{"Unnamed: 0.1":11000,"Unnamed: 0":11000.0,"anchor":"Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of\n  Dimensionality: a Review","positive":"  The paper characterizes classes of functions for which deep learning can be\nexponentially better than shallow learning. Deep convolutional networks are a\nspecial case of these conditions, though weight sharing is not the main reason\nfor their exponential advantage.\n","negative":"  In this paper, we propose three novel models to enhance word embedding by\nimplicitly using morphological information. Experiments on word similarity and\nsyntactic analogy show that the implicit models are superior to traditional\nexplicit ones. Our models outperform all state-of-the-art baselines and\nsignificantly improve the performance on both tasks. Moreover, our performance\non the smallest corpus is similar to the performance of CBOW on the corpus\nwhich is five times the size of ours. Parameter analysis indicates that the\nimplicit models can supplement semantic information during the word embedding\ntraining process.\n","id":0}
{"Unnamed: 0.1":11001,"Unnamed: 0":11001.0,"anchor":"Quantum Laplacian Eigenmap","positive":"  Laplacian eigenmap algorithm is a typical nonlinear model for dimensionality\nreduction in classical machine learning. We propose an efficient quantum\nLaplacian eigenmap algorithm to exponentially speed up the original\ncounterparts. In our work, we demonstrate that the Hermitian chain product\nproposed in quantum linear discriminant analysis (arXiv:1510.00113,2015) can be\napplied to implement quantum Laplacian eigenmap algorithm. While classical\nLaplacian eigenmap algorithm requires polynomial time to solve the eigenvector\nproblem, our algorithm is able to exponentially speed up nonlinear\ndimensionality reduction.\n","negative":"  Despite the huge spread and economical importance of configurable software\nsystems, there is unsatisfactory support in utilizing the full potential of\nthese systems with respect to finding performance-optimal configurations. Prior\nwork on predicting the performance of software configurations suffered from\neither (a) requiring far too many sample configurations or (b) large variances\nin their predictions. Both these problems can be avoided using the WHAT\nspectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of\nthe distance matrix between the configurations of a configurable software\nsystem, to perform dimensionality reduction. Within that reduced configuration\nspace, many closely associated configurations can be studied by executing only\na few sample configurations. For the subject systems studied here, a few dozen\nsamples yield accurate and stable predictors - less than 10% prediction error,\nwith a standard deviation of less than 2%. When compared to the state of the\nart, WHAT (a) requires 2 to 10 times fewer samples to achieve similar\nprediction accuracies, and (b) its predictions are more stable (i.e., have\nlower standard deviation). Furthermore, we demonstrate that predictive models\ngenerated by WHAT can be used by optimizers to discover system configurations\nthat closely approach the optimal performance.\n","id":1}
{"Unnamed: 0.1":11002,"Unnamed: 0":11002.0,"anchor":"Temporal Matrix Completion with Locally Linear Latent Factors for\n  Medical Applications","positive":"  Regular medical records are useful for medical practitioners to analyze and\nmonitor patient health status especially for those with chronic disease, but\nsuch records are usually incomplete due to unpunctuality and absence of\npatients. In order to resolve the missing data problem over time, tensor-based\nmodel is suggested for missing data imputation in recent papers because this\napproach makes use of low rank tensor assumption for highly correlated data.\nHowever, when the time intervals between records are long, the data correlation\nis not high along temporal direction and such assumption is not valid. To\naddress this problem, we propose to decompose a matrix with missing data into\nits latent factors. Then, the locally linear constraint is imposed on these\nfactors for matrix completion in this paper. By using a publicly available\ndataset and two medical datasets collected from hospital, experimental results\nshow that the proposed algorithm achieves the best performance by comparing\nwith the existing methods.\n","negative":"  We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.\n","id":2}
{"Unnamed: 0.1":11003,"Unnamed: 0":11003.0,"anchor":"Multidimensional Binary Search for Contextual Decision-Making","positive":"  We consider a multidimensional search problem that is motivated by questions\nin contextual decision-making, such as dynamic pricing and personalized\nmedicine. Nature selects a state from a $d$-dimensional unit ball and then\ngenerates a sequence of $d$-dimensional directions. We are given access to the\ndirections, but not access to the state. After receiving a direction, we have\nto guess the value of the dot product between the state and the direction. Our\ngoal is to minimize the number of times when our guess is more than $\\epsilon$\naway from the true answer. We construct a polynomial time algorithm that we\ncall Projected Volume achieving regret $O(d\\log(d\/\\epsilon))$, which is optimal\nup to a $\\log d$ factor. The algorithm combines a volume cutting strategy with\na new geometric technique that we call cylindrification.\n","negative":"  We propose a multigrid extension of convolutional neural networks (CNNs).\nRather than manipulating representations living on a single spatial grid, our\nnetwork layers operate across scale space, on a pyramid of grids. They consume\nmultigrid inputs and produce multigrid outputs; convolutional filters\nthemselves have both within-scale and cross-scale extent. This aspect is\ndistinct from simple multiscale designs, which only process the input at\ndifferent scales. Viewed in terms of information flow, a multigrid network\npasses messages across a spatial pyramid. As a consequence, receptive field\nsize grows exponentially with depth, facilitating rapid integration of context.\nMost critically, multigrid structure enables networks to learn internal\nattention and dynamic routing mechanisms, and use them to accomplish tasks on\nwhich modern CNNs fail.\n  Experiments demonstrate wide-ranging performance advantages of multigrid. On\nCIFAR and ImageNet classification tasks, flipping from a single grid to\nmultigrid within the standard CNN paradigm improves accuracy, while being\ncompute and parameter efficient. Multigrid is independent of other\narchitectural choices; we show synergy in combination with residual\nconnections. Multigrid yields dramatic improvement on a synthetic semantic\nsegmentation dataset. Most strikingly, relatively shallow multigrid networks\ncan learn to directly perform spatial transformation tasks, where, in contrast,\ncurrent CNNs fail. Together, our results suggest that continuous evolution of\nfeatures on a multigrid pyramid is a more powerful alternative to existing CNN\ndesigns on a flat grid.\n","id":3}
{"Unnamed: 0.1":11004,"Unnamed: 0":11004.0,"anchor":"Initialization and Coordinate Optimization for Multi-way Matching","positive":"  We consider the problem of consistently matching multiple sets of elements to\neach other, which is a common task in fields such as computer vision. To solve\nthe underlying NP-hard objective, existing methods often relax or approximate\nit, but end up with unsatisfying empirical performance due to a misaligned\nobjective. We propose a coordinate update algorithm that directly optimizes the\ntarget objective. By using pairwise alignment information to build an\nundirected graph and initializing the permutation matrices along the edges of\nits Maximum Spanning Tree, our algorithm successfully avoids bad local optima.\nTheoretically, with high probability our algorithm guarantees an optimal\nsolution under reasonable noise assumptions. Empirically, our algorithm\nconsistently and significantly outperforms existing methods on several\nbenchmark tasks on real datasets.\n","negative":"  Generative adversarial networks (GANs) are a framework for producing a\ngenerative model by way of a two-player minimax game. In this paper, we propose\nthe \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that\nextends GANs to multiple discriminators. In previous work, the successful\ntraining of GANs requires modifying the minimax objective to accelerate\ntraining early on. In contrast, GMAN can be reliably trained with the original,\nuntampered objective. We explore a number of design perspectives with the\ndiscriminator role ranging from formidable adversary to forgiving teacher.\nImage generation tasks comparing the proposed framework to standard GANs\ndemonstrate GMAN produces higher quality samples in a fraction of the\niterations when measured by a pairwise GAM-type metric.\n","id":4}
{"Unnamed: 0.1":11005,"Unnamed: 0":11005.0,"anchor":"Deep Convolutional Neural Network Design Patterns","positive":"  Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https:\/\/github.com\/iPhysicist\/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work.\n","negative":"  We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the additional\ndegree of freedom to control the focus of its attention from a laser sharp\nattention to a broad attention. It is applicable whenever we can assume that\nthe distance in the latent space reflects some notion of semantics. We use the\nproposed attention model as a scoring function for the embedding of a knowledge\nbase into a continuous vector space and then train a model that performs\nquestion answering about the entities in the knowledge base. The proposed\nattention model can handle both the propagation of uncertainty when following a\nseries of relations and also the conjunction of conditions in a natural way. On\na dataset of soccer players who participated in the FIFA World Cup 2014, we\ndemonstrate that our model can handle both path queries and conjunctive queries\nwell.\n","id":5}
{"Unnamed: 0.1":11006,"Unnamed: 0":11006.0,"anchor":"Quantile Reinforcement Learning","positive":"  In reinforcement learning, the standard criterion to evaluate policies in a\nstate is the expectation of (discounted) sum of rewards. However, this\ncriterion may not always be suitable, we consider an alternative criterion\nbased on the notion of quantiles. In the case of episodic reinforcement\nlearning problems, we propose an algorithm based on stochastic approximation\nwith two timescales. We evaluate our proposition on a simple model of the TV\nshow, Who wants to be a millionaire.\n","negative":"  With large volumes of health care data comes the research area of\ncomputational phenotyping, making use of techniques such as machine learning to\ndescribe illnesses and other clinical concepts from the data itself. The\n\"traditional\" approach of using supervised learning relies on a domain expert,\nand has two main limitations: requiring skilled humans to supply correct labels\nlimits its scalability and accuracy, and relying on existing clinical\ndescriptions limits the sorts of patterns that can be found. For instance, it\nmay fail to acknowledge that a disease treated as a single condition may really\nhave several subtypes with different phenotypes, as seems to be the case with\nasthma and heart disease. Some recent papers cite successes instead using\nunsupervised learning. This shows great potential for finding patterns in\nElectronic Health Records that would otherwise be hidden and that can lead to\ngreater understanding of conditions and treatments. This work implements a\nmethod derived strongly from Lasko et al., but implements it in Apache Spark\nand Python and generalizes it to laboratory time-series data in MIMIC-III. It\nis released as an open-source tool for exploration, analysis, and\nvisualization, available at https:\/\/github.com\/Hodapp87\/mimic3_phenotyping\n","id":6}
{"Unnamed: 0.1":11007,"Unnamed: 0":11007.0,"anchor":"Extracting Actionability from Machine Learning Models by Sub-optimal\n  Deterministic Planning","positive":"  A main focus of machine learning research has been improving the\ngeneralization accuracy and efficiency of prediction models. Many models such\nas SVM, random forest, and deep neural nets have been proposed and achieved\ngreat success. However, what emerges as missing in many applications is\nactionability, i.e., the ability to turn prediction results into actions. For\nexample, in applications such as customer relationship management, clinical\nprediction, and advertisement, the users need not only accurate prediction, but\nalso actionable instructions which can transfer an input to a desirable goal\n(e.g., higher profit repays, lower morbidity rates, higher ads hit rates).\nExisting effort in deriving such actionable knowledge is few and limited to\nsimple action models which restricted to only change one attribute for each\naction. The dilemma is that in many real applications those action models are\noften more complex and harder to extract an optimal solution.\n  In this paper, we propose a novel approach that achieves actionability by\ncombining learning with planning, two core areas of AI. In particular, we\npropose a framework to extract actionable knowledge from random forest, one of\nthe most widely used and best off-the-shelf classifiers. We formulate the\nactionability problem to a sub-optimal action planning (SOAP) problem, which is\nto find a plan to alter certain features of a given input so that the random\nforest would yield a desirable output, while minimizing the total costs of\nactions. Technically, the SOAP problem is formulated in the SAS+ planning\nformalism, and solved using a Max-SAT based approach. Our experimental results\ndemonstrate the effectiveness and efficiency of the proposed approach on a\npersonal credit dataset and other benchmarks. Our work represents a new\napplication of automated planning on an emerging and challenging machine\nlearning paradigm.\n","negative":"  Finite-sum optimization problems are ubiquitous in machine learning, and are\ncommonly solved using first-order methods which rely on gradient computations.\nRecently, there has been growing interest in \\emph{second-order} methods, which\nrely on both gradients and Hessians. In principle, second-order methods can\nrequire much fewer iterations than first-order methods, and hold the promise\nfor more efficient algorithms. Although computing and manipulating Hessians is\nprohibitive for high-dimensional problems in general, the Hessians of\nindividual functions in finite-sum problems can often be efficiently computed,\ne.g. because they possess a low-rank structure. Can second-order information\nindeed be used to solve such problems more efficiently? In this paper, we\nprovide evidence that the answer -- perhaps surprisingly -- is negative, at\nleast in terms of worst-case guarantees. However, we also discuss what\nadditional assumptions and algorithmic approaches might potentially circumvent\nthis negative result.\n","id":7}
{"Unnamed: 0.1":11008,"Unnamed: 0":11008.0,"anchor":"Low Rank Approximation with Entrywise $\\ell_1$-Norm Error","positive":"  We study the $\\ell_1$-low rank approximation problem, where for a given $n\n\\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to\noutput a rank-$k$ matrix $\\widehat{A}$ for which\n  $$\\|A-\\widehat{A}\\|_1 \\leq \\alpha \\cdot \\min_{\\textrm{rank-}k\\textrm{\nmatrices}~A'}\\|A-A'\\|_1,$$ where for an $n \\times d$ matrix $C$, we let\n$\\|C\\|_1 = \\sum_{i=1}^n \\sum_{j=1}^d |C_{i,j}|$. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is\nindicated in models where Gaussian assumptions on the noise may not apply. The\nproblem was shown to be NP-hard by Gillis and Vavasis and a number of\nheuristics have been proposed. It was asked in multiple places if there are any\napproximation algorithms.\n  We give the first provable approximation algorithms for $\\ell_1$-low rank\napproximation, showing that it is possible to achieve approximation factor\n$\\alpha = (\\log d) \\cdot \\mathrm{poly}(k)$ in $\\mathrm{nnz}(A) + (n+d)\n\\mathrm{poly}(k)$ time, where $\\mathrm{nnz}(A)$ denotes the number of non-zero\nentries of $A$. If $k$ is constant, we further improve the approximation ratio\nto $O(1)$ with a $\\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time\nHypothesis, we show there is no $\\mathrm{poly}(nd)$-time algorithm achieving a\n$(1+\\frac{1}{\\log^{1+\\gamma}(nd)})$-approximation, for $\\gamma > 0$ an\narbitrarily small constant, even when $k = 1$.\n  We give a number of additional results for $\\ell_1$-low rank approximation:\nnearly tight upper and lower bounds for column subset selection, CUR\ndecompositions, extensions to low rank approximation with respect to\n$\\ell_p$-norms for $1 \\leq p < 2$ and earthmover distance, low-communication\ndistributed protocols and low-memory streaming algorithms, algorithms with\nlimited randomness, and bicriteria algorithms. We also give a preliminary\nempirical evaluation.\n","negative":"  Influential node detection is a central research topic in social network\nanalysis. Many existing methods rely on the assumption that the network\nstructure is completely known \\textit{a priori}. However, in many applications,\nnetwork structure is unavailable to explain the underlying information\ndiffusion phenomenon. To address the challenge of information diffusion\nanalysis with incomplete knowledge of network structure, we develop a\nmulti-task low rank linear influence model. By exploiting the relationships\nbetween contagions, our approach can simultaneously predict the volume (i.e.\ntime series prediction) for each contagion (or topic) and automatically\nidentify the most influential nodes for each contagion. The proposed model is\nvalidated using synthetic data and an ISIS twitter dataset. In addition to\nimproving the volume prediction performance significantly, we show that the\nproposed approach can reliably infer the most influential users for specific\ncontagions.\n","id":8}
{"Unnamed: 0.1":11009,"Unnamed: 0":11009.0,"anchor":"Fast Eigenspace Approximation using Random Signals","positive":"  We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors.\n","negative":"  The alternating direction method of multipliers (ADMM) is a common\noptimization tool for solving constrained and non-differentiable problems. We\nprovide an empirical study of the practical performance of ADMM on several\nnonconvex applications, including l0 regularized linear regression, l0\nregularized image denoising, phase retrieval, and eigenvector computation. Our\nexperiments suggest that ADMM performs well on a broad class of non-convex\nproblems. Moreover, recently proposed adaptive ADMM methods, which\nautomatically tune penalty parameters as the method runs, can improve algorithm\nefficiency and solution quality compared to ADMM with a non-tuned penalty.\n","id":9}
{"Unnamed: 0.1":11010,"Unnamed: 0":11010.0,"anchor":"Multitask Protein Function Prediction Through Task Dissimilarity","positive":"  Automated protein function prediction is a challenging problem with\ndistinctive features, such as the hierarchical organization of protein\nfunctions and the scarcity of annotated proteins for most biological functions.\nWe propose a multitask learning algorithm addressing both issues. Unlike\nstandard multitask algorithms, which use task (protein functions) similarity\ninformation as a bias to speed up learning, we show that dissimilarity\ninformation enforces separation of rare class labels from frequent class\nlabels, and for this reason is better suited for solving unbalanced protein\nfunction prediction problems. We support our claim by showing that a multitask\nextension of the label propagation algorithm empirically works best when the\ntask relatedness information is represented using a dissimilarity matrix as\nopposed to a similarity matrix. Moreover, the experimental comparison carried\nout on three model organism shows that our method has a more stable performance\nin both \"protein-centric\" and \"function-centric\" evaluation settings.\n","negative":"  Deluge Networks (DelugeNets) are deep neural networks which efficiently\nfacilitate massive cross-layer information inflows from preceding layers to\nsucceeding layers. The connections between layers in DelugeNets are established\nthrough cross-layer depthwise convolutional layers with learnable filters,\nacting as a flexible yet efficient selection mechanism. DelugeNets can\npropagate information across many layers with greater flexibility and utilize\nnetwork parameters more effectively compared to ResNets, whilst being more\nefficient than DenseNets. Remarkably, a DelugeNet model with just model\ncomplexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve\nclassification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset\nrespectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on\nImageNet dataset, despite costing merely half of the computations needed by the\nlatter.\n","id":10}
{"Unnamed: 0.1":11011,"Unnamed: 0":11011.0,"anchor":"Learning to Pivot with Adversarial Networks","positive":"  Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics.\n","negative":"  Stochastic gradient descent~(SGD) and its variants have attracted much\nattention in machine learning due to their efficiency and effectiveness for\noptimization. To handle large-scale problems, researchers have recently\nproposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for\nmulti-core systems. However, existing works have only proved the convergence of\nthese LF-PSGD methods for convex problems. To the best of our knowledge, no\nwork has proved the convergence of the LF-PSGD methods for non-convex problems.\nIn this paper, we provide the theoretical proof about the convergence of two\nrepresentative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems.\nEmpirical results also show that both Hogwild! and AsySVRG are convergent on\nnon-convex problems, which successfully verifies our theoretical results.\n","id":11}
{"Unnamed: 0.1":11012,"Unnamed: 0":11012.0,"anchor":"Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space\n  Matter?","positive":"  The use of deep reinforcement learning allows for high-dimensional state\ndescriptors, but little is known about how the choice of action representation\nimpacts the learning difficulty and the resulting performance. We compare the\nimpact of four different action parameterizations (torques, muscle-activations,\ntarget joint angles, and target joint-angle velocities) in terms of learning\ntime, policy robustness, motion quality, and policy query rates. Our results\nare evaluated on a gait-cycle imitation task for multiple planar articulated\nfigures and multiple gaits. We demonstrate that the local feedback provided by\nhigher-level action parameterizations can significantly impact the learning,\nrobustness, and quality of the resulting policies.\n","negative":"  Humans are remarkably adept at interpreting the gaze direction of other\nindividuals in their surroundings. This skill is at the core of the ability to\nengage in joint visual attention, which is essential for establishing social\ninteractions. How accurate are humans in determining the gaze direction of\nothers in lifelike scenes, when they can move their heads and eyes freely, and\nwhat are the sources of information for the underlying perceptual processes?\nThese questions pose a challenge from both empirical and computational\nperspectives, due to the complexity of the visual input in real-life\nsituations. Here we measure empirically human accuracy in perceiving the gaze\ndirection of others in lifelike scenes, and study computationally the sources\nof information and representations underlying this cognitive capacity. We show\nthat humans perform better in face-to-face conditions compared with recorded\nconditions, and that this advantage is not due to the availability of input\ndynamics. We further show that humans are still performing well when only the\neyes-region is visible, rather than the whole face. We develop a computational\nmodel, which replicates the pattern of human performance, including the finding\nthat the eyes-region contains on its own, the required information for\nestimating both head orientation and direction of gaze. Consistent with\nneurophysiological findings on task-specific face regions in the brain, the\nlearned computational representations reproduce perceptual effects such as the\nWollaston illusion, when trained to estimate direction of gaze, but not when\ntrained to recognize objects or faces.\n","id":12}
{"Unnamed: 0.1":11013,"Unnamed: 0":11013.0,"anchor":"A-Ward_p\\b{eta}: Effective hierarchical clustering using the Minkowski\n  metric and a fast k -means initialisation","positive":"  In this paper we make two novel contributions to hierarchical clustering.\nFirst, we introduce an anomalous pattern initialisation method for hierarchical\nclustering algorithms, called A-Ward, capable of substantially reducing the\ntime they take to converge. This method generates an initial partition with a\nsufficiently large number of clusters. This allows the cluster merging process\nto start from this partition rather than from a trivial partition composed\nsolely of singletons. Our second contribution is an extension of the Ward and\nWard p algorithms to the situation where the feature weight exponent can differ\nfrom the exponent of the Minkowski distance. This new method, called A-Ward\np\\b{eta} , is able to generate a much wider variety of clustering solutions. We\nalso demonstrate that its parameters can be estimated reasonably well by using\na cluster validity index. We perform numerous experiments using data sets with\ntwo types of noise, insertion of noise features and blurring within-cluster\nvalues of some features. These experiments allow us to conclude: (i) our\nanomalous pattern initialisation method does indeed reduce the time a\nhierarchical clustering algorithm takes to complete, without negatively\nimpacting its cluster recovery ability; (ii) A-Ward p\\b{eta} provides better\ncluster recovery than both Ward and Ward p.\n","negative":"  Cross-entropy loss together with softmax is arguably one of the most common\nused supervision components in convolutional neural networks (CNNs). Despite\nits simplicity, popularity and excellent performance, the component does not\nexplicitly encourage discriminative learning of features. In this paper, we\npropose a generalized large-margin softmax (L-Softmax) loss which explicitly\nencourages intra-class compactness and inter-class separability between learned\nfeatures. Moreover, L-Softmax not only can adjust the desired margin but also\ncan avoid overfitting. We also show that the L-Softmax loss can be optimized by\ntypical stochastic gradient descent. Extensive experiments on four benchmark\ndatasets demonstrate that the deeply-learned features with L-softmax loss\nbecome more discriminative, hence significantly boosting the performance on a\nvariety of visual classification and verification tasks.\n","id":13}
{"Unnamed: 0.1":11014,"Unnamed: 0":11014.0,"anchor":"Cross: Efficient Low-rank Tensor Completion","positive":"  The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging.\n","negative":"  This paper presents the development of several models of a deep convolutional\nauto-encoder in the Caffe deep learning framework and their experimental\nevaluation on the example of MNIST dataset. We have created five models of a\nconvolutional auto-encoder which differ architecturally by the presence or\nabsence of pooling and unpooling layers in the auto-encoder's encoder and\ndecoder parts. Our results show that the developed models provide very good\nresults in dimensionality reduction and unsupervised clustering tasks, and\nsmall classification errors when we used the learned internal code as an input\nof a supervised linear classifier and multi-layer perceptron. The best results\nwere provided by a model where the encoder part contains convolutional and\npooling layers, followed by an analogous decoder part with deconvolution and\nunpooling layers without the use of switch variables in the decoder part. The\npaper also discusses practical details of the creation of a deep convolutional\nauto-encoder in the very popular Caffe deep learning framework. We believe that\nour approach and results presented in this paper could help other researchers\nto build efficient deep neural network architectures in the future.\n","id":14}
{"Unnamed: 0.1":11015,"Unnamed: 0":11015.0,"anchor":"Using a Deep Reinforcement Learning Agent for Traffic Signal Control","positive":"  Ensuring transportation systems are efficient is a priority for modern\nsociety. Technological advances have made it possible for transportation\nsystems to collect large volumes of varied data on an unprecedented scale. We\npropose a traffic signal control system which takes advantage of this new, high\nquality data, with minimal abstraction compared to other proposed systems. We\napply modern deep reinforcement learning methods to build a truly adaptive\ntraffic signal control agent in the traffic microsimulator SUMO. We propose a\nnew state space, the discrete traffic state encoding, which is information\ndense. The discrete traffic state encoding is used as input to a deep\nconvolutional neural network, trained using Q-learning with experience replay.\nOur agent was compared against a one hidden layer neural network traffic signal\ncontrol agent and reduces average cumulative delay by 82%, average queue length\nby 66% and average travel time by 20%.\n","negative":"  We present an approach to model time series data from resting state fMRI for\nautism spectrum disorder (ASD) severity classification. We propose to adopt\nkernel machines and employ graph kernels that define a kernel dot product\nbetween two graphs. This enables us to take advantage of spatio-temporal\ninformation to capture the dynamics of the brain network, as opposed to\naggregating them in the spatial or temporal dimension. In addition to the\nconventional similarity graphs, we explore the use of L1 graph using sparse\ncoding, and the persistent homology of time delay embeddings, in the proposed\npipeline for ASD classification. In our experiments on two datasets from the\nABIDE collection, we demonstrate a consistent and significant advantage in\nusing graph kernels over traditional linear or non linear kernels for a variety\nof time series features.\n","id":15}
{"Unnamed: 0.1":11016,"Unnamed: 0":11016.0,"anchor":"Categorical Reparameterization with Gumbel-Softmax","positive":"  Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification.\n","negative":"  The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem.\n","id":16}
{"Unnamed: 0.1":11017,"Unnamed: 0":11017.0,"anchor":"PrivLogit: Efficient Privacy-preserving Logistic Regression by Tailoring\n  Numerical Optimizers","positive":"  Safeguarding privacy in machine learning is highly desirable, especially in\ncollaborative studies across many organizations. Privacy-preserving distributed\nmachine learning (based on cryptography) is popular to solve the problem.\nHowever, existing cryptographic protocols still incur excess computational\noverhead. Here, we make a novel observation that this is partially due to naive\nadoption of mainstream numerical optimization (e.g., Newton method) and failing\nto tailor for secure computing. This work presents a contrasting perspective:\ncustomizing numerical optimization specifically for secure settings. We propose\na seemingly less-favorable optimization method that can in fact significantly\naccelerate privacy-preserving logistic regression. Leveraging this new method,\nwe propose two new secure protocols for conducting logistic regression in a\nprivacy-preserving and distributed manner. Extensive theoretical and empirical\nevaluations prove the competitive performance of our two secure proposals while\nwithout compromising accuracy or privacy: with speedup up to 2.3x and 8.1x,\nrespectively, over state-of-the-art; and even faster as data scales up. Such\ndrastic speedup is on top of and in addition to performance improvements from\nexisting (and future) state-of-the-art cryptography. Our work provides a new\nway towards efficient and practical privacy-preserving logistic regression for\nlarge-scale studies which are common for modern science.\n","negative":"  The majority of methods used to compute approximations to the\nHamilton-Jacobi-Isaacs partial differential equation (HJI PDE) rely on the\ndiscretization of the state space to perform dynamic programming updates. This\ntype of approach is known to suffer from the curse of dimensionality due to the\nexponential growth in grid points with the state dimension. In this work we\npresent an approximate dynamic programming algorithm that computes an\napproximation of the solution of the HJI PDE by alternating between solving a\nregression problem and solving a minimax problem using a feedforward neural\nnetwork as the function approximator. We find that this method requires less\nmemory to run and to store the approximation than traditional gridding methods,\nand we test it on a few systems of two, three and six dimensions.\n","id":17}
{"Unnamed: 0.1":11018,"Unnamed: 0":11018.0,"anchor":"Demystifying ResNet","positive":"  The Residual Network (ResNet), proposed in He et al. (2015), utilized\nshortcut connections to significantly reduce the difficulty of training, which\nresulted in great performance boosts in terms of both training and\ngeneralization error.\n  It was empirically observed in He et al. (2015) that stacking more layers of\nresidual blocks with shortcut 2 results in smaller training error, while it is\nnot true for shortcut of length 1 or 3. We provide a theoretical explanation\nfor the uniqueness of shortcut 2.\n  We show that with or without nonlinearities, by adding shortcuts that have\ndepth two, the condition number of the Hessian of the loss function at the zero\ninitial point is depth-invariant, which makes training very deep models no more\ndifficult than shallow ones. Shortcuts of higher depth result in an extremely\nflat (high-order) stationary point initially, from which the optimization\nalgorithm is hard to escape. The shortcut 1, however, is essentially equivalent\nto no shortcuts, which has a condition number exploding to infinity as the\nnumber of layers grows. We further argue that as the number of layers tends to\ninfinity, it suffices to only look at the loss function at the zero initial\npoint.\n  Extensive experiments are provided accompanying our theoretical results. We\nshow that initializing the network to small weights with shortcut 2 achieves\nsignificantly better results than random Gaussian (Xavier) initialization,\northogonal initialization, and shortcuts of deeper depth, from various\nperspectives ranging from final loss, learning dynamics and stability, to the\nbehavior of the Hessian along the learning process.\n","negative":"  In this paper we obtain several informative error bounds on function\napproximation for the policy evaluation algorithm proposed by Basu et al. when\nthe aim is to find the risk-sensitive cost represented using exponential\nutility. The main idea is to use classical Bapat's inequality and to use\nPerron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to\nget the new bounds. The novelty of our approach is that we use the\nirreduciblity of Markov chain to get the new bounds whereas the earlier work by\nBasu et al. used spectral variation bound which is true for any matrix. We also\ngive examples where all our bounds achieve the \"actual error\" whereas the\nearlier bound given by Basu et al. is much weaker in comparison. We show that\nthis happens due to the absence of difference term in the earlier bound which\nis always present in all our bounds when the state space is large.\nAdditionally, we discuss how all our bounds compare with each other. As a\ncorollary of our main result we provide a bound between largest eigenvalues of\ntwo irreducibile matrices in terms of the matrix entries.\n","id":18}
{"Unnamed: 0.1":11019,"Unnamed: 0":11019.0,"anchor":"Conspiracies between Learning Algorithms, Circuit Lower Bounds and\n  Pseudorandomness","positive":"  We prove several results giving new and stronger connections between\nlearning, circuit lower bounds and pseudorandomness. Among other results, we\nshow a generic learning speedup lemma, equivalences between various learning\nmodels in the exponential time and subexponential time regimes, a dichotomy\nbetween learning and pseudorandomness, consequences of non-trivial learning for\ncircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,\nand NC$^1$-hardness for the Minimum Circuit Size Problem.\n","negative":"  Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.\n","id":19}
{"Unnamed: 0.1":11020,"Unnamed: 0":11020.0,"anchor":"Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear","positive":"  Many practical environments contain catastrophic states that an optimal agent\nwould visit infrequently or never. Even on toy problems, Deep Reinforcement\nLearning (DRL) agents tend to periodically revisit these states upon forgetting\ntheir existence under a new policy. We introduce intrinsic fear (IF), a learned\nreward shaping that guards DRL agents against periodic catastrophes. IF agents\npossess a fear model trained to predict the probability of imminent\ncatastrophe. This score is then used to penalize the Q-learning objective. Our\ntheoretical analysis bounds the reduction in average return due to learning on\nthe perturbed objective. We also prove robustness to classification errors. As\na bonus, IF models tend to learn faster, owing to reward shaping. Experiments\ndemonstrate that intrinsic-fear DQNs solve otherwise pathological environments\nand improve on several Atari games.\n","negative":"  In this paper we propose a fast online Kernel SVM algorithm under tight\nbudget constraints. We propose to split the input space using LVQ and train a\nKernel SVM in each cluster. To allow for online training, we propose to limit\nthe size of the support vector set of each cluster using different strategies.\nWe show in the experiment that our algorithm is able to achieve high accuracy\nwhile having a very high number of samples processed per second both in\ntraining and in the evaluation.\n","id":20}
{"Unnamed: 0.1":11021,"Unnamed: 0":11021.0,"anchor":"Sample Efficient Actor-Critic with Experience Replay","positive":"  This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.\n","negative":"  MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.\n","id":21}
{"Unnamed: 0.1":11022,"Unnamed: 0":11022.0,"anchor":"Deep Information Propagation","positive":"  We study the behavior of untrained neural networks whose weights and biases\nare randomly distributed using mean field theory. We show the existence of\ndepth scales that naturally limit the maximum depth of signal propagation\nthrough these random networks. Our main practical result is to show that random\nnetworks may be trained precisely when information can travel through them.\nThus, the depth scales that we identify provide bounds on how deep a network\nmay be trained for a specific choice of hyperparameters. As a corollary to\nthis, we argue that in networks at the edge of chaos, one of these depth scales\ndiverges. Thus arbitrarily deep networks may be trained only sufficiently close\nto criticality. We show that the presence of dropout destroys the\norder-to-chaos critical point and therefore strongly limits the maximum\ntrainable depth for random networks. Finally, we develop a mean field theory\nfor backpropagation and we show that the ordered and chaotic phases correspond\nto regions of vanishing and exploding gradient respectively.\n","negative":"  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank matrix, in settings where {}{the data} are noisy, or where\nthe overall matrix has missing elements. We propose a randomized two-step\ninference framework, and establish sufficient conditions on the required sample\ncomplexities under which these methods succeed (with high probability) in\naccurately locating the outliers for each task. Comprehensive numerical\nexperimental results are provided to verify the theoretical bounds and\ndemonstrate the computational efficiency of the proposed algorithm.\n","id":22}
{"Unnamed: 0.1":11023,"Unnamed: 0":11023.0,"anchor":"Adversarial Machine Learning at Scale","positive":"  Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess.\n","negative":"  We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures.\n","id":23}
{"Unnamed: 0.1":11024,"Unnamed: 0":11024.0,"anchor":"Reparameterization trick for discrete variables","positive":"  Low-variance gradient estimation is crucial for learning directed graphical\nmodels parameterized by neural networks, where the reparameterization trick is\nwidely used for those with continuous variables. While this technique gives\nlow-variance gradient estimates, it has not been directly applicable to\ndiscrete variables, the sampling of which inherently requires discontinuous\noperations. We argue that the discontinuity can be bypassed by marginalizing\nout the variable of interest, which results in a new reparameterization trick\nfor discrete variables. This reparameterization greatly reduces the variance,\nwhich is understood by regarding the method as an application of common random\nnumbers to the estimation. The resulting estimator is theoretically guaranteed\nto have a variance not larger than that of the likelihood-ratio method with the\noptimal input-dependent baseline. We give empirical results for variational\nlearning of sigmoid belief networks.\n","negative":"  The scale of modern datasets necessitates the development of efficient\ndistributed optimization methods for machine learning. We present a\ngeneral-purpose framework for distributed computing environments, CoCoA, that\nhas an efficient communication scheme and is applicable to a wide variety of\nproblems in machine learning and signal processing. We extend the framework to\ncover general non-strongly-convex regularizers, including L1-regularized\nproblems like lasso, sparse logistic regression, and elastic net\nregularization, and show how earlier work can be derived as a special case. We\nprovide convergence guarantees for the class of convex regularized loss\nminimization objectives, leveraging a novel approach in handling\nnon-strongly-convex regularizers and non-smooth loss functions. The resulting\nframework has markedly improved performance over state-of-the-art methods, as\nwe illustrate with an extensive set of experiments on real distributed\ndatasets.\n","id":24}
{"Unnamed: 0.1":11025,"Unnamed: 0":11025.0,"anchor":"Generalized Topic Modeling","positive":"  Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning.\n","negative":"  Learning from examples is one of the key problems in science and engineering.\nIt deals with function reconstruction from a finite set of direct and noisy\nsamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely\nused to solve this task and includes powerful estimators such as regularization\nnetworks. Recent achievements include the proof of the statistical consistency\nof these kernel- based approaches. Parallel to this, many different system\nidentification techniques have been developed but the interaction with machine\nlearning does not appear so strong yet. One reason is that the RKHSs usually\nemployed in machine learning do not embed the information available on dynamic\nsystems, e.g. BIBO stability. In addition, in system identification the\nindependent data assumptions routinely adopted in machine learning are never\nsatisfied in practice. This paper provides new results which strengthen the\nconnection between system identification and machine learning. Our starting\npoint is the introduction of RKHSs of dynamic systems. They contain functionals\nover spaces defined by system inputs and allow to interpret system\nidentification as learning from examples. In both linear and nonlinear\nsettings, it is shown that this perspective permits to derive in a relatively\nsimple way conditions on RKHS stability (i.e. the property of containing only\nBIBO stable systems or predictors), also facilitating the design of new kernels\nfor system identification. Furthermore, we prove the convergence of the\nregularized estimator to the optimal predictor under conditions typical of\ndynamic systems.\n","id":25}
{"Unnamed: 0.1":11026,"Unnamed: 0":11026.0,"anchor":"Learning Identity Mappings with Residual Gates","positive":"  We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively.\n","negative":"  Non-negative matrix factorization (NMF) is a new knowledge discovery method\nthat is used for text mining, signal processing, bioinformatics, and consumer\nanalysis. However, its basic property as a learning machine is not yet\nclarified, as it is not a regular statistical model, resulting that theoretical\noptimization method of NMF has not yet established. In this paper, we study the\nreal log canonical threshold of NMF and give an upper bound of the\ngeneralization error in Bayesian learning. The results show that the\ngeneralization error of the matrix factorization can be made smaller than\nregular statistical models if Bayesian learning is applied.\n","id":26}
{"Unnamed: 0.1":11027,"Unnamed: 0":11027.0,"anchor":"Semantic Noise Modeling for Better Representation Learning","positive":"  Latent representation learned from multi-layered neural networks via\nhierarchical feature abstraction enables recent success of deep learning. Under\nthe deep learning framework, generalization performance highly depends on the\nlearned latent representation which is obtained from an appropriate training\nscenario with a task-specific objective on a designed network model. In this\nwork, we propose a novel latent space modeling method to learn better latent\nrepresentation. We designed a neural network model based on the assumption that\ngood base representation can be attained by maximizing the total correlation\nbetween the input, latent, and output variables. From the base model, we\nintroduce a semantic noise modeling method which enables class-conditional\nperturbation on latent space to enhance the representational power of learned\nlatent feature. During training, latent vector representation can be\nstochastically perturbed by a modeled class-conditional additive noise while\nmaintaining its original semantic feature. It implicitly brings the effect of\nsemantic augmentation on the latent space. The proposed model can be easily\nlearned by back-propagation with common gradient-based optimization algorithms.\nExperimental results show that the proposed method helps to achieve performance\nbenefits against various previous approaches. We also provide the empirical\nanalyses for the proposed class-conditional perturbation process including\nt-SNE visualization.\n","negative":"  The study of complex systems benefits from graph models and their analysis.\nIn particular, the eigendecomposition of the graph Laplacian lets emerge\nproperties of global organization from local interactions; e.g., the Fiedler\nvector has the smallest non-zero eigenvalue and plays a key role for graph\nclustering. Graph signal processing focusses on the analysis of signals that\nare attributed to the graph nodes. The eigendecomposition of the graph\nLaplacian allows to define the graph Fourier transform and extend conventional\nsignal-processing operations to graphs. Here, we introduce the design of\nSlepian graph signals, by maximizing energy concentration in a predefined\nsubgraph for a graph spectral bandlimit. We establish a novel link with\nclassical Laplacian embedding and graph clustering, which provides a meaning to\nlocalized graph frequencies.\n","id":27}
{"Unnamed: 0.1":11028,"Unnamed: 0":11028.0,"anchor":"A Communication-Efficient Parallel Algorithm for Decision Tree","positive":"  Decision tree (and its extensions such as Gradient Boosting Decision Trees\nand Random Forest) is a widely used machine learning algorithm, due to its\npractical effectiveness and model interpretability. With the emergence of big\ndata, there is an increasing need to parallelize the training process of\ndecision tree. However, most existing attempts along this line suffer from high\ncommunication costs. In this paper, we propose a new algorithm, called\n\\emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After\npartitioning the training data onto a number of (e.g., $M$) machines, this\nalgorithm performs both local voting and global voting in each iteration. For\nlocal voting, the top-$k$ attributes are selected from each machine according\nto its local data. Then, globally top-$2k$ attributes are determined by a\nmajority voting among these local candidates. Finally, the full-grained\nhistograms of the globally top-$2k$ attributes are collected from local\nmachines in order to identify the best (most informative) attribute and its\nsplit point. PV-Tree can achieve a very low communication cost (independent of\nthe total number of attributes) and thus can scale out very well. Furthermore,\ntheoretical analysis shows that this algorithm can learn a near optimal\ndecision tree, since it can find the best attribute with a large probability.\nOur experiments on real-world datasets show that PV-Tree significantly\noutperforms the existing parallel decision tree algorithms in the trade-off\nbetween accuracy and efficiency.\n","negative":"  This work leverages recent advances in probabilistic machine learning to\ndiscover conservation laws expressed by parametric linear equations. Such\nequations involve, but are not limited to, ordinary and partial differential,\nintegro-differential, and fractional order operators. Here, Gaussian process\npriors are modified according to the particular form of such operators and are\nemployed to infer parameters of the linear equations from scarce and possibly\nnoisy observations. Such observations may come from experiments or \"black-box\"\ncomputer simulations.\n","id":28}
{"Unnamed: 0.1":11029,"Unnamed: 0":11029.0,"anchor":"Information Dropout: Learning Optimal Representations Through Noisy\n  Computation","positive":"  The cross-entropy loss commonly used in deep learning is closely related to\nthe defining properties of optimal representations, but does not enforce some\nof the key properties. We show that this can be solved by adding a\nregularization term, which is in turn related to injecting multiplicative noise\nin the activations of a Deep Neural Network, a special case of which is the\ncommon practice of dropout. We show that our regularized loss function can be\nefficiently minimized using Information Dropout, a generalization of dropout\nrooted in information theoretic principles that automatically adapts to the\ndata and can better exploit architectures of limited capacity. When the task is\nthe reconstruction of the input, we show that our loss function yields a\nVariational Autoencoder as a special case, thus providing a link between\nrepresentation learning, information theory and variational inference. Finally,\nwe prove that we can promote the creation of disentangled representations\nsimply by enforcing a factorized prior, a fact that has been observed\nempirically in recent work. Our experiments validate the theoretical intuitions\nbehind our method, and we find that information dropout achieves a comparable\nor better generalization performance than binary dropout, especially on smaller\nmodels, since it can automatically adapt the noise to the structure of the\nnetwork, as well as to the test sample.\n","negative":"  Many machine learning applications use latent variable models to explain\nstructure in data, whereby visible variables (= coordinates of the given\ndatapoint) are explained as a probabilistic function of some hidden variables.\nFinding parameters with the maximum likelihood is NP-hard even in very simple\nsettings. In recent years, provably efficient algorithms were nevertheless\ndeveloped for models with linear structures: topic models, mixture models,\nhidden markov models, etc. These algorithms use matrix or tensor decomposition,\nand make some reasonable assumptions about the parameters of the underlying\nmodel.\n  But matrix or tensor decomposition seems of little use when the latent\nvariable model has nonlinearities. The current paper shows how to make\nprogress: tensor decomposition is applied for learning the single-layer {\\em\nnoisy or} network, which is a textbook example of a Bayes net, and used for\nexample in the classic QMR-DT software for diagnosing which disease(s) a\npatient may have by observing the symptoms he\/she exhibits.\n  The technical novelty here, which should be useful in other settings in\nfuture, is analysis of tensor decomposition in presence of systematic error\n(i.e., where the noise\/error is correlated with the signal, and doesn't\ndecrease as number of samples goes to infinity). This requires rethinking all\nsteps of tensor decomposition methods from the ground up.\n  For simplicity our analysis is stated assuming that the network parameters\nwere chosen from a probability distribution but the method seems more generally\napplicable.\n","id":29}
{"Unnamed: 0.1":11030,"Unnamed: 0":11030.0,"anchor":"Learning to Rank Scientific Documents from the Crowd","positive":"  Finding related published articles is an important task in any science, but\nwith the explosion of new work in the biomedical domain it has become\nespecially challenging. Most existing methodologies use text similarity metrics\nto identify whether two articles are related or not. However biomedical\nknowledge discovery is hypothesis-driven. The most related articles may not be\nones with the highest text similarities. In this study, we first develop an\ninnovative crowd-sourcing approach to build an expert-annotated\ndocument-ranking corpus. Using this corpus as the gold standard, we then\nevaluate the approaches of using text similarity to rank the relatedness of\narticles. Finally, we develop and evaluate a new supervised model to\nautomatically rank related scientific articles. Our results show that authors'\nranking differ significantly from rankings by text-similarity-based models. By\ntraining a learning-to-rank model on a subset of the annotated corpus, we found\nthe best supervised learning-to-rank model (SVM-Rank) significantly surpassed\nstate-of-the-art baseline systems.\n","negative":"  We formulate learning of a binary autoencoder as a biconvex optimization\nproblem which learns from the pairwise correlations between encoded and decoded\nbits. Among all possible algorithms that use this information, ours finds the\nautoencoder that reconstructs its inputs with worst-case optimal loss. The\noptimal decoder is a single layer of artificial neurons, emerging entirely from\nthe minimax loss minimization, and with weights learned by convex optimization.\nAll this is reflected in competitive experimental results, demonstrating that\nbinary autoencoding can be done efficiently by conveying information in\npairwise correlations in an optimal fashion.\n","id":30}
{"Unnamed: 0.1":11031,"Unnamed: 0":11031.0,"anchor":"Information-Theoretic Bounds and Approximations in Neural Population\n  Coding","positive":"  While Shannon's mutual information has widespread applications in many\ndisciplines, for practical applications it is often difficult to calculate its\nvalue accurately for high-dimensional variables because of the curse of\ndimensionality. This paper is focused on effective approximation methods for\nevaluating mutual information in the context of neural population coding. For\nlarge but finite neural populations, we derive several information-theoretic\nasymptotic bounds and approximation formulas that remain valid in\nhigh-dimensional spaces. We prove that optimizing the population density\ndistribution based on these approximation formulas is a convex optimization\nproblem which allows efficient numerical solutions. Numerical simulation\nresults confirmed that our asymptotic formulas were highly accurate for\napproximating mutual information for large neural populations. In special\ncases, the approximation formulas are exactly equal to the true mutual\ninformation. We also discuss techniques of variable transformation and\ndimensionality reduction to facilitate computation of the approximations.\n","negative":"  This paper presents a novel form of policy gradient for model-free\nreinforcement learning (RL) with improved exploration properties. Current\npolicy-based methods use entropy regularization to encourage undirected\nexploration of the reward landscape, which is ineffective in high dimensional\nspaces with sparse rewards. We propose a more directed exploration strategy\nthat promotes exploration of under-appreciated reward regions. An action\nsequence is considered under-appreciated if its log-probability under the\ncurrent policy under-estimates its resulting reward. The proposed exploration\nstrategy is easy to implement, requiring small modifications to an\nimplementation of the REINFORCE algorithm. We evaluate the approach on a set of\nalgorithmic tasks that have long challenged RL methods. Our approach reduces\nhyper-parameter sensitivity and demonstrates significant improvements over\nbaseline methods. Our algorithm successfully solves a benchmark multi-digit\naddition task and generalizes to long sequences. This is, to our knowledge, the\nfirst time that a pure RL method has solved addition using only reward\nfeedback.\n","id":31}
{"Unnamed: 0.1":11032,"Unnamed: 0":11032.0,"anchor":"Learning Continuous Semantic Representations of Symbolic Expressions","positive":"  Combining abstract, symbolic reasoning with continuous neural reasoning is a\ngrand challenge of representation learning. As a step in this direction, we\npropose a new architecture, called neural equivalence networks, for the problem\nof learning continuous semantic representations of algebraic and logical\nexpressions. These networks are trained to represent semantic equivalence, even\nof expressions that are syntactically very different. The challenge is that\nsemantic representations must be computed in a syntax-directed manner, because\nsemantics is compositional, but at the same time, small changes in syntax can\nlead to very large changes in semantics, which can be difficult for continuous\nneural architectures. We perform an exhaustive evaluation on the task of\nchecking equivalence on a highly diverse class of symbolic algebraic and\nboolean expression types, showing that our model significantly outperforms\nexisting architectures.\n","negative":"  We present a novel extension of Thompson Sampling for stochastic sequential\ndecision problems with graph feedback, even when the graph structure itself is\nunknown and\/or changing. We provide theoretical guarantees on the Bayesian\nregret of the algorithm, linking its performance to the underlying properties\nof the graph. Thompson Sampling has the advantage of being applicable without\nthe need to construct complicated upper confidence bounds for different\nproblems. We illustrate its performance through extensive experimental results\non real and simulated networks with graph feedback. More specifically, we\ntested our algorithms on power law, planted partitions and Erdo's-Renyi graphs,\nas well as on graphs derived from Facebook and Flixster data. These all show\nthat our algorithms clearly outperform related methods that employ upper\nconfidence bounds, even if the latter use more information about the graph.\n","id":32}
{"Unnamed: 0.1":11033,"Unnamed: 0":11033.0,"anchor":"Sparsely-Connected Neural Networks: Towards Efficient VLSI\n  Implementation of Deep Neural Networks","positive":"  Recently deep neural networks have received considerable attention due to\ntheir ability to extract and represent high-level abstractions in data sets.\nDeep neural networks such as fully-connected and convolutional neural networks\nhave shown excellent performance on a wide range of recognition and\nclassification tasks. However, their hardware implementations currently suffer\nfrom large silicon area and high power consumption due to the their high degree\nof complexity. The power\/energy consumption of neural networks is dominated by\nmemory accesses, the majority of which occur in fully-connected networks. In\nfact, they contain most of the deep neural network parameters. In this paper,\nwe propose sparsely-connected networks, by showing that the number of\nconnections in fully-connected networks can be reduced by up to 90% while\nimproving the accuracy performance on three popular datasets (MNIST, CIFAR10\nand SVHN). We then propose an efficient hardware architecture based on\nlinear-feedback shift registers to reduce the memory requirements of the\nproposed sparsely-connected networks. The proposed architecture can save up to\n90% of memory compared to the conventional implementations of fully-connected\nneural networks. Moreover, implementation results show up to 84% reduction in\nthe energy consumption of a single neuron of the proposed sparsely-connected\nnetworks compared to a single neuron of fully-connected neural networks.\n","negative":"  This paper surveys quantum learning theory: the theoretical aspects of\nmachine learning using quantum computers. We describe the main results known\nfor three models of learning: exact learning from membership queries, and\nProbably Approximately Correct (PAC) and agnostic learning from classical or\nquantum examples.\n","id":33}
{"Unnamed: 0.1":11034,"Unnamed: 0":11034.0,"anchor":"Semi-supervised deep learning by metric embedding","positive":"  Deep networks are successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are usually much less suited for semi-supervised\nproblems because of their tendency to overfit easily when trained on small\namounts of data. In this work we will explore a new training objective that is\ntargeting a semi-supervised regime with only a small subset of labeled data.\nThis criterion is based on a deep metric embedding over distance relations\nwithin the set of labeled samples, together with constraints over the\nembeddings of the unlabeled set. The final learned representations are\ndiscriminative in euclidean space, and hence can be used with subsequent\nnearest-neighbor classification using the labeled samples.\n","negative":"  It's useful to automatically transform an image from its original form to\nsome synthetic form (style, partial contents, etc.), while keeping the original\nstructure or semantics. We define this requirement as the \"image-to-image\ntranslation\" problem, and propose a general approach to achieve it, based on\ndeep convolutional and conditional generative adversarial networks (GANs),\nwhich has gained a phenomenal success to learn mapping images from noise input\nsince 2014. In this work, we develop a two step (unsupervised) learning method\nto translate images between different domains by using unlabeled images without\nspecifying any correspondence between them, so that to avoid the cost of\nacquiring labeled data. Compared with prior works, we demonstrated the capacity\nof generality in our model, by which variance of translations can be conduct by\na single type of model. Such capability is desirable in applications like\nbidirectional translation\n","id":34}
{"Unnamed: 0.1":11035,"Unnamed: 0":11035.0,"anchor":"Ways of Conditioning Generative Adversarial Networks","positive":"  The GANs are generative models whose random samples realistically reflect\nnatural images. It also can generate samples with specific attributes by\nconcatenating a condition vector into the input, yet research on this field is\nnot well studied. We propose novel methods of conditioning generative\nadversarial networks (GANs) that achieve state-of-the-art results on MNIST and\nCIFAR-10. We mainly introduce two models: an information retrieving model that\nextracts conditional information from the samples, and a spatial bilinear\npooling model that forms bilinear features derived from the spatial cross\nproduct of an image and a condition vector. These methods significantly enhance\nlog-likelihood of test data under the conditional distributions compared to the\nmethods of concatenation.\n","negative":"  In this paper we make two novel contributions to hierarchical clustering.\nFirst, we introduce an anomalous pattern initialisation method for hierarchical\nclustering algorithms, called A-Ward, capable of substantially reducing the\ntime they take to converge. This method generates an initial partition with a\nsufficiently large number of clusters. This allows the cluster merging process\nto start from this partition rather than from a trivial partition composed\nsolely of singletons. Our second contribution is an extension of the Ward and\nWard p algorithms to the situation where the feature weight exponent can differ\nfrom the exponent of the Minkowski distance. This new method, called A-Ward\np\\b{eta} , is able to generate a much wider variety of clustering solutions. We\nalso demonstrate that its parameters can be estimated reasonably well by using\na cluster validity index. We perform numerous experiments using data sets with\ntwo types of noise, insertion of noise features and blurring within-cluster\nvalues of some features. These experiments allow us to conclude: (i) our\nanomalous pattern initialisation method does indeed reduce the time a\nhierarchical clustering algorithm takes to complete, without negatively\nimpacting its cluster recovery ability; (ii) A-Ward p\\b{eta} provides better\ncluster recovery than both Ward and Ward p.\n","id":35}
{"Unnamed: 0.1":11036,"Unnamed: 0":11036.0,"anchor":"Learning heat diffusion graphs","positive":"  Effective information analysis generally boils down to properly identifying\nthe structure or geometry of the data, which is often represented by a graph.\nIn some applications, this structure may be partly determined by design\nconstraints or pre-determined sensing arrangements, like in road transportation\nnetworks for example. In general though, the data structure is not readily\navailable and becomes pretty difficult to define. In particular, the global\nsmoothness assumptions, that most of the existing works adopt, are often too\ngeneral and unable to properly capture localized properties of data. In this\npaper, we go beyond this classical data model and rather propose to represent\ninformation as a sparse combination of localized functions that live on a data\nstructure represented by a graph. Based on this model, we focus on the problem\nof inferring the connectivity that best explains the data samples at different\nvertices of a graph that is a priori unknown. We concentrate on the case where\nthe observed data is actually the sum of heat diffusion processes, which is a\nquite common model for data on networks or other irregular structures. We cast\na new graph learning problem and solve it with an efficient nonconvex\noptimization algorithm. Experiments on both synthetic and real world data\nfinally illustrate the benefits of the proposed graph learning framework and\nconfirm that the data structure can be efficiently learned from data\nobservations only. We believe that our algorithm will help solving key\nquestions in diverse application domains such as social and biological network\nanalysis where it is crucial to unveil proper geometry for data understanding\nand inference.\n","negative":"  Topological data analysis (TDA) has emerged as one of the most promising\ntechniques to reconstruct the unknown shapes of high-dimensional spaces from\nobserved data samples. TDA, thus, yields key shape descriptors in the form of\npersistent topological features that can be used for any supervised or\nunsupervised learning task, including multi-way classification. Sparse\nsampling, on the other hand, provides a highly efficient technique to\nreconstruct signals in the spatial-temporal domain from just a few\ncarefully-chosen samples. Here, we present a new method, referred to as the\nSparse-TDA algorithm, that combines favorable aspects of the two techniques.\nThis combination is realized by selecting an optimal set of sparse pixel\nsamples from the persistent features generated by a vector-based TDA algorithm.\nThese sparse samples are selected from a low-rank matrix representation of\npersistent features using QR pivoting. We show that the Sparse-TDA method\ndemonstrates promising performance on three benchmark problems related to human\nposture recognition and image texture classification.\n","id":36}
{"Unnamed: 0.1":11037,"Unnamed: 0":11037.0,"anchor":"Multi-task learning with deep model based reinforcement learning","positive":"  In recent years, model-free methods that use deep learning have achieved\ngreat success in many different reinforcement learning environments. Most\nsuccessful approaches focus on solving a single task, while multi-task\nreinforcement learning remains an open problem. In this paper, we present a\nmodel based approach to deep reinforcement learning which we use to solve\ndifferent tasks simultaneously. We show that our approach not only does not\ndegrade but actually benefits from learning multiple tasks. For our model, we\nalso present a new kind of recurrent neural network inspired by residual\nnetworks that decouples memory from computation allowing to model complex\nenvironments that do not require lots of memory.\n","negative":"  Training deep neural networks for solving machine learning problems is one\ngreat challenge in the field, mainly due to its associated optimisation problem\nbeing highly non-convex. Recent developments have suggested that many training\nalgorithms do not suffer from undesired local minima under certain scenario,\nand consequently led to great efforts in pursuing mathematical explanations for\nsuch observations. This work provides an alternative mathematical understanding\nof the challenge from a smooth optimisation perspective. By assuming exact\nlearning of finite samples, sufficient conditions are identified via a critical\npoint analysis to ensure any local minimum to be globally minimal as well.\nFurthermore, a state of the art algorithm, known as the Generalised\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\nNewton's algorithm, which shares the property of being locally quadratically\nconvergent to a global minimum under the condition of exact learning.\n","id":37}
{"Unnamed: 0.1":11038,"Unnamed: 0":11038.0,"anchor":"Tying Word Vectors and Word Classifiers: A Loss Framework for Language\n  Modeling","positive":"  Recurrent neural networks have been very successful at predicting sequences\nof words in tasks such as language modeling. However, all such models are based\non the conventional classification framework, where the model is trained\nagainst one-hot targets, and each word is represented both as an input and as\nan output in isolation. This causes inefficiencies in learning both in terms of\nutilizing all of the information and in terms of the number of parameters\nneeded to train. We introduce a novel theoretical framework that facilitates\nbetter learning in language modeling, and show that our framework leads to\ntying together the input embedding and the output projection matrices, greatly\nreducing the number of trainable variables. Our framework leads to state of the\nart performance on the Penn Treebank with a variety of network models.\n","negative":"  Modern convolutional networks, incorporating rectifiers and max-pooling, are\nneither smooth nor convex; standard guarantees therefore do not apply.\nNevertheless, methods from convex optimization such as gradient descent and\nAdam are widely used as building blocks for deep learning algorithms. This\npaper provides the first convergence guarantee applicable to modern convnets,\nwhich furthermore matches a lower bound for convex nonsmooth functions. The key\ntechnical tool is the neural Taylor approximation -- a straightforward\napplication of Taylor expansions to neural networks -- and the associated\nTaylor loss. Experiments on a range of optimizers, layers, and tasks provide\nevidence that the analysis accurately captures the dynamics of neural\noptimization. The second half of the paper applies the Taylor approximation to\nisolate the main difficulty in training rectifier nets -- that gradients are\nshattered -- and investigates the hypothesis that, by exploring the space of\nactivation configurations more thoroughly, adaptive optimizers such as RMSProp\nand Adam are able to converge to better solutions.\n","id":38}
{"Unnamed: 0.1":11039,"Unnamed: 0":11039.0,"anchor":"Understanding Deep Neural Networks with Rectified Linear Units","positive":"  In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory.\n","negative":"  Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an\nimplementation f that meets both a semantic constraint given by a logical\nformula $\\varphi$ in a background theory T, and a syntactic constraint given by\na grammar G, which specifies the allowed set of candidate implementations. Such\na synthesis problem can be formally defined in SyGuS-IF, a language that is\nbuilt on top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to\nfacilitate, bring together and accelerate research and development of efficient\nsolvers for SyGuS by providing a platform for evaluating different synthesis\ntechniques on a comprehensive set of benchmarks. In this year's competition we\nadded a new track devoted to programming by examples. This track consisted of\ntwo categories, one using the theory of bit-vectors and one using the theory of\nstrings. This paper presents and analyses the results of SyGuS-Comp'16.\n","id":39}
{"Unnamed: 0.1":11040,"Unnamed: 0":11040.0,"anchor":"Protein Secondary Structure Prediction Using Deep Multi-scale\n  Convolutional Neural Networks and Next-Step Conditioning","positive":"  Recently developed deep learning techniques have significantly improved the\naccuracy of various speech and image recognition systems. In this paper we\nadapt some of these techniques for protein secondary structure prediction. We\nfirst train a series of deep neural networks to predict eight-class secondary\nstructure labels given a protein's amino acid sequence information and find\nthat using recent methods for regularization, such as dropout and weight-norm\nconstraining, leads to measurable gains in accuracy. We then adapt recent\nconvolutional neural network architectures--Inception, ReSNet, and DenseNet\nwith Batch Normalization--to the problem of protein structure prediction. These\nconvolutional architectures make heavy use of multi-scale filter layers that\nsimultaneously compute features on several scales, and use residual connections\nto prevent underfitting. Using a carefully modified version of these\narchitectures, we achieve state-of-the-art performance of 70.0% per amino acid\naccuracy on the public CB513 benchmark dataset. Finally, we explore additions\nfrom sequence-to-sequence learning, altering the model to make its predictions\nconditioned on both the protein's amino acid sequence and its past secondary\nstructure labels. We introduce a new method of ensembling such a conditional\nmodel with our convolutional model, an approach which reaches 70.6% Q8 accuracy\non CB513. We argue that these results can be further refined for larger boosts\nin prediction accuracy through more sophisticated attempts to control\noverfitting of conditional models. We aim to release the code for these\nexperiments as part of the TensorFlow repository.\n","negative":"  This paper introduces the probabilistic module interface, which allows\nencapsulation of complex probabilistic models with latent variables alongside\ncustom stochastic approximate inference machinery, and provides a\nplatform-agnostic abstraction barrier separating the model internals from the\nhost probabilistic inference system. The interface can be seen as a stochastic\ngeneralization of a standard simulation and density interface for probabilistic\nprimitives. We show that sound approximate inference algorithms can be\nconstructed for networks of probabilistic modules, and we demonstrate that the\ninterface can be implemented using learned stochastic inference networks and\nMCMC and SMC approximate inference programs.\n","id":40}
{"Unnamed: 0.1":11041,"Unnamed: 0":11041.0,"anchor":"Estimating Causal Direction and Confounding of Two Discrete Variables","positive":"  We propose a method to classify the causal relationship between two discrete\nvariables given only the joint distribution of the variables, acknowledging\nthat the method is subject to an inherent baseline error. We assume that the\ncausal system is acyclicity, but we do allow for hidden common causes. Our\nalgorithm presupposes that the probability distributions $P(C)$ of a cause $C$\nis independent from the probability distribution $P(E\\mid C)$ of the\ncause-effect mechanism. While our classifier is trained with a Bayesian\nassumption of flat hyperpriors, we do not make this assumption about our test\ndata. This work connects to recent developments on the identifiability of\ncausal models over continuous variables under the assumption of \"independent\nmechanisms\". Carefully-commented Python notebooks that reproduce all our\nexperiments are available online at\nhttp:\/\/vision.caltech.edu\/~kchalupk\/code.html.\n","negative":"  We propose a general theory for studying the \\xl{landscape} of nonconvex\n\\xl{optimization} with underlying symmetric structures \\tz{for a class of\nmachine learning problems (e.g., low-rank matrix factorization, phase\nretrieval, and deep linear neural networks)}. In specific, we characterize the\nlocations of stationary points and the null space of Hessian matrices \\xl{of\nthe objective function} via the lens of invariant groups\\removed{for associated\noptimization problems, including low-rank matrix factorization, phase\nretrieval, and deep linear neural networks}. As a major motivating example, we\napply the proposed general theory to characterize the global \\xl{landscape} of\nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In\nparticular, we illustrate how the rotational symmetry group gives rise to\ninfinitely many nonisolated strict saddle points and equivalent global minima\nof the objective function. By explicitly identifying all stationary points, we\ndivide the entire parameter space into three regions: ($\\cR_1$) the region\ncontaining the neighborhoods of all strict saddle points, where the objective\nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all\nglobal minima, where the objective enjoys strong convexity along certain\ndirections; and ($\\cR_3$) the complement of the above regions, where the\ngradient has sufficiently large magnitudes. We further extend our result to the\nmatrix sensing problem. Such global landscape implies strong global convergence\nguarantees for popular iterative algorithms with arbitrary initial solutions.\n","id":41}
{"Unnamed: 0.1":11042,"Unnamed: 0":11042.0,"anchor":"Eve: A Gradient Based Optimization Method with Locally and Globally\n  Adaptive Learning Rates","positive":"  Adaptive gradient methods for stochastic optimization adjust the learning\nrate for each parameter locally. However, there is also a global learning rate\nwhich must be tuned in order to get the best performance. In this paper, we\npresent a new algorithm that adapts the learning rate locally for each\nparameter separately, and also globally for all parameters together.\nSpecifically, we modify Adam, a popular method for training deep learning\nmodels, with a coefficient that captures properties of the objective function.\nEmpirically, we show that our method, which we call Eve, outperforms Adam and\nother popular methods in training deep neural networks, like convolutional\nneural networks for image classification, and recurrent neural networks for\nlanguage tasks.\n","negative":"  We investigate iterated compositions of weighted sums of Gaussian kernels and\nprovide an interpretation of the construction that shows some similarities with\nthe architectures of deep neural networks. On the theoretical side, we show\nthat these kernels are universal and that SVMs using these kernels are\nuniversally consistent. We further describe a parameter optimization method for\nthe kernel parameters and empirically compare this method to SVMs, random\nforests, a multiple kernel learning approach, and to some deep neural networks.\n","id":42}
{"Unnamed: 0.1":11043,"Unnamed: 0":11043.0,"anchor":"Topology and Geometry of Half-Rectified Network Optimization","positive":"  The loss surface of deep neural networks has recently attracted interest in\nthe optimization and machine learning communities as a prime example of\nhigh-dimensional non-convex problem. Some insights were recently gained using\nspin glass models and mean-field approximations, but at the expense of strongly\nsimplifying the nonlinear nature of the model.\n  In this work, we do not make any such assumption and study conditions on the\ndata distribution and model architecture that prevent the existence of bad\nlocal minima. Our theoretical work quantifies and formalizes two important\n\\emph{folklore} facts: (i) the landscape of deep linear networks has a\nradically different topology from that of deep half-rectified ones, and (ii)\nthat the energy landscape in the non-linear case is fundamentally controlled by\nthe interplay between the smoothness of the data distribution and model\nover-parametrization. Our main theoretical contribution is to prove that\nhalf-rectified single layer networks are asymptotically connected, and we\nprovide explicit bounds that reveal the aforementioned interplay.\n  The conditioning of gradient descent is the next challenge we address. We\nstudy this question through the geometry of the level sets, and we introduce an\nalgorithm to efficiently estimate the regularity of such sets on large-scale\nnetworks. Our empirical results show that these level sets remain connected\nthroughout all the learning phase, suggesting a near convex behavior, but they\nbecome exponentially more curvy as the energy level decays, in accordance to\nwhat is observed in practice with very low curvature attractors.\n","negative":"  Compressive sensing (CS) is a promising technology for realizing\nenergy-efficient wireless sensors for long-term health monitoring. However,\nconventional model-driven CS frameworks suffer from limited compression ratio\nand reconstruction quality when dealing with physiological signals due to\ninaccurate models and the overlook of individual variability. In this paper, we\npropose a data-driven CS framework that can learn signal characteristics and\npersonalized features from any individual recording of physiologic signals to\nenhance CS performance with a minimized number of measurements. Such\nimprovements are accomplished by a co-training approach that optimizes the\nsensing matrix and the dictionary towards improved restricted isometry property\nand signal sparsity, respectively. Experimental results upon ECG signals show\nthat the proposed method, at a compression ratio of 10x, successfully reduces\nthe isometry constant of the trained sensing matrices by 86% against random\nmatrices and improves the overall reconstructed signal-to-noise ratio by 15dB\nover conventional model-driven approaches.\n","id":43}
{"Unnamed: 0.1":11044,"Unnamed: 0":11044.0,"anchor":"Classification with Ultrahigh-Dimensional Features","positive":"  Although much progress has been made in classification with high-dimensional\nfeatures \\citep{Fan_Fan:2008, JGuo:2010, CaiSun:2014, PRXu:2014},\nclassification with ultrahigh-dimensional features, wherein the features much\noutnumber the sample size, defies most existing work. This paper introduces a\nnovel and computationally feasible multivariate screening and classification\nmethod for ultrahigh-dimensional data. Leveraging inter-feature correlations,\nthe proposed method enables detection of marginally weak and sparse signals and\nrecovery of the true informative feature set, and achieves asymptotic optimal\nmisclassification rates. We also show that the proposed procedure provides more\npowerful discovery boundaries compared to those in \\citet{CaiSun:2014} and\n\\citet{JJin:2009}. The performance of the proposed procedure is evaluated using\nsimulation studies and demonstrated via classification of patients with\ndifferent post-transplantation renal functional types.\n","negative":"  We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute.\n","id":44}
{"Unnamed: 0.1":11045,"Unnamed: 0":11045.0,"anchor":"Automated Generation of Multilingual Clusters for the Evaluation of\n  Distributed Representations","positive":"  We propose a language-agnostic way of automatically generating sets of\nsemantically similar clusters of entities along with sets of \"outlier\"\nelements, which may then be used to perform an intrinsic evaluation of word\nembeddings in the outlier detection task. We used our methodology to create a\ngold-standard dataset, which we call WikiSem500, and evaluated multiple\nstate-of-the-art embeddings. The results show a correlation between performance\non this dataset and performance on sentiment analysis.\n","negative":"  Algorithms with fast convergence, small number of data access, and low\nper-iteration complexity are particularly favorable in the big data era, due to\nthe demand for obtaining \\emph{highly accurate solutions} to problems with\n\\emph{a large number of samples} in \\emph{ultra-high} dimensional space.\nExisting algorithms lack at least one of these qualities, and thus are\ninefficient in handling such big data challenge. In this paper, we propose a\nmethod enjoying all these merits with an accelerated convergence rate\n$O(\\frac{1}{k^2})$. Empirical studies on large scale datasets with more than\none million features are conducted to show the effectiveness of our methods in\npractice.\n","id":45}
{"Unnamed: 0.1":11046,"Unnamed: 0":11046.0,"anchor":"Quasi-Recurrent Neural Networks","positive":"  Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.\n","negative":"  Feature pooling layers (e.g., max pooling) in convolutional neural networks\n(CNNs) serve the dual purpose of providing increasingly abstract\nrepresentations as well as yielding computational savings in subsequent\nconvolutional layers. We view the pooling operation in CNNs as a two-step\nprocedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature\nmap with stride one which leaves the spatial resolution intact, and second,\ndownsampling is performed by selecting one pixel from each non-overlapping\npooling window in an often uniform and deterministic (e.g., top-left) manner.\nOur starting point in this work is the observation that this regularly spaced\ndownsampling arising from non-overlapping windows, although intuitive from a\nsignal processing perspective (which has the goal of signal reconstruction), is\nnot necessarily optimal for \\emph{learning} (where the goal is to generalize).\nWe study this aspect and propose a novel pooling strategy with stochastic\nspatial sampling (S3Pool), where the regular downsampling is replaced by a more\ngeneral stochastic version. We observe that this general stochasticity acts as\na strong regularizer, and can also be seen as doing implicit data augmentation\nby introducing distortions in the feature maps. We further introduce a\nmechanism to control the amount of distortion to suit different datasets and\narchitectures. To demonstrate the effectiveness of the proposed approach, we\nperform extensive experiments on several popular image classification\nbenchmarks, observing excellent improvements over baseline models. Experimental\ncode is available at https:\/\/github.com\/Shuangfei\/s3pool.\n","id":46}
{"Unnamed: 0.1":11047,"Unnamed: 0":11047.0,"anchor":"Neural Architecture Search with Reinforcement Learning","positive":"  Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.\n","negative":"  We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.\n","id":47}
{"Unnamed: 0.1":11048,"Unnamed: 0":11048.0,"anchor":"Class-prior Estimation for Learning from Positive and Unlabeled Data","positive":"  We consider the problem of estimating the class prior in an unlabeled\ndataset. Under the assumption that an additional labeled dataset is available,\nthe class prior can be estimated by fitting a mixture of class-wise data\ndistributions to the unlabeled data distribution. However, in practice, such an\nadditional labeled dataset is often not available. In this paper, we show that,\nwith additional samples coming only from the positive class, the class prior of\nthe unlabeled dataset can be estimated correctly. Our key idea is to use\nproperly penalized divergences for model fitting to cancel the error caused by\nthe absence of negative samples. We further show that the use of the penalized\n$L_1$-distance gives a computationally efficient algorithm with an analytic\nsolution. The consistency, stability, and estimation error are theoretically\nanalyzed. Finally, we experimentally demonstrate the usefulness of the proposed\nmethod.\n","negative":"  We illustrate the potential applications in machine learning of the\nChristoffel function, or more precisely, its empirical counterpart associated\nwith a counting measure uniformly supported on a finite set of points. Firstly,\nwe provide a thresholding scheme which allows to approximate the support of a\nmeasure from a finite subset of its moments with strong asymptotic guaranties.\nSecondly, we provide a consistency result which relates the empirical\nChristoffel function and its population counterpart in the limit of large\nsamples. Finally, we illustrate the relevance of our results on simulated and\nreal world datasets for several applications in statistics and machine\nlearning: (a) density and support estimation from finite samples, (b) outlier\nand novelty detection and (c) affine matching.\n","id":48}
{"Unnamed: 0.1":11049,"Unnamed: 0":11049.0,"anchor":"LipNet: End-to-End Sentence-level Lipreading","positive":"  Lipreading is the task of decoding text from the movement of a speaker's\nmouth. Traditional approaches separated the problem into two stages: designing\nor learning visual features, and prediction. More recent deep lipreading\napproaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman,\n2016a). However, existing work on models trained end-to-end perform only word\nclassification, rather than sentence-level sequence prediction. Studies have\nshown that human lipreading performance increases for longer words (Easton &\nBasala, 1982), indicating the importance of features capturing temporal context\nin an ambiguous communication channel. Motivated by this observation, we\npresent LipNet, a model that maps a variable-length sequence of video frames to\ntext, making use of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. To the\nbest of our knowledge, LipNet is the first end-to-end sentence-level lipreading\nmodel that simultaneously learns spatiotemporal visual features and a sequence\nmodel. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level,\noverlapped speaker split task, outperforming experienced human lipreaders and\nthe previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).\n","negative":"  We study the classic $k$-median and $k$-means clustering objectives in the\nbeyond-worst-case scenario. We consider three well-studied notions of\nstructured data that aim at characterizing real-world inputs: Distribution\nStability (introduced by Awasthi, Blum, and Sheffet, FOCS 2010), Spectral\nSeparability (introduced by Kumar and Kannan, FOCS 2010), Perturbation\nResilience (introduced by Bilu and Linial, ICS 2010).\n  We prove structural results showing that inputs satisfying at least one of\nthe conditions are inherently \"local\". Namely, for any such input, any local\noptimum is close both in term of structure and in term of objective value to\nthe global optima.\n  As a corollary we obtain that the widely-used Local Search algorithm has\nstrong performance guarantees for both the tasks of recovering the underlying\noptimal clustering and obtaining a clustering of small cost. This is a\nsignificant step toward understanding the success of local search heuristics in\nclustering applications.\n","id":49}
{"Unnamed: 0.1":11050,"Unnamed: 0":11050.0,"anchor":"Loss-aware Binarization of Deep Networks","positive":"  Deep neural network models, though very powerful and highly successful, are\ncomputationally expensive in terms of space and time. Recently, there have been\na number of attempts on binarizing the network weights and activations. This\ngreatly reduces the network size, and replaces the underlying multiplications\nto additions or even XNOR bit operations. However, existing binarization\nschemes are based on simple matrix approximation and ignore the effect of\nbinarization on the loss. In this paper, we propose a proximal Newton algorithm\nwith diagonal Hessian approximation that directly minimizes the loss w.r.t. the\nbinarized weights. The underlying proximal step has an efficient closed-form\nsolution, and the second-order information can be efficiently obtained from the\nsecond moments already computed by the Adam optimizer. Experiments on both\nfeedforward and recurrent networks show that the proposed loss-aware\nbinarization algorithm outperforms existing binarization schemes, and is also\nmore robust for wide and deep networks.\n","negative":"  We present an architecture which lets us train deep, directed generative\nmodels with many layers of latent variables. We include deterministic paths\nbetween all latent variables and the generated output, and provide a richer set\nof connections between computations for inference and generation, which enables\nmore effective communication of information throughout the model during\ntraining. To improve performance on natural images, we incorporate a\nlightweight autoregressive model in the reconstruction distribution. These\ntechniques permit end-to-end training of models with 10+ layers of latent\nvariables. Experiments show that our approach achieves state-of-the-art\nperformance on standard image modelling benchmarks, can expose latent class\nstructure in the absence of label information, and can provide convincing\nimputations of occluded regions in natural images.\n","id":50}
{"Unnamed: 0.1":11051,"Unnamed: 0":11051.0,"anchor":"Learning to Play in a Day: Faster Deep Reinforcement Learning by\n  Optimality Tightening","positive":"  We propose a novel training algorithm for reinforcement learning which\ncombines the strength of deep Q-learning with a constrained optimization\napproach to tighten optimality and encourage faster reward propagation. Our\nnovel technique makes deep reinforcement learning more practical by drastically\nreducing the training time. We evaluate the performance of our approach on the\n49 games of the challenging Arcade Learning Environment, and report significant\nimprovements in both training time and accuracy.\n","negative":"  We present the Neural Physics Engine (NPE), a framework for learning\nsimulators of intuitive physics that naturally generalize across variable\nobject count and different scene configurations. We propose a factorization of\na physical scene into composable object-based representations and a neural\nnetwork architecture whose compositional structure factorizes object dynamics\ninto pairwise interactions. Like a symbolic physics engine, the NPE is endowed\nwith generic notions of objects and their interactions; realized as a neural\nnetwork, it can be trained via stochastic gradient descent to adapt to specific\nobject properties and dynamics of different worlds. We evaluate the efficacy of\nour approach on simple rigid body dynamics in two-dimensional worlds. By\ncomparing to less structured architectures, we show that the NPE's\ncompositional representation of the structure in physical interactions improves\nits ability to predict movement, generalize across variable object count and\ndifferent scene configurations, and infer latent properties of objects such as\nmass.\n","id":51}
{"Unnamed: 0.1":11052,"Unnamed: 0":11052.0,"anchor":"Combining policy gradient and Q-learning","positive":"  Policy gradient is an efficient technique for improving a policy in a\nreinforcement learning setting. However, vanilla online variants are on-policy\nonly and not able to take advantage of off-policy data. In this paper we\ndescribe a new technique that combines policy gradient with off-policy\nQ-learning, drawing experience from a replay buffer. This is motivated by\nmaking a connection between the fixed points of the regularized policy gradient\nalgorithm and the Q-values. This connection allows us to estimate the Q-values\nfrom the action preferences of the policy, to which we apply Q-learning\nupdates. We refer to the new technique as 'PGQL', for policy gradient and\nQ-learning. We also establish an equivalency between action-value fitting\ntechniques and actor-critic algorithms, showing that regularized policy\ngradient techniques can be interpreted as advantage function learning\nalgorithms. We conclude with some numerical examples that demonstrate improved\ndata efficiency and stability of PGQL. In particular, we tested PGQL on the\nfull suite of Atari games and achieved performance exceeding that of both\nasynchronous advantage actor-critic (A3C) and Q-learning.\n","negative":"  Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017).\n","id":52}
{"Unnamed: 0.1":11053,"Unnamed: 0":11053.0,"anchor":"Robustly representing uncertainty in deep neural networks through\n  sampling","positive":"  As deep neural networks (DNNs) are applied to increasingly challenging\nproblems, they will need to be able to represent their own uncertainty.\nModeling uncertainty is one of the key features of Bayesian methods. Using\nBernoulli dropout with sampling at prediction time has recently been proposed\nas an efficient and well performing variational inference method for DNNs.\nHowever, sampling from other multiplicative noise based variational\ndistributions has not been investigated in depth. We evaluated Bayesian DNNs\ntrained with Bernoulli or Gaussian multiplicative masking of either the units\n(dropout) or the weights (dropconnect). We tested the calibration of the\nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on\nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of\nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or\nBernoulli, led to more robust representation of uncertainty compared to\nsampling of units. However, using either Gaussian or Bernoulli dropout led to\nincreased test set classification accuracy. Based on these findings we used\nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show\napproximates the use of a spike-and-slab variational distribution without\nincreasing the number of learned parameters. We found that spike-and-slab\nsampling had higher test set performance than Gaussian dropconnect and more\nrobustly represented its uncertainty compared to Bernoulli dropout.\n","negative":"  Recurrent neural networks (RNNs) are powerful and effective for processing\nsequential data. However, RNNs are usually considered \"black box\" models whose\ninternal structure and learned parameters are not interpretable. In this paper,\nwe propose an interpretable RNN based on the sequential iterative\nsoft-thresholding algorithm (SISTA) for solving the sequential sparse recovery\nproblem, which models a sequence of correlated observations with a sequence of\nsparse latent vectors. The architecture of the resulting SISTA-RNN is\nimplicitly defined by the computational structure of SISTA, which results in a\nnovel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are\nperfectly interpretable as the parameters of a principled statistical model,\nwhich in this case include a sparsifying dictionary, iterative step size, and\nregularization parameters. In addition, on a particular sequential compressive\nsensing task, the SISTA-RNN trains faster and achieves better performance than\nconventional state-of-the-art black box RNNs, including long-short term memory\n(LSTM) RNNs.\n","id":53}
{"Unnamed: 0.1":11054,"Unnamed: 0":11054.0,"anchor":"Twenty (simple) questions","positive":"  A basic combinatorial interpretation of Shannon's entropy function is via the\n\"20 questions\" game. This cooperative game is played by two players, Alice and\nBob: Alice picks a distribution $\\pi$ over the numbers $\\{1,\\ldots,n\\}$, and\nannounces it to Bob. She then chooses a number $x$ according to $\\pi$, and Bob\nattempts to identify $x$ using as few Yes\/No queries as possible, on average.\n  An optimal strategy for the \"20 questions\" game is given by a Huffman code\nfor $\\pi$: Bob's questions reveal the codeword for $x$ bit by bit. This\nstrategy finds $x$ using fewer than $H(\\pi)+1$ questions on average. However,\nthe questions asked by Bob could be arbitrary. In this paper, we investigate\nthe following question: Are there restricted sets of questions that match the\nperformance of Huffman codes, either exactly or approximately?\n  Our first main result shows that for every distribution $\\pi$, Bob has a\nstrategy that uses only questions of the form \"$x < c$?\" and \"$x = c$?\", and\nuncovers $x$ using at most $H(\\pi)+1$ questions on average, matching the\nperformance of Huffman codes in this sense. We also give a natural set of\n$O(rn^{1\/r})$ questions that achieve a performance of at most $H(\\pi)+r$, and\nshow that $\\Omega(rn^{1\/r})$ questions are required to achieve such a\nguarantee.\n  Our second main result gives a set $\\mathcal{Q}$ of $1.25^{n+o(n)}$ questions\nsuch that for every distribution $\\pi$, Bob can implement an optimal strategy\nfor $\\pi$ using only questions from $\\mathcal{Q}$. We also show that\n$1.25^{n-o(n)}$ questions are needed, for infinitely many $n$. If we allow a\nsmall slack of $r$ over the optimal strategy, then roughly $(rn)^{\\Theta(1\/r)}$\nquestions are necessary and sufficient.\n","negative":"  We introduce a new family of minmax rank aggregation problems under two\ndistance measures, the Kendall {\\tau} and the Spearman footrule. As the\nproblems are NP-hard, we proceed to describe a number of constant-approximation\nalgorithms for solving them. We conclude with illustrative applications of the\naggregation methods on the Mallows model and genomic data.\n","id":54}
{"Unnamed: 0.1":11055,"Unnamed: 0":11055.0,"anchor":"Generative Multi-Adversarial Networks","positive":"  Generative adversarial networks (GANs) are a framework for producing a\ngenerative model by way of a two-player minimax game. In this paper, we propose\nthe \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that\nextends GANs to multiple discriminators. In previous work, the successful\ntraining of GANs requires modifying the minimax objective to accelerate\ntraining early on. In contrast, GMAN can be reliably trained with the original,\nuntampered objective. We explore a number of design perspectives with the\ndiscriminator role ranging from formidable adversary to forgiving teacher.\nImage generation tasks comparing the proposed framework to standard GANs\ndemonstrate GMAN produces higher quality samples in a fraction of the\niterations when measured by a pairwise GAM-type metric.\n","negative":"  We are surrounded by huge amounts of large-scale high dimensional data. It is\ndesirable to reduce the dimensionality of data for many learning tasks due to\nthe curse of dimensionality. Feature selection has shown its effectiveness in\nmany applications by building simpler and more comprehensive model, improving\nlearning performance, and preparing clean, understandable data. Recently, some\nunique characteristics of big data such as data velocity and data variety\npresent challenges to the feature selection problem. In this paper, we envision\nthese challenges of feature selection for big data analytics. In particular, we\nfirst give a brief introduction about feature selection and then detail the\nchallenges of feature selection for structured, heterogeneous and streaming\ndata as well as its scalability and stability issues. At last, to facilitate\nand promote the feature selection research, we present an open-source feature\nselection repository (scikit-feature), which consists of most of current\npopular feature selection algorithms.\n","id":55}
{"Unnamed: 0.1":11056,"Unnamed: 0":11056.0,"anchor":"Comparing learning algorithms in neural network for diagnosing\n  cardiovascular disease","positive":"  Today data mining techniques are exploited in medical science for diagnosing,\novercoming and treating diseases. Neural network is one of the techniques which\nare widely used for diagnosis in medical field. In this article efficiency of\nnine algorithms, which are basis of neural network learning in diagnosing\ncardiovascular diseases, will be assessed. Algorithms are assessed in terms of\naccuracy, sensitivity, transparency, AROC and convergence rate by means of 10\nfold cross validation. The results suggest that in training phase, Lonberg-M\nalgorithm has the best efficiency in terms of all metrics, algorithm OSS has\nmaximum accuracy in testing phase, algorithm SCG has the maximum transparency\nand algorithm CGB has the maximum sensitivity.\n","negative":"  Finding the main product of a chemical reaction is one of the important\nproblems of organic chemistry. This paper describes a method of applying a\nneural machine translation model to the prediction of organic chemical\nreactions. In order to translate 'reactants and reagents' to 'products', a\ngated recurrent unit based sequence-to-sequence model and a parser to generate\ninput tokens for model from reaction SMILES strings were built. Training sets\nare composed of reactions from the patent databases, and reactions manually\ngenerated applying the elementary reactions in an organic chemistry textbook of\nWade. The trained models were tested by examples and problems in the textbook.\nThe prediction process does not need manual encoding of rules (e.g., SMARTS\ntransformations) to predict products, hence it only needs sufficient training\nreaction sets to learn new types of reactions.\n","id":56}
{"Unnamed: 0.1":11057,"Unnamed: 0":11057.0,"anchor":"Oracle-Efficient Online Learning and Auction Design","positive":"  We consider the design of computationally efficient online learning\nalgorithms in an adversarial setting in which the learner has access to an\noffline optimization oracle. We present an algorithm called Generalized\nFollow-the-Perturbed-Leader and provide conditions under which it is\noracle-efficient while achieving vanishing regret. Our results make significant\nprogress on an open problem raised by Hazan and Koren, who showed that\noracle-efficient algorithms do not exist in general and asked whether one can\nidentify properties under which oracle-efficient online learning may be\npossible.\n  Our auction-design framework considers an auctioneer learning an optimal\nauction for a sequence of adversarially selected valuations with the goal of\nachieving revenue that is almost as good as the optimal auction in hindsight,\namong a class of auctions. We give oracle-efficient learning results for: (1)\nVCG auctions with bidder-specific reserves in single-parameter settings, (2)\nenvy-free item pricing in multi-item auctions, and (3) s-level auctions of\nMorgenstern and Roughgarden for single-item settings. The last result leads to\nan approximation of the overall optimal Myerson auction when bidders'\nvaluations are drawn according to a fast-mixing Markov process, extending prior\nwork that only gave such guarantees for the i.i.d. setting.\n  Finally, we derive various extensions, including: (1) oracle-efficient\nalgorithms for the contextual learning setting in which the learner has access\nto side information (such as bidder demographics), (2) learning with\napproximate oracles such as those based on Maximal-in-Range algorithms, and (3)\nno-regret bidding in simultaneous auctions, resolving an open problem of\nDaskalakis and Syrgkanis.\n","negative":"  Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.\n","id":57}
{"Unnamed: 0.1":11058,"Unnamed: 0":11058.0,"anchor":"TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency","positive":"  In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based\nlanguage model designed to directly capture the global semantic meaning\nrelating words in a document via latent topics. Because of their sequential\nnature, RNNs are good at capturing the local structure of a word sequence -\nboth semantic and syntactic - but might face difficulty remembering long-range\ndependencies. Intuitively, these long-range dependencies are of semantic\nnature. In contrast, latent topic models are able to capture the global\nunderlying semantic structure of a document but do not account for word\nordering. The proposed TopicRNN model integrates the merits of RNNs and latent\ntopic models: it captures local (syntactic) dependencies using an RNN and\nglobal (semantic) dependencies using latent topics. Unlike previous work on\ncontextual RNN language modeling, our model is learned end-to-end. Empirical\nresults on word prediction show that TopicRNN outperforms existing contextual\nRNN baselines. In addition, TopicRNN can be used as an unsupervised feature\nextractor for documents. We do this for sentiment analysis on the IMDB movie\nreview dataset and report an error rate of $6.28\\%$. This is comparable to the\nstate-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally,\nTopicRNN also yields sensible topics, making it a useful alternative to\ndocument models such as latent Dirichlet allocation.\n","negative":"  Controlling Chaos could be a big factor in getting great stable amounts of\nenergy out of small amounts of not necessarily stable resources. By definition,\nChaos is getting huge changes in the system's output due to unpredictable small\nchanges in initial conditions, and that means we could take advantage of this\nfact and select the proper control system to manipulate system's initial\nconditions and inputs in general and get a desirable output out of otherwise a\nChaotic system. That was accomplished by first building some known chaotic\ncircuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN\ncontrol system. It was shown that this technique can also be used to stabilize\nsome hard to stabilize electronic systems.\n","id":58}
{"Unnamed: 0.1":11059,"Unnamed: 0":11059.0,"anchor":"Detecting Dependencies in Sparse, Multivariate Databases Using\n  Probabilistic Programming and Non-parametric Bayes","positive":"  Datasets with hundreds of variables and many missing values are commonplace.\nIn this setting, it is both statistically and computationally challenging to\ndetect true predictive relationships between variables and also to suppress\nfalse positives. This paper proposes an approach that combines probabilistic\nprogramming, information theory, and non-parametric Bayes. It shows how to use\nBayesian non-parametric modeling to (i) build an ensemble of joint probability\nmodels for all the variables; (ii) efficiently detect marginal independencies;\nand (iii) estimate the conditional mutual information between arbitrary subsets\nof variables, subject to a broad class of constraints. Users can access these\ncapabilities using BayesDB, a probabilistic programming platform for\nprobabilistic data analysis, by writing queries in a simple, SQL-like language.\nThis paper demonstrates empirically that the method can (i) detect\ncontext-specific (in)dependencies on challenging synthetic problems and (ii)\nyield improved sensitivity and specificity over baselines from statistics and\nmachine learning, on a real-world database of over 300 sparsely observed\nindicators of macroeconomic development and public health.\n","negative":"  This paper presents a concept of a novel method for adjusting\nhyper-parameters in Deep Learning (DL) algorithms. An external agent-observer\nmonitors a performance of a selected Deep Learning algorithm. The observer\nlearns to model the DL algorithm using a series of random experiments.\nConsequently, it may be used for predicting a response of the DL algorithm in\nterms of a selected quality measurement to a set of hyper-parameters. This\nallows to construct an ensemble composed of a series of evaluators which\nconstitute an observer-assisted architecture. The architecture may be used to\ngradually iterate towards to the best achievable quality score in tiny steps\ngoverned by a unit of progress. The algorithm is stopped when the maximum\nnumber of steps is reached or no further progress is made.\n","id":59}
{"Unnamed: 0.1":11060,"Unnamed: 0":11060.0,"anchor":"Beyond Fine Tuning: A Modular Approach to Learning on Small Data","positive":"  In this paper we present a technique to train neural network models on small\namounts of data. Current methods for training neural networks on small amounts\nof rich data typically rely on strategies such as fine-tuning a pre-trained\nneural network or the use of domain-specific hand-engineered features. Here we\ntake the approach of treating network layers, or entire networks, as modules\nand combine pre-trained modules with untrained modules, to learn the shift in\ndistributions between data sets. The central impact of using a modular approach\ncomes from adding new representations to a network, as opposed to replacing\nrepresentations via fine-tuning. Using this technique, we are able surpass\nresults using standard fine-tuning transfer learning approaches, and we are\nalso able to significantly increase performance over such approaches when using\nsmaller amounts of data.\n","negative":"  Malicious URL, a.k.a. malicious website, is a common and serious threat to\ncybersecurity. Malicious URLs host unsolicited content (spam, phishing,\ndrive-by exploits, etc.) and lure unsuspecting users to become victims of scams\n(monetary loss, theft of private information, and malware installation), and\ncause losses of billions of dollars every year. It is imperative to detect and\nact on such threats in a timely manner. Traditionally, this detection is done\nmostly through the usage of blacklists. However, blacklists cannot be\nexhaustive, and lack the ability to detect newly generated malicious URLs. To\nimprove the generality of malicious URL detectors, machine learning techniques\nhave been explored with increasing attention in recent years. This article aims\nto provide a comprehensive survey and a structural understanding of Malicious\nURL Detection techniques using machine learning. We present the formal\nformulation of Malicious URL Detection as a machine learning task, and\ncategorize and review the contributions of literature studies that addresses\ndifferent dimensions of this problem (feature representation, algorithm design,\netc.). Further, this article provides a timely and comprehensive survey for a\nrange of different audiences, not only for machine learning researchers and\nengineers in academia, but also for professionals and practitioners in\ncybersecurity industry, to help them understand the state of the art and\nfacilitate their own research and practical applications. We also discuss\npractical issues in system design, open research challenges, and point out some\nimportant directions for future research.\n","id":60}
{"Unnamed: 0.1":11061,"Unnamed: 0":11061.0,"anchor":"Learning to Draw Samples: With Application to Amortized MLE for\n  Generative Adversarial Learning","positive":"  We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient that maximumly decreases the\nKL divergence with the target distribution. Our method works for any target\ndistribution specified by their unnormalized density function, and can train\nany black-box architectures that are differentiable in terms of the parameters\nwe want to adapt. As an application of our method, we propose an amortized MLE\nalgorithm for training deep energy model, where a neural sampler is adaptively\ntrained to approximate the likelihood function. Our method mimics an\nadversarial game between the deep energy model and the neural sampler, and\nobtains realistic-looking images competitive with the state-of-the-art results.\n","negative":"  Network quantization is one of network compression techniques to reduce the\nredundancy of deep neural networks. It reduces the number of distinct network\nparameter values by quantization in order to save the storage for them. In this\npaper, we design network quantization schemes that minimize the performance\nloss due to quantization given a compression ratio constraint. We analyze the\nquantitative relation of quantization errors to the neural network loss\nfunction and identify that the Hessian-weighted distortion measure is locally\nthe right objective function for the optimization of network quantization. As a\nresult, Hessian-weighted k-means clustering is proposed for clustering network\nparameters to quantize. When optimal variable-length binary codes, e.g.,\nHuffman codes, are employed for further compression, we derive that the network\nquantization problem can be related to the entropy-constrained scalar\nquantization (ECSQ) problem in information theory and consequently propose two\nsolutions of ECSQ for network quantization, i.e., uniform quantization and an\niterative solution similar to Lloyd's algorithm. Finally, using the simple\nuniform quantization followed by Huffman coding, we show from our experiments\nthat the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,\n32-layer ResNet and AlexNet, respectively.\n","id":61}
{"Unnamed: 0.1":11062,"Unnamed: 0":11062.0,"anchor":"Words or Characters? Fine-grained Gating for Reading Comprehension","positive":"  Previous work combines word-level and character-level representations using\nconcatenation or scalar weighting, which is suboptimal for high-level tasks\nlike reading comprehension. We present a fine-grained gating mechanism to\ndynamically combine word-level and character-level representations based on\nproperties of the words. We also extend the idea of fine-grained gating to\nmodeling the interaction between questions and paragraphs for reading\ncomprehension. Experiments show that our approach can improve the performance\non reading comprehension tasks, achieving new state-of-the-art results on the\nChildren's Book Test dataset. To demonstrate the generality of our gating\nmechanism, we also show improved results on a social media tag prediction task.\n","negative":"  Obtaining common representations from different modalities is important in\nthat they are interchangeable with each other in a classification problem. For\nexample, we can train a classifier on image features in the common\nrepresentations and apply it to the testing of the text features in the\nrepresentations. Existing multi-modal representation learning methods mainly\naim to extract rich information from paired samples and train a classifier by\nthe corresponding labels; however, collecting paired samples and their labels\nsimultaneously involves high labor costs. Addressing paired modal samples\nwithout their labels and single modal data with their labels independently is\nmuch easier than addressing labeled multi-modal data. To obtain the common\nrepresentations under such a situation, we propose to make the distributions\nover different modalities similar in the learned representations, namely\nmodality-invariant representations. In particular, we propose a novel algorithm\nfor modality-invariant representation learning, named Deep Modality Invariant\nAdversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation\n(DA). Using the modality-invariant representations learned by DeMIAN, we\nachieved better classification accuracy than with the state-of-the-art methods,\nespecially for some benchmark datasets of zero-shot learning.\n","id":62}
{"Unnamed: 0.1":11063,"Unnamed: 0":11063.0,"anchor":"LSTM-Based System-Call Language Modeling and Robust Ensemble Method for\n  Designing Host-Based Intrusion Detection Systems","positive":"  In computer security, designing a robust intrusion detection system is one of\nthe most fundamental and important problems. In this paper, we propose a\nsystem-call language-modeling approach for designing anomaly-based host\nintrusion detection systems. To remedy the issue of high false-alarm rates\ncommonly arising in conventional methods, we employ a novel ensemble method\nthat blends multiple thresholding classifiers into a single one, making it\npossible to accumulate 'highly normal' sequences. The proposed system-call\nlanguage model has various advantages leveraged by the fact that it can learn\nthe semantic meaning and interactions of each system call that existing methods\ncannot effectively consider. Through diverse experiments on public benchmark\ndatasets, we demonstrate the validity and effectiveness of the proposed method.\nMoreover, we show that our model possesses high portability, which is one of\nthe key aspects of realizing successful intrusion detection systems.\n","negative":"  Combining abstract, symbolic reasoning with continuous neural reasoning is a\ngrand challenge of representation learning. As a step in this direction, we\npropose a new architecture, called neural equivalence networks, for the problem\nof learning continuous semantic representations of algebraic and logical\nexpressions. These networks are trained to represent semantic equivalence, even\nof expressions that are syntactically very different. The challenge is that\nsemantic representations must be computed in a syntax-directed manner, because\nsemantics is compositional, but at the same time, small changes in syntax can\nlead to very large changes in semantics, which can be difficult for continuous\nneural architectures. We perform an exhaustive evaluation on the task of\nchecking equivalence on a highly diverse class of symbolic algebraic and\nboolean expression types, showing that our model significantly outperforms\nexisting architectures.\n","id":63}
{"Unnamed: 0.1":11064,"Unnamed: 0":11064.0,"anchor":"Learning a Static Analyzer from Data","positive":"  To be practically useful, modern static analyzers must precisely model the\neffect of both, statements in the programming language as well as frameworks\nused by the program under analysis. While important, manually addressing these\nchallenges is difficult for at least two reasons: (i) the effects on the\noverall analysis can be non-trivial, and (ii) as the size and complexity of\nmodern libraries increase, so is the number of cases the analysis must handle.\n  In this paper we present a new, automated approach for creating static\nanalyzers: instead of manually providing the various inference rules of the\nanalyzer, the key idea is to learn these rules from a dataset of programs. Our\nmethod consists of two ingredients: (i) a synthesis algorithm capable of\nlearning a candidate analyzer from a given dataset, and (ii) a counter-example\nguided learning procedure which generates new programs beyond those in the\ninitial dataset, critical for discovering corner cases and ensuring the learned\nanalysis generalizes to unseen programs.\n  We implemented and instantiated our approach to the task of learning\nJavaScript static analysis rules for a subset of points-to analysis and for\nallocation sites analysis. These are challenging yet important problems that\nhave received significant research attention. We show that our approach is\neffective: our system automatically discovered practical and useful inference\nrules for many cases that are tricky to manually identify and are missed by\nstate-of-the-art, manually tuned analyzers.\n","negative":"  Information cascades, effectively facilitated by most social network\nplatforms, are recognized as a major factor in almost every social success and\ndisaster in these networks. Can cascades be predicted? While many believe that\nthey are inherently unpredictable, recent work has shown that some key\nproperties of information cascades, such as size, growth, and shape, can be\npredicted by a machine learning algorithm that combines many features. These\npredictors all depend on a bag of hand-crafting features to represent the\ncascade network and the global network structure. Such features, always\ncarefully and sometimes mysteriously designed, are not easy to extend or to\ngeneralize to a different platform or domain.\n  Inspired by the recent successes of deep learning in multiple data mining\ntasks, we investigate whether an end-to-end deep learning approach could\neffectively predict the future size of cascades. Such a method automatically\nlearns the representation of individual cascade graphs in the context of the\nglobal network structure, without hand-crafted features and heuristics. We find\nthat node embeddings fall short of predictive power, and it is critical to\nlearn the representation of a cascade graph as a whole. We present algorithms\nthat learn the representation of cascade graphs in an end-to-end manner, which\nsignificantly improve the performance of cascade prediction over strong\nbaselines that include feature based methods, node embedding methods, and graph\nkernel methods. Our results also provide interesting implications for cascade\nprediction in general.\n","id":64}
{"Unnamed: 0.1":11065,"Unnamed: 0":11065.0,"anchor":"Learning to Act by Predicting the Future","positive":"  We present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments.\n","negative":"  Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.\n","id":65}
{"Unnamed: 0.1":11066,"Unnamed: 0":11066.0,"anchor":"Learning to superoptimize programs","positive":"  Code super-optimization is the task of transforming any given program to a\nmore efficient version while preserving its input-output behaviour. In some\nsense, it is similar to the paraphrase problem from natural language processing\nwhere the intention is to change the syntax of an utterance without changing\nits semantics. Code-optimization has been the subject of years of research that\nhas resulted in the development of rule-based transformation strategies that\nare used by compilers. More recently, however, a class of stochastic search\nbased methods have been shown to outperform these strategies. This approach\ninvolves repeated sampling of modifications to the program from a proposal\ndistribution, which are accepted or rejected based on whether they preserve\ncorrectness, and the improvement they achieve. These methods, however, neither\nlearn from past behaviour nor do they try to leverage the semantics of the\nprogram under consideration. Motivated by this observation, we present a novel\nlearning based approach for code super-optimization. Intuitively, our method\nworks by learning the proposal distribution using unbiased estimators of the\ngradient of the expected improvement. Experiments on benchmarks comprising of\nautomatically generated as well as existing (\"Hacker's Delight\") programs show\nthat the proposed method is able to significantly outperform state of the art\napproaches for code super-optimization.\n","negative":"  Heart diseases constitute a global health burden, and the problem is\nexacerbated by the error-prone nature of listening to and interpreting heart\nsounds. This motivates the development of automated classification to screen\nfor abnormal heart sounds. Existing machine learning-based systems achieve\naccurate classification of heart sound recordings but rely on expert features\nthat have not been thoroughly evaluated on noisy recordings. Here we propose a\nsegmental convolutional neural network architecture that achieves automatic\nfeature learning from noisy heart sound recordings. Our experiments show that\nour best model, trained on noisy recording segments acquired with an existing\nhidden semi-markov model-based approach, attains a classification accuracy of\n87.5% on the 2016 PhysioNet\/CinC Challenge dataset, compared to the 84.6%\naccuracy of the state-of-the-art statistical classifier trained and evaluated\non the same dataset. Our results indicate the potential of using neural\nnetwork-based methods to increase the accuracy of automated classification of\nheart sound recordings for improved screening of heart diseases.\n","id":66}
{"Unnamed: 0.1":11067,"Unnamed: 0":11067.0,"anchor":"Modular Multitask Reinforcement Learning with Policy Sketches","positive":"  We describe a framework for multitask deep reinforcement learning guided by\npolicy sketches. Sketches annotate tasks with sequences of named subtasks,\nproviding information about high-level structural relationships among tasks but\nnot how to implement them---specifically not providing the detailed guidance\nused by much previous work on learning policy abstractions for RL (e.g.\nintermediate rewards, subtask completion signals, or intrinsic motivations). To\nlearn from sketches, we present a model that associates every subtask with a\nmodular subpolicy, and jointly maximizes reward over full task-specific\npolicies by tying parameters across shared subpolicies. Optimization is\naccomplished via a decoupled actor--critic training objective that facilitates\nlearning common behaviors from multiple dissimilar reward functions. We\nevaluate the effectiveness of our approach in three environments featuring both\ndiscrete and continuous control, and with sparse rewards that can be obtained\nonly after completing a number of high-level subgoals. Experiments show that\nusing our approach to learn policies guided by sketches gives better\nperformance than existing techniques for learning task-specific or shared\npolicies, while naturally inducing a library of interpretable primitive\nbehaviors that can be recombined to rapidly adapt to new tasks.\n","negative":"  Deep reinforcement learning has emerged as a promising and powerful technique\nfor automatically acquiring control policies that can process raw sensory\ninputs, such as images, and perform complex behaviors. However, extending deep\nRL to real-world robotic tasks has proven challenging, particularly in\nsafety-critical domains such as autonomous flight, where a trial-and-error\nlearning process is often impractical. In this paper, we explore the following\nquestion: can we train vision-based navigation policies entirely in simulation,\nand then transfer them into the real world to achieve real-world flight without\na single real training image? We propose a learning method that we call\nCAD$^2$RL, which can be used to perform collision-free indoor flight in the\nreal world while being trained entirely on 3D CAD models. Our method uses\nsingle RGB images from a monocular camera, without needing to explicitly\nreconstruct the 3D geometry of the environment or perform explicit motion\nplanning. Our learned collision avoidance policy is represented by a deep\nconvolutional neural network that directly processes raw monocular images and\noutputs velocity commands. This policy is trained entirely on simulated images,\nwith a Monte Carlo policy evaluation algorithm that directly optimizes the\nnetwork's ability to produce collision-free flight. By highly randomizing the\nrendering settings for our simulated training set, we show that we can train a\npolicy that generalizes to the real world, without requiring the simulator to\nbe particularly realistic or high-fidelity. We evaluate our method by flying a\nreal quadrotor through indoor environments, and further evaluate the design\nchoices in our simulator through a series of ablation studies on depth\nprediction. For supplementary video see: https:\/\/youtu.be\/nXBWmzFrj5s\n","id":67}
{"Unnamed: 0.1":11068,"Unnamed: 0":11068.0,"anchor":"Generative Adversarial Networks as Variational Training of Energy Based\n  Models","positive":"  In this paper, we study deep generative models for effective unsupervised\nlearning. We propose VGAN, which works by minimizing a variational lower bound\nof the negative log likelihood (NLL) of an energy based model (EBM), where the\nmodel density $p(\\mathbf{x})$ is approximated by a variational distribution\n$q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two\nstep procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize\nthe lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn\nfrom $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the\ngenerative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to\nthe discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with\nseveral notable differences. We hence name our model variational GANs (VGANs).\nVGAN provides a practical solution to training deep EBMs in high dimensional\nspace, by eliminating the need of MCMC sampling. From this view, we are also\nable to identify causes to the difficulty of training GANs and propose viable\nsolutions. \\footnote{Experimental code is available at\nhttps:\/\/github.com\/Shuangfei\/vgan}\n","negative":"  As high-throughput biological sequencing becomes faster and cheaper, the need\nto extract useful information from sequencing becomes ever more paramount,\noften limited by low-throughput experimental characterizations. For proteins,\naccurate prediction of their functions directly from their primary amino-acid\nsequences has been a long standing challenge. Here, machine learning using\nartificial recurrent neural networks (RNN) was applied towards classification\nof protein function directly from primary sequence without sequence alignment,\nheuristic scoring or feature engineering. The RNN models containing\nlong-short-term-memory (LSTM) units trained on public, annotated datasets from\nUniProt achieved high performance for in-class prediction of four important\nprotein functions tested, particularly compared to other machine learning\nalgorithms using sequence-derived protein features. RNN models were used also\nfor out-of-class predictions of phylogenetically distinct protein families with\nsimilar functions, including proteins of the CRISPR-associated nuclease,\nferritin-like iron storage and cytochrome P450 families. Applying the trained\nRNN models on the partially unannotated UniRef100 database predicted not only\ncandidates validated by existing annotations but also currently unannotated\nsequences. Some RNN predictions for the ferritin-like iron sequestering\nfunction were experimentally validated, even though their sequences differ\nsignificantly from known, characterized proteins and from each other and cannot\nbe easily predicted using popular bioinformatics methods. As sequencing and\nexperimental characterization data increases rapidly, the machine-learning\napproach based on RNN could be useful for discovery and prediction of\nhomologues for a wide range of protein functions.\n","id":68}
{"Unnamed: 0.1":11069,"Unnamed: 0":11069.0,"anchor":"Entropy-SGD: Biasing Gradient Descent Into Wide Valleys","positive":"  This paper proposes a new optimization algorithm called Entropy-SGD for\ntraining deep neural networks that is motivated by the local geometry of the\nenergy landscape. Local extrema with low generalization error have a large\nproportion of almost-zero eigenvalues in the Hessian with very few positive or\nnegative eigenvalues. We leverage upon this observation to construct a\nlocal-entropy-based objective function that favors well-generalizable solutions\nlying in large flat regions of the energy landscape, while avoiding\npoorly-generalizable solutions located in the sharp valleys. Conceptually, our\nalgorithm resembles two nested loops of SGD where we use Langevin dynamics in\nthe inner loop to compute the gradient of the local entropy before each update\nof the weights. We show that the new objective has a smoother energy landscape\nand show improved generalization over SGD using uniform stability, under\ncertain assumptions. Our experiments on convolutional and recurrent networks\ndemonstrate that Entropy-SGD compares favorably to state-of-the-art techniques\nin terms of generalization error and training time.\n","negative":"  An associative memory is a framework of content-addressable memory that\nstores a collection of message vectors (or a dataset) over a neural network\nwhile enabling a neurally feasible mechanism to recover any message in the\ndataset from its noisy version. Designing an associative memory requires\naddressing two main tasks: 1) learning phase: given a dataset, learn a concise\nrepresentation of the dataset in the form of a graphical model (or a neural\nnetwork), 2) recall phase: given a noisy version of a message vector from the\ndataset, output the correct message vector via a neurally feasible algorithm\nover the network learnt during the learning phase. This paper studies the\nproblem of designing a class of neural associative memories which learns a\nnetwork representation for a large dataset that ensures correction against a\nlarge number of adversarial errors during the recall phase. Specifically, the\nassociative memories designed in this paper can store dataset containing\n$\\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can\ntolerate $\\Omega(\\frac{n}{{\\rm polylog} n})$ adversarial errors. This paper\ncarries out this memory design by mapping the learning phase and recall phase\nto the tasks of dictionary learning with a square dictionary and iterative\nerror correction in an expander code, respectively.\n","id":69}
{"Unnamed: 0.1":11070,"Unnamed: 0":11070.0,"anchor":"Learning to Perform Physics Experiments via Deep Reinforcement Learning","positive":"  When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.\n","negative":"  Characterizing patient somatic mutations through next-generation sequencing\ntechnologies opens up possibilities for refining cancer subtypes. However,\ncatalogues of mutations reveal that only a small fraction of the genes are\naltered frequently in patients. On the other hand different genomic alterations\nmay perturb the same pathways. We propose a novel clustering procedure that\nquantifies the similarities of patients from their mutational profile on\npathways via a novel graph kernel. We represent each KEGG pathway as an\nundirected graph. For each patient the vertex labels are assigned based on her\naltered genes. Smoothed shortest path graph kernel (smSPK) evaluates each pair\nof patients by comparing their vertex labeled pathway graphs. Our clustering\nprocedure involves two steps: the smSPK kernel matrix derived for each pathway\nare input to kernel k-means algorithm and each pathway is evaluated\nindividually. In the next step, only those pathways that are successful are\ncombined in to a single kernel input to kernel k-means to stratify patients.\nEvaluating the procedure on simulated data showed that smSPK clusters patients\nup to 88\\% accuracy. Finally to identify ovarian cancer patient subgroups, we\napply our methodology to the cancer genome atlas ovarian data that involves 481\npatients. The identified subgroups are evaluated through survival analysis.\nGrouping patients into four clusters results with patients groups that are\nsignificantly different in their survival times ($p$-value $\\le 0.005$).\n","id":70}
{"Unnamed: 0.1":11071,"Unnamed: 0":11071.0,"anchor":"Challenges of Feature Selection for Big Data Analytics","positive":"  We are surrounded by huge amounts of large-scale high dimensional data. It is\ndesirable to reduce the dimensionality of data for many learning tasks due to\nthe curse of dimensionality. Feature selection has shown its effectiveness in\nmany applications by building simpler and more comprehensive model, improving\nlearning performance, and preparing clean, understandable data. Recently, some\nunique characteristics of big data such as data velocity and data variety\npresent challenges to the feature selection problem. In this paper, we envision\nthese challenges of feature selection for big data analytics. In particular, we\nfirst give a brief introduction about feature selection and then detail the\nchallenges of feature selection for structured, heterogeneous and streaming\ndata as well as its scalability and stability issues. At last, to facilitate\nand promote the feature selection research, we present an open-source feature\nselection repository (scikit-feature), which consists of most of current\npopular feature selection algorithms.\n","negative":"  The cross-entropy loss commonly used in deep learning is closely related to\nthe defining properties of optimal representations, but does not enforce some\nof the key properties. We show that this can be solved by adding a\nregularization term, which is in turn related to injecting multiplicative noise\nin the activations of a Deep Neural Network, a special case of which is the\ncommon practice of dropout. We show that our regularized loss function can be\nefficiently minimized using Information Dropout, a generalization of dropout\nrooted in information theoretic principles that automatically adapts to the\ndata and can better exploit architectures of limited capacity. When the task is\nthe reconstruction of the input, we show that our loss function yields a\nVariational Autoencoder as a special case, thus providing a link between\nrepresentation learning, information theory and variational inference. Finally,\nwe prove that we can promote the creation of disentangled representations\nsimply by enforcing a factorized prior, a fact that has been observed\nempirically in recent work. Our experiments validate the theoretical intuitions\nbehind our method, and we find that information dropout achieves a comparable\nor better generalization performance than binary dropout, especially on smaller\nmodels, since it can automatically adapt the noise to the structure of the\nnetwork, as well as to the test sample.\n","id":71}
{"Unnamed: 0.1":11072,"Unnamed: 0":11072.0,"anchor":"An Information-Theoretic Framework for Fast and Robust Unsupervised\n  Learning via Neural Population Infomax","positive":"  A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.\n","negative":"  The success of deep convolutional neural networks on image classification and\nrecognition tasks has led to new applications in very diversified contexts,\nincluding the field of medical imaging. In this paper we investigate and\npropose neural network architectures for automated multi-class segmentation of\nanatomical organs in chest radiographs, namely for lungs, clavicles and heart.\nWe address several open challenges including model overfitting, reducing number\nof parameters and handling of severely imbalanced data in CXR by fusing recent\nconcepts in convolutional networks and adapting them to the segmentation\nproblem task in CXR. We demonstrate that our architecture combining delayed\nsubsampling, exponential linear units, highly restrictive regularization and a\nlarge number of high resolution low level abstract features outperforms\nstate-of-the-art methods on all considered organs, as well as the human\nobserver on lungs and heart. The models use a multi-class configuration with\nthree target classes and are trained and tested on the publicly available JSRT\ndatabase, consisting of 247 X-ray images the ground-truth masks for which are\navailable in the SCR database. Our best performing model, trained with the loss\nfunction based on the Dice coefficient, reached mean Jaccard overlap scores of\n95.0\\% for lungs, 86.8\\% for clavicles and 88.2\\% for heart. This architecture\noutperformed the human observer results for lungs and heart.\n","id":72}
{"Unnamed: 0.1":11073,"Unnamed: 0":11073.0,"anchor":"Joint Multimodal Learning with Deep Generative Models","positive":"  We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. Recently, some studies handle multiple modalities on deep generative\nmodels, such as variational autoencoders (VAEs). However, these models\ntypically assume that modalities are forced to have a conditioned relation,\ni.e., we can only generate modalities in one direction. To achieve our\nobjective, we should extract a joint representation that captures high-level\nconcepts among all modalities and through which we can exchange them\nbi-directionally. As described herein, we propose a joint multimodal\nvariational autoencoder (JMVAE), in which all modalities are independently\nconditioned on joint representation. In other words, it models a joint\ndistribution of modalities. Furthermore, to be able to generate missing\nmodalities from the remaining modalities properly, we develop an additional\nmethod, JMVAE-kl, that is trained by reducing the divergence between JMVAE's\nencoder and prepared networks of respective modalities. Our experiments show\nthat our proposed method can obtain appropriate joint representation from\nmultiple modalities and that it can generate and reconstruct them more properly\nthan conventional VAEs. We further demonstrate that JMVAE can generate multiple\nmodalities bi-directionally.\n","negative":"  A few notes on the use of machine learning in medicine and the related\nunintended consequences.\n","id":73}
{"Unnamed: 0.1":11074,"Unnamed: 0":11074.0,"anchor":"Decision Tree Classification with Differential Privacy: A Survey","positive":"  Data mining information about people is becoming increasingly important in\nthe data-driven society of the 21st century. Unfortunately, sometimes there are\nreal-world considerations that conflict with the goals of data mining;\nsometimes the privacy of the people being data mined needs to be considered.\nThis necessitates that the output of data mining algorithms be modified to\npreserve privacy while simultaneously not ruining the predictive power of the\noutputted model. Differential privacy is a strong, enforceable definition of\nprivacy that can be used in data mining algorithms, guaranteeing that nothing\nwill be learned about the people in the data that could not already be\ndiscovered without their participation. In this survey, we focus on one\nparticular data mining algorithm -- decision trees -- and how differential\nprivacy interacts with each of the components that constitute decision tree\nalgorithms. We analyze both greedy and random decision trees, and the conflicts\nthat arise when trying to balance privacy requirements with the accuracy of the\nmodel.\n","negative":"  Kernel fusion is a popular and effective approach for combining multiple\nfeatures that characterize different aspects of data. Traditional approaches\nfor Multiple Kernel Learning (MKL) attempt to learn the parameters for\ncombining the kernels through sophisticated optimization procedures. In this\npaper, we propose an alternative approach that creates dense embeddings for\ndata using the kernel similarities and adopts a deep neural network\narchitecture for fusing the embeddings. In order to improve the effectiveness\nof this network, we introduce the kernel dropout regularization strategy\ncoupled with the use of an expanded set of composition kernels. Experiment\nresults on a real-world activity recognition dataset show that the proposed\narchitecture is effective in fusing kernels and achieves state-of-the-art\nperformance.\n","id":74}
{"Unnamed: 0.1":11075,"Unnamed: 0":11075.0,"anchor":"Averaged-DQN: Variance Reduction and Stabilization for Deep\n  Reinforcement Learning","positive":"  Instability and variability of Deep Reinforcement Learning (DRL) algorithms\ntend to adversely affect their performance. Averaged-DQN is a simple extension\nto the DQN algorithm, based on averaging previously learned Q-values estimates,\nwhich leads to a more stable training procedure and improved performance by\nreducing approximation error variance in the target values. To understand the\neffect of the algorithm, we examine the source of value function estimation\nerrors and provide an analytical comparison within a simplified model. We\nfurther present experiments on the Arcade Learning Environment benchmark that\ndemonstrate significantly improved stability and performance due to the\nproposed extension.\n","negative":"  We connect high-dimensional subset selection and submodular maximization. Our\nresults extend the work of Das and Kempe (2011) from the setting of linear\nregression to arbitrary objective functions. For greedy feature selection, this\nconnection allows us to obtain strong multiplicative performance bounds on\nseveral methods without statistical modeling assumptions. We also derive\nrecovery guarantees of this form under standard assumptions. Our work shows\nthat greedy algorithms perform within a constant factor from the best possible\nsubset-selection solution for a broad class of general objective functions. Our\nmethods allow a direct control over the number of obtained features as opposed\nto regularization parameters that only implicitly control sparsity. Our proof\ntechnique uses the concept of weak submodularity initially defined by Das and\nKempe. We draw a connection between convex analysis and submodular set function\ntheory which may be of independent interest for other statistical learning\napplications that have combinatorial structure.\n","id":75}
{"Unnamed: 0.1":11076,"Unnamed: 0":11076.0,"anchor":"DeepSense: A Unified Deep Learning Framework for Time-Series Mobile\n  Sensing Data Processing","positive":"  Mobile sensing applications usually require time-series inputs from sensors.\nSome applications, such as tracking, can use sensed acceleration and rate of\nrotation to calculate displacement based on physical system models. Other\napplications, such as activity recognition, extract manually designed features\nfrom sensor inputs for classification. Such applications face two challenges.\nOn one hand, on-device sensor measurements are noisy. For many mobile\napplications, it is hard to find a distribution that exactly describes the\nnoise in practice. Unfortunately, calculating target quantities based on\nphysical system and noise models is only as accurate as the noise assumptions.\nSimilarly, in classification applications, although manually designed features\nhave proven to be effective, it is not always straightforward to find the most\nrobust features to accommodate diverse sensor noise patterns and user\nbehaviors. To this end, we propose DeepSense, a deep learning framework that\ndirectly addresses the aforementioned noise and feature customization\nchallenges in a unified manner. DeepSense integrates convolutional and\nrecurrent neural networks to exploit local interactions among similar mobile\nsensors, merge local interactions of different sensory modalities into global\ninteractions, and extract temporal relationships to model signal dynamics.\nDeepSense thus provides a general signal estimation and classification\nframework that accommodates a wide range of applications. We demonstrate the\neffectiveness of DeepSense using three representative and challenging tasks:\ncar tracking with motion sensors, heterogeneous human activity recognition, and\nuser identification with biometric motion analysis. DeepSense significantly\noutperforms the state-of-the-art methods for all three tasks. In addition,\nDeepSense is feasible to implement on smartphones due to its moderate energy\nconsumption and low latency\n","negative":"  Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.\n","id":76}
{"Unnamed: 0.1":11077,"Unnamed: 0":11077.0,"anchor":"Linear Convergence of SVRG in Statistical Estimation","positive":"  SVRG and its variants are among the state of art optimization algorithms for\nlarge scale machine learning problems. It is well known that SVRG converges\nlinearly when the objective function is strongly convex. However this setup can\nbe restrictive, and does not include several important formulations such as\nLasso, group Lasso, logistic regression, and some non-convex models including\ncorrected Lasso and SCAD. In this paper, we prove that, for a class of\nstatistical M-estimators covering examples mentioned above, SVRG solves the\nformulation with {\\em a linear convergence rate} without strong convexity or\neven convexity. Our analysis makes use of {\\em restricted strong convexity},\nunder which we show that SVRG converges linearly to the fundamental statistical\nprecision of the model, i.e., the difference between true unknown parameter\n$\\theta^*$ and the optimal solution $\\hat{\\theta}$ of the model.\n","negative":"  The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.\n","id":77}
{"Unnamed: 0.1":11078,"Unnamed: 0":11078.0,"anchor":"Log-time and Log-space Extreme Classification","positive":"  We present LTLS, a technique for multiclass and multilabel prediction that\ncan perform training and inference in logarithmic time and space. LTLS embeds\nlarge classification problems into simple structured prediction problems and\nrelies on efficient dynamic programming algorithms for inference. We train LTLS\nwith stochastic gradient descent on a number of multiclass and multilabel\ndatasets and show that despite its small memory footprint it is often\ncompetitive with existing approaches.\n","negative":"  We introduce a library of geometric voxel features for CAD surface\nrecognition\/retrieval tasks. Our features include local versions of the\nintrinsic volumes (the usual 3D volume, surface area, integrated mean and\nGaussian curvature) and a few closely related quantities. We also compute Haar\nwavelet and statistical distribution features by aggregating raw voxel\nfeatures. We apply our features to object classification on the ESB data set\nand demonstrate accurate results with a small number of shallow decision trees.\n","id":78}
{"Unnamed: 0.1":11079,"Unnamed: 0":11079.0,"anchor":"Regularizing CNNs with Locally Constrained Decorrelations","positive":"  Regularization is key for deep learning since it allows training more complex\nmodels while keeping lower levels of overfitting. However, the most prevalent\nregularizations do not leverage all the capacity of the models since they rely\non reducing the effective number of parameters. Feature decorrelation is an\nalternative for using the full capacity of the models but the overfitting\nreduction margins are too narrow given the overhead it introduces. In this\npaper, we show that regularizing negatively correlated features is an obstacle\nfor effective decorrelation and present OrthoReg, a novel regularization\ntechnique that locally enforces feature orthogonality. As a result, imposing\nlocality constraints in feature decorrelation removes interferences between\nnegatively correlated feature weights, allowing the regularizer to reach higher\ndecorrelation bounds, and reducing the overfitting more effectively. In\nparticular, we show that the models regularized with OrthoReg have higher\naccuracy bounds even when batch normalization and dropout are present.\nMoreover, since our regularization is directly performed on the weights, it is\nespecially suitable for fully convolutional neural networks, where the weight\nspace is constant compared to the feature map space. As a result, we are able\nto reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and\nSVHN.\n","negative":"  The support vector machine is a flexible optimization-based technique widely\nused for classification problems. In practice, its training part becomes\ncomputationally expensive on large-scale data sets because of such reasons as\nthe complexity and number of iterations in parameter fitting methods,\nunderlying optimization solvers, and nonlinearity of kernels. We introduce a\nfast multilevel framework for solving support vector machine models that is\ninspired by the algebraic multigrid. Significant improvement in the running has\nbeen achieved without any loss in the quality. The proposed technique is highly\nbeneficial on imbalanced sets. We demonstrate computational results on publicly\navailable and industrial data sets.\n","id":79}
{"Unnamed: 0.1":11080,"Unnamed: 0":11080.0,"anchor":"One Class Splitting Criteria for Random Forests","positive":"  Random Forests (RFs) are strong machine learning tools for classification and\nregression. However, they remain supervised algorithms, and no extension of RFs\nto the one-class setting has been proposed, except for techniques based on\nsecond-class sampling. This work fills this gap by proposing a natural\nmethodology to extend standard splitting criteria to the one-class setting,\nstructurally generalizing RFs to one-class classification. An extensive\nbenchmark of seven state-of-the-art anomaly detection algorithms is also\npresented. This empirically demonstrates the relevance of our approach.\n","negative":"  Prediction in a small-sized sample with a large number of covariates, the\n\"small n, large p\" problem, is challenging. This setting is encountered in\nmultiple applications, such as precision medicine, where obtaining additional\nsamples can be extremely costly or even impossible, and extensive research\neffort has recently been dedicated to finding principled solutions for accurate\nprediction. However, a valuable source of additional information, domain\nexperts, has not yet been efficiently exploited. We formulate knowledge\nelicitation generally as a probabilistic inference process, where expert\nknowledge is sequentially queried to improve predictions. In the specific case\nof sparse linear regression, where we assume the expert has knowledge about the\nvalues of the regression coefficients or about the relevance of the features,\nwe propose an algorithm and computational approximation for fast and efficient\ninteraction, which sequentially identifies the most informative features on\nwhich to query expert knowledge. Evaluations of our method in experiments with\nsimulated and real users show improved prediction accuracy already with a small\neffort from the expert.\n","id":80}
{"Unnamed: 0.1":11081,"Unnamed: 0":11081.0,"anchor":"Fixed-point Factorized Networks","positive":"  In recent years, Deep Neural Networks (DNN) based methods have achieved\nremarkable performance in a wide range of tasks and have been among the most\npowerful and widely used techniques in computer vision. However, DNN-based\nmethods are both computational-intensive and resource-consuming, which hinders\nthe application of these methods on embedded systems like smart phones. To\nalleviate this problem, we introduce a novel Fixed-point Factorized Networks\n(FFN) for pretrained models to reduce the computational complexity as well as\nthe storage requirement of networks. The resulting networks have only weights\nof -1, 0 and 1, which significantly eliminates the most resource-consuming\nmultiply-accumulate operations (MACs). Extensive experiments on large-scale\nImageNet classification task show the proposed FFN only requires one-thousandth\nof multiply operations with comparable accuracy.\n","negative":"  Policy gradient is an efficient technique for improving a policy in a\nreinforcement learning setting. However, vanilla online variants are on-policy\nonly and not able to take advantage of off-policy data. In this paper we\ndescribe a new technique that combines policy gradient with off-policy\nQ-learning, drawing experience from a replay buffer. This is motivated by\nmaking a connection between the fixed points of the regularized policy gradient\nalgorithm and the Q-values. This connection allows us to estimate the Q-values\nfrom the action preferences of the policy, to which we apply Q-learning\nupdates. We refer to the new technique as 'PGQL', for policy gradient and\nQ-learning. We also establish an equivalency between action-value fitting\ntechniques and actor-critic algorithms, showing that regularized policy\ngradient techniques can be interpreted as advantage function learning\nalgorithms. We conclude with some numerical examples that demonstrate improved\ndata efficiency and stability of PGQL. In particular, we tested PGQL on the\nfull suite of Atari games and achieved performance exceeding that of both\nasynchronous advantage actor-critic (A3C) and Q-learning.\n","id":81}
{"Unnamed: 0.1":11082,"Unnamed: 0":11082.0,"anchor":"Differentiable Functional Program Interpreters","positive":"  Programming by Example (PBE) is the task of inducing computer programs from\ninput-output examples. It can be seen as a type of machine learning where the\nhypothesis space is the set of legal programs in some programming language.\nRecent work on differentiable interpreters relaxes the discrete space of\nprograms into a continuous space so that search over programs can be performed\nusing gradient-based optimization. While conceptually powerful, so far\ndifferentiable interpreter-based program synthesis has only been capable of\nsolving very simple problems. In this work, we study modeling choices that\narise when constructing a differentiable programming language and their impact\non the success of synthesis. The main motivation for the modeling choices comes\nfrom functional programming: we study the effect of memory allocation schemes,\nimmutable data, type systems, and built-in control-flow structures. Empirically\nwe show that incorporating functional programming ideas into differentiable\nprogramming languages allows us to learn much more complex programs than is\npossible with existing differentiable languages.\n","negative":"  Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess.\n","id":82}
{"Unnamed: 0.1":11083,"Unnamed: 0":11083.0,"anchor":"DeepCoder: Learning to Write Programs","positive":"  We develop a first line of attack for solving programming competition-style\nproblems from input-output examples using deep learning. The approach is to\ntrain a neural network to predict properties of the program that generated the\noutputs from the inputs. We use the neural network's predictions to augment\nsearch techniques from the programming languages community, including\nenumerative search and an SMT-based solver. Empirically, we show that our\napproach leads to an order of magnitude speedup over the strong non-augmented\nbaselines and a Recurrent Neural Network approach, and that we are able to\nsolve problems of difficulty comparable to the simplest problems on programming\ncompetition websites.\n","negative":"  We address the problem of semi-supervised learning in relational networks,\nnetworks in which nodes are entities and links are the relationships or\ninteractions between them. Typically this problem is confounded with the\nproblem of graph-based semi-supervised learning (GSSL), because both problems\nrepresent the data as a graph and predict the missing class labels of nodes.\nHowever, not all graphs are created equally. In GSSL a graph is constructed,\noften from independent data, based on similarity. As such, edges tend to\nconnect instances with the same class label. Relational networks, however, can\nbe more heterogeneous and edges do not always indicate similarity. For\ninstance, instead of links being more likely to connect nodes with the same\nclass label, they may occur more frequently between nodes with different class\nlabels (link-heterogeneity). Or nodes with the same class label do not\nnecessarily have the same type of connectivity across the whole network\n(class-heterogeneity), e.g. in a network of sexual interactions we may observe\nlinks between opposite genders in some parts of the graph and links between the\nsame genders in others. Performing classification in networks with different\ntypes of heterogeneity is a hard problem that is made harder still when we do\nnot know a-priori the type or level of heterogeneity. Here we present two\nscalable approaches for graph-based semi-supervised learning for the more\ngeneral case of relational networks. We demonstrate these approaches on\nsynthetic and real-world networks that display different link patterns within\nand between classes. Compared to state-of-the-art approaches, ours give better\nclassification performance without prior knowledge of how classes interact. In\nparticular, our two-step label propagation algorithm gives consistently good\naccuracy and runs on networks of over 1.6 million nodes and 30 million edges in\naround 12 seconds.\n","id":83}
{"Unnamed: 0.1":11084,"Unnamed: 0":11084.0,"anchor":"Multi-view Generative Adversarial Networks","positive":"  Learning over multi-view data is a challenging problem with strong practical\napplications. Most related studies focus on the classification point of view\nand assume that all the views are available at any time. We consider an\nextension of this framework in two directions. First, based on the BiGAN model,\nthe Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from\nmulti-view inputs. Second, it can deal with missing views and is able to update\nits prediction when additional views are provided. We illustrate these\nproperties on a set of experiments over different datasets.\n","negative":"  A graph-based classification method is proposed for semi-supervised learning\nin the case of Euclidean data and for classification in the case of graph data.\nOur manifold learning technique is based on a convex optimization problem\ninvolving a convex quadratic regularization term and a concave quadratic loss\nfunction with a trade-off parameter carefully chosen so that the objective\nfunction remains convex. As shown empirically, the advantage of considering a\nconcave loss function is that the learning problem becomes more robust in the\npresence of noisy labels. Furthermore, the loss function considered here is\nthen more similar to a classification loss while several other methods treat\ngraph-based classification problems as regression problems.\n","id":84}
{"Unnamed: 0.1":11085,"Unnamed: 0":11085.0,"anchor":"Does Distributionally Robust Supervised Learning Give Robust\n  Classifiers?","positive":"  Distributionally Robust Supervised Learning (DRSL) is necessary for building\nreliable machine learning systems. When machine learning is deployed in the\nreal world, its performance can be significantly degraded because test data may\nfollow a different distribution from training data. DRSL with f-divergences\nexplicitly considers the worst-case distribution shift by minimizing the\nadversarially reweighted training loss. In this paper, we analyze this DRSL,\nfocusing on the classification scenario. Since the DRSL is explicitly\nformulated for a distribution shift scenario, we naturally expect it to give a\nrobust classifier that can aggressively handle shifted distributions. However,\nsurprisingly, we prove that the DRSL just ends up giving a classifier that\nexactly fits the given training distribution, which is too pessimistic. This\npessimism comes from two sources: the particular losses used in classification\nand the fact that the variety of distributions to which the DRSL tries to be\nrobust is too wide. Motivated by our analysis, we propose simple DRSL that\novercomes this pessimism and empirically demonstrate its effectiveness.\n","negative":"  Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.\n","id":85}
{"Unnamed: 0.1":11086,"Unnamed: 0":11086.0,"anchor":"Reinforcement Learning Approach for Parallelization in Filters\n  Aggregation Based Feature Selection Algorithms","positive":"  One of the classical problems in machine learning and data mining is feature\nselection. A feature selection algorithm is expected to be quick, and at the\nsame time it should show high performance. MeLiF algorithm effectively solves\nthis problem using ensembles of ranking filters. This article describes two\ndifferent ways to improve MeLiF algorithm performance with parallelization.\nExperiments show that proposed schemes significantly improves algorithm\nperformance and increase feature selection quality.\n","negative":"  We give the first dimension-efficient algorithms for learning Rectified\nLinear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto\n\\max(0, \\mathbf{w} \\cdot \\mathbf{x})$ with $\\mathbf{w} \\in \\mathbb{S}^{n-1}$.\nOur algorithm works in the challenging Reliable Agnostic learning model of\nKalai, Kanade, and Mansour (2009) where the learner is given access to a\ndistribution $\\cal{D}$ on labeled examples but the labeling may be arbitrary.\nWe construct a hypothesis that simultaneously minimizes the false-positive rate\nand the loss on inputs given positive labels by $\\cal{D}$, for any convex,\nbounded, and Lipschitz loss function.\n  The algorithm runs in polynomial-time (in $n$) with respect to any\ndistribution on $\\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for\nany error parameter $\\epsilon = \\Omega(1\/\\log n)$ (this yields a PTAS for a\nquestion raised by F. Bach on the complexity of maximizing ReLUs). These\nresults are in contrast to known efficient algorithms for reliably learning\nlinear threshold functions, where $\\epsilon$ must be $\\Omega(1)$ and strong\nassumptions are required on the marginal distribution. We can compose our\nresults to obtain the first set of efficient algorithms for learning\nconstant-depth networks of ReLUs.\n  Our techniques combine kernel methods and polynomial approximations with a\n\"dual-loss\" approach to convex programming. As a byproduct we obtain a number\nof applications including the first set of efficient algorithms for \"convex\npiecewise-linear fitting\" and the first efficient algorithms for noisy\npolynomial reconstruction of low-weight polynomials on the unit sphere.\n","id":86}
{"Unnamed: 0.1":11087,"Unnamed: 0":11087.0,"anchor":"Reinforcement-based Simultaneous Algorithm and its Hyperparameters\n  Selection","positive":"  Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA.\n","negative":"  Natural language understanding and dialogue policy learning are both\nessential in conversational systems that predict the next system actions in\nresponse to a current user utterance. Conventional approaches aggregate\nseparate models of natural language understanding (NLU) and system action\nprediction (SAP) as a pipeline that is sensitive to noisy outputs of\nerror-prone NLU. To address the issues, we propose an end-to-end deep recurrent\nneural network with limited contextual dialogue memory by jointly training NLU\nand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our\nproposed model significantly outperforms the state-of-the-art pipeline models\nfor both NLU and SAP, which indicates that our joint model is capable of\nmitigating the affects of noisy NLU outputs, and NLU model can be refined by\nerror flows backpropagating from the extra supervised signals of system\nactions.\n","id":87}
{"Unnamed: 0.1":11088,"Unnamed: 0":11088.0,"anchor":"Distributed Coordinate Descent for Generalized Linear Models with\n  Regularization","positive":"  Generalized linear model with $L_1$ and $L_2$ regularization is a widely used\ntechnique for solving classification, class probability estimation and\nregression problems. With the numbers of both features and examples growing\nrapidly in the fields like text mining and clickstream data analysis\nparallelization and the use of cluster architectures becomes important. We\npresent a novel algorithm for fitting regularized generalized linear models in\nthe distributed environment. The algorithm splits data between nodes by\nfeatures, uses coordinate descent on each node and line search to merge results\nglobally. Convergence proof is provided. A modifications of the algorithm\naddresses slow node problem. For an important particular case of logistic\nregression we empirically compare our program with several state-of-the art\napproaches that rely on different algorithmic and data spitting methods.\nExperiments demonstrate that our approach is scalable and superior when\ntraining on large and sparse datasets.\n","negative":"  We consider the problem of learning from noisy data in practical settings\nwhere the size of data is too large to store on a single machine. More\nchallenging, the data coming from the wild may contain malicious outliers. To\naddress the scalability and robustness issues, we present an online robust\nlearning (ORL) approach. ORL is simple to implement and has provable robustness\nguarantee -- in stark contrast to existing online learning approaches that are\ngenerally fragile to outliers. We specialize the ORL approach for two concrete\ncases: online robust principal component analysis and online linear regression.\nWe demonstrate the efficiency and robustness advantages of ORL through\ncomprehensive simulations and predicting image tags on a large-scale data set.\nWe also discuss extension of the ORL to distributed learning and provide\nexperimental evaluations.\n","id":88}
{"Unnamed: 0.1":11089,"Unnamed: 0":11089.0,"anchor":"Differentiable Programs with Neural Libraries","positive":"  We develop a framework for combining differentiable programming languages\nwith neural networks. Using this framework we create end-to-end trainable\nsystems that learn to write interpretable algorithms with perceptual\ncomponents. We explore the benefits of inductive biases for strong\ngeneralization and modularity that come from the program-like structure of our\nmodels. In particular, modularity allows us to learn a library of (neural)\nfunctions which grows and improves as more tasks are solved. Empirically, we\nshow that this leads to lifelong learning systems that transfer knowledge to\nnew tasks more effectively than baselines.\n","negative":"  A basic combinatorial interpretation of Shannon's entropy function is via the\n\"20 questions\" game. This cooperative game is played by two players, Alice and\nBob: Alice picks a distribution $\\pi$ over the numbers $\\{1,\\ldots,n\\}$, and\nannounces it to Bob. She then chooses a number $x$ according to $\\pi$, and Bob\nattempts to identify $x$ using as few Yes\/No queries as possible, on average.\n  An optimal strategy for the \"20 questions\" game is given by a Huffman code\nfor $\\pi$: Bob's questions reveal the codeword for $x$ bit by bit. This\nstrategy finds $x$ using fewer than $H(\\pi)+1$ questions on average. However,\nthe questions asked by Bob could be arbitrary. In this paper, we investigate\nthe following question: Are there restricted sets of questions that match the\nperformance of Huffman codes, either exactly or approximately?\n  Our first main result shows that for every distribution $\\pi$, Bob has a\nstrategy that uses only questions of the form \"$x < c$?\" and \"$x = c$?\", and\nuncovers $x$ using at most $H(\\pi)+1$ questions on average, matching the\nperformance of Huffman codes in this sense. We also give a natural set of\n$O(rn^{1\/r})$ questions that achieve a performance of at most $H(\\pi)+r$, and\nshow that $\\Omega(rn^{1\/r})$ questions are required to achieve such a\nguarantee.\n  Our second main result gives a set $\\mathcal{Q}$ of $1.25^{n+o(n)}$ questions\nsuch that for every distribution $\\pi$, Bob can implement an optimal strategy\nfor $\\pi$ using only questions from $\\mathcal{Q}$. We also show that\n$1.25^{n-o(n)}$ questions are needed, for infinitely many $n$. If we allow a\nsmall slack of $r$ over the optimal strategy, then roughly $(rn)^{\\Theta(1\/r)}$\nquestions are necessary and sufficient.\n","id":89}
{"Unnamed: 0.1":11090,"Unnamed: 0":11090.0,"anchor":"Neural Networks Designing Neural Networks: Multi-Objective\n  Hyper-Parameter Optimization","positive":"  Artificial neural networks have gone through a recent rise in popularity,\nachieving state-of-the-art results in various fields, including image\nclassification, speech recognition, and automated control. Both the performance\nand computational complexity of such models are heavily dependant on the design\nof characteristic hyper-parameters (e.g., number of hidden layers, nodes per\nlayer, or choice of activation functions), which have traditionally been\noptimized manually. With machine learning penetrating low-power mobile and\nembedded areas, the need to optimize not only for performance (accuracy), but\nalso for implementation complexity, becomes paramount. In this work, we present\na multi-objective design space exploration method that reduces the number of\nsolution networks trained and evaluated through response surface modelling.\nGiven spaces which can easily exceed 1020 solutions, manually designing a\nnear-optimal architecture is unlikely as opportunities to reduce network\ncomplexity, while maintaining performance, may be overlooked. This problem is\nexacerbated by the fact that hyper-parameters which perform well on specific\ndatasets may yield sub-par results on others, and must therefore be designed on\na per-application basis. In our work, machine learning is leveraged by training\nan artificial neural network to predict the performance of future candidate\nnetworks. The method is evaluated on the MNIST and CIFAR-10 image datasets,\noptimizing for both recognition accuracy and computational complexity.\nExperimental results demonstrate that the proposed method can closely\napproximate the Pareto-optimal front, while only exploring a small fraction of\nthe design space.\n","negative":"  Learning over multi-view data is a challenging problem with strong practical\napplications. Most related studies focus on the classification point of view\nand assume that all the views are available at any time. We consider an\nextension of this framework in two directions. First, based on the BiGAN model,\nthe Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from\nmulti-view inputs. Second, it can deal with missing views and is able to update\nits prediction when additional views are provided. We illustrate these\nproperties on a set of experiments over different datasets.\n","id":90}
{"Unnamed: 0.1":11091,"Unnamed: 0":11091.0,"anchor":"Unrolled Generative Adversarial Networks","positive":"  We introduce a method to stabilize Generative Adversarial Networks (GANs) by\ndefining the generator objective with respect to an unrolled optimization of\nthe discriminator. This allows training to be adjusted between using the\noptimal discriminator in the generator's objective, which is ideal but\ninfeasible in practice, and using the current value of the discriminator, which\nis often unstable and leads to poor solutions. We show how this technique\nsolves the common problem of mode collapse, stabilizes training of GANs with\ncomplex recurrent generators, and increases diversity and coverage of the data\ndistribution by the generator.\n","negative":"  Kernel Ridge Regression (KRR) is a simple yet powerful technique for\nnon-parametric regression whose computation amounts to solving a linear system.\nThis system is usually dense and highly ill-conditioned. In addition, the\ndimensions of the matrix are the same as the number of data points, so direct\nmethods are unrealistic for large-scale datasets. In this paper, we propose a\npreconditioning technique for accelerating the solution of the aforementioned\nlinear system. The preconditioner is based on random feature maps, such as\nrandom Fourier features, which have recently emerged as a powerful technique\nfor speeding up and scaling the training of kernel-based methods, such as\nkernel ridge regression, by resorting to approximations. However, random\nfeature maps only provide crude approximations to the kernel function, so\ndelivering state-of-the-art results by directly solving the approximated system\nrequires the number of random features to be very large. We show that random\nfeature maps can be much more effective in forming preconditioners, since under\ncertain conditions a not-too-large number of random features is sufficient to\nyield an effective preconditioner. We empirically evaluate our method and show\nit is highly effective for datasets of up to one million training examples.\n","id":91}
{"Unnamed: 0.1":11092,"Unnamed: 0":11092.0,"anchor":"Designing Neural Network Architectures using Reinforcement Learning","positive":"  At present, designing convolutional neural network (CNN) architectures\nrequires both human expertise and labor. New architectures are handcrafted by\ncareful experimentation or modified from a handful of existing networks. We\nintroduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to\nautomatically generate high-performing CNN architectures for a given learning\ntask. The learning agent is trained to sequentially choose CNN layers using\n$Q$-learning with an $\\epsilon$-greedy exploration strategy and experience\nreplay. The agent explores a large but finite space of possible architectures\nand iteratively discovers designs with improved performance on the learning\ntask. On image classification benchmarks, the agent-designed networks\n(consisting of only standard convolution, pooling, and fully-connected layers)\nbeat existing networks designed with the same layer types and are competitive\nagainst the state-of-the-art methods that use more complex layer types. We also\noutperform existing meta-modeling approaches for network design on image\nclassification tasks.\n","negative":"  One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.\n","id":92}
{"Unnamed: 0.1":11093,"Unnamed: 0":11093.0,"anchor":"Using Social Dynamics to Make Individual Predictions: Variational\n  Inference with a Stochastic Kinetic Model","positive":"  Social dynamics is concerned primarily with interactions among individuals\nand the resulting group behaviors, modeling the temporal evolution of social\nsystems via the interactions of individuals within these systems. In\nparticular, the availability of large-scale data from social networks and\nsensor networks offers an unprecedented opportunity to predict state-changing\nevents at the individual level. Examples of such events include disease\ntransmission, opinion transition in elections, and rumor propagation. Unlike\nprevious research focusing on the collective effects of social systems, this\nstudy makes efficient inferences at the individual level. In order to cope with\ndynamic interactions among a large number of individuals, we introduce the\nstochastic kinetic model to capture adaptive transition probabilities and\npropose an efficient variational inference algorithm the complexity of which\ngrows linearly --- rather than exponentially --- with the number of\nindividuals. To validate this method, we have performed epidemic-dynamics\nexperiments on wireless sensor network data collected from more than ten\nthousand people over three years. The proposed algorithm was used to track\ndisease transmission and predict the probability of infection for each\nindividual. Our results demonstrate that this method is more efficient than\nsampling while nonetheless achieving high accuracy.\n","negative":"  Sparse signal representations based on linear combinations of learned atoms\nhave been used to obtain state-of-the-art results in several practical signal\nprocessing applications. Approximation methods are needed to process\nhigh-dimensional signals in this way because the problem to calculate optimal\natoms for sparse coding is NP-hard. Here we study greedy algorithms for\nunsupervised learning of dictionaries of shift-invariant atoms and propose a\nnew method where each atom is selected with the same probability on average,\nwhich corresponds to the homeostatic regulation of a recurrent convolutional\nneural network. Equiprobable selection can be used with several greedy\nalgorithms for dictionary learning to ensure that all atoms adapt during\ntraining and that no particular atom is more likely to take part in the linear\ncombination on average. We demonstrate via simulation experiments that\ndictionary learning with equiprobable selection results in higher entropy of\nthe sparse representation and lower reconstruction and denoising errors, both\nin the case of ordinary matching pursuit and orthogonal matching pursuit with\nshift-invariant dictionaries. Furthermore, we show that the computational costs\nof the matching pursuits are lower with equiprobable selection, leading to\nfaster and more accurate dictionary learning algorithms.\n","id":93}
{"Unnamed: 0.1":11094,"Unnamed: 0":11094.0,"anchor":"Trusting SVM for Piecewise Linear CNNs","positive":"  We present a novel layerwise optimization algorithm for the learning\nobjective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large\nclass of convolutional neural networks. Specifically, PL-CNNs employ piecewise\nlinear non-linearities such as the commonly used ReLU and max-pool, and an SVM\nclassifier as the final layer. The key observation of our approach is that the\nproblem corresponding to the parameter estimation of a layer can be formulated\nas a difference-of-convex (DC) program, which happens to be a latent structured\nSVM. We optimize the DC program using the concave-convex procedure, which\nrequires us to iteratively solve a structured SVM problem. This allows to\ndesign an optimization algorithm with an optimal learning rate that does not\nrequire any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that\nour approach always improves over the state of the art variants of\nbackpropagation and scales to large data and large network settings.\n","negative":"  Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence.\n","id":94}
{"Unnamed: 0.1":11095,"Unnamed: 0":11095.0,"anchor":"CoCoA: A General Framework for Communication-Efficient Distributed\n  Optimization","positive":"  The scale of modern datasets necessitates the development of efficient\ndistributed optimization methods for machine learning. We present a\ngeneral-purpose framework for distributed computing environments, CoCoA, that\nhas an efficient communication scheme and is applicable to a wide variety of\nproblems in machine learning and signal processing. We extend the framework to\ncover general non-strongly-convex regularizers, including L1-regularized\nproblems like lasso, sparse logistic regression, and elastic net\nregularization, and show how earlier work can be derived as a special case. We\nprovide convergence guarantees for the class of convex regularized loss\nminimization objectives, leveraging a novel approach in handling\nnon-strongly-convex regularizers and non-smooth loss functions. The resulting\nframework has markedly improved performance over state-of-the-art methods, as\nwe illustrate with an extensive set of experiments on real distributed\ndatasets.\n","negative":"  Recently published methods enable training of bitwise neural networks which\nallow reduced representation of down to a single bit per weight. We present a\nmethod that exploits ensemble decisions based on multiple stochastically\nsampled network models to increase performance figures of bitwise neural\nnetworks in terms of classification accuracy at inference. Our experiments with\nthe CIFAR-10 and GTSRB datasets show that the performance of such network\nensembles surpasses the performance of the high-precision base model. With this\ntechnique we achieve 5.81% best classification error on CIFAR-10 test set using\nbitwise networks. Concerning inference on embedded systems we evaluate these\nbitwise networks using a hardware efficient stochastic rounding procedure. Our\nwork contributes to efficient embedded bitwise neural networks.\n","id":95}
{"Unnamed: 0.1":11096,"Unnamed: 0":11096.0,"anchor":"Playing SNES in the Retro Learning Environment","positive":"  Mastering a video game requires skill, tactics and strategy. While these\nattributes may be acquired naturally by human players, teaching them to a\ncomputer program is a far more challenging task. In recent years, extensive\nresearch was carried out in the field of reinforcement learning and numerous\nalgorithms were introduced, aiming to learn how to perform human tasks such as\nplaying video games. As a result, the Arcade Learning Environment (ALE)\n(Bellemare et al., 2013) has become a commonly used benchmark environment\nallowing algorithms to train on various Atari 2600 games. In many games the\nstate-of-the-art algorithms outperform humans. In this paper we introduce a new\nlearning environment, the Retro Learning Environment --- RLE, that can run\ngames on the Super Nintendo Entertainment System (SNES), Sega Genesis and\nseveral other gaming consoles. The environment is expandable, allowing for more\nvideo games and consoles to be easily added to the environment, while\nmaintaining the same interface as ALE. Moreover, RLE is compatible with Python\nand Torch. SNES games pose a significant challenge to current algorithms due to\ntheir higher level of complexity and versatility.\n","negative":"  This paper presents an infinite variational autoencoder (VAE) whose capacity\nadapts to suit the input data. This is achieved using a mixture model where the\nmixing coefficients are modeled by a Dirichlet process, allowing us to\nintegrate over the coefficients when performing inference. Critically, this\nthen allows us to automatically vary the number of autoencoders in the mixture\nbased on the data. Experiments show the flexibility of our method, particularly\nfor semi-supervised learning, where only a small number of training samples are\navailable.\n","id":96}
{"Unnamed: 0.1":11097,"Unnamed: 0":11097.0,"anchor":"Minimax-optimal semi-supervised regression on unknown manifolds","positive":"  We consider semi-supervised regression when the predictor variables are drawn\nfrom an unknown manifold. A simple two step approach to this problem is to: (i)\nestimate the manifold geodesic distance between any pair of points using both\nthe labeled and unlabeled instances; and (ii) apply a k nearest neighbor\nregressor based on these distance estimates. We prove that given sufficiently\nmany unlabeled points, this simple method of geodesic kNN regression achieves\nthe optimal finite-sample minimax bound on the mean squared error, as if the\nmanifold were known. Furthermore, we show how this approach can be efficiently\nimplemented, requiring only O(k N log N) operations to estimate the regression\nfunction at all N labeled and unlabeled points. We illustrate this approach on\ntwo datasets with a manifold structure: indoor localization using WiFi\nfingerprints and facial pose estimation. In both cases, geodesic kNN is more\naccurate and much faster than the popular Laplacian eigenvector regressor.\n","negative":"  Recent machine learning methods make it possible to model potential energy of\natomic configurations with chemical-level accuracy (as calculated from\nab-initio calculations) and at speeds suitable for molecular dynam- ics\nsimulation. Best performance is achieved when the known physical constraints\nare encoded in the machine learning models. For example, the atomic energy is\ninvariant under global translations and rotations, it is also invariant to\npermutations of same-species atoms. Although simple to state, these symmetries\nare complicated to encode into machine learning algorithms. In this paper, we\npresent a machine learning approach based on graph theory that naturally\nincorporates translation, rotation, and permutation symmetries. Specifically,\nwe use a random walk graph kernel to measure the similarity of two adjacency\nmatrices, each of which represents a local atomic environment. This Graph\nApproximated Energy (GRAPE) approach is flexible and admits many possible\nextensions. We benchmark a simple version of GRAPE by predicting atomization\nenergies on a standard dataset of organic molecules.\n","id":97}
{"Unnamed: 0.1":11098,"Unnamed: 0":11098.0,"anchor":"Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic","positive":"  Model-free deep reinforcement learning (RL) methods have been successful in a\nwide variety of simulated domains. However, a major obstacle facing deep RL in\nthe real world is their high sample complexity. Batch policy gradient methods\noffer stable learning, but at the cost of high variance, which often requires\nlarge batches. TD-style methods, such as off-policy actor-critic and\nQ-learning, are more sample-efficient but biased, and often require costly\nhyperparameter sweeps to stabilize. In this work, we aim to develop methods\nthat combine the stability of policy gradients with the efficiency of\noff-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor\nexpansion of the off-policy critic as a control variate. Q-Prop is both sample\nefficient and stable, and effectively combines the benefits of on-policy and\noff-policy methods. We analyze the connection between Q-Prop and existing\nmodel-free algorithms, and use control variate theory to derive two variants of\nQ-Prop with conservative and aggressive adaptation. We show that conservative\nQ-Prop provides substantial gains in sample efficiency over trust region policy\noptimization (TRPO) with generalized advantage estimation (GAE), and improves\nstability over deep deterministic policy gradient (DDPG), the state-of-the-art\non-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control\nenvironments.\n","negative":"  In this paper we propose a novel learning framework called Supervised and\nWeakly Supervised Learning where the goal is to learn simultaneously from\nweakly and strongly labeled data. Strongly labeled data can be simply\nunderstood as fully supervised data where all labeled instances are available.\nIn weakly supervised learning only data is weakly labeled which prevents one\nfrom directly applying supervised learning methods. Our proposed framework is\nmotivated by the fact that a small amount of strongly labeled data can give\nconsiderable improvement over only weakly supervised learning. The primary\nproblem domain focus of this paper is acoustic event and scene detection in\naudio recordings. We first propose a naive formulation for leveraging labeled\ndata in both forms. We then propose a more general framework for Supervised and\nWeakly Supervised Learning (SWSL). Based on this general framework, we propose\na graph based approach for SWSL. Our main method is based on manifold\nregularization on graphs in which we show that the unified learning can be\nformulated as a constraint optimization problem which can be solved by\niterative concave-convex procedure (CCCP). Our experiments show that our\nproposed framework can address several concerns of audio content analysis using\nweakly labeled data.\n","id":98}
{"Unnamed: 0.1":11099,"Unnamed: 0":11099.0,"anchor":"Hierarchical compositional feature learning","positive":"  We introduce the hierarchical compositional network (HCN), a directed\ngenerative model able to discover and disentangle, without supervision, the\nbuilding blocks of a set of binary images. The building blocks are binary\nfeatures defined hierarchically as a composition of some of the features in the\nlayer immediately below, arranged in a particular manner. At a high level, HCN\nis similar to a sigmoid belief network with pooling. Inference and learning in\nHCN are very challenging and existing variational approximations do not work\nsatisfactorily. A main contribution of this work is to show that both can be\naddressed using max-product message passing (MPMP) with a particular schedule\n(no EM required). Also, using MPMP as an inference engine for HCN makes new\ntasks simple: adding supervision information, classifying images, or performing\ninpainting all correspond to clamping some variables of the model to their\nknown values and running MPMP on the rest. When used for classification, fast\ninference with HCN has exactly the same functional form as a convolutional\nneural network (CNN) with linear activations and binary weights. However, HCN's\nfeatures are qualitatively very different.\n","negative":"  We show that every symmetric normed space admits an efficient nearest\nneighbor search data structure with doubly-logarithmic approximation.\nSpecifically, for every $n$, $d = n^{o(1)}$, and every $d$-dimensional\nsymmetric norm $\\|\\cdot\\|$, there exists a data structure for\n$\\mathrm{poly}(\\log \\log n)$-approximate nearest neighbor search over\n$\\|\\cdot\\|$ for $n$-point datasets achieving $n^{o(1)}$ query time and\n$n^{1+o(1)}$ space. The main technical ingredient of the algorithm is a\nlow-distortion embedding of a symmetric norm into a low-dimensional iterated\nproduct of top-$k$ norms.\n  We also show that our techniques cannot be extended to general norms.\n","id":99}
{"Unnamed: 0.1":11100,"Unnamed: 0":11100.0,"anchor":"Learning Time Series Detection Models from Temporally Imprecise Labels","positive":"  In this paper, we consider a new low-quality label learning problem: learning\ntime series detection models from temporally imprecise labels. In this problem,\nthe data consist of a set of input time series, and supervision is provided by\na sequence of noisy time stamps corresponding to the occurrence of positive\nclass events. Such temporally imprecise labels commonly occur in areas like\nmobile health research where human annotators are tasked with labeling the\noccurrence of very short duration events. We propose a general learning\nframework for this problem that can accommodate different base classifiers and\nnoise models. We present results on real mobile health data showing that the\nproposed framework significantly outperforms a number of alternatives including\nassuming that the label time stamps are noise-free, transforming the problem\ninto the multiple instance learning framework, and learning on labels that were\nmanually re-aligned.\n","negative":"  In this paper we present the greedy step averaging(GSA) method, a\nparameter-free stochastic optimization algorithm for a variety of machine\nlearning problems. As a gradient-based optimization method, GSA makes use of\nthe information from the minimizer of a single sample's loss function, and\ntakes average strategy to calculate reasonable learning rate sequence. While\nmost existing gradient-based algorithms introduce an increasing number of hyper\nparameters or try to make a trade-off between computational cost and\nconvergence rate, GSA avoids the manual tuning of learning rate and brings in\nno more hyper parameters or extra cost. We perform exhaustive numerical\nexperiments for logistic and softmax regression to compare our method with the\nother state of the art ones on 16 datasets. Results show that GSA is robust on\nvarious scenarios.\n","id":100}
{"Unnamed: 0.1":11101,"Unnamed: 0":11101.0,"anchor":"Memory-augmented Attention Modelling for Videos","positive":"  We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures.\n","negative":"  Neural network based sequence-to-sequence models in an encoder-decoder\nframework have been successfully applied to solve Question Answering (QA)\nproblems, predicting answers from statements and questions. However, almost all\nprevious models have failed to consider detailed context information and\nunknown states under which systems do not have enough information to answer\ngiven questions. These scenarios with incomplete or ambiguous information are\nvery common in the setting of Interactive Question Answering (IQA). To address\nthis challenge, we develop a novel model, employing context-dependent\nword-level attention for more accurate statement representations and\nquestion-guided sentence-level attention for better context modeling. We also\ngenerate unique IQA datasets to test our model, which will be made publicly\navailable. Employing these attention mechanisms, our model accurately\nunderstands when it can output an answer or when it requires generating a\nsupplementary question for additional input depending on different contexts.\nWhen available, user's feedback is encoded and directly applied to update\nsentence-level attention to infer an answer. Extensive experiments on QA and\nIQA datasets quantitatively demonstrate the effectiveness of our model with\nsignificant improvement over state-of-the-art conventional QA models.\n","id":101}
{"Unnamed: 0.1":11102,"Unnamed: 0":11102.0,"anchor":"Gaussian Attention Model and Its Application to Knowledge Base Embedding\n  and Question Answering","positive":"  We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the additional\ndegree of freedom to control the focus of its attention from a laser sharp\nattention to a broad attention. It is applicable whenever we can assume that\nthe distance in the latent space reflects some notion of semantics. We use the\nproposed attention model as a scoring function for the embedding of a knowledge\nbase into a continuous vector space and then train a model that performs\nquestion answering about the entities in the knowledge base. The proposed\nattention model can handle both the propagation of uncertainty when following a\nseries of relations and also the conjunction of conditions in a natural way. On\na dataset of soccer players who participated in the FIFA World Cup 2014, we\ndemonstrate that our model can handle both path queries and conjunctive queries\nwell.\n","negative":"  ReLU neural networks define piecewise linear functions of their inputs.\nHowever, initializing and training a neural network is very different from\nfitting a linear spline. In this paper, we expand empirically upon previous\ntheoretical work to demonstrate features of trained neural networks. Standard\nnetwork initialization and training produce networks vastly simpler than a\nnaive parameter count would suggest and can impart odd features to the trained\nnetwork. However, we also show the forced simplicity is beneficial and, indeed,\ncritical for the wide success of these networks.\n","id":102}
{"Unnamed: 0.1":11103,"Unnamed: 0":11103.0,"anchor":"Optimal Binary Autoencoding with Pairwise Correlations","positive":"  We formulate learning of a binary autoencoder as a biconvex optimization\nproblem which learns from the pairwise correlations between encoded and decoded\nbits. Among all possible algorithms that use this information, ours finds the\nautoencoder that reconstructs its inputs with worst-case optimal loss. The\noptimal decoder is a single layer of artificial neurons, emerging entirely from\nthe minimax loss minimization, and with weights learned by convex optimization.\nAll this is reflected in competitive experimental results, demonstrating that\nbinary autoencoding can be done efficiently by conveying information in\npairwise correlations in an optimal fashion.\n","negative":"  We analyze the necessary number of samples for sparse vector recovery in a\nnoisy linear prediction setup. This model includes problems such as linear\nregression and classification. We focus on structured graph models. In\nparticular, we prove that sufficient number of samples for the weighted graph\nmodel proposed by Hegde and others is also necessary. We use the Fano's\ninequality on well constructed ensembles as our main tool in establishing\ninformation theoretic lower bounds.\n","id":103}
{"Unnamed: 0.1":11104,"Unnamed: 0":11104.0,"anchor":"Learning Influence Functions from Incomplete Observations","positive":"  We study the problem of learning influence functions under incomplete\nobservations of node activations. Incomplete observations are a major concern\nas most (online and real-world) social networks are not fully observable. We\nestablish both proper and improper PAC learnability of influence functions\nunder randomly missing observations. Proper PAC learnability under the\nDiscrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade\n(DIC) models is established by reducing incomplete observations to complete\nobservations in a modified graph. Our improper PAC learnability result applies\nfor the DLT and DIC models as well as the Continuous-Time Independent Cascade\n(CIC) model. It is based on a parametrization in terms of reachability\nfeatures, and also gives rise to an efficient and practical heuristic.\nExperiments on synthetic and real-world datasets demonstrate the ability of our\nmethod to compensate even for a fairly large fraction of missing observations.\n","negative":"  This article will devise data-driven, mathematical laws that generate\noptimal, statistical classification systems which achieve minimum error rates\nfor data distributions with unchanging statistics. Thereby, I will design\nlearning machines that minimize the expected risk or probability of\nmisclassification. I will devise a system of fundamental equations of binary\nclassification for a classification system in statistical equilibrium. I will\nuse this system of equations to formulate the problem of learning unknown,\nlinear and quadratic discriminant functions from data as a locus problem,\nthereby formulating geometric locus methods within a statistical framework.\nSolving locus problems involves finding equations of curves or surfaces defined\nby given properties and finding graphs or loci of given equations. I will\ndevise three systems of data-driven, locus equations that generate optimal,\nstatistical classification systems. Each class of learning machines satisfies\nfundamental statistical laws for a classification system in statistical\nequilibrium. Thereby, I will formulate three classes of learning machines that\nare scalable modules for optimal, statistical pattern recognition systems, all\nof which are capable of performing a wide variety of statistical pattern\nrecognition tasks, where any given M-class statistical pattern recognition\nsystem exhibits optimal generalization performance for an M-class feature\nspace.\n","id":104}
{"Unnamed: 0.1":11105,"Unnamed: 0":11105.0,"anchor":"Learning from Untrusted Data","positive":"  The vast majority of theoretical results in machine learning and statistics\nassume that the available training data is a reasonably reliable reflection of\nthe phenomena to be learned or estimated. Similarly, the majority of machine\nlearning and statistical techniques used in practice are brittle to the\npresence of large amounts of biased or malicious data. In this work we consider\ntwo frameworks in which to study estimation, learning, and optimization in the\npresence of significant fractions of arbitrary data.\n  The first framework, list-decodable learning, asks whether it is possible to\nreturn a list of answers, with the guarantee that at least one of them is\naccurate. For example, given a dataset of $n$ points for which an unknown\nsubset of $\\alpha n$ points are drawn from a distribution of interest, and no\nassumptions are made about the remaining $(1-\\alpha)n$ points, is it possible\nto return a list of $\\operatorname{poly}(1\/\\alpha)$ answers, one of which is\ncorrect? The second framework, which we term the semi-verified learning model,\nconsiders the extent to which a small dataset of trusted data (drawn from the\ndistribution in question) can be leveraged to enable the accurate extraction of\ninformation from a much larger but untrusted dataset (of which only an\n$\\alpha$-fraction is drawn from the distribution).\n  We show strong positive results in both settings, and provide an algorithm\nfor robust learning in a very general stochastic optimization setting. This\ngeneral result has immediate implications for robust estimation in a number of\nsettings, including for robustly estimating the mean of distributions with\nbounded second moments, robustly learning mixtures of such distributions, and\nrobustly finding planted partitions in random graphs in which significant\nportions of the graph have been perturbed by an adversary.\n","negative":"  Passive microseismic data are commonly buried in noise, which presents a\nsignificant challenge for signal detection and recovery. For recordings from a\nsurface sensor array where each trace contains a time-delayed arrival from the\nevent, we propose an autocorrelation-based stacking method that designs a\ndenoising filter from all the traces, as well as a multi-channel detection\nscheme. This approach circumvents the issue of time aligning the traces prior\nto stacking because every trace's autocorrelation is centered at zero in the\nlag domain. The effect of white noise is concentrated near zero lag, so the\nfilter design requires a predictable adjustment of the zero-lag value.\nTruncation of the autocorrelation is employed to smooth the impulse response of\nthe denoising filter. In order to extend the applicability of the algorithm, we\nalso propose a noise prewhitening scheme that addresses cases with colored\nnoise. The simplicity and robustness of this method are validated with\nsynthetic and real seismic traces.\n","id":105}
{"Unnamed: 0.1":11106,"Unnamed: 0":11106.0,"anchor":"Adversarial Ladder Networks","positive":"  The use of unsupervised data in addition to supervised data in training\ndiscriminative neural networks has improved the performance of this clas-\nsification scheme. However, the best results were achieved with a training\nprocess that is divided in two parts: first an unsupervised pre-training step\nis done for initializing the weights of the network and after these weights are\nrefined with the use of supervised data. On the other hand adversarial noise\nhas improved the results of clas- sical supervised learning. Recently, a new\nneural network topology called Ladder Network, where the key idea is based in\nsome properties of hierar- chichal latent variable models, has been proposed as\na technique to train a neural network using supervised and unsupervised data at\nthe same time with what is called semi-supervised learning. This technique has\nreached state of the art classification. In this work we add adversarial noise\nto the ladder network and get state of the art classification, with several\nimportant conclusions on how adversarial noise can help in addition with new\npossible lines of investi- gation. We also propose an alternative to add\nadversarial noise to unsu- pervised data.\n","negative":"  We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches.\n","id":106}
{"Unnamed: 0.1":11107,"Unnamed: 0":11107.0,"anchor":"Neural Taylor Approximations: Convergence and Exploration in Rectifier\n  Networks","positive":"  Modern convolutional networks, incorporating rectifiers and max-pooling, are\nneither smooth nor convex; standard guarantees therefore do not apply.\nNevertheless, methods from convex optimization such as gradient descent and\nAdam are widely used as building blocks for deep learning algorithms. This\npaper provides the first convergence guarantee applicable to modern convnets,\nwhich furthermore matches a lower bound for convex nonsmooth functions. The key\ntechnical tool is the neural Taylor approximation -- a straightforward\napplication of Taylor expansions to neural networks -- and the associated\nTaylor loss. Experiments on a range of optimizers, layers, and tasks provide\nevidence that the analysis accurately captures the dynamics of neural\noptimization. The second half of the paper applies the Taylor approximation to\nisolate the main difficulty in training rectifier nets -- that gradients are\nshattered -- and investigates the hypothesis that, by exploring the space of\nactivation configurations more thoroughly, adaptive optimizers such as RMSProp\nand Adam are able to converge to better solutions.\n","negative":"  Automated extraction of concepts from patient clinical records is an\nessential facilitator of clinical research. For this reason, the 2010 i2b2\/VA\nNatural Language Processing Challenges for Clinical Records introduced a\nconcept extraction task aimed at identifying and classifying concepts into\npredefined categories (i.e., treatments, tests and problems). State-of-the-art\nconcept extraction approaches heavily rely on handcrafted features and\ndomain-specific resources which are hard to collect and define. For this\nreason, this paper proposes an alternative, streamlined approach: a recurrent\nneural network (the bidirectional LSTM with CRF decoding) initialized with\ngeneral-purpose, off-the-shelf word embeddings. The experimental results\nachieved on the 2010 i2b2\/VA reference corpora using the proposed framework\noutperform all recent methods and ranks closely to the best submission from the\noriginal 2010 i2b2\/VA challenge.\n","id":107}
{"Unnamed: 0.1":11108,"Unnamed: 0":11108.0,"anchor":"NonSTOP: A NonSTationary Online Prediction Method for Time Series","positive":"  We present online prediction methods for time series that let us explicitly\nhandle nonstationary artifacts (e.g. trend and seasonality) present in most\nreal time series. Specifically, we show that applying appropriate\ntransformations to such time series before prediction can lead to improved\ntheoretical and empirical prediction performance. Moreover, since these\ntransformations are usually unknown, we employ the learning with experts\nsetting to develop a fully online method (NonSTOP-NonSTationary Online\nPrediction) for predicting nonstationary time series. This framework allows for\nseasonality and\/or other trends in univariate time series and cointegration in\nmultivariate time series. Our algorithms and regret analysis subsume recent\nrelated work while significantly expanding the applicability of such methods.\nFor all the methods, we provide sub-linear regret bounds using relaxed\nassumptions. The theoretical guarantees do not fully capture the benefits of\nthe transformations, thus we provide a data-dependent analysis of the\nfollow-the-leader algorithm that provides insight into the success of using\nsuch transformations. We support all of our results with experiments on\nsimulated and real data.\n","negative":"  This paper proposes a novel approach for constructing effective personalized\npolicies when the observed data lacks counter-factual information, is biased\nand possesses many features. The approach is applicable in a wide variety of\nsettings from healthcare to advertising to education to finance. These settings\nhave in common that the decision maker can observe, for each previous instance,\nan array of features of the instance, the action taken in that instance, and\nthe reward realized -- but not the rewards of actions that were not taken: the\ncounterfactual information. Learning in such settings is made even more\ndifficult because the observed data is typically biased by the existing policy\n(that generated the data) and because the array of features that might affect\nthe reward in a particular instance -- and hence should be taken into account\nin deciding on an action in each particular instance -- is often vast. The\napproach presented here estimates propensity scores for the observed data,\ninfers counterfactuals, identifies a (relatively small) number of features that\nare (most) relevant for each possible action and instance, and prescribes a\npolicy to be followed. Comparison of the proposed algorithm against the\nstate-of-art algorithm on actual datasets demonstrates that the proposed\nalgorithm achieves a significant improvement in performance.\n","id":108}
{"Unnamed: 0.1":11109,"Unnamed: 0":11109.0,"anchor":"Divide and Conquer Networks","positive":"  We consider the learning of algorithmic tasks by mere observation of\ninput-output pairs. Rather than studying this as a black-box discrete\nregression problem with no assumption whatsoever on the input-output mapping,\nwe concentrate on tasks that are amenable to the principle of divide and\nconquer, and study what are its implications in terms of learning. This\nprinciple creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two\nscale-invariant atomic operations: how to split a given input into smaller\nsets, and how to merge two partially solved tasks into a larger partial\nsolution. Our model can be trained in weakly supervised environments, namely by\njust observing input-output pairs, and in even weaker environments, using a\nnon-differentiable reward signal. Moreover, thanks to the dynamic aspect of our\narchitecture, we can incorporate the computational complexity as a\nregularization term that can be optimized by backpropagation. We demonstrate\nthe flexibility and efficiency of the Divide-and-Conquer Network on several\ncombinatorial and geometric tasks: convex hull, clustering, knapsack and\neuclidean TSP. Thanks to the dynamic programming nature of our model, we show\nsignificant improvements in terms of generalization error and computational\ncomplexity.\n","negative":"  In settings where only unlabelled speech data is available, zero-resource\nspeech technology needs to be developed without transcriptions, pronunciation\ndictionaries, or language modelling text. There are two central problems in\nzero-resource speech processing: (i) finding frame-level feature\nrepresentations which make it easier to discriminate between linguistic units\n(phones or words), and (ii) segmenting and clustering unlabelled speech into\nmeaningful units. In this thesis, we argue that a combination of top-down and\nbottom-up modelling is advantageous in tackling these two problems.\n  To address the problem of frame-level representation learning, we present the\ncorrespondence autoencoder (cAE), a neural network trained with weak top-down\nsupervision from an unsupervised term discovery system. By combining this\ntop-down supervision with unsupervised bottom-up initialization, the cAE yields\nmuch more discriminative features than previous approaches. We then present our\nunsupervised segmental Bayesian model that segments and clusters unlabelled\nspeech into hypothesized words. By imposing a consistent top-down segmentation\nwhile also using bottom-up knowledge from detected syllable boundaries, our\nsystem outperforms several others on multi-speaker conversational English and\nXitsonga speech data. Finally, we show that the clusters discovered by the\nsegmental Bayesian model can be made less speaker- and gender-specific by using\nfeatures from the cAE instead of traditional acoustic features.\n  In summary, the different models and systems presented in this thesis show\nthat both top-down and bottom-up modelling can improve representation learning,\nsegmentation and clustering of unlabelled speech data.\n","id":109}
{"Unnamed: 0.1":11110,"Unnamed: 0":11110.0,"anchor":"An Efficient Approach to Boosting Performance of Deep Spiking Network\n  Training","positive":"  Nowadays deep learning is dominating the field of machine learning with\nstate-of-the-art performance in various application areas. Recently, spiking\nneural networks (SNNs) have been attracting a great deal of attention, notably\nowning to their power efficiency, which can potentially allow us to implement a\nlow-power deep learning engine suitable for real-time\/mobile applications.\nHowever, implementing SNN-based deep learning remains challenging, especially\ngradient-based training of SNNs by error backpropagation. We cannot simply\npropagate errors through SNNs in conventional way because of the property of\nSNNs that process discrete data in the form of a series. Consequently, most of\nthe previous studies employ a workaround technique, which first trains a\nconventional weighted-sum deep neural network and then maps the learning\nweights to the SNN under training, instead of training SNN parameters directly.\nIn order to eliminate this workaround, recently proposed is a new class of SNN\nnamed deep spiking networks (DSNs), which can be trained directly (without a\nmapping from conventional deep networks) by error backpropagation with\nstochastic gradient descent. In this paper, we show that the initialization of\nthe membrane potential on the backward path is an important step in DSN\ntraining, through diverse experiments performed under various conditions.\nFurthermore, we propose a simple and efficient method that can improve DSN\ntraining by controlling the initial membrane potential on the backward path. In\nour experiments, adopting the proposed approach allowed us to boost the\nperformance of DSN training in terms of converging time and accuracy.\n","negative":"  We characterize the sample size required for accurate graphical model\nselection from non-stationary samples. The observed data is modeled as a\nvector-valued zero-mean Gaussian random process whose samples are uncorrelated\nbut have different covariance matrices. This model contains as special cases\nthe standard setting of i.i.d. samples as well as the case of samples forming a\nstationary or underspread (non-stationary) processes. More generally, our model\napplies to any process model for which an efficient decorrelation can be\nobtained. By analyzing a particular model selection method, we derive a\nsufficient condition on the required sample size for accurate graphical model\nselection based on non-stationary data.\n","id":110}
{"Unnamed: 0.1":11111,"Unnamed: 0":11111.0,"anchor":"Domain Adaptation with L2 constraints for classifying images from\n  different endoscope systems","positive":"  This paper proposes a method for domain adaptation that extends the maximum\nmargin domain transfer (MMDT) proposed by Hoffman et al., by introducing L2\ndistance constraints between samples of different domains; thus, our method is\ndenoted as MMDTL2. Motivated by the differences between the images taken by\nnarrow band imaging (NBI) endoscopic devices, we utilize different NBI devices\nas different domains and estimate the transformations between samples of\ndifferent domains, i.e., image samples taken by different NBI endoscope\nsystems. We first formulate the problem in the primal form, and then derive the\ndual form with much lesser computational costs as compared to the naive\napproach. From our experimental results using NBI image datasets from two\ndifferent NBI endoscopic devices, we find that MMDTL2 is better than MMDT and\nalso support vector machines without adaptation, especially when NBI image\nfeatures are high-dimensional and the per-class training samples are greater\nthan 20.\n","negative":"  Bottom-Up (BU) saliency models do not perform well in complex interactive\nenvironments where humans are actively engaged in tasks (e.g., sandwich making\nand playing the video games). In this paper, we leverage Reinforcement Learning\n(RL) to highlight task-relevant locations of input frames. We propose a soft\nattention mechanism combined with the Deep Q-Network (DQN) model to teach an RL\nagent how to play a game and where to look by focusing on the most pertinent\nparts of its visual input. Our evaluations on several Atari 2600 games show\nthat the soft attention based model could predict fixation locations\nsignificantly better than bottom-up models such as Itti-Kochs saliency and\nGraph-Based Visual Saliency (GBVS) models.\n","id":111}
{"Unnamed: 0.1":11112,"Unnamed: 0":11112.0,"anchor":"Cognitive Discriminative Mappings for Rapid Learning","positive":"  Humans can learn concepts or recognize items from just a handful of examples,\nwhile machines require many more samples to perform the same task. In this\npaper, we build a computational model to investigate the possibility of this\nkind of rapid learning. The proposed method aims to improve the learning task\nof input from sensory memory by leveraging the information retrieved from\nlong-term memory. We present a simple and intuitive technique called cognitive\ndiscriminative mappings (CDM) to explore the cognitive problem. First, CDM\nseparates and clusters the data instances retrieved from long-term memory into\ndistinct classes with a discrimination method in working memory when a sensory\ninput triggers the algorithm. CDM then maps each sensory data instance to be as\nclose as possible to the median point of the data group with the same class.\nThe experimental results demonstrate that the CDM approach is effective for\nlearning the discriminative features of supervised classifications with few\ntraining sensory input instances.\n","negative":"  Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost.\n","id":112}
{"Unnamed: 0.1":11113,"Unnamed: 0":11113.0,"anchor":"PixelSNE: Visualizing Fast with Just Enough Precision via Pixel-Aligned\n  Stochastic Neighbor Embedding","positive":"  Embedding and visualizing large-scale high-dimensional data in a\ntwo-dimensional space is an important problem since such visualization can\nreveal deep insights out of complex data. Most of the existing embedding\napproaches, however, run on an excessively high precision, ignoring the fact\nthat at the end, embedding outputs are converted into coarse-grained discrete\npixel coordinates in a screen space. Motivated by such an observation and\ndirectly considering pixel coordinates in an embedding optimization process, we\naccelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding\n(BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel\nmethod called PixelSNE, a highly-efficient, screen resolution-driven 2D\nembedding method with a linear computational complexity in terms of the number\nof data items. Our experimental results show the significantly fast running\ntime of PixelSNE by a large margin against BH-SNE, while maintaining the\nminimal degradation in the embedding quality. Finally, the source code of our\nmethod is publicly available at https:\/\/github.com\/awesome-davian\/PixelSNE\n","negative":"  Various kinds of k-nearest neighbor (KNN) based classification methods are\nthe bases of many well-established and high-performance pattern-recognition\ntechniques, but both of them are vulnerable to their parameter choice.\nEssentially, the challenge is to detect the neighborhood of various data sets,\nwhile utterly ignorant of the data characteristic. This article introduces a\nnew supervised classification method: the extend natural neighbor (ENaN)\nmethod, and shows that it provides a better classification result without\nchoosing the neighborhood parameter artificially. Unlike the original KNN based\nmethod which needs a prior k, the ENaNE method predicts different k in\ndifferent stages. Therefore, the ENaNE method is able to learn more from\nflexible neighbor information both in training stage and testing stage, and\nprovide a better classification result.\n","id":113}
{"Unnamed: 0.1":11114,"Unnamed: 0":11114.0,"anchor":"Gradients of Counterfactuals","positive":"  Gradients have been used to quantify feature importance in machine learning\nmodels. Unfortunately, in nonlinear deep networks, not only individual neurons\nbut also the whole network can saturate, and as a result an important input\nfeature can have a tiny gradient. We study various networks, and observe that\nthis phenomena is indeed widespread, across many inputs.\n  We propose to examine interior gradients, which are gradients of\ncounterfactual inputs constructed by scaling down the original input. We apply\nour method to the GoogleNet architecture for object recognition in images, as\nwell as a ligand-based virtual screening network with categorical features and\nan LSTM based language model for the Penn Treebank dataset. We visualize how\ninterior gradients better capture feature importance. Furthermore, interior\ngradients are applicable to a wide variety of deep networks, and have the\nattribution property that the feature importance scores sum to the the\nprediction score.\n  Best of all, interior gradients can be computed just as easily as gradients.\nIn contrast, previous methods are complex to implement, which hinders practical\nadoption.\n","negative":"  Recently, there has been an increasing interest in designing distributed\nconvex optimization algorithms under the setting where the data matrix is\npartitioned on features. Algorithms under this setting sometimes have many\nadvantages over those under the setting where data is partitioned on samples,\nespecially when the number of features is huge. Therefore, it is important to\nunderstand the inherent limitations of these optimization problems. In this\npaper, with certain restrictions on the communication allowed in the\nprocedures, we develop tight lower bounds on communication rounds for a broad\nclass of non-incremental algorithms under this setting. We also provide a lower\nbound on communication rounds for a class of (randomized) incremental\nalgorithms.\n","id":114}
{"Unnamed: 0.1":11115,"Unnamed: 0":11115.0,"anchor":"Deep Unsupervised Clustering with Gaussian Mixture Variational\n  Autoencoders","positive":"  We study a variant of the variational autoencoder model (VAE) with a Gaussian\nmixture as a prior distribution, with the goal of performing unsupervised\nclustering through deep generative models. We observe that the known problem of\nover-regularisation that has been shown to arise in regular VAEs also manifests\nitself in our model and leads to cluster degeneracy. We show that a heuristic\ncalled minimum information constraint that has been shown to mitigate this\neffect in VAEs can also be applied to improve unsupervised clustering\nperformance with our model. Furthermore we analyse the effect of this heuristic\nand provide an intuition of the various processes with the help of\nvisualizations. Finally, we demonstrate the performance of our model on\nsynthetic data, MNIST and SVHN, showing that the obtained clusters are\ndistinct, interpretable and result in achieving competitive performance on\nunsupervised clustering to the state-of-the-art results.\n","negative":"  Survival analysis is a fundamental tool in medical research to identify\npredictors of adverse events and develop systems for clinical decision support.\nIn order to leverage large amounts of patient data, efficient optimisation\nroutines are paramount. We propose an efficient training algorithm for the\nkernel survival support vector machine (SSVM). We directly optimise the primal\nobjective function and employ truncated Newton optimisation and order statistic\ntrees to significantly lower computational costs compared to previous training\nalgorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with\n$n$ samples and $p$ features. Our results demonstrate that our proposed\noptimisation scheme allows analysing data of a much larger scale with no loss\nin prediction performance. Experiments on synthetic and 5 real-world datasets\nshow that our technique outperforms existing kernel SSVM formulations if the\namount of right censoring is high ($\\geq85\\%$), and performs comparably\notherwise.\n","id":115}
{"Unnamed: 0.1":11116,"Unnamed: 0":11116.0,"anchor":"Sentence Ordering and Coherence Modeling using Recurrent Neural Networks","positive":"  Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.\n","negative":"  In many data exploration tasks it is meaningful to identify groups of\nattribute interactions that are specific to a variable of interest. For\ninstance, in a dataset where the attributes are medical markers and the\nvariable of interest (class variable) is binary indicating presence\/absence of\ndisease, we would like to know which medical markers interact with respect to\nthe binary class label. These interactions are useful in several practical\napplications, for example, to gain insight into the structure of the data, in\nfeature selection, and in data anonymisation. We present a novel method, based\non statistical significance testing, that can be used to verify if the data set\nhas been created by a given factorised class-conditional joint distribution,\nwhere the distribution is parametrised by a partition of its attributes.\nFurthermore, we provide a method, named ASTRID, for automatically finding a\npartition of attributes describing the distribution that has generated the\ndata. State-of-the-art classifiers are utilised to capture the interactions\npresent in the data by systematically breaking attribute interactions and\nobserving the effect of this breaking on classifier performance. We empirically\ndemonstrate the utility of the proposed method with examples using real and\nsynthetic data.\n","id":116}
{"Unnamed: 0.1":11117,"Unnamed: 0":11117.0,"anchor":"Unsupervised Pretraining for Sequence to Sequence Learning","positive":"  This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models. Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the art results on the\nWMT English$\\rightarrow$German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from the previous best models on\nboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner.\n","negative":"  Support vector machines (SVMs) have been recognized as a potential tool for\nsupervised classification analyses in different domains of research. In\nessence, SVM is a binary classifier. Therefore, in case of a multiclass\nproblem, the problem is divided into a series of binary problems which are\nsolved by binary classifiers, and finally the classification results are\ncombined following either the one-against-one or one-against-all strategies. In\nthis paper, an attempt has been made to classify lithology using a multiclass\nSVM based framework using well logs as predictor variables. Here, the lithology\nis classified into four classes such as sand, shaly sand, sandy shale and shale\nbased on the relative values of sand and shale fractions as suggested by an\nexpert geologist. The available dataset consisting well logs (gamma ray,\nneutron porosity, density, and P-sonic) and class information from four closely\nspaced wells from an onshore hydrocarbon field is divided into training and\ntesting sets. We have used one-against-all strategy to combine the results of\nmultiple binary classifiers. The reported results established the superiority\nof multiclass SVM compared to other classifiers in terms of classification\naccuracy. The selection of kernel function and associated parameters has also\nbeen investigated here. It can be envisaged from the results achieved in this\nstudy that the proposed framework based on multiclass SVM can further be used\nto solve classification problems. In future research endeavor, seismic\nattributes can be introduced in the framework to classify the lithology\nthroughout a study area from seismic inputs.\n","id":117}
{"Unnamed: 0.1":11118,"Unnamed: 0":11118.0,"anchor":"Variational Lossy Autoencoder","positive":"  Representation learning seeks to expose certain aspects of observed data in a\nlearned representation that's amenable to downstream tasks like classification.\nFor instance, a good representation for 2D images might be one that describes\nonly global structure and discards information about detailed texture. In this\npaper, we present a simple but principled method to learn such global\nrepresentations by combining Variational Autoencoder (VAE) with neural\nautoregressive models such as RNN, MADE and PixelRNN\/CNN. Our proposed VAE\nmodel allows us to have control over what the global latent code can learn and\n, by designing the architecture accordingly, we can force the global latent\ncode to discard irrelevant information such as texture in 2D images, and hence\nthe VAE only \"autoencodes\" data in a lossy fashion. In addition, by leveraging\nautoregressive models as both prior distribution $p(z)$ and decoding\ndistribution $p(x|z)$, we can greatly improve generative modeling performance\nof VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and\nCaltech-101 Silhouettes density estimation tasks.\n","negative":"  Decision tree (and its extensions such as Gradient Boosting Decision Trees\nand Random Forest) is a widely used machine learning algorithm, due to its\npractical effectiveness and model interpretability. With the emergence of big\ndata, there is an increasing need to parallelize the training process of\ndecision tree. However, most existing attempts along this line suffer from high\ncommunication costs. In this paper, we propose a new algorithm, called\n\\emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After\npartitioning the training data onto a number of (e.g., $M$) machines, this\nalgorithm performs both local voting and global voting in each iteration. For\nlocal voting, the top-$k$ attributes are selected from each machine according\nto its local data. Then, globally top-$2k$ attributes are determined by a\nmajority voting among these local candidates. Finally, the full-grained\nhistograms of the globally top-$2k$ attributes are collected from local\nmachines in order to identify the best (most informative) attribute and its\nsplit point. PV-Tree can achieve a very low communication cost (independent of\nthe total number of attributes) and thus can scale out very well. Furthermore,\ntheoretical analysis shows that this algorithm can learn a near optimal\ndecision tree, since it can find the best attribute with a large probability.\nOur experiments on real-world datasets show that PV-Tree significantly\noutperforms the existing parallel decision tree algorithms in the trade-off\nbetween accuracy and efficiency.\n","id":118}
{"Unnamed: 0.1":11119,"Unnamed: 0":11119.0,"anchor":"Recursive Regression with Neural Networks: Approximating the HJI PDE\n  Solution","positive":"  The majority of methods used to compute approximations to the\nHamilton-Jacobi-Isaacs partial differential equation (HJI PDE) rely on the\ndiscretization of the state space to perform dynamic programming updates. This\ntype of approach is known to suffer from the curse of dimensionality due to the\nexponential growth in grid points with the state dimension. In this work we\npresent an approximate dynamic programming algorithm that computes an\napproximation of the solution of the HJI PDE by alternating between solving a\nregression problem and solving a minimax problem using a feedforward neural\nnetwork as the function approximator. We find that this method requires less\nmemory to run and to store the approximation than traditional gridding methods,\nand we test it on a few systems of two, three and six dimensions.\n","negative":"  When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.\n","id":119}
{"Unnamed: 0.1":11120,"Unnamed: 0":11120.0,"anchor":"Recursive Decomposition for Nonconvex Optimization","positive":"  Continuous optimization is an important problem in many areas of AI,\nincluding vision, robotics, probabilistic inference, and machine learning.\nUnfortunately, most real-world optimization problems are nonconvex, causing\nstandard convex techniques to find only local optima, even with extensions like\nrandom restarts and simulated annealing. We observe that, in many cases, the\nlocal modes of the objective function have combinatorial structure, and thus\nideas from combinatorial optimization can be brought to bear. Based on this, we\npropose a problem-decomposition approach to nonconvex optimization. Similarly\nto DPLL-style SAT solvers and recursive conditioning in probabilistic\ninference, our algorithm, RDIS, recursively sets variables so as to simplify\nand decompose the objective function into approximately independent\nsub-functions, until the remaining functions are simple enough to be optimized\nby standard techniques like gradient descent. The variables to set are chosen\nby graph partitioning, ensuring decomposition whenever possible. We show\nanalytically that RDIS can solve a broad class of nonconvex optimization\nproblems exponentially faster than gradient descent with random restarts.\nExperimentally, RDIS outperforms standard techniques on problems like structure\nfrom motion and protein folding.\n","negative":"  Structural causal models (SCMs), also known as (nonparametric) structural\nequation models (SEMs), are widely used for causal modeling purposes. In\nparticular, acyclic SCMs, also known as recursive SEMs, form a well-studied\nsubclass of SCMs that generalize causal Bayesian networks to allow for latent\nconfounders. In this paper, we investigate SCMs in a more general setting,\nallowing for the presence of both latent confounders and cycles. We show that\nin the presence of cycles, many of the convenient properties of acyclic SCMs do\nnot hold in general: they do not always have a solution; they do not always\ninduce unique observational, interventional and counterfactual distributions; a\nmarginalization does not always exist, and if it exists the marginal model does\nnot always respect the latent projection; they do not always satisfy a Markov\nproperty; and their graphs are not always consistent with their causal\nsemantics. We prove that for SCMs in general each of these properties does hold\nunder certain solvability conditions. Our work generalizes results for SCMs\nwith cycles that were only known for certain special cases so far. We introduce\nthe class of simple SCMs that extends the class of acyclic SCMs to the cyclic\nsetting, while preserving many of the convenient properties of acyclic SCMs.\nWith this paper we aim to provide the foundations for a general theory of\nstatistical causal modeling with SCMs.\n","id":120}
{"Unnamed: 0.1":11121,"Unnamed: 0":11121.0,"anchor":"Delving into Transferable Adversarial Examples and Black-box Attacks","positive":"  An intriguing property of deep neural networks is the existence of\nadversarial examples, which can transfer among different architectures. These\ntransferable adversarial examples may severely hinder deep neural network-based\napplications. Previous works mostly study the transferability using small scale\ndatasets. In this work, we are the first to conduct an extensive study of the\ntransferability over large models and a large scale dataset, and we are also\nthe first to study the transferability of targeted adversarial examples with\ntheir target labels. We study both non-targeted and targeted adversarial\nexamples, and show that while transferable non-targeted adversarial examples\nare easy to find, targeted adversarial examples generated using existing\napproaches almost never transfer with their target labels. Therefore, we\npropose novel ensemble-based approaches to generating transferable adversarial\nexamples. Using such approaches, we observe a large proportion of targeted\nadversarial examples that are able to transfer with their target labels for the\nfirst time. We also present some geometric studies to help understanding the\ntransferable adversarial examples. Finally, we show that the adversarial\nexamples generated using ensemble-based approaches can successfully attack\nClarifai.com, which is a black-box image classification system.\n","negative":"  To cope with changing environments, recent developments in online learning\nhave introduced the concepts of adaptive regret and dynamic regret\nindependently. In this paper, we illustrate an intrinsic connection between\nthese two concepts by showing that the dynamic regret can be expressed in terms\nof the adaptive regret and the functional variation. This observation implies\nthat strongly adaptive algorithms can be directly leveraged to minimize the\ndynamic regret. As a result, we present a series of strongly adaptive\nalgorithms that have small dynamic regrets for convex functions, exponentially\nconcave functions, and strongly convex functions, respectively. To the best of\nour knowledge, this is the first time that exponential concavity is utilized to\nupper bound the dynamic regret. Moreover, all of those adaptive algorithms do\nnot need any prior knowledge of the functional variation, which is a\nsignificant advantage over previous specialized methods for minimizing dynamic\nregret.\n","id":121}
{"Unnamed: 0.1":11122,"Unnamed: 0":11122.0,"anchor":"RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning","positive":"  Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.\n","negative":"  Providing Reinforcement Learning agents with expert advice can dramatically\nimprove various aspects of learning. Prior work has developed teaching\nprotocols that enable agents to learn efficiently in complex environments; many\nof these methods tailor the teacher's guidance to agents with a particular\nrepresentation or underlying learning scheme, offering effective but\nspecialized teaching procedures. In this work, we explore protocol programs, an\nagent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is\nto incorporate the beneficial properties of a human teacher into Reinforcement\nLearning without making strong assumptions about the inner workings of the\nagent. We show how to represent existing approaches such as action pruning,\nreward shaping, and training in simulation as special cases of our schema and\nconduct preliminary experiments on simple domains.\n","id":122}
{"Unnamed: 0.1":11123,"Unnamed: 0":11123.0,"anchor":"Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models\n  with KL-control","positive":"  This paper proposes a general method for improving the structure and quality\nof sequences generated by a recurrent neural network (RNN), while maintaining\ninformation originally learned from data, as well as sample diversity. An RNN\nis first pre-trained on data using maximum likelihood estimation (MLE), and the\nprobability distribution over the next token in the sequence learned by this\nmodel is treated as a prior policy. Another RNN is then trained using\nreinforcement learning (RL) to generate higher-quality outputs that account for\ndomain-specific incentives while retaining proximity to the prior policy of the\nMLE RNN. To formalize this objective, we derive novel off-policy RL methods for\nRNNs from KL-control. The effectiveness of the approach is demonstrated on two\napplications; 1) generating novel musical melodies, and 2) computational\nmolecular generation. For both problems, we show that the proposed method\nimproves the desired properties and structure of the generated sequences, while\nmaintaining information learned from data.\n","negative":"  Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment.\n","id":123}
{"Unnamed: 0.1":11124,"Unnamed: 0":11124.0,"anchor":"Online Learning for Wireless Distributed Computing","positive":"  There has been a growing interest for Wireless Distributed Computing (WDC),\nwhich leverages collaborative computing over multiple wireless devices. WDC\nenables complex applications that a single device cannot support individually.\nHowever, the problem of assigning tasks over multiple devices becomes\nchallenging in the dynamic environments encountered in real-world settings,\nconsidering that the resource availability and channel conditions change over\ntime in unpredictable ways due to mobility and other factors. In this paper, we\nformulate a task assignment problem as an online learning problem using an\nadversarial multi-armed bandit framework. We propose MABSTA, a novel online\nlearning algorithm that learns the performance of unknown devices and channel\nqualities continually through exploratory probing and makes task assignment\ndecisions by exploiting the gained knowledge. For maximal adaptability, MABSTA\nis designed to make no stochastic assumption about the environment. We analyze\nit mathematically and provide a worst-case performance guarantee for any\ndynamic environment. We also compare it with the optimal offline policy as well\nas other baselines via emulations on trace-data obtained from a wireless IoT\ntestbed, and show that it offers competitive and robust performance in all\ncases. To the best of our knowledge, MABSTA is the first online algorithm in\nthis domain of task assignment problems and provides provable performance\nguarantee.\n","negative":"  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods requires great effort to annotate the\ntraining data by costly manual labeling and specialized computational models to\ndetect these annotations during test. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\nfor labeling a set of instances\/patches, we propose end-to-end trained deep\nmulti-instance networks for mass classification based on whole mammogram\nwithout the aforementioned costly need to annotate the training data. We\nexplore three different schemes to construct deep multi-instance networks for\nwhole mammogram classification. Experimental results on the INbreast dataset\ndemonstrate the robustness of proposed deep networks compared to previous work\nusing segmentation and detection annotations in the training.\n","id":124}
{"Unnamed: 0.1":11125,"Unnamed: 0":11125.0,"anchor":"Lie-Access Neural Turing Machines","positive":"  External neural memory structures have recently become a popular tool for\nalgorithmic deep learning (Graves et al. 2014, Weston et al. 2014). These\nmodels generally utilize differentiable versions of traditional discrete\nmemory-access structures (random access, stacks, tapes) to provide the storage\nnecessary for computational tasks. In this work, we argue that these neural\nmemory systems lack specific structure important for relative indexing, and\npropose an alternative model, Lie-access memory, that is explicitly designed\nfor the neural setting. In this paradigm, memory is accessed using a continuous\nhead in a key-space manifold. The head is moved via Lie group actions, such as\nshifts or rotations, generated by a controller, and memory access is performed\nby linear smoothing in key space. We argue that Lie groups provide a natural\ngeneralization of discrete memory structures, such as Turing machines, as they\nprovide inverse and identity operators while maintaining differentiability. To\nexperiment with this approach, we implement a simplified Lie-access neural\nTuring machine (LANTM) with different Lie groups. We find that this approach is\nable to perform well on a range of algorithmic tasks.\n","negative":"  A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice.\n","id":125}
{"Unnamed: 0.1":11126,"Unnamed: 0":11126.0,"anchor":"Audio Visual Speech Recognition using Deep Recurrent Neural Networks","positive":"  In this work, we propose a training algorithm for an audio-visual automatic\nspeech recognition (AV-ASR) system using deep recurrent neural network\n(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal\nClassification (CTC) objective function. The frame labels obtained from the\nacoustic model are then used to perform a non-linear dimensionality reduction\nof the visual features using a deep bottleneck network. Audio and visual\nfeatures are fused and used to train a fusion RNN. The use of bottleneck\nfeatures for visual modality helps the model to converge properly during\ntraining. Our system is evaluated on GRID corpus. Our results show that\npresence of visual modality gives significant improvement in character error\nrate (CER) at various levels of noise even when the model is trained without\nnoisy data. We also provide a comparison of two fusion methods: feature fusion\nand decision fusion.\n","negative":"  We consider the problem of consistently matching multiple sets of elements to\neach other, which is a common task in fields such as computer vision. To solve\nthe underlying NP-hard objective, existing methods often relax or approximate\nit, but end up with unsatisfying empirical performance due to a misaligned\nobjective. We propose a coordinate update algorithm that directly optimizes the\ntarget objective. By using pairwise alignment information to build an\nundirected graph and initializing the permutation matrices along the edges of\nits Maximum Spanning Tree, our algorithm successfully avoids bad local optima.\nTheoretically, with high probability our algorithm guarantees an optimal\nsolution under reasonable noise assumptions. Empirically, our algorithm\nconsistently and significantly outperforms existing methods on several\nbenchmark tasks on real datasets.\n","id":126}
{"Unnamed: 0.1":11127,"Unnamed: 0":11127.0,"anchor":"Heter-LP: A heterogeneous label propagation algorithm and its\n  application in drug repositioning","positive":"  Drug repositioning offers an effective solution to drug discovery, saving\nboth time and resources by finding new indications for existing drugs.\nTypically, a drug takes effect via its protein targets in the cell. As a\nresult, it is necessary for drug development studies to conduct an\ninvestigation into the interrelationships of drugs, protein targets, and\ndiseases. Although previous studies have made a strong case for the\neffectiveness of integrative network-based methods for predicting these\ninterrelationships, little progress has been achieved in this regard within\ndrug repositioning research. Moreover, the interactions of new drugs and\ntargets (lacking any known targets and drugs, respectively) cannot be\naccurately predicted by most established methods. In this paper, we propose a\nnovel semi-supervised heterogeneous label propagation algorithm named Heter-LP,\nwhich applies both local as well as global network features for data\nintegration. To predict drug-target, disease-target, and drug-disease\nassociations, we use information about drugs, diseases, and targets as\ncollected from multiple sources at different levels. Our algorithm integrates\nthese various types of data into a heterogeneous network and implements a label\npropagation algorithm to find new interactions. Statistical analyses of 10-fold\ncross-validation results and experimental analysis support the effectiveness of\nthe proposed algorithm.\n","negative":"  One of the key tasks of sentiment analysis of product reviews is to extract\nproduct aspects or features that users have expressed opinions on. In this\nwork, we focus on using supervised sequence labeling as the base approach to\nperforming the task. Although several extraction methods using sequence\nlabeling methods such as Conditional Random Fields (CRF) and Hidden Markov\nModels (HMM) have been proposed, we show that this supervised approach can be\nsignificantly improved by exploiting the idea of concept sharing across\nmultiple domains. For example, \"screen\" is an aspect in iPhone, but not only\niPhone has a screen, many electronic devices have screens too. When \"screen\"\nappears in a review of a new domain (or product), it is likely to be an aspect\ntoo. Knowing this information enables us to do much better extraction in the\nnew domain. This paper proposes a novel extraction method exploiting this idea\nin the context of supervised sequence labeling. Experimental results show that\nit produces markedly better results than without using the past information.\n","id":127}
{"Unnamed: 0.1":11128,"Unnamed: 0":11128.0,"anchor":"A Unified Maximum Likelihood Approach for Optimal Distribution Property\n  Estimation","positive":"  The advent of data science has spurred interest in estimating properties of\ndistributions over large alphabets. Fundamental symmetric properties such as\nsupport size, support coverage, entropy, and proximity to uniformity, received\nmost attention, with each property estimated using a different technique and\noften intricate analysis tools.\n  We prove that for all these properties, a single, simple, plug-in\nestimator---profile maximum likelihood (PML)---performs as well as the best\nspecialized techniques. This raises the possibility that PML may optimally\nestimate many other symmetric properties.\n","negative":"  Continuous optimization is an important problem in many areas of AI,\nincluding vision, robotics, probabilistic inference, and machine learning.\nUnfortunately, most real-world optimization problems are nonconvex, causing\nstandard convex techniques to find only local optima, even with extensions like\nrandom restarts and simulated annealing. We observe that, in many cases, the\nlocal modes of the objective function have combinatorial structure, and thus\nideas from combinatorial optimization can be brought to bear. Based on this, we\npropose a problem-decomposition approach to nonconvex optimization. Similarly\nto DPLL-style SAT solvers and recursive conditioning in probabilistic\ninference, our algorithm, RDIS, recursively sets variables so as to simplify\nand decompose the objective function into approximately independent\nsub-functions, until the remaining functions are simple enough to be optimized\nby standard techniques like gradient descent. The variables to set are chosen\nby graph partitioning, ensuring decomposition whenever possible. We show\nanalytically that RDIS can solve a broad class of nonconvex optimization\nproblems exponentially faster than gradient descent with random restarts.\nExperimentally, RDIS outperforms standard techniques on problems like structure\nfrom motion and protein folding.\n","id":128}
{"Unnamed: 0.1":11129,"Unnamed: 0":11129.0,"anchor":"Attributing Hacks","positive":"  In this paper we describe an algorithm for estimating the provenance of hacks\non websites. That is, given properties of sites and the temporal occurrence of\nattacks, we are able to attribute individual attacks to joint causes and\nvulnerabilities, as well as estimating the evolution of these vulnerabilities\nover time. Specifically, we use hazard regression with a time-varying additive\nhazard function parameterized in a generalized linear form. The activation\ncoefficients on each feature are continuous-time functions over time. We\nformulate the problem of learning these functions as a constrained variational\nmaximum likelihood estimation problem with total variation penalty and show\nthat the optimal solution is a 0th order spline (a piecewise constant function)\nwith a finite number of known knots. This allows the inference problem to be\nsolved efficiently and at scale by solving a finite dimensional optimization\nproblem. Extensive experiments on real data sets show that our method\nsignificantly outperforms Cox's proportional hazard model. We also conduct a\ncase study and verify that the fitted functions are indeed recovering\nvulnerable features and real-life events such as the release of code to exploit\nthese features in hacker blogs.\n","negative":"  Motivation: Biological data and knowledge bases increasingly rely on Semantic\nWeb technologies and the use of knowledge graphs for data integration,\nretrieval and federated queries. In the past years, feature learning methods\nthat are applicable to graph-structured data are becoming available, but have\nnot yet widely been applied and evaluated on structured biological knowledge.\nResults: We develop a novel method for feature learning on biological knowledge\ngraphs. Our method combines symbolic methods, in particular knowledge\nrepresentation using symbolic logic and automated reasoning, with neural\nnetworks to generate embeddings of nodes that encode for related information\nwithin knowledge graphs. Through the use of symbolic logic, these embeddings\ncontain both explicit and implicit information. We apply these embeddings to\nthe prediction of edges in the knowledge graph representing problems of\nfunction prediction, finding candidate genes of diseases, protein-protein\ninteractions, or drug target relations, and demonstrate performance that\nmatches and sometimes outperforms traditional approaches based on manually\ncrafted features. Our method can be applied to any biological knowledge graph,\nand will thereby open up the increasing amount of Semantic Web based knowledge\nbases in biology to use in machine learning and data analytics. Availability\nand Implementation:\nhttps:\/\/github.com\/bio-ontology-research-group\/walking-rdf-and-owl Contact:\nrobert.hoehndorf@kaust.edu.sa\n","id":129}
{"Unnamed: 0.1":11130,"Unnamed: 0":11130.0,"anchor":"Incremental Sequence Learning","positive":"  Deep learning research over the past years has shown that by increasing the\nscope or difficulty of the learning problem over time, increasingly complex\nlearning problems can be addressed. We study incremental learning in the\ncontext of sequence learning, using generative RNNs in the form of multi-layer\nrecurrent Mixture Density Networks. While the potential of incremental or\ncurriculum learning to enhance learning is known, indiscriminate application of\nthe principle does not necessarily lead to improvement, and it is essential\ntherefore to know which forms of incremental or curriculum learning have a\npositive effect. This research contributes to that aim by comparing three\ninstantiations of incremental or curriculum learning.\n  We introduce Incremental Sequence Learning, a simple incremental approach to\nsequence learning. Incremental Sequence Learning starts out by using only the\nfirst few steps of each sequence as training data. Each time a performance\ncriterion has been reached, the length of the parts of the sequences used for\ntraining is increased.\n  We introduce and make available a novel sequence learning task and data set:\npredicting and classifying MNIST pen stroke sequences. We find that Incremental\nSequence Learning greatly speeds up sequence learning and reaches the best test\nperformance level of regular sequence learning 20 times faster, reduces the\ntest error by 74%, and in general performs more robustly; it displays lower\nvariance and achieves sustained progress after all three comparison methods\nhave stopped improving. The other instantiations of curriculum learning do not\nresult in any noticeable improvement. A trained sequence prediction model is\nalso used in transfer learning to the task of sequence classification, where it\nis found that transfer learning realizes improved classification performance\ncompared to methods that learn to classify from scratch.\n","negative":"  We develop a framework for combining differentiable programming languages\nwith neural networks. Using this framework we create end-to-end trainable\nsystems that learn to write interpretable algorithms with perceptual\ncomponents. We explore the benefits of inductive biases for strong\ngeneralization and modularity that come from the program-like structure of our\nmodels. In particular, modularity allows us to learn a library of (neural)\nfunctions which grows and improves as more tasks are solved. Empirically, we\nshow that this leads to lifelong learning systems that transfer knowledge to\nnew tasks more effectively than baselines.\n","id":130}
{"Unnamed: 0.1":11131,"Unnamed: 0":11131.0,"anchor":"Fairness in Reinforcement Learning","positive":"  We initiate the study of fairness in reinforcement learning, where the\nactions of a learning algorithm may affect its environment and future rewards.\nOur fairness constraint requires that an algorithm never prefers one action\nover another if the long-term (discounted) reward of choosing the latter action\nis higher. Our first result is negative: despite the fact that fairness is\nconsistent with the optimal policy, any learning algorithm satisfying fairness\nmust take time exponential in the number of states to achieve non-trivial\napproximation to the optimal policy. We then provide a provably fair polynomial\ntime algorithm under an approximate notion of fairness, thus establishing an\nexponential gap between exact and approximate fairness\n","negative":"  In the (deletion-channel) trace reconstruction problem, there is an unknown\n$n$-bit source string $x$. An algorithm is given access to independent traces\nof $x$, where a trace is formed by deleting each bit of~$x$ independently with\nprobability~$\\delta$. The goal of the algorithm is to recover~$x$ exactly (with\nhigh probability), while minimizing samples (number of traces) and running\ntime.\n  Previously, the best known algorithm for the trace reconstruction problem was\ndue to Holenstein~et~al.; it uses $\\exp(\\tilde{O}(n^{1\/2}))$ samples and\nrunning time for any fixed $0 < \\delta < 1$. It is also what we call a\n\"mean-based algorithm\", meaning that it only uses the empirical means of the\nindividual bits of the traces. Holenstein~et~al.~also gave a lower bound,\nshowing that any mean-based algorithm must use at least $n^{\\tilde{\\Omega}(\\log\nn)}$ samples.\n  In this paper we improve both of these results, obtaining matching upper and\nlower bounds for mean-based trace reconstruction. For any constant deletion\nrate $0 < \\delta < 1$, we give a mean-based algorithm that uses\n$\\exp(O(n^{1\/3}))$ time and traces; we also prove that any mean-based algorithm\nmust use at least $\\exp(\\Omega(n^{1\/3}))$ traces. In fact, we obtain matching\nupper and lower bounds even for $\\delta$ subconstant and $\\rho := 1-\\delta$\nsubconstant: when $(\\log^3 n)\/n \\ll \\delta \\leq 1\/2$ the bound is\n$\\exp(-\\Theta(\\delta n)^{1\/3})$, and when $1\/\\sqrt{n} \\ll \\rho \\leq 1\/2$ the\nbound is $\\exp(-\\Theta(n\/\\rho)^{1\/3})$.\n  Our proofs involve estimates for the maxima of Littlewood polynomials on\ncomplex disks. We show that these techniques can also be used to perform trace\nreconstruction with random insertions and bit-flips in addition to deletions.\nWe also find a surprising result: for deletion probabilities $\\delta > 1\/2$,\nthe presence of insertions can actually help with trace reconstruction.\n","id":131}
{"Unnamed: 0.1":11132,"Unnamed: 0":11132.0,"anchor":"Energy-efficient Machine Learning in Silicon: A Communications-inspired\n  Approach","positive":"  This position paper advocates a communications-inspired approach to the\ndesign of machine learning systems on energy-constrained embedded `always-on'\nplatforms. The communications-inspired approach has two versions - 1) a\ndeterministic version where existing low-power communication IC design methods\nare repurposed, and 2) a stochastic version referred to as Shannon-inspired\nstatistical information processing employing information-based metrics,\nstatistical error compensation (SEC), and retraining-based methods to implement\nML systems on stochastic circuit\/device fabrics operating at the limits of\nenergy-efficiency. The communications-inspired approach has the potential to\nfully leverage the opportunities afforded by ML algorithms and applications in\norder to address the challenges inherent in their deployment on\nenergy-constrained platforms.\n","negative":"  Convolutional neural networks excel in image recognition tasks, but this\ncomes at the cost of high computational and memory complexity. To tackle this\nproblem, [1] developed a tensor factorization framework to compress\nfully-connected layers. In this paper, we focus on compressing convolutional\nlayers. We show that while the direct application of the tensor framework [1]\nto the 4-dimensional kernel of convolution does compress the layer, we can do\nbetter. We reshape the convolutional kernel into a tensor of higher order and\nfactorize it. We combine the proposed approach with the previous work to\ncompress both convolutional and fully-connected layers of a network and achieve\n80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.\n","id":132}
{"Unnamed: 0.1":11133,"Unnamed: 0":11133.0,"anchor":"A Modular Theory of Feature Learning","positive":"  Learning representations of data, and in particular learning features for a\nsubsequent prediction task, has been a fruitful area of research delivering\nimpressive empirical results in recent years. However, relatively little is\nunderstood about what makes a representation `good'. We propose the idea of a\nrisk gap induced by representation learning for a given prediction context,\nwhich measures the difference in the risk of some learner using the learned\nfeatures as compared to the original inputs. We describe a set of sufficient\nconditions for unsupervised representation learning to provide a benefit, as\nmeasured by this risk gap. These conditions decompose the problem of when\nrepresentation learning works into its constituent parts, which can be\nseparately evaluated using an unlabeled sample, suitable domain-specific\nassumptions about the joint distribution, and analysis of the feature learner\nand subsequent supervised learner. We provide two examples of such conditions\nin the context of specific properties of the unlabeled distribution, namely\nwhen the data lies close to a low-dimensional manifold and when it forms\nclusters. We compare our approach to a recently proposed analysis of\nsemi-supervised learning.\n","negative":"  Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https:\/\/github.com\/iPhysicist\/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work.\n","id":133}
{"Unnamed: 0.1":11134,"Unnamed: 0":11134.0,"anchor":"Diverse Neural Network Learns True Target Functions","positive":"  Neural networks are a powerful class of functions that can be trained with\nsimple gradient descent to achieve state-of-the-art performance on a variety of\napplications. Despite their practical success, there is a paucity of results\nthat provide theoretical guarantees on why they are so effective. Lying in the\ncenter of the problem is the difficulty of analyzing the non-convex loss\nfunction with potentially numerous local minima and saddle points. Can neural\nnetworks corresponding to the stationary points of the loss function learn the\ntrue target function? If yes, what are the key factors contributing to such\nnice optimization properties?\n  In this paper, we answer these questions by analyzing one-hidden-layer neural\nnetworks with ReLU activation, and show that despite the non-convexity, neural\nnetworks with diverse units have no spurious local minima. We bypass the\nnon-convexity issue by directly analyzing the first order optimality condition,\nand show that the loss can be made arbitrarily small if the minimum singular\nvalue of the \"extended feature matrix\" is large enough. We make novel use of\ntechniques from kernel methods and geometric discrepancy, and identify a new\nrelation linking the smallest singular value to the spectrum of a kernel\nfunction associated with the activation function and to the diversity of the\nunits. Our results also suggest a novel regularization function to promote unit\ndiversity for potentially better generalization.\n","negative":"  We study the behavior of untrained neural networks whose weights and biases\nare randomly distributed using mean field theory. We show the existence of\ndepth scales that naturally limit the maximum depth of signal propagation\nthrough these random networks. Our main practical result is to show that random\nnetworks may be trained precisely when information can travel through them.\nThus, the depth scales that we identify provide bounds on how deep a network\nmay be trained for a specific choice of hyperparameters. As a corollary to\nthis, we argue that in networks at the edge of chaos, one of these depth scales\ndiverges. Thus arbitrarily deep networks may be trained only sufficiently close\nto criticality. We show that the presence of dropout destroys the\norder-to-chaos critical point and therefore strongly limits the maximum\ntrainable depth for random networks. Finally, we develop a mean field theory\nfor backpropagation and we show that the ordered and chaotic phases correspond\nto regions of vanishing and exploding gradient respectively.\n","id":134}
{"Unnamed: 0.1":11135,"Unnamed: 0":11135.0,"anchor":"Using Neural Networks to Compute Approximate and Guaranteed Feasible\n  Hamilton-Jacobi-Bellman PDE Solutions","positive":"  To sidestep the curse of dimensionality when computing solutions to\nHamilton-Jacobi-Bellman partial differential equations (HJB PDE), we propose an\nalgorithm that leverages a neural network to approximate the value function. We\nshow that our final approximation of the value function generates near optimal\ncontrols which are guaranteed to successfully drive the system to a target\nstate. Our framework is not dependent on state space discretization, leading to\na significant reduction in computation time and space complexity in comparison\nwith dynamic programming-based approaches. Using this grid-free approach also\nenables us to plan over longer time horizons with relatively little additional\ncomputation overhead. Unlike many previous neural network HJB PDE approximating\nformulations, our approximation is strictly conservative and hence any\ntrajectories we generate will be strictly feasible. For demonstration, we\nspecialize our new general framework to the Dubins car model and discuss how\nthe framework can be applied to other models with higher-dimensional state\nspaces.\n","negative":"  Credit assignment in traditional recurrent neural networks usually involves\nback-propagating through a long chain of tied weight matrices. The length of\nthis chain scales linearly with the number of time-steps as the same network is\nrun at each time-step. This creates many problems, such as vanishing gradients,\nthat have been well studied. In contrast, a NNEM's architecture recurrent\nactivity doesn't involve a long chain of activity (though some architectures\nsuch as the NTM do utilize a traditional recurrent architecture as a\ncontroller). Rather, the externally stored embedding vectors are used at each\ntime-step, but no messages are passed from previous time-steps. This means that\nvanishing gradients aren't a problem, as all of the necessary gradient paths\nare short. However, these paths are extremely numerous (one per embedding\nvector in memory) and reused for a very long time (until it leaves the memory).\nThus, the forward-pass information of each memory must be stored for the entire\nduration of the memory. This is problematic as this additional storage far\nsurpasses that of the actual memories, to the extent that large memories on\ninfeasible to back-propagate through in high dimensional settings. One way to\nget around the need to hold onto forward-pass information is to recalculate the\nforward-pass whenever gradient information is available. However, if the\nobservations are too large to store in the domain of interest, direct\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\nautoencoder to reinstate the observation, and then use the embedding network to\nrecalculate the forward-pass. Since the recalculated embedding vector is\nunlikely to perfectly match the one stored in memory, we try out 2\napproximations to utilize error gradient w.r.t. the vector in memory.\n","id":135}
{"Unnamed: 0.1":11136,"Unnamed: 0":11136.0,"anchor":"SoK: Applying Machine Learning in Security - A Survey","positive":"  The idea of applying machine learning(ML) to solve problems in security\ndomains is almost 3 decades old. As information and communications grow more\nubiquitous and more data become available, many security risks arise as well as\nappetite to manage and mitigate such risks. Consequently, research on applying\nand designing ML algorithms and systems for security has grown fast, ranging\nfrom intrusion detection systems(IDS) and malware classification to security\npolicy management(SPM) and information leak checking. In this paper, we\nsystematically study the methods, algorithms, and system designs in academic\npublications from 2008-2015 that applied ML in security domains. 98 percent of\nthe surveyed papers appeared in the 6 highest-ranked academic security\nconferences and 1 conference known for pioneering ML applications in security.\nWe examine the generalized system designs, underlying assumptions,\nmeasurements, and use cases in active research. Our examinations lead to 1) a\ntaxonomy on ML paradigms and security domains for future exploration and\nexploitation, and 2) an agenda detailing open and upcoming challenges. Based on\nour survey, we also suggest a point of view that treats security as a game\ntheory problem instead of a batch-trained ML problem.\n","negative":"  Objective: The advent of Electronic Medical Records (EMR) with large\nelectronic imaging databases along with advances in deep neural networks with\nmachine learning has provided a unique opportunity to achieve milestones in\nautomated image analysis. Optical coherence tomography (OCT) is the most\ncommonly obtained imaging modality in ophthalmology and represents a dense and\nrich dataset when combined with labels derived from the EMR. We sought to\ndetermine if deep learning could be utilized to distinguish normal OCT images\nfrom images from patients with Age-related Macular Degeneration (AMD). Methods:\nAutomated extraction of an OCT imaging database was performed and linked to\nclinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg\nSpectralis, and each OCT scan was linked to EMR clinical endpoints extracted\nfrom EPIC. The central 11 images were selected from each OCT scan of two\ncohorts of patients: normal and AMD. Cross-validation was performed using a\nrandom subset of patients. Area under receiver operator curves (auROC) were\nconstructed at an independent image level, macular OCT level, and patient\nlevel. Results: Of an extraction of 2.6 million OCT images linked to clinical\ndatapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were\nselected. A deep neural network was trained to categorize images as either\nnormal or AMD. At the image level, we achieved an auROC of 92.78% with an\naccuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an\naccuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an\naccuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were\n92.64% and 93.69% respectively. Conclusions: Deep learning techniques are\neffective for classifying OCT images. These findings have important\nimplications in utilizing OCT in automated screening and computer aided\ndiagnosis tools.\n","id":136}
{"Unnamed: 0.1":11137,"Unnamed: 0":11137.0,"anchor":"Low Data Drug Discovery with One-shot Learning","positive":"  Recent advances in machine learning have made significant contributions to\ndrug discovery. Deep neural networks in particular have been demonstrated to\nprovide significant boosts in predictive power when inferring the properties\nand activities of small-molecule compounds. However, the applicability of these\ntechniques has been limited by the requirement for large amounts of training\ndata. In this work, we demonstrate how one-shot learning can be used to\nsignificantly lower the amounts of data required to make meaningful predictions\nin drug discovery applications. We introduce a new architecture, the residual\nLSTM embedding, that, when combined with graph convolutional neural networks,\nsignificantly improves the ability to learn meaningful distance metrics over\nsmall-molecules. We open source all models introduced in this work as part of\nDeepChem, an open-source framework for deep-learning in drug discovery.\n","negative":"  Research has shown that convolutional neural networks contain significant\nredundancy, and high classification accuracy can be obtained even when weights\nand activations are reduced from floating point to binary values. In this\npaper, we present FINN, a framework for building fast and flexible FPGA\naccelerators using a flexible heterogeneous streaming architecture. By\nutilizing a novel set of optimizations that enable efficient mapping of\nbinarized neural networks to hardware, we implement fully connected,\nconvolutional and pooling layers, with per-layer compute resources being\ntailored to user-provided throughput requirements. On a ZC706 embedded FPGA\nplatform drawing less than 25 W total system power, we demonstrate up to 12.3\nmillion image classifications per second with 0.31 {\\mu}s latency on the MNIST\ndataset with 95.8% accuracy, and 21906 image classifications per second with\n283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1%\nand 94.9% accuracy. To the best of our knowledge, ours are the fastest\nclassification rates reported to date on these benchmarks.\n","id":137}
{"Unnamed: 0.1":11138,"Unnamed: 0":11138.0,"anchor":"Ultimate tensorization: compressing convolutional and FC layers alike","positive":"  Convolutional neural networks excel in image recognition tasks, but this\ncomes at the cost of high computational and memory complexity. To tackle this\nproblem, [1] developed a tensor factorization framework to compress\nfully-connected layers. In this paper, we focus on compressing convolutional\nlayers. We show that while the direct application of the tensor framework [1]\nto the 4-dimensional kernel of convolution does compress the layer, we can do\nbetter. We reshape the convolutional kernel into a tensor of higher order and\nfactorize it. We combine the proposed approach with the previous work to\ncompress both convolutional and fully-connected layers of a network and achieve\n80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.\n","negative":"  In this paper, a novel architecture for a deep recurrent neural network,\nresidual LSTM is introduced. A plain LSTM has an internal memory cell that can\nlearn long term dependencies of sequential data. It also provides a temporal\nshortcut path to avoid vanishing or exploding gradients in the temporal domain.\nThe residual LSTM provides an additional spatial shortcut path from lower\nlayers for efficient training of deep networks with multiple LSTM layers.\nCompared with the previous work, highway LSTM, residual LSTM separates a\nspatial shortcut path with temporal one by using output layers, which can help\nto avoid a conflict between spatial and temporal-domain gradient flows.\nFurthermore, residual LSTM reuses the output projection matrix and the output\ngate of LSTM to control the spatial information flow instead of additional gate\nnetworks, which effectively reduces more than 10% of network parameters. An\nexperiment for distant speech recognition on the AMI SDM corpus shows that\n10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in\nWER over 3-layer aselines, respectively. On the contrary, 10-layer residual\nLSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%\nWER reduction over plain and highway LSTM networks, respectively.\n","id":138}
{"Unnamed: 0.1":11139,"Unnamed: 0":11139.0,"anchor":"Learning to Play Guess Who? and Inventing a Grounded Language as a\n  Consequence","positive":"  Acquiring your first language is an incredible feat and not easily\nduplicated. Learning to communicate using nothing but a few pictureless books,\na corpus, would likely be impossible even for humans. Nevertheless, this is the\ndominating approach in most natural language processing today. As an\nalternative, we propose the use of situated interactions between agents as a\ndriving force for communication, and the framework of Deep Recurrent Q-Networks\nfor evolving a shared language grounded in the provided environment. We task\nthe agents with interactive image search in the form of the game Guess Who?.\nThe images from the game provide a non trivial environment for the agents to\ndiscuss and a natural grounding for the concepts they decide to encode in their\ncommunication. Our experiments show that the agents learn not only to encode\nphysical concepts in their words, i.e. grounding, but also that the agents\nlearn to hold a multi-step dialogue remembering the state of the dialogue from\nstep to step.\n","negative":"  Deep neural networks with lots of parameters are typically used for\nlarge-scale computer vision tasks such as image classification. This is a\nresult of using dense matrix multiplications and convolutions. However, sparse\ncomputations are known to be much more efficient. In this work, we train and\nbuild neural networks which implicitly use sparse computations. We introduce\nadditional gate variables to perform parameter selection and show that this is\nequivalent to using a spike-and-slab prior. We experimentally validate our\nmethod on both small and large networks and achieve state-of-the-art\ncompression results for sparse neural network models.\n","id":139}
{"Unnamed: 0.1":11140,"Unnamed: 0":11140.0,"anchor":"Faster Kernel Ridge Regression Using Sketching and Preconditioning","positive":"  Kernel Ridge Regression (KRR) is a simple yet powerful technique for\nnon-parametric regression whose computation amounts to solving a linear system.\nThis system is usually dense and highly ill-conditioned. In addition, the\ndimensions of the matrix are the same as the number of data points, so direct\nmethods are unrealistic for large-scale datasets. In this paper, we propose a\npreconditioning technique for accelerating the solution of the aforementioned\nlinear system. The preconditioner is based on random feature maps, such as\nrandom Fourier features, which have recently emerged as a powerful technique\nfor speeding up and scaling the training of kernel-based methods, such as\nkernel ridge regression, by resorting to approximations. However, random\nfeature maps only provide crude approximations to the kernel function, so\ndelivering state-of-the-art results by directly solving the approximated system\nrequires the number of random features to be very large. We show that random\nfeature maps can be much more effective in forming preconditioners, since under\ncertain conditions a not-too-large number of random features is sufficient to\nyield an effective preconditioner. We empirically evaluate our method and show\nit is highly effective for datasets of up to one million training examples.\n","negative":"  This work aims to investigate the use of deep neural network to detect\ncommercial hobby drones in real-life environments by analyzing their sound\ndata. The purpose of work is to contribute to a system for detecting drones\nused for malicious purposes, such as for terrorism. Specifically, we present a\nmethod capable of detecting the presence of commercial hobby drones as a binary\nclassification problem based on sound event detection. We recorded the sound\nproduced by a few popular commercial hobby drones, and then augmented this data\nwith diverse environmental sound data to remedy the scarcity of drone sound\ndata in diverse environments. We investigated the effectiveness of\nstate-of-the-art event sound classification methods, i.e., a Gaussian Mixture\nModel (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network\n(RNN), for drone sound detection. Our empirical results, which were obtained\nwith a testing dataset collected on an urban street, confirmed the\neffectiveness of these models for operating in a real environment. In summary,\nour RNN models showed the best detection performance with an F-Score of 0.8009\nwith 240 ms of input audio with a short processing time, indicating their\napplicability to real-time detection systems.\n","id":140}
{"Unnamed: 0.1":11141,"Unnamed: 0":11141.0,"anchor":"Sharper Bounds for Regularized Data Fitting","positive":"  We study matrix sketching methods for regularized variants of linear\nregression, low rank approximation, and canonical correlation analysis. Our\nmain focus is on sketching techniques which preserve the objective function\nvalue for regularized problems, which is an area that has remained largely\nunexplored. We study regularization both in a fairly broad setting, and in the\nspecific context of the popular and widely used technique of ridge\nregularization; for the latter, as applied to each of these problems, we show\nalgorithmic resource bounds in which the {\\em statistical dimension} appears in\nplaces where in previous bounds the rank would appear. The statistical\ndimension is always smaller than the rank, and decreases as the amount of\nregularization increases. In particular, for the ridge low-rank approximation\nproblem $\\min_{Y,X} \\lVert YX - A \\rVert_F^2 + \\lambda \\lVert Y\\rVert_F^2 +\n\\lambda\\lVert X \\rVert_F^2$, where $Y\\in\\mathbb{R}^{n\\times k}$ and\n$X\\in\\mathbb{R}^{k\\times d}$, we give an approximation algorithm needing \\[\nO(\\mathtt{nnz}(A)) + \\tilde{O}((n+d)\\varepsilon^{-1}k \\min\\{k,\n\\varepsilon^{-1}\\mathtt{sd}_\\lambda(Y^*)\\})+\n\\mathtt{poly}(\\mathtt{sd}_\\lambda(Y^*) \\varepsilon^{-1}) \\] time, where\n$s_{\\lambda}(Y^*)\\le k$ is the statistical dimension of $Y^*$, $Y^*$ is an\noptimal $Y$, $\\varepsilon$ is an error parameter, and $\\mathtt{nnz}(A)$ is the\nnumber of nonzero entries of $A$.This is faster than prior work, even when\n$\\lambda=0$.\n  We also study regularization in a much more general setting. For example, we\nobtain sketching-based algorithms for the low-rank approximation problem\n$\\min_{X,Y} \\lVert YX - A \\rVert_F^2 + f(Y,X)$ where $f(\\cdot,\\cdot)$ is a\nregularizing function satisfying some very general conditions (chiefly,\ninvariance under orthogonal transformations).\n","negative":"  In lexicon-based classification, documents are assigned labels by comparing\nthe number of words that appear from two opposed lexicons, such as positive and\nnegative sentiment. Creating such words lists is often easier than labeling\ninstances, and they can be debugged by non-experts if classification\nperformance is unsatisfactory. However, there is little analysis or\njustification of this classification heuristic. This paper describes a set of\nassumptions that can be used to derive a probabilistic justification for\nlexicon-based classification, as well as an analysis of its expected accuracy.\nOne key assumption behind lexicon-based classification is that all words in\neach lexicon are equally predictive. This is rarely true in practice, which is\nwhy lexicon-based approaches are usually outperformed by supervised classifiers\nthat learn distinct weights on each word from labeled instances. This paper\nshows that it is possible to learn such weights without labeled data, by\nleveraging co-occurrence statistics across the lexicons. This offers the best\nof both worlds: light supervision in the form of lexicons, and data-driven\nclassification with higher accuracy than traditional word-counting heuristics.\n","id":141}
{"Unnamed: 0.1":11142,"Unnamed: 0":11142.0,"anchor":"Policy Search with High-Dimensional Context Variables","positive":"  Direct contextual policy search methods learn to improve policy parameters\nand simultaneously generalize these parameters to different context or task\nvariables. However, learning from high-dimensional context variables, such as\ncamera images, is still a prominent problem in many real-world tasks. A naive\napplication of unsupervised dimensionality reduction methods to the context\nvariables, such as principal component analysis, is insufficient as\ntask-relevant input may be ignored. In this paper, we propose a contextual\npolicy search method in the model-based relative entropy stochastic search\nframework with integrated dimensionality reduction. We learn a model of the\nreward that is locally quadratic in both the policy parameters and the context\nvariables. Furthermore, we perform supervised linear dimensionality reduction\non the context variables by nuclear norm regularization. The experimental\nresults show that the proposed method outperforms naive dimensionality\nreduction via principal component analysis and a state-of-the-art contextual\npolicy search method.\n","negative":"  A key drawback of the current generation of artificial decision-makers is\nthat they do not adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, will optimize behavior with respect to an\nobjective function that must be evaluated and learned through simulations. Once\nthis objective function has been modeled, the agent can then choose its desired\nbehavior in different situations. Bayesian optimization with a Gaussian Process\nsurrogate is used as the method for investigating the objective function. One\nkey benefit is that during optimization the Gaussian Process learns a global\nestimate of the true objective function, with predicted outcomes and a\nstatistical measure of confidence in areas that haven't been investigated yet.\nHowever, standard Bayesian optimization does not perform consistently or\nprovide an accurate Gaussian Process surrogate function for highly volatile\nobjective functions. We treat these problems by introducing a novel sampling\ntechnique called Hybrid Repeat\/Multi-point Sampling. This technique gives the\nAI ability to learn optimum behaviors in a highly uncertain environment. More\nimportantly, it not only improves the reliability of the optimization, but also\ncreates a better model of the entire objective surface. With this improved\nmodel the agent is equipped to better adapt behaviors.\n","id":142}
{"Unnamed: 0.1":11143,"Unnamed: 0":11143.0,"anchor":"Disentangling factors of variation in deep representations using\n  adversarial training","positive":"  We introduce a conditional generative model for learning to disentangle the\nhidden factors of variation within a set of labeled observations, and separate\nthem into complementary codes. One code summarizes the specified factors of\nvariation associated with the labels. The other summarizes the remaining\nunspecified variability. During training, the only available source of\nsupervision comes from our ability to distinguish among different observations\nbelonging to the same class. Examples of such observations include images of a\nset of labeled objects captured at different viewpoints, or recordings of set\nof speakers dictating multiple phrases. In both instances, the intra-class\ndiversity is the source of the unspecified factors of variation: each object is\nobserved at multiple viewpoints, and each speaker dictates multiple phrases.\nLearning to disentangle the specified factors from the unspecified ones becomes\neasier when strong supervision is possible. Suppose that during training, we\nhave access to pairs of images, where each pair shows two different objects\ncaptured from the same viewpoint. This source of alignment allows us to solve\nour task using existing methods. However, labels for the unspecified factors\nare usually unavailable in realistic scenarios where data acquisition is not\nstrictly controlled. We address the problem of disentanglement in this more\ngeneral setting by combining deep convolutional autoencoders with a form of\nadversarial training. Both factors of variation are implicitly captured in the\norganization of the learned embedding space, and can be used for solving\nsingle-image analogies. Experimental results on synthetic and real datasets\nshow that the proposed method is capable of generalizing to unseen classes and\nintra-class variabilities.\n","negative":"  A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat\/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately\/efficiently\npredict performance in unexplored scenarios.\n","id":143}
{"Unnamed: 0.1":11144,"Unnamed: 0":11144.0,"anchor":"Learning an Astronomical Catalog of the Visible Universe through\n  Scalable Bayesian Inference","positive":"  Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer.\n","negative":"  This paper presents a new method to learn online policies in continuous\nstate, continuous action, model-free Markov decision processes, with two\nproperties that are crucial for practical applications. First, the policies are\nimplementable with a very low computational cost: once the policy is computed,\nthe action corresponding to a given state is obtained in logarithmic time with\nrespect to the number of samples used. Second, our method is versatile: it does\nnot rely on any a priori knowledge of the structure of optimal policies. We\nbuild upon the Fitted Q-iteration algorithm which represents the $Q$-value as\nthe average of several regression trees. Our algorithm, the Fitted Policy\nForest algorithm (FPF), computes a regression forest representing the Q-value\nand transforms it into a single tree representing the policy, while keeping\ncontrol on the size of the policy using resampling and leaf merging. We\nintroduce an adaptation of Multi-Resolution Exploration (MRE) which is\nparticularly suited to FPF. We assess the performance of FPF on three classical\nbenchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double\nIntegrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other\nalgorithms, although these algorithms rely on the use of particular\nrepresentations of the policies, especially chosen in order to fit each of the\nthree problems. Finally, we exhibit that the combination of FPF and MRE allows\nto find nearly optimal solutions in problems where $\\epsilon$-greedy approaches\nwould fail.\n","id":144}
{"Unnamed: 0.1":11145,"Unnamed: 0":11145.0,"anchor":"Binomial Checkpointing for Arbitrary Programs with No User Annotation","positive":"  Heretofore, automatic checkpointing at procedure-call boundaries, to reduce\nthe space complexity of reverse mode, has been provided by systems like\nTapenade. However, binomial checkpointing, or treeverse, has only been provided\nin Automatic Differentiation (AD) systems in special cases, e.g., through\nuser-provided pragmas on DO loops in Tapenade, or as the nested taping\nmechanism in adol-c for time integration processes, which requires that user\ncode be refactored. We present a framework for applying binomial checkpointing\nto arbitrary code with no special annotation or refactoring required. This is\naccomplished by applying binomial checkpointing directly to a program trace.\nThis trace is produced by a general-purpose checkpointing mechanism that is\northogonal to AD.\n","negative":"  Previous work combines word-level and character-level representations using\nconcatenation or scalar weighting, which is suboptimal for high-level tasks\nlike reading comprehension. We present a fine-grained gating mechanism to\ndynamically combine word-level and character-level representations based on\nproperties of the words. We also extend the idea of fine-grained gating to\nmodeling the interaction between questions and paragraphs for reading\ncomprehension. Experiments show that our approach can improve the performance\non reading comprehension tasks, achieving new state-of-the-art results on the\nChildren's Book Test dataset. To demonstrate the generality of our gating\nmechanism, we also show improved results on a social media tag prediction task.\n","id":145}
{"Unnamed: 0.1":11146,"Unnamed: 0":11146.0,"anchor":"DiffSharp: An AD Library for .NET Languages","positive":"  DiffSharp is an algorithmic differentiation or automatic differentiation (AD)\nlibrary for the .NET ecosystem, which is targeted by the C# and F# languages,\namong others. The library has been designed with machine learning applications\nin mind, allowing very succinct implementations of models and optimization\nroutines. DiffSharp is implemented in F# and exposes forward and reverse AD\noperators as general nestable higher-order functions, usable by any .NET\nlanguage. It provides high-performance linear algebra primitives---scalars,\nvectors, and matrices, with a generalization to tensors underway---that are\nfully supported by all the AD operators, and which use a BLAS\/LAPACK backend\nvia the highly optimized OpenBLAS library. DiffSharp currently uses operator\noverloading, but we are developing a transformation-based version of the\nlibrary using F#'s \"code quotation\" metaprogramming facility. Work on a\nCUDA-based GPU backend is also underway.\n","negative":"  We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework.\n","id":146}
{"Unnamed: 0.1":11147,"Unnamed: 0":11147.0,"anchor":"Multi-Task Multiple Kernel Relationship Learning","positive":"  This paper presents a novel multitask multiple kernel learning framework that\nefficiently learns the kernel weights leveraging the relationship across\nmultiple tasks. The idea is to automatically infer this task relationship in\nthe \\textit{RKHS} space corresponding to the given base kernels. The problem is\nformulated as a regularization-based approach called \\textit{Multi-Task\nMultiple Kernel Relationship Learning} (\\textit{MK-MTRL}), which models the\ntask relationship matrix from the weights learned from latent feature spaces of\ntask-specific base kernels. Unlike in previous work, the proposed formulation\nallows one to incorporate prior knowledge for simultaneously learning several\nrelated tasks. We propose an alternating minimization algorithm to learn the\nmodel parameters, kernel weights and task relationship matrix. In order to\ntackle large-scale problems, we further propose a two-stage \\textit{MK-MTRL}\nonline learning algorithm and show that it significantly reduces the\ncomputational time, and also achieves performance comparable to that of the\njoint learning framework. Experimental results on benchmark datasets show that\nthe proposed formulations outperform several state-of-the-art multitask\nlearning methods.\n","negative":"  With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data.\n","id":147}
{"Unnamed: 0.1":11148,"Unnamed: 0":11148.0,"anchor":"Importance Sampling with Unequal Support","positive":"  Importance sampling is often used in machine learning when training and\ntesting data come from different distributions. In this paper we propose a new\nvariant of importance sampling that can reduce the variance of importance\nsampling-based estimates by orders of magnitude when the supports of the\ntraining and testing distributions differ. After motivating and presenting our\nnew importance sampling estimator, we provide a detailed theoretical analysis\nthat characterizes both its bias and variance relative to the ordinary\nimportance sampling estimator (in various settings, which include cases where\nordinary importance sampling is biased, while our new estimator is not, and\nvice versa). We conclude with an example of how our new importance sampling\nestimator can be used to improve estimates of how well a new treatment policy\nfor diabetes will work for an individual, using only data from when the\nindividual used a previous treatment policy.\n","negative":"  Correlation clustering is a technique for aggregating data based on\nqualitative information about which pairs of objects are labeled 'similar' or\n'dissimilar.' Because the optimization problem is NP-hard, much of the previous\nliterature focuses on finding approximation algorithms. In this paper we\nexplore how to solve the correlation clustering objective exactly when the data\nto be clustered can be represented by a low-rank matrix. We prove in particular\nthat correlation clustering can be solved in polynomial time when the\nunderlying matrix is positive semidefinite with small constant rank, but that\nthe task remains NP-hard in the presence of even one negative eigenvalue. Based\non our theoretical results, we develop an algorithm for efficiently \"solving\"\nlow-rank positive semidefinite correlation clustering by employing a procedure\nfor zonotope vertex enumeration. We demonstrate the effectiveness and speed of\nour algorithm by using it to solve several clustering problems on both\nsynthetic and real-world data.\n","id":148}
{"Unnamed: 0.1":11149,"Unnamed: 0":11149.0,"anchor":"Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures","positive":"  We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing.\n","negative":"  We present a confidence-based single-layer feed-forward learning algorithm\nSPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of\nactivation spikes. We adaptively update a weight vector relying on confidence\nestimates and activation offsets relative to previous activity. We regularize\nupdates proportionally to item-level confidence and weight-specific support,\nloosely inspired by the observation from neurophysiology that high spike rates\nare sometimes accompanied by low temporal precision. Our experiments suggest\nthat the new learning algorithm SPIRAL is more robust and less prone to\noverfitting than both the averaged perceptron and AROW.\n","id":149}
{"Unnamed: 0.1":11150,"Unnamed: 0":11150.0,"anchor":"Understanding deep learning requires rethinking generalization","positive":"  Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.\n","negative":"  Regularization is key for deep learning since it allows training more complex\nmodels while keeping lower levels of overfitting. However, the most prevalent\nregularizations do not leverage all the capacity of the models since they rely\non reducing the effective number of parameters. Feature decorrelation is an\nalternative for using the full capacity of the models but the overfitting\nreduction margins are too narrow given the overhead it introduces. In this\npaper, we show that regularizing negatively correlated features is an obstacle\nfor effective decorrelation and present OrthoReg, a novel regularization\ntechnique that locally enforces feature orthogonality. As a result, imposing\nlocality constraints in feature decorrelation removes interferences between\nnegatively correlated feature weights, allowing the regularizer to reach higher\ndecorrelation bounds, and reducing the overfitting more effectively. In\nparticular, we show that the models regularized with OrthoReg have higher\naccuracy bounds even when batch normalization and dropout are present.\nMoreover, since our regularization is directly performed on the weights, it is\nespecially suitable for fully convolutional neural networks, where the weight\nspace is constant compared to the feature map space. As a result, we are able\nto reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and\nSVHN.\n","id":150}
{"Unnamed: 0.1":11151,"Unnamed: 0":11151.0,"anchor":"The Sum-Product Theorem: A Foundation for Learning Tractable Models","positive":"  Inference in expressive probabilistic models is generally intractable, which\nmakes them difficult to learn and limits their applicability. Sum-product\nnetworks are a class of deep models where, surprisingly, inference remains\ntractable even when an arbitrary number of hidden layers are present. In this\npaper, we generalize this result to a much broader set of learning problems:\nall those where inference consists of summing a function over a semiring. This\nincludes satisfiability, constraint satisfaction, optimization, integration,\nand others. In any semiring, for summation to be tractable it suffices that the\nfactors of every product have disjoint scopes. This unifies and extends many\nprevious results in the literature. Enforcing this condition at learning time\nthus ensures that the learned models are tractable. We illustrate the power and\ngenerality of this approach by applying it to a new type of structured\nprediction problem: learning a nonconvex function that can be globally\noptimized in polynomial time. We show empirically that this greatly outperforms\nthe standard approach of learning without regard to the cost of optimization.\n","negative":"  In this paper, we consider stochastic dual coordinate (SDCA) {\\em without}\nstrongly convex assumption or convex assumption. We show that SDCA converges\nlinearly under mild conditions termed restricted strong convexity. This covers\na wide array of popular statistical models including Lasso, group Lasso, and\nlogistic regression with $\\ell_1$ regularization, corrected Lasso and linear\nregression with SCAD regularizer. This significantly improves previous\nconvergence results on SDCA for problems that are not strongly convex. As a by\nproduct, we derive a dual free form of SDCA that can handle general\nregularization term, which is of interest by itself.\n","id":151}
{"Unnamed: 0.1":11152,"Unnamed: 0":11152.0,"anchor":"Simple and Efficient Parallelization for Probabilistic Temporal Tensor\n  Factorization","positive":"  Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm\nto model the temporal tensor data. It leverages a time constraint to capture\nthe evolving properties of tensor data. Nowadays the exploding dataset demands\na large scale PTTF analysis, and a parallel solution is critical to accommodate\nthe trend. Whereas, the parallelization of PTTF still remains unexplored. In\nthis paper, we propose a simple yet efficient Parallel Probabilistic Temporal\nTensor Factorization, referred to as P$^2$T$^2$F, to provide a scalable PTTF\nsolution. P$^2$T$^2$F is fundamentally disparate from existing parallel tensor\nfactorizations by considering the probabilistic decomposition and the temporal\neffects of tensor data. It adopts a new tensor data split strategy to subdivide\na large tensor into independent sub-tensors, the computation of which is\ninherently parallel. We train P$^2$T$^2$F with an efficient algorithm of\nstochastic Alternating Direction Method of Multipliers, and show that the\nconvergence is guaranteed. Experiments on several real-word tensor datasets\ndemonstrate that P$^2$T$^2$F is a highly effective and efficiently scalable\nalgorithm dedicated for large scale probabilistic temporal tensor analysis.\n","negative":"  Machine learning analysis of neuroimaging data can accurately predict\nchronological age in healthy people and deviations from healthy brain ageing\nhave been associated with cognitive impairment and disease. Here we sought to\nfurther establish the credentials of \"brain-predicted age\" as a biomarker of\nindividual differences in the brain ageing process, using a predictive\nmodelling approach based on deep learning, and specifically convolutional\nneural networks (CNN), and applied to both pre-processed and raw T1-weighted\nMRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted\nage using a large dataset of healthy adults (N = 2001). Next, we sought to\nestablish the heritability of brain-predicted age using a sample of monozygotic\nand dizygotic female twins (N = 62). Thirdly, we examined the test-retest and\nmulti-centre reliability of brain-predicted age using two samples\n(within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were\ngenerated and compared to a Gaussian Process Regression (GPR) approach, on all\ndatasets. Input data were grey matter (GM) or white matter (WM) volumetric maps\ngenerated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted\nage represents an accurate, highly reliable and genetically-valid phenotype,\nthat has potential to be used as a biomarker of brain ageing. Moreover, age\npredictions can be accurately generated on raw T1-MRI data, substantially\nreducing computation time for novel data, bringing the process closer to giving\nreal-time information on brain health in clinical settings.\n","id":152}
{"Unnamed: 0.1":11153,"Unnamed: 0":11153.0,"anchor":"Collision-based Testers are Optimal for Uniformity and Closeness","positive":"  We study the fundamental problems of (i) uniformity testing of a discrete\ndistribution, and (ii) closeness testing between two discrete distributions\nwith bounded $\\ell_2$-norm. These problems have been extensively studied in\ndistribution testing and sample-optimal estimators are known for\nthem~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n  In this work, we show that the original collision-based testers proposed for\nthese problems ~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant\nfactors. Previous analyses showed sample complexity upper bounds for these\ntesters that are optimal as a function of the domain size $n$, but suboptimal\nby polynomial factors in the error parameter $\\epsilon$. Our main contribution\nis a new tight analysis establishing that these collision-based testers are\ninformation-theoretically optimal, up to constant factors, both in the\ndependence on $n$ and in the dependence on $\\epsilon$.\n","negative":"  We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text.\n","id":153}
{"Unnamed: 0.1":11154,"Unnamed: 0":11154.0,"anchor":"UTCNN: a Deep Learning Model of Stance Classificationon on Social Media\n  Text","positive":"  Most neural network models for document classification on social media focus\non text infor-mation to the neglect of other information on these platforms. In\nthis paper, we classify post stance on social media channels and develop UTCNN,\na neural network model that incorporates user tastes, topic tastes, and user\ncomments on posts. UTCNN not only works on social media texts, but also\nanalyzes texts in forums and message boards. Experiments performed on Chinese\nFacebook data and English online debate forum data show that UTCNN achieves a\n0.755 macro-average f-score for supportive, neutral, and unsupportive stance\nclasses on Facebook data, which is significantly better than models in which\neither user, topic, or comment information is withheld. This model design\ngreatly mitigates the lack of data for the minor class without the use of\noversampling. In addition, UTCNN yields a 0.842 accuracy on English online\ndebate forum data, which also significantly outperforms results from previous\nwork as well as other deep learning models, showing that UTCNN performs well\nregardless of language or platform.\n","negative":"  We develop a first line of attack for solving programming competition-style\nproblems from input-output examples using deep learning. The approach is to\ntrain a neural network to predict properties of the program that generated the\noutputs from the inputs. We use the neural network's predictions to augment\nsearch techniques from the programming languages community, including\nenumerative search and an SMT-based solver. Empirically, we show that our\napproach leads to an order of magnitude speedup over the strong non-augmented\nbaselines and a Recurrent Neural Network approach, and that we are able to\nsolve problems of difficulty comparable to the simplest problems on programming\ncompetition websites.\n","id":154}
{"Unnamed: 0.1":11155,"Unnamed: 0":11155.0,"anchor":"Greedy Step Averaging: A parameter-free stochastic optimization method","positive":"  In this paper we present the greedy step averaging(GSA) method, a\nparameter-free stochastic optimization algorithm for a variety of machine\nlearning problems. As a gradient-based optimization method, GSA makes use of\nthe information from the minimizer of a single sample's loss function, and\ntakes average strategy to calculate reasonable learning rate sequence. While\nmost existing gradient-based algorithms introduce an increasing number of hyper\nparameters or try to make a trade-off between computational cost and\nconvergence rate, GSA avoids the manual tuning of learning rate and brings in\nno more hyper parameters or extra cost. We perform exhaustive numerical\nexperiments for logistic and softmax regression to compare our method with the\nother state of the art ones on 16 datasets. Results show that GSA is robust on\nvarious scenarios.\n","negative":"  The introduction of data analytics into medicine has changed the nature of\npatient treatment. In this, patients are asked to disclose personal information\nsuch as genetic markers, lifestyle habits, and clinical history. This data is\nthen used by statistical models to predict personalized treatments. However,\ndue to privacy concerns, patients often desire to withhold sensitive\ninformation. This self-censorship can impede proper diagnosis and treatment,\nwhich may lead to serious health complications and even death over time. In\nthis paper, we present privacy distillation, a mechanism which allows patients\nto control the type and amount of information they wish to disclose to the\nhealthcare providers for use in statistical models. Meanwhile, it retains the\naccuracy of models that have access to all patient data under a sufficient but\nnot full set of privacy-relevant information. We validate privacy distillation\nusing a corpus of patients prescribed to warfarin for a personalized dosage. We\nuse a deep neural network to implement privacy distillation for training and\nmaking dose predictions. We find that privacy distillation with sufficient\nprivacy-relevant information i) retains accuracy almost as good as having all\npatient data (only 3\\% worse), and ii) is effective at preventing errors that\nintroduce health-related risks (only 3.9\\% worse under- or over-prescriptions).\n","id":155}
{"Unnamed: 0.1":11156,"Unnamed: 0":11156.0,"anchor":"Learning to Navigate in Complex Environments","positive":"  Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities.\n","negative":"  Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.\n","id":156}
{"Unnamed: 0.1":11157,"Unnamed: 0":11157.0,"anchor":"Hierarchical Object Detection with Deep Reinforcement Learning","positive":"  We present a method for performing hierarchical object detection in images\nguided by a deep reinforcement learning agent. The key idea is to focus on\nthose parts of the image that contain richer information and zoom on them. We\ntrain an intelligent agent that, given an image window, is capable of deciding\nwhere to focus the attention among five different predefined region candidates\n(smaller windows). This procedure is iterated providing a hierarchical image\nanalysis.We compare two different candidate proposal strategies to guide the\nobject search: with and without overlap. Moreover, our work compares two\ndifferent strategies to extract features from a convolutional neural network\nfor each region proposal: a first one that computes new feature maps for each\nregion proposal, and a second one that computes the feature maps for the whole\nimage to later generate crops for each region proposal. Experiments indicate\nbetter results for the overlapping candidate proposal strategy and a loss of\nperformance for the cropped image features due to the loss of spatial\nresolution. We argue that, while this loss seems unavoidable when working with\nlarge amounts of object candidates, the much more reduced amount of region\nproposals generated by our reinforcement learning agent allows considering to\nextract features for each location without sharing convolutional computation\namong regions.\n","negative":"  In this paper, we propose a framework for solving a single-agent task by\nusing multiple agents, each focusing on different aspects of the task. This\napproach has two main advantages: 1) it allows for training specialized agents\non different parts of the task, and 2) it provides a new way to transfer\nknowledge, by transferring trained agents. Our framework generalizes the\ntraditional hierarchical decomposition, in which, at any moment in time, a\nsingle agent has control until it has solved its particular subtask. We\nillustrate our framework with empirical experiments on two domains.\n","id":157}
{"Unnamed: 0.1":11158,"Unnamed: 0":11158.0,"anchor":"Tricks from Deep Learning","positive":"  The deep learning community has devised a diverse set of methods to make\ngradient optimization, using large datasets, of large and highly complex models\nwith deeply cascaded nonlinearities, practical. Taken as a whole, these methods\nconstitute a breakthrough, allowing computational structures which are quite\nwide, very deep, and with an enormous number and variety of free parameters to\nbe effectively optimized. The result now dominates much of practical machine\nlearning, with applications in machine translation, computer vision, and speech\nrecognition. Many of these methods, viewed through the lens of algorithmic\ndifferentiation (AD), can be seen as either addressing issues with the gradient\nitself, or finding ways of achieving increased efficiency using tricks that are\nAD-related, but not provided by current AD systems.\n  The goal of this paper is to explain not just those methods of most relevance\nto AD, but also the technical constraints and mindset which led to their\ndiscovery. After explaining this context, we present a \"laundry list\" of\nmethods developed by the deep learning community. Two of these are discussed in\nfurther mathematical detail: a way to dramatically reduce the size of the tape\nwhen performing reverse-mode AD on a (theoretically) time-reversible process\nlike an ODE integrator; and a new mathematical insight that allows for the\nimplementation of a stochastic Newton's method.\n","negative":"  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes, but it has not yet been successfully used for automotive\napplications. There has recently been a revival of interest in the topic,\nhowever, driven by the ability of deep learning algorithms to learn good\nrepresentations of the environment. Motivated by Google DeepMind's successful\ndemonstrations of learning for games from Breakout to Go, we will propose\ndifferent methods for autonomous driving using deep reinforcement learning.\nThis is of particular interest as it is difficult to pose autonomous driving as\na supervised learning problem as it has a strong interaction with the\nenvironment including other vehicles, pedestrians and roadworks. As this is a\nrelatively new area of research for autonomous driving, we will formulate two\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\nactions category. For the discrete actions category, we will deal with Deep\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\nthat, We will also discover the performance of these two categories on an open\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\ncar Simulator. Our simulation results demonstrate learning of autonomous\nmaneuvering in a scenario of complex road curvatures and simple interaction\nwith other vehicles. Finally, we explain the effect of some restricted\nconditions, put on the car during the learning phase, on the convergence time\nfor finishing its learning phase.\n","id":158}
{"Unnamed: 0.1":11159,"Unnamed: 0":11159.0,"anchor":"Towards the Science of Security and Privacy in Machine Learning","positive":"  Advances in machine learning (ML) in recent years have enabled a dizzying\narray of applications such as data analytics, autonomous systems, and security\ndiagnostics. ML is now pervasive---new systems and models are being deployed in\nevery domain imaginable, leading to rapid and widespread deployment of software\nbased inference and decision making. There is growing recognition that ML\nexposes new vulnerabilities in software systems, yet the technical community's\nunderstanding of the nature and extent of these vulnerabilities remains\nlimited. We systematize recent findings on ML security and privacy, focusing on\nattacks identified on these systems and defenses crafted to date. We articulate\na comprehensive threat model for ML, and categorize attacks and defenses within\nan adversarial framework. Key insights resulting from works both in the ML and\nsecurity communities are identified and the effectiveness of approaches are\nrelated to structural elements of ML algorithms and the data used to train\nthem. We conclude by formally exploring the opposing relationship between model\naccuracy and resilience to adversarial manipulation. Through these\nexplorations, we show that there are (possibly unavoidable) tensions between\nmodel complexity, accuracy, and resilience that must be calibrated for the\nenvironments in which they will be used.\n","negative":"  We consider the problem of predicting the next observation given a sequence\nof past observations, and consider the extent to which accurate prediction\nrequires complex algorithms that explicitly leverage long-range dependencies.\nPerhaps surprisingly, our positive results show that for a broad class of\nsequences, there is an algorithm that predicts well on average, and bases its\npredictions only on the most recent few observation together with a set of\nsimple summary statistics of the past observations. Specifically, we show that\nfor any distribution over observations, if the mutual information between past\nobservations and future observations is upper bounded by $I$, then a simple\nMarkov model over the most recent $I\/\\epsilon$ observations obtains expected KL\nerror $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to\nthe optimal predictor that has access to the entire past and knows the data\ngenerating distribution. For a Hidden Markov Model with $n$ hidden states, $I$\nis bounded by $\\log n$, a quantity that does not depend on the mixing time, and\nwe show that the trivial prediction algorithm based on the empirical\nfrequencies of length $O(\\log n\/\\epsilon)$ windows of observations achieves\nthis error, provided the length of the sequence is $d^{\\Omega(\\log\nn\/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n  We also establish that this result cannot be improved upon, even for the\nclass of HMMs, in the following two senses: First, for HMMs with $n$ hidden\nstates, a window length of $\\log n\/\\epsilon$ is information-theoretically\nnecessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the\n$d^{\\Theta(\\log n\/\\epsilon)}$ samples required to estimate the Markov model for\nan observation alphabet of size $d$ is necessary for any computationally\ntractable learning algorithm, assuming the hardness of strongly refuting a\ncertain class of CSPs.\n","id":159}
{"Unnamed: 0.1":11160,"Unnamed: 0":11160.0,"anchor":"Recovery Guarantee of Non-negative Matrix Factorization via Alternating\n  Updates","positive":"  Non-negative matrix factorization is a popular tool for decomposing data into\nfeature and weight matrices under non-negativity constraints. It enjoys\npractical success but is poorly understood theoretically. This paper proposes\nan algorithm that alternates between decoding the weights and updating the\nfeatures, and shows that assuming a generative model of the data, it provably\nrecovers the ground-truth under fairly mild conditions. In particular, its only\nessential requirement on features is linear independence. Furthermore, the\nalgorithm uses ReLU to exploit the non-negativity for decoding the weights, and\nthus can tolerate adversarial noise that can potentially be as large as the\nsignal, and can tolerate unbiased noise much larger than the signal. The\nanalysis relies on a carefully designed coupling between two potential\nfunctions, which we believe is of independent interest.\n","negative":"  We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. Recently, some studies handle multiple modalities on deep generative\nmodels, such as variational autoencoders (VAEs). However, these models\ntypically assume that modalities are forced to have a conditioned relation,\ni.e., we can only generate modalities in one direction. To achieve our\nobjective, we should extract a joint representation that captures high-level\nconcepts among all modalities and through which we can exchange them\nbi-directionally. As described herein, we propose a joint multimodal\nvariational autoencoder (JMVAE), in which all modalities are independently\nconditioned on joint representation. In other words, it models a joint\ndistribution of modalities. Furthermore, to be able to generate missing\nmodalities from the remaining modalities properly, we develop an additional\nmethod, JMVAE-kl, that is trained by reducing the divergence between JMVAE's\nencoder and prepared networks of respective modalities. Our experiments show\nthat our proposed method can obtain appropriate joint representation from\nmultiple modalities and that it can generate and reconstruct them more properly\nthan conventional VAEs. We further demonstrate that JMVAE can generate multiple\nmodalities bi-directionally.\n","id":160}
{"Unnamed: 0.1":11161,"Unnamed: 0":11161.0,"anchor":"Learning to Learn without Gradient Descent by Gradient Descent","positive":"  We learn recurrent neural network optimizers trained on simple synthetic\nfunctions by gradient descent. We show that these learned optimizers exhibit a\nremarkable degree of transfer in that they can be used to efficiently optimize\na broad range of derivative-free black-box functions, including Gaussian\nprocess bandits, simple control objectives, global optimization benchmarks and\nhyper-parameter tuning tasks. Up to the training horizon, the learned\noptimizers learn to trade-off exploration and exploitation, and compare\nfavourably with heavily engineered Bayesian optimization packages for\nhyper-parameter tuning.\n","negative":"  Despite the great successes of deep learning, the effectiveness of deep\nneural networks has not been understood at any theoretical depth. This work is\nmotivated by the thrust of developing a deeper understanding of recurrent\nneural networks, particularly LSTM\/GRU-like networks. As the highly complex\nstructure of the recurrent unit in LSTM and GRU networks makes them difficult\nto analyze, our methodology in this research theme is to construct an\nalternative recurrent unit that is as simple as possible and yet also captures\nthe key components of LSTM\/GRU recurrent units. Such a unit can then be used\nfor the study of recurrent networks and its structural simplicity may allow\neasier analysis. Towards that goal, we take a system-theoretic perspective to\ndesign a new recurrent unit, which we call the prototypical recurrent unit\n(PRU). Not only having minimal complexity, PRU is demonstrated experimentally\nto have comparable performance to GRU and LSTM unit. This establishes PRU\nnetworks as a prototype for future study of LSTM\/GRU-like recurrent networks.\nThis paper also studies the memorization abilities of LSTM, GRU and PRU\nnetworks, motivated by the folk belief that such networks possess long-term\nmemory. For this purpose, we design a simple and controllable task, called\n``memorization problem'', where the networks are trained to memorize certain\ntargeted information. We show that the memorization performance of all three\nnetworks depends on the amount of targeted information, the amount of\n``interfering\" information, and the state space dimension of the recurrent\nunit. Experiments are also performed for another controllable task, the adding\nproblem, and similar conclusions are obtained.\n","id":161}
{"Unnamed: 0.1":11162,"Unnamed: 0":11162.0,"anchor":"A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models","positive":"  Generative adversarial networks (GANs) are a recently proposed class of\ngenerative models in which a generator is trained to optimize a cost function\nthat is being simultaneously learned by a discriminator. While the idea of\nlearning cost functions is relatively new to the field of generative modeling,\nlearning costs has long been studied in control and reinforcement learning (RL)\ndomains, typically for imitation learning from demonstrations. In these fields,\nlearning cost function underlying observed behavior is known as inverse\nreinforcement learning (IRL) or inverse optimal control. While at first the\nconnection between cost learning in RL and cost learning in generative modeling\nmay appear to be a superficial one, we show in this paper that certain IRL\nmethods are in fact mathematically equivalent to GANs. In particular, we\ndemonstrate an equivalence between a sample-based algorithm for maximum entropy\nIRL and a GAN in which the generator's density can be evaluated and is provided\nas an additional input to the discriminator. Interestingly, maximum entropy IRL\nis a special case of an energy-based model. We discuss the interpretation of\nGANs as an algorithm for training energy-based models, and relate this\ninterpretation to other recent work that seeks to connect GANs and EBMs. By\nformally highlighting the connection between GANs, IRL, and EBMs, we hope that\nresearchers in all three communities can better identify and apply transferable\nideas from one domain to another, particularly for developing more stable and\nscalable algorithms: a major challenge in all three domains.\n","negative":"  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification, and clustering. For regression and cluster\nanalyses of continuous data, MoE usually use normal experts following the\nGaussian distribution. However, for a set of data containing a group or groups\nof observations with heavy tails or atypical observations, the use of normal\nexperts is unsuitable and can unduly affect the fit of the MoE model. We\nintroduce a robust MoE modeling using the $t$ distribution. The proposed $t$\nMoE (TMoE) deals with these issues regarding heavy-tailed and noisy data. We\ndevelop a dedicated expectation-maximization (EM) algorithm to estimate the\nparameters of the proposed model by monotonically maximizing the observed data\nlog-likelihood. We describe how the presented model can be used in prediction\nand in model-based clustering of regression data. The proposed model is\nvalidated on numerical experiments carried out on simulated data, which show\nthe effectiveness and the robustness of the proposed model in terms of modeling\nnon-linear regression functions as well as in model-based clustering. Then, it\nis applied to the real-world data of tone perception for musical data analysis,\nand the one of temperature anomalies for the analysis of climate change data.\nThe obtained results show the usefulness of the TMoE model for practical\napplications.\n","id":162}
{"Unnamed: 0.1":11163,"Unnamed: 0":11163.0,"anchor":"Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM","positive":"  Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is\nused as the building block in energy-based deep generative models. Due to\nnumerical stability and quantifiability of the likelihood, RBM is commonly used\nwith Bernoulli units. Here, we consider an alternative member of exponential\nfamily RBM with leaky rectified linear units -- called leaky RBM. We first\nstudy the joint and marginal distributions of leaky RBM under different\nleakiness, which provides us important insights by connecting the leaky RBM\nmodel and truncated Gaussian distributions. The connection leads us to a simple\nyet efficient method for sampling from this model, where the basic idea is to\nanneal the leakiness rather than the energy; -- i.e., start from a fully\nGaussian\/Linear unit and gradually decrease the leakiness over iterations. This\nserves as an alternative to the annealing of the temperature parameter and\nenables numerical estimation of the likelihood that are more efficient and more\naccurate than the commonly used annealed importance sampling (AIS). We further\ndemonstrate that the proposed sampling algorithm enjoys faster mixing property\nthan contrastive divergence algorithm, which benefits the training without any\nadditional computational cost.\n","negative":"  Predictive State Representations (PSRs) are powerful techniques for modelling\ndynamical systems, which represent a state as a vector of predictions about\nfuture observable events (tests). In PSRs, one of the fundamental problems is\nthe learning of the PSR model of the underlying system. Recently, spectral\nmethods have been successfully used to address this issue by treating the\nlearning problem as the task of computing an singular value decomposition (SVD)\nover a submatrix of a special type of matrix called the Hankel matrix. Under\nthe assumptions that the rows and columns of the submatrix of the Hankel Matrix\nare sufficient~(which usually means a very large number of rows and columns,\nand almost fails in practice) and the entries of the matrix can be estimated\naccurately, it has been proven that the spectral approach for learning PSRs is\nstatistically consistent and the learned parameters can converge to the true\nparameters. However, in practice, due to the limit of the computation ability,\nonly a finite set of rows or columns can be chosen to be used for the spectral\nlearning. While different sets of columns usually lead to variant accuracy of\nthe learned model, in this paper, we propose an approach for selecting the set\nof columns, namely basis selection, by adopting a concept of model entropy to\nmeasure the accuracy of the learned model. Experimental results are shown to\ndemonstrate the effectiveness of the proposed approach.\n","id":163}
{"Unnamed: 0.1":11164,"Unnamed: 0":11164.0,"anchor":"Unsupervised Learning For Effective User Engagement on Social Media","positive":"  In this paper, we investigate the effectiveness of unsupervised feature\nlearning techniques in predicting user engagement on social media.\nSpecifically, we compare two methods to predict the number of feedbacks (i.e.,\ncomments) that a blog post is likely to receive. We compare Principal Component\nAnalysis (PCA) and sparse Autoencoder to a baseline method where the data are\nonly centered and scaled, on each of two models: Linear Regression and\nRegression Tree. We find that unsupervised learning techniques significantly\nimprove the prediction accuracy on both models. For the Linear Regression\nmodel, sparse Autoencoder achieves the best result, with an improvement in the\nroot mean squared error (RMSE) on the test set of 42% over the baseline method.\nFor the Regression Tree model, PCA achieves the best result, with an\nimprovement in RMSE of 15% over the baseline.\n","negative":"  In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.\n","id":164}
{"Unnamed: 0.1":11165,"Unnamed: 0":11165.0,"anchor":"Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly\n  Likelihood","positive":"  We develop a supervised machine learning model that detects anomalies in\nsystems in real time. Our model processes unbounded streams of data into time\nseries which then form the basis of a low-latency anomaly detection model.\nMoreover, we extend our preliminary goal of just anomaly detection to\nsimultaneous anomaly prediction. We approach this very challenging problem by\ndeveloping a Bayesian Network framework that captures the information about the\nparameters of the lagged regressors calibrated in the first part of our\napproach and use this structure to learn local conditional probability\ndistributions.\n","negative":"  This research presents an innovative and unique way of solving the\nadvertisement prediction problem which is considered as a learning problem over\nthe past several years. Online advertising is a multi-billion-dollar industry\nand is growing every year with a rapid pace. The goal of this research is to\nenhance click through rate of the contextual advertisements using Linear\nRegression. In order to address this problem, a new technique propose in this\npaper to predict the CTR which will increase the overall revenue of the system\nby serving the advertisements more suitable to the viewers with the help of\nfeature extraction and displaying the advertisements based on context of the\npublishers. The important steps include the data collection, feature\nextraction, CTR prediction and advertisement serving. The statistical results\nobtained from the dynamically used technique show an efficient outcome by\nfitting the data close to perfection for the LR technique using optimized\nfeature selection.\n","id":165}
{"Unnamed: 0.1":11166,"Unnamed: 0":11166.0,"anchor":"Reinforcement Learning in Rich-Observation MDPs using Spectral Methods","positive":"  Reinforcement learning (RL) in Markov decision processes (MDPs) with large\nstate spaces is a challenging problem. The performance of standard RL\nalgorithms degrades drastically with the dimensionality of state space.\nHowever, in practice, these large MDPs typically incorporate a latent or hidden\nlow-dimensional structure. In this paper, we study the setting of\nrich-observation Markov decision processes (ROMDP), where there are a small\nnumber of hidden states which possess an injective mapping to the observation\nstates. In other words, every observation state is generated through a single\nhidden state, and this mapping is unknown a priori. We introduce a spectral\ndecomposition method that consistently learns this mapping, and more\nimportantly, achieves it with low regret. The estimated mapping is integrated\ninto an optimistic RL algorithm (UCRL), which operates on the estimated hidden\nspace. We derive finite-time regret bounds for our algorithm with a weak\ndependence on the dimensionality of the observed space. In fact, our algorithm\nasymptotically achieves the same average regret as the oracle UCRL algorithm,\nwhich has the knowledge of the mapping from hidden to observed spaces. Thus, we\nderive an efficient spectral RL algorithm for ROMDPs.\n","negative":"  Convolutional Neural Networks (CNNs) has shown a great success in many areas\nincluding complex image classification tasks. However, they need a lot of\nmemory and computational cost, which hinders them from running in relatively\nlow-end smart devices such as smart phones. We propose a CNN compression method\nbased on CP-decomposition and Tensor Power Method. We also propose an iterative\nfine tuning, with which we fine-tune the whole network after decomposing each\nlayer, but before decomposing the next layer. Significant reduction in memory\nand computation cost is achieved compared to state-of-the-art previous work\nwith no more accuracy loss.\n","id":166}
{"Unnamed: 0.1":11167,"Unnamed: 0":11167.0,"anchor":"Personalized Donor-Recipient Matching for Organ Transplantation","positive":"  Organ transplants can improve the life expectancy and quality of life for the\nrecipient but carries the risk of serious post-operative complications, such as\nseptic shock and organ rejection. The probability of a successful transplant\ndepends in a very subtle fashion on compatibility between the donor and the\nrecipient but current medical practice is short of domain knowledge regarding\nthe complex nature of recipient-donor compatibility. Hence a data-driven\napproach for learning compatibility has the potential for significant\nimprovements in match quality. This paper proposes a novel system\n(ConfidentMatch) that is trained using data from electronic health records.\nConfidentMatch predicts the success of an organ transplant (in terms of the 3\nyear survival rates) on the basis of clinical and demographic traits of the\ndonor and recipient. ConfidentMatch captures the heterogeneity of the donor and\nrecipient traits by optimally dividing the feature space into clusters and\nconstructing different optimal predictive models to each cluster. The system\ncontrols the complexity of the learned predictive model in a way that allows\nfor assuring more granular and confident predictions for a larger number of\npotential recipient-donor pairs, thereby ensuring that predictions are\n\"personalized\" and tailored to individual characteristics to the finest\npossible granularity. Experiments conducted on the UNOS heart transplant\ndataset show the superiority of the prognostic value of ConfidentMatch to other\ncompeting benchmarks; ConfidentMatch can provide predictions of success with\n95% confidence for 5,489 patients of a total population of 9,620 patients,\nwhich corresponds to 410 more patients than the most competitive benchmark\nalgorithm (DeepBoost).\n","negative":"  State-of-the-art i-vector based speaker verification relies on variants of\nProbabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We\nare mainly motivated by the recent work of the joint Bayesian (JB) method,\nwhich is originally proposed for discriminant analysis in face verification. We\napply JB to speaker verification and make three contributions beyond the\noriginal JB. 1) In contrast to the EM iterations with approximated statistics\nin the original JB, the EM iterations with exact statistics are employed and\ngive better performance. 2) We propose to do simultaneous diagonalization (SD)\nof the within-class and between-class covariance matrices to achieve efficient\ntesting, which has broader application scope than the SVD-based efficient\ntesting method in the original JB. 3) We scrutinize similarities and\ndifferences between various Gaussian PLDAs and JB, complementing the previous\nanalysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are\nconducted on NIST SRE10 core condition 5, empirically validating the\nsuperiority of JB with faster convergence rate and 9-13% EER reduction compared\nwith state-of-the-art PLDA.\n","id":167}
{"Unnamed: 0.1":11168,"Unnamed: 0":11168.0,"anchor":"Anomaly Detection in Bitcoin Network Using Unsupervised Learning Methods","positive":"  The problem of anomaly detection has been studied for a long time. In short,\nanomalies are abnormal or unlikely things. In financial networks, thieves and\nillegal activities are often anomalous in nature. Members of a network want to\ndetect anomalies as soon as possible to prevent them from harming the network's\ncommunity and integrity. Many Machine Learning techniques have been proposed to\ndeal with this problem; some results appear to be quite promising but there is\nno obvious superior method. In this paper, we consider anomaly detection\nparticular to the Bitcoin transaction network. Our goal is to detect which\nusers and transactions are the most suspicious; in this case, anomalous\nbehavior is a proxy for suspicious behavior. To this end, we use three\nunsupervised learning methods including k-means clustering, Mahalanobis\ndistance, and Unsupervised Support Vector Machine (SVM) on two graphs generated\nby the Bitcoin transaction network: one graph has users as nodes, and the other\nhas transactions as nodes.\n","negative":"  In the light of regularized dynamic time warping kernels, this paper\nre-considers the concept of time elastic centroid for a setof time series. We\nderive a new algorithm based on a probabilistic interpretation of kernel\nalignment matrices. This algorithm expressesthe averaging process in terms of a\nstochastic alignment automata. It uses an iterative agglomerative heuristic\nmethod for averagingthe aligned samples, while also averaging the times of\noccurrence of the aligned samples. By comparing classification accuracies for45\nheterogeneous time series datasets obtained by first nearest centroid\/medoid\nclassifiers we show that: i) centroid-basedapproaches significantly outperform\nmedoid-based approaches, ii) for the considered datasets, our algorithm that\ncombines averagingin the sample space and along the time axes, emerges as the\nmost significantly robust model for time-elastic averaging with apromising\nnoise reduction capability. We also demonstrate its benefit in an isolated\ngesture recognition experiment and its ability tosignificantly reduce the size\nof training instance sets. Finally we highlight its denoising capability using\ndemonstrative synthetic data:we show that it is possible to retrieve, from few\nnoisy instances, a signal whose components are scattered in a wide spectral\nband.\n","id":168}
{"Unnamed: 0.1":11169,"Unnamed: 0":11169.0,"anchor":"An Introduction to MM Algorithms for Machine Learning and Statistical","positive":"  MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.\n","negative":"  Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution\n","id":169}
{"Unnamed: 0.1":11170,"Unnamed: 0":11170.0,"anchor":"Dual Teaching: A Practical Semi-supervised Wrapper Method","positive":"  Semi-supervised wrapper methods are concerned with building effective\nsupervised classifiers from partially labeled data. Though previous works have\nsucceeded in some fields, it is still difficult to apply semi-supervised\nwrapper methods to practice because the assumptions those methods rely on tend\nto be unrealistic in practice. For practical use, this paper proposes a novel\nsemi-supervised wrapper method, Dual Teaching, whose assumptions are easy to\nset up. Dual Teaching adopts two external classifiers to estimate the false\npositives and false negatives of the base learner. Only if the recall of every\nexternal classifier is greater than zero and the sum of the precision is\ngreater than one, Dual Teaching will train a base learner from partially\nlabeled data as effectively as the fully-labeled-data-trained classifier. The\neffectiveness of Dual Teaching is proved in both theory and practice.\n","negative":"  Embedding and visualizing large-scale high-dimensional data in a\ntwo-dimensional space is an important problem since such visualization can\nreveal deep insights out of complex data. Most of the existing embedding\napproaches, however, run on an excessively high precision, ignoring the fact\nthat at the end, embedding outputs are converted into coarse-grained discrete\npixel coordinates in a screen space. Motivated by such an observation and\ndirectly considering pixel coordinates in an embedding optimization process, we\naccelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding\n(BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel\nmethod called PixelSNE, a highly-efficient, screen resolution-driven 2D\nembedding method with a linear computational complexity in terms of the number\nof data items. Our experimental results show the significantly fast running\ntime of PixelSNE by a large margin against BH-SNE, while maintaining the\nminimal degradation in the embedding quality. Finally, the source code of our\nmethod is publicly available at https:\/\/github.com\/awesome-davian\/PixelSNE\n","id":170}
{"Unnamed: 0.1":11171,"Unnamed: 0":11171.0,"anchor":"Riemannian Tensor Completion with Side Information","positive":"  By restricting the iterate on a nonlinear manifold, the recently proposed\nRiemannian optimization methods prove to be both efficient and effective in low\nrank tensor completion problems. However, existing methods fail to exploit the\neasily accessible side information, due to their format mismatch. Consequently,\nthere is still room for improvement in such methods. To fill the gap, in this\npaper, a novel Riemannian model is proposed to organically integrate the\noriginal model and the side information by overcoming their inconsistency. For\nthis particular model, an efficient Riemannian conjugate gradient descent\nsolver is devised based on a new metric that captures the curvature of the\nobjective.Numerical experiments suggest that our solver is more accurate than\nthe state-of-the-art without compromising the efficiency.\n","negative":"  The use of unsupervised data in addition to supervised data in training\ndiscriminative neural networks has improved the performance of this clas-\nsification scheme. However, the best results were achieved with a training\nprocess that is divided in two parts: first an unsupervised pre-training step\nis done for initializing the weights of the network and after these weights are\nrefined with the use of supervised data. On the other hand adversarial noise\nhas improved the results of clas- sical supervised learning. Recently, a new\nneural network topology called Ladder Network, where the key idea is based in\nsome properties of hierar- chichal latent variable models, has been proposed as\na technique to train a neural network using supervised and unsupervised data at\nthe same time with what is called semi-supervised learning. This technique has\nreached state of the art classification. In this work we add adversarial noise\nto the ladder network and get state of the art classification, with several\nimportant conclusions on how adversarial noise can help in addition with new\npossible lines of investi- gation. We also propose an alternative to add\nadversarial noise to unsu- pervised data.\n","id":171}
{"Unnamed: 0.1":11172,"Unnamed: 0":11172.0,"anchor":"Prognostics of Surgical Site Infections using Dynamic Health Data","positive":"  Surgical Site Infection (SSI) is a national priority in healthcare research.\nMuch research attention has been attracted to develop better SSI risk\nprediction models. However, most of the existing SSI risk prediction models are\nbuilt on static risk factors such as comorbidities and operative factors. In\nthis paper, we investigate the use of the dynamic wound data for SSI risk\nprediction. There have been emerging mobile health (mHealth) tools that can\nclosely monitor the patients and generate continuous measurements of many\nwound-related variables and other evolving clinical variables. Since existing\nprediction models of SSI have quite limited capacity to utilize the evolving\nclinical data, we develop the corresponding solution to equip these mHealth\ntools with decision-making capabilities for SSI prediction with a seamless\nassembly of several machine learning models to tackle the analytic challenges\narising from the spatial-temporal data. The basic idea is to exploit the\nlow-rank property of the spatial-temporal data via the bilinear formulation,\nand further enhance it with automatic missing data imputation by the matrix\ncompletion technique. We derive efficient optimization algorithms to implement\nthese models and demonstrate the superior performances of our new predictive\nmodel on a real-world dataset of SSI, compared to a range of state-of-the-art\nmethods.\n","negative":"  The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging.\n","id":172}
{"Unnamed: 0.1":11173,"Unnamed: 0":11173.0,"anchor":"GANS for Sequences of Discrete Elements with the Gumbel-softmax\n  Distribution","positive":"  Generative Adversarial Networks (GAN) have limitations when the goal is to\ngenerate sequences of discrete elements. The reason for this is that samples\nfrom a distribution on discrete objects such as the multinomial are not\ndifferentiable with respect to the distribution parameters. This problem can be\navoided by using the Gumbel-softmax distribution, which is a continuous\napproximation to a multinomial distribution parameterized in terms of the\nsoftmax function. In this work, we evaluate the performance of GANs based on\nrecurrent neural networks with Gumbel-softmax output distributions in the task\nof generating sequences of discrete elements.\n","negative":"  With the resurgence of interest in neural networks, representation learning\nhas re-emerged as a central focus in artificial intelligence. Representation\nlearning refers to the discovery of useful encodings of data that make\ndomain-relevant information explicit. Factorial representations identify\nunderlying independent causal factors of variation in data. A factorial\nrepresentation is compact and faithful, makes the causal factors explicit, and\nfacilitates human interpretation of data. Factorial representations support a\nvariety of applications, including the generation of novel examples, indexing\nand search, novelty detection, and transfer learning.\n  This article surveys various constraints that encourage a learning algorithm\nto discover factorial representations. I dichotomize the constraints in terms\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\nexploit assumptions about the environment, such as the statistical distribution\nof factor coefficients, assumptions about the perturbations a factor should be\ninvariant to (e.g. a representation of an object can be invariant to rotation,\ntranslation or scaling), and assumptions about how factors are combined to\nsynthesize an observation. Supervised inductive biases are constraints on the\nrepresentations based on additional information connected to observations.\nSupervisory labels come in variety of types, which vary in how strongly they\nconstrain the representation, how many factors are labeled, how many\nobservations are labeled, and whether or not we know the associations between\nthe constraints and the factors they are related to.\n  This survey brings together a wide variety of models that all touch on the\nproblem of learning factorial representations and lays out a framework for\ncomparing these models based on the strengths of the underlying supervised and\nunsupervised inductive biases.\n","id":173}
{"Unnamed: 0.1":11174,"Unnamed: 0":11174.0,"anchor":"Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly\n  Accelerated Dynamic Imaging","positive":"  Sparsity-based approaches have been popular in many applications in image\nprocessing and imaging. Compressed sensing exploits the sparsity of images in a\ntransform domain or dictionary to improve image recovery from undersampled\nmeasurements. In the context of inverse problems in dynamic imaging, recent\nresearch has demonstrated the promise of sparsity and low-rank techniques. For\nexample, the patches of the underlying data are modeled as sparse in an\nadaptive dictionary domain, and the resulting image and dictionary estimation\nfrom undersampled measurements is called dictionary-blind compressed sensing,\nor the dynamic image sequence is modeled as a sum of low-rank and sparse (in\nsome transform domain) components (L+S model) that are estimated from limited\nmeasurements. In this work, we investigate a data-adaptive extension of the L+S\nmodel, dubbed LASSI, where the temporal image sequence is decomposed into a\nlow-rank component and a component whose spatiotemporal (3D) patches are sparse\nin some adaptive dictionary domain. We investigate various formulations and\nefficient methods for jointly estimating the underlying dynamic signal\ncomponents and the spatiotemporal dictionary from limited measurements. We also\nobtain efficient sparsity penalized dictionary-blind compressed sensing methods\nas special cases of our LASSI approaches. Our numerical experiments demonstrate\nthe promising performance of LASSI schemes for dynamic magnetic resonance image\nreconstruction from limited k-t space data compared to recent methods such as\nk-t SLR and L+S, and compared to the proposed dictionary-blind compressed\nsensing method.\n","negative":"  By restricting the iterate on a nonlinear manifold, the recently proposed\nRiemannian optimization methods prove to be both efficient and effective in low\nrank tensor completion problems. However, existing methods fail to exploit the\neasily accessible side information, due to their format mismatch. Consequently,\nthere is still room for improvement in such methods. To fill the gap, in this\npaper, a novel Riemannian model is proposed to organically integrate the\noriginal model and the side information by overcoming their inconsistency. For\nthis particular model, an efficient Riemannian conjugate gradient descent\nsolver is devised based on a new metric that captures the curvature of the\nobjective.Numerical experiments suggest that our solver is more accurate than\nthe state-of-the-art without compromising the efficiency.\n","id":174}
{"Unnamed: 0.1":11175,"Unnamed: 0":11175.0,"anchor":"Batched Gaussian Process Bandit Optimization via Determinantal Point\n  Processes","positive":"  Gaussian Process bandit optimization has emerged as a powerful tool for\noptimizing noisy black box functions. One example in machine learning is\nhyper-parameter optimization where each evaluation of the target function\nrequires training a model which may involve days or even weeks of computation.\nMost methods for this so-called \"Bayesian optimization\" only allow sequential\nexploration of the parameter space. However, it is often desirable to propose\nbatches or sets of parameter values to explore simultaneously, especially when\nthere are large parallel processing facilities at our disposal. Batch methods\nrequire modeling the interaction between the different evaluations in the\nbatch, which can be expensive in complex scenarios. In this paper, we propose a\nnew approach for parallelizing Bayesian optimization by modeling the diversity\nof a batch via Determinantal point processes (DPPs) whose kernels are learned\nautomatically. This allows us to generalize a previous result as well as prove\nbetter regret bounds based on DPP sampling. Our experiments on a variety of\nsynthetic and real-world robotics and hyper-parameter optimization tasks\nindicate that our DPP-based methods, especially those based on DPP sampling,\noutperform state-of-the-art methods.\n","negative":"  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n","id":175}
{"Unnamed: 0.1":11176,"Unnamed: 0":11176.0,"anchor":"Accelerated Variance Reduced Block Coordinate Descent","positive":"  Algorithms with fast convergence, small number of data access, and low\nper-iteration complexity are particularly favorable in the big data era, due to\nthe demand for obtaining \\emph{highly accurate solutions} to problems with\n\\emph{a large number of samples} in \\emph{ultra-high} dimensional space.\nExisting algorithms lack at least one of these qualities, and thus are\ninefficient in handling such big data challenge. In this paper, we propose a\nmethod enjoying all these merits with an accelerated convergence rate\n$O(\\frac{1}{k^2})$. Empirical studies on large scale datasets with more than\none million features are conducted to show the effectiveness of our methods in\npractice.\n","negative":"  We propose a general approach for supervised learning with structured output\nspaces, such as combinatorial and polyhedral sets, that is based on minimizing\nestimated conditional risk functions. Given a loss function defined over pairs\nof output labels, we first estimate the conditional risk function by solving a\n(possibly infinite) collection of regularized least squares problems. A\nprediction is made by solving an inference problem that minimizes the estimated\nconditional risk function over the output space. We show that this approach\nenables, in some cases, efficient training and inference without explicitly\nintroducing a convex surrogate for the original loss function, even when it is\ndiscontinuous. Empirical evaluations on real-world and synthetic data sets\ndemonstrate the effectiveness of our method in adapting to a variety of loss\nfunctions.\n","id":176}
{"Unnamed: 0.1":11177,"Unnamed: 0":11177.0,"anchor":"Realistic risk-mitigating recommendations via inverse classification","positive":"  Inverse classification, the process of making meaningful perturbations to a\ntest point such that it is more likely to have a desired classification, has\npreviously been addressed using data from a single static point in time. Such\nan approach yields inflated probability estimates, stemming from an implicitly\nmade assumption that recommendations are implemented instantaneously. We\npropose using longitudinal data to alleviate such issues in two ways. First, we\nuse past outcome probabilities as features in the present. Use of such past\nprobabilities ties historical behavior to the present, allowing for more\ninformation to be taken into account when making initial probability estimates\nand subsequently performing inverse classification. Secondly, following inverse\nclassification application, optimized instances' unchangeable features\n(e.g.,~age) are updated using values from the next longitudinal time period.\nOptimized test instance probabilities are then reassessed. Updating the\nunchangeable features in this manner reflects the notion that improvements in\noutcome likelihood, which result from following the inverse classification\nrecommendations, do not materialize instantaneously. As our experiments\ndemonstrate, more realistic estimates of probability can be obtained by\nfactoring in such considerations.\n","negative":"  The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain.\n","id":177}
{"Unnamed: 0.1":11178,"Unnamed: 0":11178.0,"anchor":"CAD2RL: Real Single-Image Flight without a Single Real Image","positive":"  Deep reinforcement learning has emerged as a promising and powerful technique\nfor automatically acquiring control policies that can process raw sensory\ninputs, such as images, and perform complex behaviors. However, extending deep\nRL to real-world robotic tasks has proven challenging, particularly in\nsafety-critical domains such as autonomous flight, where a trial-and-error\nlearning process is often impractical. In this paper, we explore the following\nquestion: can we train vision-based navigation policies entirely in simulation,\nand then transfer them into the real world to achieve real-world flight without\na single real training image? We propose a learning method that we call\nCAD$^2$RL, which can be used to perform collision-free indoor flight in the\nreal world while being trained entirely on 3D CAD models. Our method uses\nsingle RGB images from a monocular camera, without needing to explicitly\nreconstruct the 3D geometry of the environment or perform explicit motion\nplanning. Our learned collision avoidance policy is represented by a deep\nconvolutional neural network that directly processes raw monocular images and\noutputs velocity commands. This policy is trained entirely on simulated images,\nwith a Monte Carlo policy evaluation algorithm that directly optimizes the\nnetwork's ability to produce collision-free flight. By highly randomizing the\nrendering settings for our simulated training set, we show that we can train a\npolicy that generalizes to the real world, without requiring the simulator to\nbe particularly realistic or high-fidelity. We evaluate our method by flying a\nreal quadrotor through indoor environments, and further evaluate the design\nchoices in our simulator through a series of ablation studies on depth\nprediction. For supplementary video see: https:\/\/youtu.be\/nXBWmzFrj5s\n","negative":"  It is argued that deep learning is efficient for data that is generated from\nhierarchal generative models. Examples of such generative models include\nwavelet scattering networks, functions of compositional structure, and deep\nrendering models. Unfortunately so far, for all such models, it is either not\nrigorously known that they can be learned efficiently, or it is not known that\n\"deep algorithms\" are required in order to learn them.\n  We propose a simple family of \"generative hierarchal models\" which can be\nefficiently learned and where \"deep\" algorithm are necessary for learning. Our\ndefinition of \"deep\" algorithms is based on the empirical observation that deep\nnets necessarily use correlations between features. More formally, we show that\nin a semi-supervised setting, given access to low-order moments of the labeled\ndata and all of the unlabeled data, it is information theoretically impossible\nto perform classification while at the same time there is an efficient\nalgorithm, that given all labelled and unlabeled data, perfectly labels all\nunlabelled data with high probability.\n  For the proof, we use and strengthen the fact that Belief Propagation does\nnot admit a good approximation in terms of linear functions.\n","id":178}
{"Unnamed: 0.1":11179,"Unnamed: 0":11179.0,"anchor":"Preference Completion from Partial Rankings","positive":"  We propose a novel and efficient algorithm for the collaborative preference\ncompletion problem, which involves jointly estimating individualized rankings\nfor a set of entities over a shared set of items, based on a limited number of\nobserved affinity values. Our approach exploits the observation that while\npreferences are often recorded as numerical scores, the predictive quantity of\ninterest is the underlying rankings. Thus, attempts to closely match the\nrecorded scores may lead to overfitting and impair generalization performance.\nInstead, we propose an estimator that directly fits the underlying preference\norder, combined with nuclear norm constraints to encourage low--rank\nparameters. Besides (approximate) correctness of the ranking order, the\nproposed estimator makes no generative assumption on the numerical scores of\nthe observations. One consequence is that the proposed estimator can fit any\nconsistent partial ranking over a subset of the items represented as a directed\nacyclic graph (DAG), generalizing standard techniques that can only fit\npreference scores. Despite this generality, for supervision representing total\nor blockwise total orders, the computational complexity of our algorithm is\nwithin a $\\log$ factor of the standard algorithms for nuclear norm\nregularization based estimates for matrix completion. We further show promising\nempirical results for a novel and challenging application of collaboratively\nranking of the associations between brain--regions and cognitive neuroscience\nterms.\n","negative":"  We devise the Unit Commitment Nearest Neighbor (UCNN) algorithm to be used as\na proxy for quickly approximating outcomes of short-term decisions, to make\ntractable hierarchical long-term assessment and planning for large power\nsystems. Experimental results on updated versions of IEEE-RTS79 and IEEE-RTS96\nshow high accuracy measured on operational cost, achieved in runtimes that are\nlower in several orders of magnitude than the traditional approach.\n","id":179}
{"Unnamed: 0.1":11180,"Unnamed: 0":11180.0,"anchor":"Learning Sparse, Distributed Representations using the Hebbian Principle","positive":"  The \"fire together, wire together\" Hebbian model is a central principle for\nlearning in neuroscience, but surprisingly, it has found limited applicability\nin modern machine learning. In this paper, we take a first step towards\nbridging this gap, by developing flavors of competitive Hebbian learning which\nproduce sparse, distributed neural codes using online adaptation with minimal\ntuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning\n(AHL). We illustrate the distributed nature of the learned representations via\noutput entropy computations for synthetic data, and demonstrate superior\nperformance, compared to standard alternatives such as autoencoders, in\ntraining a deep convolutional net on standard image datasets.\n","negative":"  We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps:\/\/visualdialog.org\n","id":180}
{"Unnamed: 0.1":11181,"Unnamed: 0":11181.0,"anchor":"Identity Matters in Deep Learning","positive":"  An emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks.\n","negative":"  Training time on large datasets for deep neural networks is the principal\nworkflow bottleneck in a number of important applications of deep learning,\nsuch as object classification and detection in automatic driver assistance\nsystems (ADAS). To minimize training time, the training of a deep neural\nnetwork must be scaled beyond a single machine to as many machines as possible\nby distributing the optimization method used for training. While a number of\napproaches have been proposed for distributed stochastic gradient descent\n(SGD), at the current time synchronous approaches to distributed SGD appear to\nbe showing the greatest performance at large scale. Synchronous scaling of SGD\nsuffers from the need to synchronize all processors on each gradient step and\nis not resilient in the face of failing or lagging processors. In asynchronous\napproaches using parameter servers, training is slowed by contention to the\nparameter server. In this paper we compare the convergence of synchronous and\nasynchronous SGD for training a modern ResNet network architecture on the\nImageNet classification problem. We also propose an asynchronous method,\ngossiping SGD, that aims to retain the positive features of both systems by\nreplacing the all-reduce collective operation of synchronous training with a\ngossip aggregation algorithm. We find, perhaps counterintuitively, that\nasynchronous SGD, including both elastic averaging and gossiping, converges\nfaster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales\nbetter to more nodes (up to about 100 nodes).\n","id":181}
{"Unnamed: 0.1":11182,"Unnamed: 0":11182.0,"anchor":"On the Quantitative Analysis of Decoder-Based Generative Models","positive":"  The past several years have seen remarkable progress in generative models\nwhich produce convincing samples of images and other modalities. A shared\ncomponent of many powerful generative models is a decoder network, a parametric\ndeep neural net that defines a generative distribution. Examples include\nvariational autoencoders, generative adversarial networks, and generative\nmoment matching networks. Unfortunately, it can be difficult to quantify the\nperformance of these models because of the intractability of log-likelihood\nestimation, and inspecting samples can be misleading. We propose to use\nAnnealed Importance Sampling for evaluating log-likelihoods for decoder-based\nmodels and validate its accuracy using bidirectional Monte Carlo. The\nevaluation code is provided at https:\/\/github.com\/tonywu95\/eval_gen. Using this\ntechnique, we analyze the performance of decoder-based models, the\neffectiveness of existing log-likelihood estimators, the degree of overfitting,\nand the degree to which these models miss important modes of the data\ndistribution.\n","negative":"  Recommendation plays an increasingly important role in our daily lives.\nRecommender systems automatically suggest items to users that might be\ninteresting for them. Recent studies illustrate that incorporating social trust\nin Matrix Factorization methods demonstrably improves accuracy of rating\nprediction. Such approaches mainly use the trust scores explicitly expressed by\nusers. However, it is often challenging to have users provide explicit trust\nscores of each other. There exist quite a few works, which propose Trust\nMetrics to compute and predict trust scores between users based on their\ninteractions. In this paper, first we present how social relation can be\nextracted from users' ratings to items by describing Hellinger distance between\nusers in recommender systems. Then, we propose to incorporate the predicted\ntrust scores into social matrix factorization models. By analyzing social\nrelation extraction from three well-known real-world datasets, which both:\ntrust and recommendation data available, we conclude that using the implicit\nsocial relation in social recommendation techniques has almost the same\nperformance compared to the actual trust scores explicitly expressed by users.\nHence, we build our method, called Hell-TrustSVD, on top of the\nstate-of-the-art social recommendation technique to incorporate both the\nextracted implicit social relations and ratings given by users on the\nprediction of items for an active user. To the best of our knowledge, this is\nthe first work to extend TrustSVD with extracted social trust information. The\nexperimental results support the idea of employing implicit trust into matrix\nfactorization whenever explicit trust is not available, can perform much better\nthan the state-of-the-art approaches in user rating prediction.\n","id":182}
{"Unnamed: 0.1":11183,"Unnamed: 0":11183.0,"anchor":"Attending to Characters in Neural Sequence Labeling Models","positive":"  Sequence labeling architectures use word embeddings for capturing similarity,\nbut suffer when handling previously unseen or rare words. We investigate\ncharacter-level extensions to such models and propose a novel architecture for\ncombining alternative word representations. By using an attention mechanism,\nthe model is able to dynamically decide how much information to use from a\nword- or character-level component. We evaluated different architectures on a\nrange of sequence labeling datasets, and character-level extensions were found\nto improve performance on every benchmark. In addition, the proposed\nattention-based architecture delivered the best results even with a smaller\nnumber of trainable parameters.\n","negative":"  We consider the setting in which an electric power utility seeks to curtail\nits peak electricity demand by offering a fixed group of customers a uniform\nprice for reductions in consumption relative to their predetermined baselines.\nThe underlying demand curve, which describes the aggregate reduction in\nconsumption in response to the offered price, is assumed to be affine and\nsubject to unobservable random shocks. Assuming that both the parameters of the\ndemand curve and the distribution of the random shocks are initially unknown to\nthe utility, we investigate the extent to which the utility might dynamically\nadjust its offered prices to maximize its cumulative risk-sensitive payoff over\na finite number of $T$ days. In order to do so effectively, the utility must\ndesign its pricing policy to balance the tradeoff between the need to learn the\nunknown demand model (exploration) and maximize its payoff (exploitation) over\ntime. In this paper, we propose such a pricing policy, which is shown to\nexhibit an expected payoff loss over $T$ days that is at most\n$O(\\sqrt{T}\\log(T))$, relative to an oracle pricing policy that knows the\nunderlying demand model. Moreover, the proposed pricing policy is shown to\nyield a sequence of prices that converge to the oracle optimal prices in the\nmean square sense.\n","id":183}
{"Unnamed: 0.1":11184,"Unnamed: 0":11184.0,"anchor":"On numerical approximation schemes for expectation propagation","positive":"  Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments.\n","negative":"  We consider a large dataset of real-world, on-road driving from a 100-car\nnaturalistic study to explore the predictive power of driver glances and,\nspecifically, to answer the following question: what can be predicted about the\nstate of the driver and the state of the driving environment from a 6-second\nsequence of macro-glances? The context-based nature of such glances allows for\napplication of supervised learning to the problem of vision-based gaze\nestimation, making it robust, accurate, and reliable in messy, real-world\nconditions. So, it's valuable to ask whether such macro-glances can be used to\ninfer behavioral, environmental, and demographic variables? We analyze 27\nbinary classification problems based on these variables. The takeaway is that\nglance can be used as part of a multi-sensor real-time system to predict\nradio-tuning, fatigue state, failure to signal, talking, and several\nenvironment variables.\n","id":184}
{"Unnamed: 0.1":11185,"Unnamed: 0":11185.0,"anchor":"Generative Models and Model Criticism via Optimized Maximum Mean\n  Discrepancy","positive":"  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n","negative":"  Ensuring transportation systems are efficient is a priority for modern\nsociety. Technological advances have made it possible for transportation\nsystems to collect large volumes of varied data on an unprecedented scale. We\npropose a traffic signal control system which takes advantage of this new, high\nquality data, with minimal abstraction compared to other proposed systems. We\napply modern deep reinforcement learning methods to build a truly adaptive\ntraffic signal control agent in the traffic microsimulator SUMO. We propose a\nnew state space, the discrete traffic state encoding, which is information\ndense. The discrete traffic state encoding is used as input to a deep\nconvolutional neural network, trained using Q-learning with experience replay.\nOur agent was compared against a one hidden layer neural network traffic signal\ncontrol agent and reduces average cumulative delay by 82%, average queue length\nby 66% and average travel time by 20%.\n","id":185}
{"Unnamed: 0.1":11186,"Unnamed: 0":11186.0,"anchor":"Post Training in Deep Learning with Last Kernel","positive":"  One of the main challenges of deep learning methods is the choice of an\nappropriate training strategy. In particular, additional steps, such as\nunsupervised pre-training, have been shown to greatly improve the performances\nof deep structures. In this article, we propose an extra training step, called\npost-training, which only optimizes the last layer of the network. We show that\nthis procedure can be analyzed in the context of kernel theory, with the first\nlayers computing an embedding of the data and the last layer a statistical\nmodel to solve the task based on this embedding. This step makes sure that the\nembedding, or representation, of the data is used in the best possible way for\nthe considered task. This idea is then tested on multiple architectures with\nvarious data sets, showing that it consistently provides a boost in\nperformance.\n","negative":"  We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.\n","id":186}
{"Unnamed: 0.1":11187,"Unnamed: 0":11187.0,"anchor":"Deep Learning with Sets and Point Clouds","positive":"  We introduce a simple permutation equivariant layer for deep learning with\nset structure.This type of layer, obtained by parameter-sharing, has a simple\nimplementation and linear-time complexity in the size of each set. We use deep\npermutation-invariant networks to perform point-could classification and\nMNIST-digit summation, where in both cases the output is invariant to\npermutations of the input. In a semi-supervised setting, where the goal is make\npredictions for each instance within a set, we demonstrate the usefulness of\nthis type of layer in set-outlier detection as well as semi-supervised learning\nwith clustering side-information.\n","negative":"  To ease the development of robot learning in industry, two conditions need to\nbe fulfilled. Manipulators must be able to learn high accuracy and precision\ntasks while being safe for workers in the factory. In this paper, we extend\npreviously submitted work which consists in rapid learning of local high\naccuracy behaviors. By exploration and regression, linear and quadratic models\nare learnt for respectively the dynamics and cost function. Iterative Linear\nQuadratic Gaussian Regulator combined with cost quadratic regression can\nconverge rapidly in the final stages towards high accuracy behavior as the cost\nfunction is modelled quite precisely. In this paper, both a different cost\nfunction and a second order improvement method are implemented within this\nframework. We also propose an analysis of the algorithm parameters through\nsimulation for a positioning task. Finally, an experimental validation on a\nKUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be\neasily programmed into safety mode, which makes it qualified for the second\nindustry constraint stated above.\n","id":187}
{"Unnamed: 0.1":11188,"Unnamed: 0":11188.0,"anchor":"Normalizing the Normalizers: Comparing and Extending Network\n  Normalization Schemes","positive":"  Normalization techniques have only recently begun to be exploited in\nsupervised learning tasks. Batch normalization exploits mini-batch statistics\nto normalize the activations. This was shown to speed up training and result in\nbetter models. However its success has been very limited when dealing with\nrecurrent neural networks. On the other hand, layer normalization normalizes\nthe activations across all activities within a layer. This was shown to work\nwell in the recurrent setting. In this paper we propose a unified view of\nnormalization techniques, as forms of divisive normalization, which includes\nlayer and batch normalization as special cases. Our second contribution is the\nfinding that a small modification to these normalization schemes, in\nconjunction with a sparse regularizer on the activations, leads to significant\nbenefits over standard normalization techniques. We demonstrate the\neffectiveness of our unified divisive normalization framework in the context of\nconvolutional neural nets and recurrent neural networks, showing improvements\nover baselines in image classification, language modeling as well as\nsuper-resolution.\n","negative":"  Group-Lasso (gLasso) identifies important explanatory factors in predicting\nthe response variable by considering the grouping structure over input\nvariables. However, most existing algorithms for gLasso are not scalable to\ndeal with large-scale datasets, which are becoming a norm in many applications.\nIn this paper, we present a divide-and-conquer based parallel algorithm\n(DC-gLasso) to scale up gLasso in the tasks of regression with grouping\nstructures. DC-gLasso only needs two iterations to collect and aggregate the\nlocal estimates on subsets of the data, and is provably correct to recover the\ntrue model under certain conditions. We further extend it to deal with\noverlappings between groups. Empirical results on a wide range of synthetic and\nreal-world datasets show that DC-gLasso can significantly improve the time\nefficiency without sacrificing regression accuracy.\n","id":188}
{"Unnamed: 0.1":11189,"Unnamed: 0":11189.0,"anchor":"Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann\n  Machines","positive":"  Quantum annealing (QA) is a hardware-based heuristic optimization and\nsampling method applicable to discrete undirected graphical models. While\nsimilar to simulated annealing, QA relies on quantum, rather than thermal,\neffects to explore complex search spaces. For many classes of problems, QA is\nknown to offer computational advantages over simulated annealing. Here we\nreport on the ability of recent QA hardware to accelerate training of fully\nvisible Boltzmann machines. We characterize the sampling distribution of QA\nhardware, and show that in many cases, the quantum distributions differ\nsignificantly from classical Boltzmann distributions. In spite of this\ndifference, training (which seeks to match data and model statistics) using\nstandard classical gradient updates is still effective. We investigate the use\nof QA for seeding Markov chains as an alternative to contrastive divergence\n(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we\nshow that for problems with high-energy barriers between modes, QA-based seeds\ncan improve upon chains with CD and PCD initializations. For these hard\nproblems, QA gradient estimates are more accurate, and allow for faster\nlearning. Furthermore, and interestingly, even the case of raw QA samples (that\nis, $k=0$) achieved similar improvements. We argue that this relates to the\nfact that we are training a quantum rather than classical Boltzmann\ndistribution in this case. The learned parameters give rise to hardware QA\ndistributions closely approximating classical Boltzmann distributions that are\nhard to train with CD\/PCD.\n","negative":"  Recently, it was shown that if multiplicative weights are assigned to the\nedges of a Tanner graph used in belief propagation decoding, it is possible to\nuse deep learning techniques to find values for the weights which improve the\nerror-correction performance of the decoder. Unfortunately, this approach\nrequires many multiplications, which are generally expensive operations. In\nthis paper, we suggest a more hardware-friendly approach in which offset\nmin-sum decoding is augmented with learnable offset parameters. Our method uses\nno multiplications and has a parameter count less than half that of the\nmultiplicative algorithm. This both speeds up training and provides a feasible\npath to hardware architectures. After describing our method, we compare the\nperformance of the two neural decoding algorithms and show that our method\nachieves error-correction performance within 0.1 dB of the multiplicative\napproach and as much as 1 dB better than traditional belief propagation for the\ncodes under consideration.\n","id":189}
{"Unnamed: 0.1":11190,"Unnamed: 0":11190.0,"anchor":"Learning-Theoretic Foundations of Algorithm Configuration for\n  Combinatorial Partitioning Problems","positive":"  Max-cut, clustering, and many other partitioning problems that are of\nsignificant importance to machine learning and other scientific fields are\nNP-hard, a reality that has motivated researchers to develop a wealth of\napproximation algorithms and heuristics. Although the best algorithm to use\ntypically depends on the specific application domain, a worst-case analysis is\noften used to compare algorithms. This may be misleading if worst-case\ninstances occur infrequently, and thus there is a demand for optimization\nmethods which return the algorithm configuration best suited for the given\napplication's typical inputs. We address this problem for clustering, max-cut,\nand other partitioning problems, such as integer quadratic programming, by\ndesigning computationally efficient and sample efficient learning algorithms\nwhich receive samples from an application-specific distribution over problem\ninstances and learn a partitioning algorithm with high expected performance.\nOur algorithms learn over common integer quadratic programming and clustering\nalgorithm families: SDP rounding algorithms and agglomerative clustering\nalgorithms with dynamic programming. For our sample complexity analysis, we\nprovide tight bounds on the pseudodimension of these algorithm classes, and\nshow that surprisingly, even for classes of algorithms parameterized by a\nsingle parameter, the pseudo-dimension is superconstant. In this way, our work\nboth contributes to the foundations of algorithm configuration and pushes the\nboundaries of learning theory, since the algorithm classes we analyze consist\nof multi-stage optimization procedures and are significantly more complex than\nclasses typically studied in learning theory.\n","negative":"  Fuzzing consists of repeatedly testing an application with modified, or\nfuzzed, inputs with the goal of finding security vulnerabilities in\ninput-parsing code. In this paper, we show how to automate the generation of an\ninput grammar suitable for input fuzzing using sample inputs and\nneural-network-based statistical machine-learning techniques. We present a\ndetailed case study with a complex input format, namely PDF, and a large\ncomplex security-critical parser for this format, namely, the PDF parser\nembedded in Microsoft's new Edge browser. We discuss (and measure) the tension\nbetween conflicting learning and fuzzing goals: learning wants to capture the\nstructure of well-formed inputs, while fuzzing wants to break that structure in\norder to cover unexpected code paths and find bugs. We also present a new\nalgorithm for this learn&fuzz challenge which uses a learnt input probability\ndistribution to intelligently guide where to fuzz inputs.\n","id":190}
{"Unnamed: 0.1":11191,"Unnamed: 0":11191.0,"anchor":"Splitting matters: how monotone transformation of predictor variables\n  may improve the predictions of decision tree models","positive":"  It is widely believed that the prediction accuracy of decision tree models is\ninvariant under any strictly monotone transformation of the individual\npredictor variables. However, this statement may be false when predicting new\nobservations with values that were not seen in the training-set and are close\nto the location of the split point of a tree rule. The sensitivity of the\nprediction error to the split point interpolation is high when the split point\nof the tree is estimated based on very few observations, reaching 9%\nmisclassification error when only 10 observations are used for constructing a\nsplit, and shrinking to 1% when relying on 100 observations. This study\ncompares the performance of alternative methods for split point interpolation\nand concludes that the best choice is taking the mid-point between the two\nclosest points to the split point of the tree. Furthermore, if the (continuous)\ndistribution of the predictor variable is known, then using its probability\nintegral for transforming the variable (\"quantile transformation\") will reduce\nthe model's interpolation error by up to about a half on average. Accordingly,\nthis study provides guidelines for both developers and users of decision tree\nmodels (including bagging and random forest).\n","negative":"  We describe a neural attention model with a learnable retinal sampling\nlattice. The model is trained on a visual search task requiring the\nclassification of an object embedded in a visual scene amidst background\ndistractors using the smallest number of fixations. We explore the tiling\nproperties that emerge in the model's retinal sampling lattice after training.\nSpecifically, we show that this lattice resembles the eccentricity dependent\nsampling lattice of the primate retina, with a high resolution region in the\nfovea surrounded by a low resolution periphery. Furthermore, we find conditions\nwhere these emergent properties are amplified or eliminated providing clues to\ntheir function.\n","id":191}
{"Unnamed: 0.1":11192,"Unnamed: 0":11192.0,"anchor":"Earliness-Aware Deep Convolutional Networks for Early Time Series\n  Classification","positive":"  We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an\nend-to-end deep learning framework, for early classification of time series\ndata. Unlike most existing methods for early classification of time series\ndata, that are designed to solve this problem under the assumption of the\navailability of a good set of pre-defined (often hand-crafted) features, our\nframework can jointly perform feature learning (by learning a deep hierarchy of\n\\emph{shapelets} capturing the salient characteristics in each time series),\nalong with a dynamic truncation model to help our deep feature learning\narchitecture focus on the early parts of each time series. Consequently, our\nframework is able to make highly reliable early predictions, outperforming\nvarious state-of-the-art methods for early time series classification, while\nalso being competitive when compared to the state-of-the-art time series\nclassification algorithms that work with \\emph{fully observed} time series\ndata. To the best of our knowledge, the proposed framework is the first to\nperform data-driven (deep) feature learning in the context of early\nclassification of time series data. We perform a comprehensive set of\nexperiments, on several benchmark data sets, which demonstrate that our method\nyields significantly better predictions than various state-of-the-art methods\ndesigned for early time series classification. In addition to obtaining high\naccuracies, our experiments also show that the learned deep shapelets based\nfeatures are also highly interpretable and can help gain better understanding\nof the underlying characteristics of time series data.\n","negative":"  In this work, we study the use of logistic regression in manufacturing\nfailures detection. As a data set for the analysis, we used the data from\nKaggle competition Bosch Production Line Performance. We considered the use of\nmachine learning, linear and Bayesian models. For machine learning approach, we\nanalyzed XGBoost tree based classifier to obtain high scored classification.\nUsing the generalized linear model for logistic regression makes it possible to\nanalyze the influence of the factors under study. The Bayesian approach for\nlogistic regression gives the statistical distribution for the parameters of\nthe model. It can be useful in the probabilistic analysis, e.g. risk\nassessment.\n","id":192}
{"Unnamed: 0.1":11193,"Unnamed: 0":11193.0,"anchor":"How to scale distributed deep learning?","positive":"  Training time on large datasets for deep neural networks is the principal\nworkflow bottleneck in a number of important applications of deep learning,\nsuch as object classification and detection in automatic driver assistance\nsystems (ADAS). To minimize training time, the training of a deep neural\nnetwork must be scaled beyond a single machine to as many machines as possible\nby distributing the optimization method used for training. While a number of\napproaches have been proposed for distributed stochastic gradient descent\n(SGD), at the current time synchronous approaches to distributed SGD appear to\nbe showing the greatest performance at large scale. Synchronous scaling of SGD\nsuffers from the need to synchronize all processors on each gradient step and\nis not resilient in the face of failing or lagging processors. In asynchronous\napproaches using parameter servers, training is slowed by contention to the\nparameter server. In this paper we compare the convergence of synchronous and\nasynchronous SGD for training a modern ResNet network architecture on the\nImageNet classification problem. We also propose an asynchronous method,\ngossiping SGD, that aims to retain the positive features of both systems by\nreplacing the all-reduce collective operation of synchronous training with a\ngossip aggregation algorithm. We find, perhaps counterintuitively, that\nasynchronous SGD, including both elastic averaging and gossiping, converges\nfaster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales\nbetter to more nodes (up to about 100 nodes).\n","negative":"  The widespread availability of electronic health records (EHRs) promises to\nusher in the era of personalized medicine. However, the problem of extracting\nuseful clinical representations from longitudinal EHR data remains challenging.\nIn this paper, we explore deep neural network models with learned medical\nfeature embedding to deal with the problems of high dimensionality and\ntemporality. Specifically, we use a multi-layer convolutional neural network\n(CNN) to parameterize the model and is thus able to capture complex non-linear\nlongitudinal evolution of EHRs. Our model can effectively capture local\/short\ntemporal dependency in EHRs, which is beneficial for risk prediction. To\naccount for high dimensionality, we use the embedding medical features in the\nCNN model which hold the natural medical concepts. Our initial experiments\nproduce promising results and demonstrate the effectiveness of both the medical\nfeature embedding and the proposed convolutional neural network in risk\nprediction on cohorts of congestive heart failure and diabetes patients\ncompared with several strong baselines.\n","id":193}
{"Unnamed: 0.1":11194,"Unnamed: 0":11194.0,"anchor":"Link Prediction using Embedded Knowledge Graphs","positive":"  Since large knowledge bases are typically incomplete, missing facts need to\nbe inferred from observed facts in a task called knowledge base completion. The\nmost successful approaches to this task have typically explored explicit paths\nthrough sequences of triples. These approaches have usually resorted to\nhuman-designed sampling procedures, since large knowledge graphs produce\nprohibitively large numbers of possible paths, most of which are uninformative.\nAs an alternative approach, we propose performing a single, short sequence of\ninteractive lookup operations on an embedded knowledge graph which has been\ntrained through end-to-end backpropagation to be an optimized and compressed\nversion of the initial knowledge base. Our proposed model, called Embedded\nKnowledge Graph Network (EKGN), achieves new state-of-the-art results on\npopular knowledge base completion benchmarks.\n","negative":"  We introduce the variational graph auto-encoder (VGAE), a framework for\nunsupervised learning on graph-structured data based on the variational\nauto-encoder (VAE). This model makes use of latent variables and is capable of\nlearning interpretable latent representations for undirected graphs. We\ndemonstrate this model using a graph convolutional network (GCN) encoder and a\nsimple inner product decoder. Our model achieves competitive results on a link\nprediction task in citation networks. In contrast to most existing models for\nunsupervised learning on graph-structured data and link prediction, our model\ncan naturally incorporate node features, which significantly improves\npredictive performance on a number of benchmark datasets.\n","id":194}
{"Unnamed: 0.1":11195,"Unnamed: 0":11195.0,"anchor":"A Generic Coordinate Descent Framework for Learning from Implicit\n  Feedback","positive":"  In recent years, interest in recommender research has shifted from explicit\nfeedback towards implicit feedback data. A diversity of complex models has been\nproposed for a wide variety of applications. Despite this, learning from\nimplicit feedback is still computationally challenging. So far, most work\nrelies on stochastic gradient descent (SGD) solvers which are easy to derive,\nbut in practice challenging to apply, especially for tasks with many items. For\nthe simple matrix factorization model, an efficient coordinate descent (CD)\nsolver has been previously proposed. However, efficient CD approaches have not\nbeen derived for more complex models.\n  In this paper, we provide a new framework for deriving efficient CD\nalgorithms for complex recommender models. We identify and introduce the\nproperty of k-separable models. We show that k-separability is a sufficient\nproperty to allow efficient optimization of implicit recommender problems with\nCD. We illustrate this framework on a variety of state-of-the-art models\nincluding factorization machines and Tucker decomposition. To summarize, our\nwork provides the theory and building blocks to derive efficient implicit CD\nalgorithms for complex recommender models.\n","negative":"  Mobile sensing applications usually require time-series inputs from sensors.\nSome applications, such as tracking, can use sensed acceleration and rate of\nrotation to calculate displacement based on physical system models. Other\napplications, such as activity recognition, extract manually designed features\nfrom sensor inputs for classification. Such applications face two challenges.\nOn one hand, on-device sensor measurements are noisy. For many mobile\napplications, it is hard to find a distribution that exactly describes the\nnoise in practice. Unfortunately, calculating target quantities based on\nphysical system and noise models is only as accurate as the noise assumptions.\nSimilarly, in classification applications, although manually designed features\nhave proven to be effective, it is not always straightforward to find the most\nrobust features to accommodate diverse sensor noise patterns and user\nbehaviors. To this end, we propose DeepSense, a deep learning framework that\ndirectly addresses the aforementioned noise and feature customization\nchallenges in a unified manner. DeepSense integrates convolutional and\nrecurrent neural networks to exploit local interactions among similar mobile\nsensors, merge local interactions of different sensory modalities into global\ninteractions, and extract temporal relationships to model signal dynamics.\nDeepSense thus provides a general signal estimation and classification\nframework that accommodates a wide range of applications. We demonstrate the\neffectiveness of DeepSense using three representative and challenging tasks:\ncar tracking with motion sensors, heterogeneous human activity recognition, and\nuser identification with biometric motion analysis. DeepSense significantly\noutperforms the state-of-the-art methods for all three tasks. In addition,\nDeepSense is feasible to implement on smartphones due to its moderate energy\nconsumption and low latency\n","id":195}
{"Unnamed: 0.1":11196,"Unnamed: 0":11196.0,"anchor":"Robust Matrix Regression","positive":"  Modern technologies are producing datasets with complex intrinsic structures,\nand they can be naturally represented as matrices instead of vectors. To\npreserve the latent data structures during processing, modern regression\napproaches incorporate the low-rank property to the model and achieve\nsatisfactory performance for certain applications. These approaches all assume\nthat both predictors and labels for each pair of data within the training set\nare accurate. However, in real-world applications, it is common to see the\ntraining data contaminated by noises, which can affect the robustness of these\nmatrix regression methods. In this paper, we address this issue by introducing\na novel robust matrix regression method. We also derive efficient proximal\nalgorithms for model training. To evaluate the performance of our methods, we\napply it to real world applications with comparative studies. Our method\nachieves the state-of-the-art performance, which shows the effectiveness and\nthe practical value of our method.\n","negative":"  Graphs are fundamental mathematical structures used in various fields to\nrepresent data, signals and processes. In this paper, we propose a novel\nframework for learning\/estimating graphs from data. The proposed framework\nincludes (i) formulation of various graph learning problems, (ii) their\nprobabilistic interpretations and (iii) associated algorithms. Specifically,\ngraph learning problems are posed as estimation of graph Laplacian matrices\nfrom some observed data under given structural constraints (e.g., graph\nconnectivity and sparsity level). From a probabilistic perspective, the\nproblems of interest correspond to maximum a posteriori (MAP) parameter\nestimation of Gaussian-Markov random field (GMRF) models, whose precision\n(inverse covariance) is a graph Laplacian matrix. For the proposed graph\nlearning problems, specialized algorithms are developed by incorporating the\ngraph Laplacian and structural constraints. The experimental results\ndemonstrate that the proposed algorithms outperform the current\nstate-of-the-art methods in terms of accuracy and computational efficiency.\n","id":196}
{"Unnamed: 0.1":11197,"Unnamed: 0":11197.0,"anchor":"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement\n  Learning","positive":"  Count-based exploration algorithms are known to perform near-optimally when\nused in conjunction with tabular reinforcement learning (RL) methods for\nsolving small discrete Markov decision processes (MDPs). It is generally\nthought that count-based methods cannot be applied in high-dimensional state\nspaces, since most states will only occur once. Recent deep RL exploration\nstrategies are able to deal with high-dimensional continuous state spaces\nthrough complex heuristics, often relying on optimism in the face of\nuncertainty or intrinsic motivation. In this work, we describe a surprising\nfinding: a simple generalization of the classic count-based approach can reach\nnear state-of-the-art performance on various high-dimensional and\/or continuous\ndeep RL benchmarks. States are mapped to hash codes, which allows to count\ntheir occurrences with a hash table. These counts are then used to compute a\nreward bonus according to the classic count-based exploration theory. We find\nthat simple hash functions can achieve surprisingly good results on many\nchallenging tasks. Furthermore, we show that a domain-dependent learned hash\ncode may further improve these results. Detailed analysis reveals important\naspects of a good hash function: 1) having appropriate granularity and 2)\nencoding information relevant to solving the MDP. This exploration strategy\nachieves near state-of-the-art performance on both continuous control tasks and\nAtari 2600 games, hence providing a simple yet powerful baseline for solving\nMDPs that require considerable exploration.\n","negative":"  Owing to their low-complexity iterations, Frank-Wolfe (FW) solvers are well\nsuited for various large-scale learning tasks. When block-separable constraints\nare present, randomized block FW (RB-FW) has been shown to further reduce\ncomplexity by updating only a fraction of coordinate blocks per iteration. To\ncircumvent the limitations of existing methods, the present work develops step\nsizes for RB-FW that enable a flexible selection of the number of blocks to\nupdate per iteration while ensuring convergence and feasibility of the\niterates. To this end, convergence rates of RB-FW are established through\ncomputational bounds on a primal sub-optimality measure and on the duality gap.\nThe novel bounds extend the existing convergence analysis, which only applies\nto a step-size sequence that does not generally lead to feasible iterates.\nFurthermore, two classes of step-size sequences that guarantee feasibility of\nthe iterates are also proposed to enhance flexibility in choosing decay rates.\nThe novel convergence results are markedly broadened to encompass also\nnonconvex objectives, and further assert that RB-FW with exact line-search\nreaches a stationary point at rate $\\mathcal{O}(1\/\\sqrt{t})$. Performance of\nRB-FW with different step sizes and number of blocks is demonstrated in two\napplications, namely charging of electrical vehicles and structural support\nvector machines. Extensive simulated tests demonstrate the performance\nimprovement of RB-FW relative to existing randomized single-block FW methods.\n","id":197}
{"Unnamed: 0.1":11198,"Unnamed: 0":11198.0,"anchor":"AdversariaLib: An Open-source Library for the Security Evaluation of\n  Machine Learning Algorithms Under Attack","positive":"  We present AdversariaLib, an open-source python library for the security\nevaluation of machine learning (ML) against carefully-targeted attacks. It\nsupports the implementation of several attacks proposed thus far in the\nliterature of adversarial learning, allows for the evaluation of a wide range\nof ML algorithms, runs on multiple platforms, and has multi-processing enabled.\nThe library has a modular architecture that makes it easy to use and to extend\nby implementing novel attacks and countermeasures. It relies on other\nwidely-used open-source ML libraries, including scikit-learn and FANN.\nClassification algorithms are implemented and optimized in C\/C++, allowing for\na fast evaluation of the simulated attacks. The package is distributed under\nthe GNU General Public License v3, and it is available for download at\nhttp:\/\/sourceforge.net\/projects\/adversarialib.\n","negative":"  This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives.\n","id":198}
{"Unnamed: 0.1":11199,"Unnamed: 0":11199.0,"anchor":"The Power of Normalization: Faster Evasion of Saddle Points","positive":"  A commonly used heuristic in non-convex optimization is Normalized Gradient\nDescent (NGD) - a variant of gradient descent in which only the direction of\nthe gradient is taken into account and its magnitude ignored. We analyze this\nheuristic and show that with carefully chosen parameters and noise injection,\nthis method can provably evade saddle points. We establish the convergence of\nNGD to a local minimum, and demonstrate rates which improve upon the fastest\nknown first order algorithm due to Ge e al. (2015).\n  The effectiveness of our method is demonstrated via an application to the\nproblem of online tensor decomposition; a task for which saddle point evasion\nis known to result in convergence to global minima.\n","negative":"  Since large knowledge bases are typically incomplete, missing facts need to\nbe inferred from observed facts in a task called knowledge base completion. The\nmost successful approaches to this task have typically explored explicit paths\nthrough sequences of triples. These approaches have usually resorted to\nhuman-designed sampling procedures, since large knowledge graphs produce\nprohibitively large numbers of possible paths, most of which are uninformative.\nAs an alternative approach, we propose performing a single, short sequence of\ninteractive lookup operations on an embedded knowledge graph which has been\ntrained through end-to-end backpropagation to be an optimized and compressed\nversion of the initial knowledge base. Our proposed model, called Embedded\nKnowledge Graph Network (EKGN), achieves new state-of-the-art results on\npopular knowledge base completion benchmarks.\n","id":199}
{"Unnamed: 0.1":11200,"Unnamed: 0":11200.0,"anchor":"Multilinear Low-Rank Tensors on Graphs & Applications","positive":"  We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework.\n","negative":"  We revisit the notion of probably approximately correct implication bases\nfrom the literature and present a first formulation in the language of formal\nconcept analysis, with the goal to investigate whether such bases represent a\nsuitable substitute for exact implication bases in practical use-cases. To this\nend, we quantitatively examine the behavior of probably approximately correct\nimplication bases on artificial and real-world data sets and compare their\nprecision and recall with respect to their corresponding exact implication\nbases. Using a small example, we also provide qualitative insight that\nimplications from probably approximately correct bases can still represent\nmeaningful knowledge from a given data set.\n","id":200}
{"Unnamed: 0.1":11201,"Unnamed: 0":11201.0,"anchor":"The Power of Side-information in Subgraph Detection","positive":"  In this work, we tackle the problem of hidden community detection. We\nconsider Belief Propagation (BP) applied to the problem of detecting a hidden\nErd\\H{o}s-R\\'enyi (ER) graph embedded in a larger and sparser ER graph, in the\npresence of side-information. We derive two related algorithms based on BP to\nperform subgraph detection in the presence of two kinds of side-information.\nThe first variant of side-information consists of a set of nodes, called cues,\nknown to be from the subgraph. The second variant of side-information consists\nof a set of nodes that are cues with a given probability. It was shown in past\nworks that BP without side-information fails to detect the subgraph correctly\nwhen an effective signal-to-noise ratio (SNR) parameter falls below a\nthreshold. In contrast, in the presence of non-trivial side-information, we\nshow that the BP algorithm achieves asymptotically zero error for any value of\nthe SNR parameter. We validate our results through simulations on synthetic\ndatasets as well as on a few real world networks.\n","negative":"  We propose a cluster-based quantization method to convert pre-trained full\nprecision weights into ternary weights with minimal impact on the accuracy. In\naddition, we also constrain the activations to 8-bits thus enabling sub 8-bit\nfull integer inference pipeline. Our method uses smaller clusters of N filters\nwith a common scaling factor to minimize the quantization loss, while also\nmaximizing the number of ternary operations. We show that with a cluster size\nof N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best\nfull precision results while replacing ~85% of all multiplications with 8-bit\naccumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1\naccuracy which within 2% of the full precision result. We also study the impact\nof the size of the cluster on both performance and accuracy, larger cluster\nsizes N=64 can replace ~98% of the multiplications with ternary operations but\nintroduces significant drop in accuracy which necessitates fine tuning the\nparameters with retraining the network at lower precision. To address this we\nhave also trained low-precision Resnet-50 with 8-bit activations and ternary\nweights by pre-initializing the network with full precision weights and achieve\n68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can\nrun on a full 8-bit compute pipeline, with a potential 16x improvement in\nperformance compared to baseline full-precision models.\n","id":201}
{"Unnamed: 0.1":11202,"Unnamed: 0":11202.0,"anchor":"Constrained Low-Rank Learning Using Least Squares-Based Regularization","positive":"  Low-rank learning has attracted much attention recently due to its efficacy\nin a rich variety of real-world tasks, e.g., subspace segmentation and image\ncategorization. Most low-rank methods are incapable of capturing\nlow-dimensional subspace for supervised learning tasks, e.g., classification\nand regression. This paper aims to learn both the discriminant low-rank\nrepresentation (LRR) and the robust projecting subspace in a supervised manner.\nTo achieve this goal, we cast the problem into a constrained rank minimization\nframework by adopting the least squares regularization. Naturally, the data\nlabel structure tends to resemble that of the corresponding low-dimensional\nrepresentation, which is derived from the robust subspace projection of clean\ndata by low-rank learning. Moreover, the low-dimensional representation of\noriginal data can be paired with some informative structure by imposing an\nappropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a\nnovel constrained LRR method. The objective function is formulated as a\nconstrained nuclear norm minimization problem, which can be solved by the\ninexact augmented Lagrange multiplier algorithm. Extensive experiments on image\nclassification, human pose estimation, and robust face recovery have confirmed\nthe superiority of our method.\n","negative":"  OpenML is an online machine learning platform where researchers can easily\nshare data, machine learning tasks and experiments as well as organize them\nonline to work and collaborate more efficiently. In this paper, we present an R\npackage to interface with the OpenML platform and illustrate its usage in\ncombination with the machine learning R package mlr. We show how the OpenML\npackage allows R users to easily search, download and upload data sets and\nmachine learning tasks. Furthermore, we also show how to upload results of\nexperiments, share them with others and download results from other users.\nBeyond ensuring reproducibility of results, the OpenML platform automates much\nof the drudge work, speeds up research, facilitates collaboration and increases\nthe users' visibility online.\n","id":202}
{"Unnamed: 0.1":11203,"Unnamed: 0":11203.0,"anchor":"Audio Event and Scene Recognition: A Unified Approach using Strongly and\n  Weakly Labeled Data","positive":"  In this paper we propose a novel learning framework called Supervised and\nWeakly Supervised Learning where the goal is to learn simultaneously from\nweakly and strongly labeled data. Strongly labeled data can be simply\nunderstood as fully supervised data where all labeled instances are available.\nIn weakly supervised learning only data is weakly labeled which prevents one\nfrom directly applying supervised learning methods. Our proposed framework is\nmotivated by the fact that a small amount of strongly labeled data can give\nconsiderable improvement over only weakly supervised learning. The primary\nproblem domain focus of this paper is acoustic event and scene detection in\naudio recordings. We first propose a naive formulation for leveraging labeled\ndata in both forms. We then propose a more general framework for Supervised and\nWeakly Supervised Learning (SWSL). Based on this general framework, we propose\na graph based approach for SWSL. Our main method is based on manifold\nregularization on graphs in which we show that the unified learning can be\nformulated as a constraint optimization problem which can be solved by\niterative concave-convex procedure (CCCP). Our experiments show that our\nproposed framework can address several concerns of audio content analysis using\nweakly labeled data.\n","negative":"  The deep learning community has devised a diverse set of methods to make\ngradient optimization, using large datasets, of large and highly complex models\nwith deeply cascaded nonlinearities, practical. Taken as a whole, these methods\nconstitute a breakthrough, allowing computational structures which are quite\nwide, very deep, and with an enormous number and variety of free parameters to\nbe effectively optimized. The result now dominates much of practical machine\nlearning, with applications in machine translation, computer vision, and speech\nrecognition. Many of these methods, viewed through the lens of algorithmic\ndifferentiation (AD), can be seen as either addressing issues with the gradient\nitself, or finding ways of achieving increased efficiency using tricks that are\nAD-related, but not provided by current AD systems.\n  The goal of this paper is to explain not just those methods of most relevance\nto AD, but also the technical constraints and mindset which led to their\ndiscovery. After explaining this context, we present a \"laundry list\" of\nmethods developed by the deep learning community. Two of these are discussed in\nfurther mathematical detail: a way to dramatically reduce the size of the tape\nwhen performing reverse-mode AD on a (theoretically) time-reversible process\nlike an ODE integrator; and a new mathematical insight that allows for the\nimplementation of a stochastic Newton's method.\n","id":203}
{"Unnamed: 0.1":11204,"Unnamed: 0":11204.0,"anchor":"Unsupervised Learning with Truncated Gaussian Graphical Models","positive":"  Gaussian graphical models (GGMs) are widely used for statistical modeling,\nbecause of ease of inference and the ubiquitous use of the normal distribution\nin practical approximations. However, they are also known for their limited\nmodeling abilities, due to the Gaussian assumption. In this paper, we introduce\na novel variant of GGMs, which relaxes the Gaussian restriction and yet admits\nefficient inference. Specifically, we impose a bipartite structure on the GGM\nand govern the hidden variables by truncated normal distributions. The\nnonlinearity of the model is revealed by its connection to rectified linear\nunit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and\nappealing properties of truncated normals, we are able to train the models\nefficiently using contrastive divergence. We consider three output constructs,\naccounting for real-valued, binary and count data. We further extend the model\nto deep constructions and show that deep models can be used for unsupervised\npre-training of rectifier neural networks. Extensive experimental results are\nprovided to validate the proposed models and demonstrate their superiority over\ncompeting models.\n","negative":"  In this paper, we explore ordinal classification (in the context of deep\nneural networks) through a simple modification of the squared error loss which\nnot only allows it to not only be sensitive to class ordering, but also allows\nthe possibility of having a discrete probability distribution over the classes.\nOur formulation is based on the use of a softmax hidden layer, which has\nreceived relatively little attention in the literature. We empirically evaluate\nits performance on the Kaggle diabetic retinopathy dataset, an ordinal and\nhigh-resolution dataset and show that it outperforms all of the baselines\nemployed.\n","id":204}
{"Unnamed: 0.1":11205,"Unnamed: 0":11205.0,"anchor":"Robust Semi-Supervised Graph Classifier Learning with Negative Edge\n  Weights","positive":"  In a semi-supervised learning scenario, (possibly noisy) partially observed\nlabels are used as input to train a classifier, in order to assign labels to\nunclassified samples. In this paper, we study this classifier learning problem\nfrom a graph signal processing (GSP) perspective. Specifically, by viewing a\nbinary classifier as a piecewise constant graph-signal in a high-dimensional\nfeature space, we cast classifier learning as a signal restoration problem via\na classical maximum a posteriori (MAP) formulation. Unlike previous\ngraph-signal restoration works, we consider in addition edges with negative\nweights that signify anti-correlation between samples. One unfortunate\nconsequence is that the graph Laplacian matrix $\\mathbf{L}$ can be indefinite,\nand previously proposed graph-signal smoothness prior $\\mathbf{x}^T \\mathbf{L}\n\\mathbf{x}$ for candidate signal $\\mathbf{x}$ can lead to pathological\nsolutions. In response, we derive an optimal perturbation matrix\n$\\boldsymbol{\\Delta}$ - based on a fast lower-bound computation of the minimum\neigenvalue of $\\mathbf{L}$ via a novel application of the Haynsworth inertia\nadditivity formula---so that $\\mathbf{L} + \\boldsymbol{\\Delta}$ is positive\nsemi-definite, resulting in a stable signal prior. Further, instead of forcing\na hard binary decision for each sample, we define the notion of generalized\nsmoothness on graph that promotes ambiguity in the classifier signal. Finally,\nwe propose an algorithm based on iterative reweighted least squares (IRLS) that\nsolves the posed MAP problem efficiently. Extensive simulation results show\nthat our proposed algorithm outperforms both SVM variants and graph-based\nclassifiers using positive-edge graphs noticeably.\n","negative":"  Autonomous control systems onboard planetary rovers and spacecraft benefit\nfrom having cognitive capabilities like learning so that they can adapt to\nunexpected situations in-situ. Q-learning is a form of reinforcement learning\nand it has been efficient in solving certain class of learning problems.\nHowever, embedded systems onboard planetary rovers and spacecraft rarely\nimplement learning algorithms due to the constraints faced in the field, like\nprocessing power, chip size, convergence rate and costs due to the need for\nradiation hardening. These challenges present a compelling need for a portable,\nlow-power, area efficient hardware accelerator to make learning algorithms\npractical onboard space hardware. This paper presents a FPGA implementation of\nQ-learning with Artificial Neural Networks (ANN). This method matches the\nmassive parallelism inherent in neural network software with the fine-grain\nparallelism of an FPGA hardware thereby dramatically reducing processing time.\nMars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices\nfor image processing, pyrotechnic operation control and obstacle avoidance. We\nsimulate and program our architecture on a Xilinx Virtex 7 FPGA. The\narchitectural implementation for a single neuron Q-learning and a more complex\nMultilayer Perception (MLP) Q-learning accelerator has been demonstrated. The\nresults show up to a 43-fold speed up by Virtex 7 FPGAs compared to a\nconventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed\narchitecture using the Symphony simulator and compiler from Xilinx, and\nevaluate the performance and power consumption.\n","id":205}
{"Unnamed: 0.1":11206,"Unnamed: 0":11206.0,"anchor":"Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box\n  Models","positive":"  Predictive models are increasingly deployed for the purpose of determining\naccess to services such as credit, insurance, and employment. Despite potential\ngains in productivity and efficiency, several potential problems have yet to be\naddressed, particularly the potential for unintentional discrimination. We\npresent an iterative procedure, based on orthogonal projection of input\nattributes, for enabling interpretability of black-box predictive models.\nThrough our iterative procedure, one can quantify the relative dependence of a\nblack-box model on its input attributes.The relative significance of the inputs\nto a predictive model can then be used to assess the fairness (or\ndiscriminatory extent) of such a model.\n","negative":"  Deep neural networks are widely used in machine learning applications.\nHowever, the deployment of large neural networks models can be difficult to\ndeploy on mobile devices with limited power budgets. To solve this problem, we\npropose Trained Ternary Quantization (TTQ), a method that can reduce the\nprecision of weights in neural networks to ternary values. This method has very\nlittle accuracy degradation and can even improve the accuracy of some models\n(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet\nmodel is trained from scratch, which means it's as easy as to train normal full\nprecision model. We highlight our trained quantization method that can learn\nboth ternary values and ternary assignment. During inference, only ternary\nvalues (2-bit weights) and scaling factors are needed, therefore our models are\nnearly 16x smaller than full-precision models. Our ternary models can also be\nviewed as sparse binary weight networks, which can potentially be accelerated\nwith custom circuit. Experiments on CIFAR-10 show that the ternary models\nobtained by trained quantization method outperform full-precision models of\nResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model\noutperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and\noutperforms previous ternary models by 3%.\n","id":206}
{"Unnamed: 0.1":11207,"Unnamed: 0":11207.0,"anchor":"Oracle Complexity of Second-Order Methods for Finite-Sum Problems","positive":"  Finite-sum optimization problems are ubiquitous in machine learning, and are\ncommonly solved using first-order methods which rely on gradient computations.\nRecently, there has been growing interest in \\emph{second-order} methods, which\nrely on both gradients and Hessians. In principle, second-order methods can\nrequire much fewer iterations than first-order methods, and hold the promise\nfor more efficient algorithms. Although computing and manipulating Hessians is\nprohibitive for high-dimensional problems in general, the Hessians of\nindividual functions in finite-sum problems can often be efficiently computed,\ne.g. because they possess a low-rank structure. Can second-order information\nindeed be used to solve such problems more efficiently? In this paper, we\nprovide evidence that the answer -- perhaps surprisingly -- is negative, at\nleast in terms of worst-case guarantees. However, we also discuss what\nadditional assumptions and algorithmic approaches might potentially circumvent\nthis negative result.\n","negative":"  We consider a multidimensional search problem that is motivated by questions\nin contextual decision-making, such as dynamic pricing and personalized\nmedicine. Nature selects a state from a $d$-dimensional unit ball and then\ngenerates a sequence of $d$-dimensional directions. We are given access to the\ndirections, but not access to the state. After receiving a direction, we have\nto guess the value of the dot product between the state and the direction. Our\ngoal is to minimize the number of times when our guess is more than $\\epsilon$\naway from the true answer. We construct a polynomial time algorithm that we\ncall Projected Volume achieving regret $O(d\\log(d\/\\epsilon))$, which is optimal\nup to a $\\log d$ factor. The algorithm combines a volume cutting strategy with\na new geometric technique that we call cylindrification.\n","id":207}
{"Unnamed: 0.1":11208,"Unnamed: 0":11208.0,"anchor":"PixelVAE: A Latent Variable Model for Natural Images","positive":"  Natural image modeling is a landmark challenge of unsupervised learning.\nVariational Autoencoders (VAEs) learn a useful latent representation and model\nglobal structure well but have difficulty capturing small details. PixelCNN\nmodels details very well, but lacks a latent code and is difficult to scale for\ncapturing large structures. We present PixelVAE, a VAE model with an\nautoregressive decoder based on PixelCNN. Our model requires very few expensive\nautoregressive layers compared to PixelCNN and learns latent codes that are\nmore compressed than a standard VAE while still capturing most non-trivial\nstructure. Finally, we extend our model to a hierarchy of latent variables at\ndifferent scales. Our model achieves state-of-the-art performance on binarized\nMNIST, competitive performance on 64x64 ImageNet, and high-quality samples on\nthe LSUN bedrooms dataset.\n","negative":"  Traffic accident data are usually noisy, contain missing values, and\nheterogeneous. How to select the most important variables to improve real-time\ntraffic accident risk prediction has become a concern of many recent studies.\nThis paper proposes a novel variable selection method based on the Frequent\nPattern tree (FP tree) algorithm. First, all the frequent patterns in the\ntraffic accident dataset are discovered. Then for each frequent pattern, a new\ncriterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is\ncalculated. This ROPR is added to the importance score of the variables that\ndifferentiate one frequent pattern from the others. To test the proposed\nmethod, a dataset was compiled from the traffic accidents records detected by\nonly one detector on interstate highway I-64 in Virginia in 2005. This dataset\nwas then linked to other variables such as real-time traffic information and\nweather conditions. Both the proposed method based on the FP tree algorithm, as\nwell as the widely utilized, random forest method, were then used to identify\nthe important variables or the Virginia dataset. The results indicate that\nthere are some differences between the variables deemed important by the FP\ntree and those selected by the random forest method. Following this, two\nbaseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network)\nwere developed to predict accident risk based on the variables identified by\nboth the FP tree method and the random forest method. The results show that the\nmodels based on the variable selection using the FP tree performed better than\nthose based on the random forest method for several versions of the k-NN and\nBayesian network models.The best results were derived from a Bayesian network\nmodel using variables from FP tree. That model could predict 61.11% of\naccidents accurately while having a false alarm rate of 38.16%.\n","id":208}
{"Unnamed: 0.1":11209,"Unnamed: 0":11209.0,"anchor":"Probabilistic Failure Analysis in Model Validation & Verification","positive":"  Automated fault localization is an important issue in model validation and\nverification. It helps the end users in analyzing the origin of failure. In\nthis work, we show the early experiments with probabilistic analysis approaches\nin fault localization. Inspired by the Kullback-Leibler Divergence from\nBayesian probabilistic theory, we propose a suspiciousness factor to compute\nthe fault contribution for the transitions in the reachability graph of model\nchecking, using which to rank the potential faulty transitions. To\nautomatically locate design faults in the simulation model of detailed design,\nwe propose to use the statistical model Hidden Markov Model (HMM), which\nprovides statistically identical information to component's real behavior. The\ncore of this method is a fault localization algorithm that gives out the set of\nsuspicious ranked faulty components and a backward algorithm that computes the\nmatching degree between the HMM and the simulation model to evaluate the\nconfidence degree of the localization conclusion.\n","negative":"  We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers.\n","id":209}
{"Unnamed: 0.1":11210,"Unnamed: 0":11210.0,"anchor":"Learning Dexterous Manipulation Policies from Experience and Imitation","positive":"  We explore learning-based approaches for feedback control of a dexterous\nfive-finger hand performing non-prehensile manipulation. First, we learn local\ncontrollers that are able to perform the task starting at a predefined initial\nstate. These controllers are constructed using trajectory optimization with\nrespect to locally-linear time-varying models learned directly from sensor\ndata. In some cases, we initialize the optimizer with human demonstrations\ncollected via teleoperation in a virtual environment. We demonstrate that such\ncontrollers can perform the task robustly, both in simulation and on the\nphysical platform, for a limited range of initial conditions around the trained\nstarting state. We then consider two interpolation methods for generalizing to\na wider range of initial conditions: deep learning, and nearest neighbors. We\nfind that nearest neighbors achieve higher performance. Nevertheless, the\nneural network has its advantages: it uses only tactile and proprioceptive\nfeedback but no visual feedback about the object (i.e. it performs the task\nblind) and learns a time-invariant policy. In contrast, the nearest neighbors\nmethod switches between time-varying local controllers based on the proximity\nof initial object states sensed via motion capture. While both generalization\nmethods leave room for improvement, our work shows that (i) local\ntrajectory-based controllers for complex non-prehensile manipulation tasks can\nbe constructed from surprisingly small amounts of training data, and (ii)\ncollections of such controllers can be interpolated to form more global\ncontrollers. Results are summarized in the supplementary video:\nhttps:\/\/youtu.be\/E0wmO6deqjo\n","negative":"  Computational approaches to drug discovery can reduce the time and cost\nassociated with experimental assays and enable the screening of novel\nchemotypes. Structure-based drug design methods rely on scoring functions to\nrank and predict binding affinities and poses. The ever-expanding amount of\nprotein-ligand binding and structural data enables the use of deep machine\nlearning techniques for protein-ligand scoring.\n  We describe convolutional neural network (CNN) scoring functions that take as\ninput a comprehensive 3D representation of a protein-ligand interaction. A CNN\nscoring function automatically learns the key features of protein-ligand\ninteractions that correlate with binding. We train and optimize our CNN scoring\nfunctions to discriminate between correct and incorrect binding poses and known\nbinders and non-binders. We find that our CNN scoring function outperforms the\nAutoDock Vina scoring function when ranking poses both for pose prediction and\nvirtual screening.\n","id":210}
{"Unnamed: 0.1":11211,"Unnamed: 0":11211.0,"anchor":"Convergence rate of stochastic k-means","positive":"  We analyze online \\cite{BottouBengio} and mini-batch \\cite{Sculley} $k$-means\nvariants. Both scale up the widely used $k$-means algorithm via stochastic\napproximation, and have become popular for large-scale clustering and\nunsupervised feature learning. We show, for the first time, that starting with\nany initial solution, they converge to a \"local optimum\" at rate\n$O(\\frac{1}{t})$ (in terms of the $k$-means objective) under general\nconditions. In addition, we show if the dataset is clusterable, when\ninitialized with a simple and scalable seeding algorithm, mini-batch $k$-means\nconverges to an optimal $k$-means solution at rate $O(\\frac{1}{t})$ with high\nprobability. The $k$-means objective is non-convex and non-differentiable: we\nexploit ideas from recent work on stochastic gradient descent for non-convex\nproblems \\cite{ge:sgd_tensor, balsubramani13} by providing a novel\ncharacterization of the trajectory of $k$-means algorithm on its solution\nspace, and circumvent the non-differentiability problem via geometric insights\nabout $k$-means update.\n","negative":"  Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities.\n","id":211}
{"Unnamed: 0.1":11212,"Unnamed: 0":11212.0,"anchor":"Machine Learning Approach for Skill Evaluation in Robotic-Assisted\n  Surgery","positive":"  Evaluating surgeon skill has predominantly been a subjective task.\nDevelopment of objective methods for surgical skill assessment are of increased\ninterest. Recently, with technological advances such as robotic-assisted\nminimally invasive surgery (RMIS), new opportunities for objective and\nautomated assessment frameworks have arisen. In this paper, we applied machine\nlearning methods to automatically evaluate performance of the surgeon in RMIS.\nSix important movement features were used in the evaluation including\ncompletion time, path length, depth perception, speed, smoothness and\ncurvature. Different classification methods applied to discriminate expert and\nnovice surgeons. We test our method on real surgical data for suturing task and\ncompare the classification result with the ground truth data (obtained by\nmanual labeling). The experimental results show that the proposed framework can\nclassify surgical skill level with relatively high accuracy of 85.7%. This\nstudy demonstrates the ability of machine learning methods to automatically\nclassify expert and novice surgeons using movement features for different RMIS\ntasks. Due to the simplicity and generalizability of the introduced\nclassification method, it is easy to implement in existing trainers.\n","negative":"  We introduce a new method for finding network motifs: interesting or\ninformative subgraph patterns in a network. Subgraphs are motifs when their\nfrequency in the data is high compared to the expected frequency under a null\nmodel. To compute this expectation, a full or approximate count of the\noccurrences of a motif is normally repeated on as many as 1000 random graphs\nsampled from the null model; a prohibitively expensive step. We use ideas from\nthe Minimum Description Length (MDL) literature to define a new measure of\nmotif relevance. With our method, samples from the null model are not required.\nInstead we compute the probability of the data under the null model and compare\nthis to the probability under a specially designed alternative model. With this\nnew relevance test, we can search for motifs by random sampling, rather than\nrequiring an accurate count of all instances of a motif. This allows motif\nanalysis to scale to networks with billions of links.\n","id":212}
{"Unnamed: 0.1":11213,"Unnamed: 0":11213.0,"anchor":"S3Pool: Pooling with Stochastic Spatial Sampling","positive":"  Feature pooling layers (e.g., max pooling) in convolutional neural networks\n(CNNs) serve the dual purpose of providing increasingly abstract\nrepresentations as well as yielding computational savings in subsequent\nconvolutional layers. We view the pooling operation in CNNs as a two-step\nprocedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature\nmap with stride one which leaves the spatial resolution intact, and second,\ndownsampling is performed by selecting one pixel from each non-overlapping\npooling window in an often uniform and deterministic (e.g., top-left) manner.\nOur starting point in this work is the observation that this regularly spaced\ndownsampling arising from non-overlapping windows, although intuitive from a\nsignal processing perspective (which has the goal of signal reconstruction), is\nnot necessarily optimal for \\emph{learning} (where the goal is to generalize).\nWe study this aspect and propose a novel pooling strategy with stochastic\nspatial sampling (S3Pool), where the regular downsampling is replaced by a more\ngeneral stochastic version. We observe that this general stochasticity acts as\na strong regularizer, and can also be seen as doing implicit data augmentation\nby introducing distortions in the feature maps. We further introduce a\nmechanism to control the amount of distortion to suit different datasets and\narchitectures. To demonstrate the effectiveness of the proposed approach, we\nperform extensive experiments on several popular image classification\nbenchmarks, observing excellent improvements over baseline models. Experimental\ncode is available at https:\/\/github.com\/Shuangfei\/s3pool.\n","negative":"  Recently there has been significant interest in training machine-learning\nmodels at low precision: by reducing precision, one can reduce computation and\ncommunication by one order of magnitude. We examine training at reduced\nprecision, both from a theoretical and practical perspective, and ask: is it\npossible to train models at end-to-end low precision with provable guarantees?\nCan this lead to consistent order-of-magnitude speedups? We present a framework\ncalled ZipML to answer these questions. For linear models, the answer is yes.\nWe develop a simple framework based on one simple but novel strategy called\ndouble sampling. Our framework is able to execute training at low precision\nwith no bias, guaranteeing convergence, whereas naive quantization would\nintroduce significant bias. We validate our framework across a range of\napplications, and show that it enables an FPGA prototype that is up to 6.5x\nfaster than an implementation using full 32-bit precision. We further develop a\nvariance-optimal stochastic quantization strategy and show that it can make a\nsignificant difference in a variety of settings. When applied to linear models\ntogether with double sampling, we save up to another 1.7x in data movement\ncompared with uniform quantization. When training deep networks with quantized\nmodels, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally,\nwe extend our framework through approximation to non-linear models, such as\nSVM. We show that, although using low-precision data induces bias, we can\nappropriately bound and control the bias. We find in practice 8-bit precision\nis often sufficient to converge to the correct solution. Interestingly,\nhowever, in practice we notice that our framework does not always outperform\nthe naive rounding approach. We discuss this negative result in detail.\n","id":213}
{"Unnamed: 0.1":11214,"Unnamed: 0":11214.0,"anchor":"Training Spiking Deep Networks for Neuromorphic Hardware","positive":"  We describe a method to train spiking deep networks that can be run using\nleaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for\nspiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012\nbenchmark. Our method for transforming deep artificial neural networks into\nspiking networks is scalable and works with a wide range of neural\nnonlinearities. We achieve these results by softening the neural response\nfunction, such that its derivative remains bounded, and by training the network\nwith noise to provide robustness against the variability introduced by spikes.\nOur analysis shows that implementations of these networks on neuromorphic\nhardware will be many times more power-efficient than the equivalent\nnon-spiking networks on traditional hardware.\n","negative":"  Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,\nslowing response times and desensitising nurses to alarms (Chambrin, 2001),\ncausing true alarms to be missed (Hug et al., 2011). We compare sliding window\npredictors with recurrent predictors to classify patient state-of-health from\nICU multivariate time series; we report slightly improved performance for the\nRNN for three out of four targets.\n","id":214}
{"Unnamed: 0.1":11215,"Unnamed: 0":11215.0,"anchor":"A Semi-Markov Switching Linear Gaussian Model for Censored Physiological\n  Data","positive":"  Critically ill patients in regular wards are vulnerable to unanticipated\nclinical dete- rioration which requires timely transfer to the intensive care\nunit (ICU). To allow for risk scoring and patient monitoring in such a setting,\nwe develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the\ninpatients' physiol- ogy. The model captures the patients' latent clinical\nstates and their corresponding observable lab tests and vital signs. We present\nan efficient unsupervised learn- ing algorithm that capitalizes on the\ninformatively censored data in the electronic health records (EHR) to learn the\nparameters of the SSLGM; the learned model is then used to assess the new\ninpatients' risk for clinical deterioration in an online fashion, allowing for\ntimely ICU admission. Experiments conducted on a het- erogeneous cohort of\n6,094 patients admitted to a large academic medical center show that the\nproposed model significantly outperforms the currently deployed risk scores\nsuch as Rothman index, MEWS, SOFA and APACHE.\n","negative":"  In computational complexity, a complexity class is given by a set of problems\nor functions, and a basic challenge is to show separations of complexity\nclasses $A \\not= B$ especially when $A$ is known to be a subset of $B$. In this\npaper we introduce a homological theory of functions that can be used to\nestablish complexity separations, while also providing other interesting\nconsequences. We propose to associate a topological space $S_A$ to each class\nof functions $A$, such that, to separate complexity classes $A \\subseteq B'$,\nit suffices to observe a change in \"the number of holes\", i.e. homology, in\n$S_A$ as a subclass $B$ of $B'$ is added to $A$. In other words, if the\nhomologies of $S_A$ and $S_{A \\cup B}$ are different, then $A \\not= B'$. We\ndevelop the underlying theory of functions based on combinatorial and\nhomological commutative algebra and Stanley-Reisner theory, and recover Minsky\nand Papert's 1969 result that parity cannot be computed by nonmaximal degree\npolynomial threshold functions. In the process, we derive a \"maximal principle\"\nfor polynomial threshold functions that is used to extend this result further\nto arbitrary symmetric functions. A surprising coincidence is demonstrated,\nwhere the maximal dimension of \"holes\" in $S_A$ upper bounds the VC dimension\nof $A$, with equality for common computational cases such as the class of\npolynomial threshold functions or the class of linear functionals in $\\mathbb\nF_2$, or common algebraic cases such as when the Stanley-Reisner ring of $S_A$\nis Cohen-Macaulay. As another interesting application of our theory, we prove a\nresult that a priori has nothing to do with complexity separation: it\ncharacterizes when a vector subspace intersects the positive cone, in terms of\nhomological conditions. By analogy to Farkas' result doing the same with\n*linear conditions*, we call our theorem the Homological Farkas Lemma.\n","id":215}
{"Unnamed: 0.1":11216,"Unnamed: 0":11216.0,"anchor":"Net-Trim: Convex Pruning of Deep Neural Networks with Performance\n  Guarantee","positive":"  We introduce and analyze a new technique for model reduction for deep neural\nnetworks. While large networks are theoretically capable of learning\narbitrarily complex models, overfitting and model redundancy negatively affects\nthe prediction accuracy and model variance. Our Net-Trim algorithm prunes\n(sparsifies) a trained network layer-wise, removing connections at each layer\nby solving a convex optimization program. This program seeks a sparse set of\nweights at each layer that keeps the layer inputs and outputs consistent with\nthe originally trained model. The algorithms and associated analysis are\napplicable to neural networks operating with the rectified linear unit (ReLU)\nas the nonlinear activation. We present both parallel and cascade versions of\nthe algorithm. While the latter can achieve slightly simpler models with the\nsame generalization performance, the former can be computed in a distributed\nmanner. In both cases, Net-Trim significantly reduces the number of connections\nin the network, while also providing enough regularization to slightly reduce\nthe generalization error. We also provide a mathematical analysis of the\nconsistency between the initial network and the retrained model. To analyze the\nmodel sample complexity, we derive the general sufficient conditions for the\nrecovery of a sparse transform matrix. For a single layer taking independent\nGaussian random vectors of length $N$ as inputs, we show that if the network\nresponse can be described using a maximum number of $s$ non-zero weights per\nnode, these weights can be learned from $\\mathcal{O}(s\\log N)$ samples.\n","negative":"  In a semi-supervised learning scenario, (possibly noisy) partially observed\nlabels are used as input to train a classifier, in order to assign labels to\nunclassified samples. In this paper, we study this classifier learning problem\nfrom a graph signal processing (GSP) perspective. Specifically, by viewing a\nbinary classifier as a piecewise constant graph-signal in a high-dimensional\nfeature space, we cast classifier learning as a signal restoration problem via\na classical maximum a posteriori (MAP) formulation. Unlike previous\ngraph-signal restoration works, we consider in addition edges with negative\nweights that signify anti-correlation between samples. One unfortunate\nconsequence is that the graph Laplacian matrix $\\mathbf{L}$ can be indefinite,\nand previously proposed graph-signal smoothness prior $\\mathbf{x}^T \\mathbf{L}\n\\mathbf{x}$ for candidate signal $\\mathbf{x}$ can lead to pathological\nsolutions. In response, we derive an optimal perturbation matrix\n$\\boldsymbol{\\Delta}$ - based on a fast lower-bound computation of the minimum\neigenvalue of $\\mathbf{L}$ via a novel application of the Haynsworth inertia\nadditivity formula---so that $\\mathbf{L} + \\boldsymbol{\\Delta}$ is positive\nsemi-definite, resulting in a stable signal prior. Further, instead of forcing\na hard binary decision for each sample, we define the notion of generalized\nsmoothness on graph that promotes ambiguity in the classifier signal. Finally,\nwe propose an algorithm based on iterative reweighted least squares (IRLS) that\nsolves the posed MAP problem efficiently. Extensive simulation results show\nthat our proposed algorithm outperforms both SVM variants and graph-based\nclassifiers using positive-edge graphs noticeably.\n","id":216}
{"Unnamed: 0.1":11217,"Unnamed: 0":11217.0,"anchor":"Graph Learning from Data under Structural and Laplacian Constraints","positive":"  Graphs are fundamental mathematical structures used in various fields to\nrepresent data, signals and processes. In this paper, we propose a novel\nframework for learning\/estimating graphs from data. The proposed framework\nincludes (i) formulation of various graph learning problems, (ii) their\nprobabilistic interpretations and (iii) associated algorithms. Specifically,\ngraph learning problems are posed as estimation of graph Laplacian matrices\nfrom some observed data under given structural constraints (e.g., graph\nconnectivity and sparsity level). From a probabilistic perspective, the\nproblems of interest correspond to maximum a posteriori (MAP) parameter\nestimation of Gaussian-Markov random field (GMRF) models, whose precision\n(inverse covariance) is a graph Laplacian matrix. For the proposed graph\nlearning problems, specialized algorithms are developed by incorporating the\ngraph Laplacian and structural constraints. The experimental results\ndemonstrate that the proposed algorithms outperform the current\nstate-of-the-art methods in terms of accuracy and computational efficiency.\n","negative":"  In this document we shows a first implementation and some preliminary results\nof a new theory, facing Machine Learning problems in the frameworks of\nClassical Mechanics and Variational Calculus. We give a general formulation of\nthe problem and then we studies basic behaviors of the model on simple\npractical implementations.\n","id":217}
{"Unnamed: 0.1":11218,"Unnamed: 0":11218.0,"anchor":"Bayesian optimization of hyper-parameters in reservoir computing","positive":"  We describe a method for searching the optimal hyper-parameters in reservoir\ncomputing, which consists of a Gaussian process with Bayesian optimization. It\nprovides an alternative to other frequently used optimization methods such as\ngrid, random, or manual search. In addition to a set of optimal\nhyper-parameters, the method also provides a probability distribution of the\ncost function as a function of the hyper-parameters. We apply this method to\ntwo types of reservoirs: nonlinear delay nodes and echo state networks. It\nshows excellent performance on all considered benchmarks, either matching or\nsignificantly surpassing results found in the literature. In general, the\nalgorithm achieves optimal results in fewer iterations when compared to other\noptimization methods. We have optimized up to six hyper-parameters\nsimultaneously, which would have been infeasible using, e.g., grid search. Due\nto its automated nature, this method significantly reduces the need for expert\nknowledge when optimizing the hyper-parameters in reservoir computing. Existing\nsoftware libraries for Bayesian optimization, such as Spearmint, make the\nimplementation of the algorithm straightforward. A fork of the Spearmint\nframework along with a tutorial on how to use it in practice is available at\nhttps:\/\/bitbucket.org\/uhasseltmachinelearning\/spearmint\/\n","negative":"  We propose a new formulation for pruning convolutional kernels in neural\nnetworks to enable efficient inference. We interleave greedy criteria-based\npruning with fine-tuning by backpropagation - a computationally efficient\nprocedure that maintains good generalization in the pruned network. We propose\na new criterion based on Taylor expansion that approximates the change in the\ncost function induced by pruning network parameters. We focus on transfer\nlearning, where large pretrained networks are adapted to specialized tasks. The\nproposed criterion demonstrates superior performance compared to other\ncriteria, e.g. the norm of kernel weights or feature map activation, for\npruning large CNNs after adaptation to fine-grained classification tasks\n(Birds-200 and Flowers-102) relaying only on the first order gradient\ninformation. We also show that pruning can lead to more than 10x theoretical\n(5x practical) reduction in adapted 3D-convolutional filters with a small drop\nin accuracy in a recurrent gesture classifier. Finally, we show results for the\nlarge-scale ImageNet dataset to emphasize the flexibility of our approach.\n","id":218}
{"Unnamed: 0.1":11219,"Unnamed: 0":11219.0,"anchor":"Deep Variational Inference Without Pixel-Wise Reconstruction","positive":"  Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers.\n","negative":"  This paper considers the problem of simultaneously learning the Sensing\nMatrix and Sparsifying Dictionary (SMSD) on a large training dataset. To\naddress the formulated joint learning problem, we propose an online algorithm\nthat consists of a closed-form solution for optimizing the sensing matrix with\na fixed sparsifying dictionary and a stochastic method for learning the\nsparsifying dictionary on a large dataset when the sensing matrix is given.\nBenefiting from training on a large dataset, the obtained compressive sensing\n(CS) system by the proposed algorithm yields a much better performance in terms\nof signal recovery accuracy than the existing ones. The simulation results on\nnatural images demonstrate the effectiveness of the suggested online algorithm\ncompared with the existing methods.\n","id":219}
{"Unnamed: 0.1":11220,"Unnamed: 0":11220.0,"anchor":"A Learning Scheme for Microgrid Islanding and Reconnection","positive":"  This paper introduces a potential learning scheme that can dynamically\npredict the stability of the reconnection of sub-networks to a main grid. As\nthe future electrical power systems tend towards smarter and greener\ntechnology, the deployment of self sufficient networks, or microgrids, becomes\nmore likely. Microgrids may operate on their own or synchronized with the main\ngrid, thus control methods need to take into account islanding and reconnecting\nof said networks. The ability to optimally and safely reconnect a portion of\nthe grid is not well understood and, as of now, limited to raw synchronization\nbetween interconnection points. A support vector machine (SVM) leveraging\nreal-time data from phasor measurement units (PMUs) is proposed to predict in\nreal time whether the reconnection of a sub-network to the main grid would lead\nto stability or instability. A dynamics simulator fed with pre-acquired system\nparameters is used to create training data for the SVM in various operating\nstates. The classifier was tested on a variety of cases and operating points to\nensure diversity. Accuracies of approximately 85% were observed throughout most\nconditions when making dynamic predictions of a given network.\n","negative":"  In this paper, we propose two methods for tackling the problem of\ncross-device matching for online advertising at CIKM Cup 2016. The first method\nconsiders the matching problem as a binary classification task and solve it by\nutilizing ensemble learning techniques. The second method defines the matching\nproblem as a ranking task and effectively solve it with using learning-to-rank\nalgorithms. The results show that the proposed methods obtain promising\nresults, in which the ranking-based method outperforms the classification-based\nmethod for the task.\n","id":220}
{"Unnamed: 0.1":11221,"Unnamed: 0":11221.0,"anchor":"Approximating Wisdom of Crowds using K-RBMs","positive":"  An important way to make large training sets is to gather noisy labels from\ncrowds of non experts. We propose a method to aggregate noisy labels collected\nfrom a crowd of workers or annotators. Eliciting labels is important in tasks\nsuch as judging web search quality and rating products. Our method assumes that\nlabels are generated by a probability distribution over items and labels. We\nformulate the method by drawing parallels between Gaussian Mixture Models\n(GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of\nvote aggregation can be viewed as one of clustering. We use K-RBMs to perform\nclustering. We finally show some empirical evaluations over real datasets.\n","negative":"  Deep networks are successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are usually much less suited for semi-supervised\nproblems because of their tendency to overfit easily when trained on small\namounts of data. In this work we will explore a new training objective that is\ntargeting a semi-supervised regime with only a small subset of labeled data.\nThis criterion is based on a deep metric embedding over distance relations\nwithin the set of labeled samples, together with constraints over the\nembeddings of the unlabeled set. The final learned representations are\ndiscriminative in euclidean space, and hence can be used with subsequent\nnearest-neighbor classification using the labeled samples.\n","id":221}
{"Unnamed: 0.1":11222,"Unnamed: 0":11222.0,"anchor":"Fast On-Line Kernel Density Estimation for Active Object Localization","positive":"  A major goal of computer vision is to enable computers to interpret visual\nsituations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd\nwaiting for a bus,\" \"a picnic\") whose image instantiations are linked more by\ntheir common spatial and semantic structure than by low-level visual\nsimilarity. In this paper, we propose a novel method for prior learning and\nactive object localization for this kind of knowledge-driven search in static\nimages. In our system, prior situation knowledge is captured by a set of\nflexible, kernel-based density estimations---a situation model---that represent\nthe expected spatial structure of the given situation. These estimations are\nefficiently updated by information gained as the system searches for relevant\nobjects, allowing the system to use context as it is discovered to narrow the\nsearch.\n  More specifically, at any given time in a run on a test image, our system\nuses image features plus contextual information it has discovered to identify a\nsmall subset of training images---an importance cluster---that is deemed most\nsimilar to the given test image, given the context. This subset is used to\ngenerate an updated situation model in an on-line fashion, using an efficient\nmultipole expansion technique.\n  As a proof of concept, we apply our algorithm to a highly varied and\nchallenging dataset consisting of instances of a \"dog-walking\" situation. Our\nresults support the hypothesis that dynamically-rendered, context-based\nprobability models can support efficient object localization in visual\nsituations. Moreover, our approach is general enough to be applied to diverse\nmachine learning paradigms requiring interpretable, probabilistic\nrepresentations generated from partially observed data.\n","negative":"  This work initiates a systematic investigation of testing high-dimensional\nstructured distributions by focusing on testing Bayesian networks -- the\nprototypical family of directed graphical models. A Bayesian network is defined\nby a directed acyclic graph, where we associate a random variable with each\nnode. The value at any particular node is conditionally independent of all the\nother non-descendant nodes once its parents are fixed. Specifically, we study\nthe properties of identity testing and closeness testing of Bayesian networks.\nOur main contribution is the first non-trivial efficient testing algorithms for\nthese problems and corresponding information-theoretic lower bounds. For a wide\nrange of parameter settings, our testing algorithms have sample complexity\nsublinear in the dimension and are sample-optimal, up to constant factors.\n","id":222}
{"Unnamed: 0.1":11223,"Unnamed: 0":11223.0,"anchor":"DeepCas: an End-to-end Predictor of Information Cascades","positive":"  Information cascades, effectively facilitated by most social network\nplatforms, are recognized as a major factor in almost every social success and\ndisaster in these networks. Can cascades be predicted? While many believe that\nthey are inherently unpredictable, recent work has shown that some key\nproperties of information cascades, such as size, growth, and shape, can be\npredicted by a machine learning algorithm that combines many features. These\npredictors all depend on a bag of hand-crafting features to represent the\ncascade network and the global network structure. Such features, always\ncarefully and sometimes mysteriously designed, are not easy to extend or to\ngeneralize to a different platform or domain.\n  Inspired by the recent successes of deep learning in multiple data mining\ntasks, we investigate whether an end-to-end deep learning approach could\neffectively predict the future size of cascades. Such a method automatically\nlearns the representation of individual cascade graphs in the context of the\nglobal network structure, without hand-crafted features and heuristics. We find\nthat node embeddings fall short of predictive power, and it is critical to\nlearn the representation of a cascade graph as a whole. We present algorithms\nthat learn the representation of cascade graphs in an end-to-end manner, which\nsignificantly improve the performance of cascade prediction over strong\nbaselines that include feature based methods, node embedding methods, and graph\nkernel methods. Our results also provide interesting implications for cascade\nprediction in general.\n","negative":"  We consider the problem of optimal budget allocation for crowdsourcing\nproblems, allocating users to tasks to maximize our final confidence in the\ncrowdsourced answers. Such an optimized worker assignment method allows us to\nboost the efficacy of any popular crowdsourcing estimation algorithm. We\nconsider a mutual information interpretation of the crowdsourcing problem,\nwhich leads to a stochastic subset selection problem with a submodular\nobjective function. We present experimental simulation results which\ndemonstrate the effectiveness of our dynamic task allocation method for\nachieving higher accuracy, possibly requiring fewer labels, as well as\nimproving upon a previous method which is sensitive to the proportion of users\nto questions.\n","id":223}
{"Unnamed: 0.1":11224,"Unnamed: 0":11224.0,"anchor":"Fully-adaptive Feature Sharing in Multi-Task Networks with Applications\n  in Person Attribute Classification","positive":"  Multi-task learning aims to improve generalization performance of multiple\nprediction tasks by appropriately sharing relevant information across them. In\nthe context of deep neural networks, this idea is often realized by\nhand-designed network architectures with layers that are shared across tasks\nand branches that encode task-specific features. However, the space of possible\nmulti-task deep architectures is combinatorially large and often the final\narchitecture is arrived at by manual exploration of this space subject to\ndesigner's bias, which can be both error-prone and tedious. In this work, we\npropose a principled approach for designing compact multi-task deep learning\narchitectures. Our approach starts with a thin network and dynamically widens\nit in a greedy manner during training using a novel criterion that promotes\ngrouping of similar tasks together. Our Extensive evaluation on person\nattributes classification tasks involving facial and clothing attributes\nsuggests that the models produced by the proposed method are fast, compact and\ncan closely match or exceed the state-of-the-art accuracy from strong baselines\nby much more expensive models.\n","negative":"  Model instability and poor prediction of long-term behavior are common\nproblems when modeling dynamical systems using nonlinear \"black-box\"\ntechniques. Direct optimization of the long-term predictions, often called\nsimulation error minimization, leads to optimization problems that are\ngenerally non-convex in the model parameters and suffer from multiple local\nminima. In this work we present methods which address these problems through\nconvex optimization, based on Lagrangian relaxation, dissipation inequalities,\ncontraction theory, and semidefinite programming. We demonstrate the proposed\nmethods with a model order reduction task for electronic circuit design and the\nidentification of a pneumatic actuator from experiment.\n","id":224}
{"Unnamed: 0.1":11225,"Unnamed: 0":11225.0,"anchor":"Spectral Convolution Networks","positive":"  Previous research has shown that computation of convolution in the frequency\ndomain provides a significant speedup versus traditional convolution network\nimplementations. However, this performance increase comes at the expense of\nrepeatedly computing the transform and its inverse in order to apply other\nnetwork operations such as activation, pooling, and dropout. We show,\nmathematically, how convolution and activation can both be implemented in the\nfrequency domain using either the Fourier or Laplace transformation. The main\ncontributions are a description of spectral activation under the Fourier\ntransform and a further description of an efficient algorithm for computing\nboth convolution and activation under the Laplace transform. By computing both\nthe convolution and activation functions in the frequency domain, we can reduce\nthe number of transforms required, as well as reducing overall complexity. Our\ndescription of a spectral activation function, together with existing spectral\nanalogs of other network functions may then be used to compose a fully spectral\nimplementation of a convolution network.\n","negative":"  Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.\n","id":225}
{"Unnamed: 0.1":11226,"Unnamed: 0":11226.0,"anchor":"Reinforcement Learning with Unsupervised Auxiliary Tasks","positive":"  Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth.\n","negative":"  Across a far-reaching diversity of scientific and industrial applications, a\ngeneral key problem involves relating the structure of time-series data to a\nmeaningful outcome, such as detecting anomalous events from sensor recordings,\nor diagnosing patients from physiological time-series measurements like heart\nrate or brain activity. Currently, researchers must devote considerable effort\nmanually devising, or searching for, properties of their time series that are\nsuitable for the particular analysis problem at hand. Addressing this\nnon-systematic and time-consuming procedure, here we introduce a new tool,\nhctsa, that selects interpretable and useful properties of time series\nautomatically, by comparing implementations over 7700 time-series features\ndrawn from diverse scientific literatures. Using two exemplar biological\napplications, we show how hctsa allows researchers to leverage decades of\ntime-series research to quantify and understand informative structure in their\ntime-series data.\n","id":226}
{"Unnamed: 0.1":11227,"Unnamed: 0":11227.0,"anchor":"The ZipML Framework for Training Models with End-to-End Low Precision:\n  The Cans, the Cannots, and a Little Bit of Deep Learning","positive":"  Recently there has been significant interest in training machine-learning\nmodels at low precision: by reducing precision, one can reduce computation and\ncommunication by one order of magnitude. We examine training at reduced\nprecision, both from a theoretical and practical perspective, and ask: is it\npossible to train models at end-to-end low precision with provable guarantees?\nCan this lead to consistent order-of-magnitude speedups? We present a framework\ncalled ZipML to answer these questions. For linear models, the answer is yes.\nWe develop a simple framework based on one simple but novel strategy called\ndouble sampling. Our framework is able to execute training at low precision\nwith no bias, guaranteeing convergence, whereas naive quantization would\nintroduce significant bias. We validate our framework across a range of\napplications, and show that it enables an FPGA prototype that is up to 6.5x\nfaster than an implementation using full 32-bit precision. We further develop a\nvariance-optimal stochastic quantization strategy and show that it can make a\nsignificant difference in a variety of settings. When applied to linear models\ntogether with double sampling, we save up to another 1.7x in data movement\ncompared with uniform quantization. When training deep networks with quantized\nmodels, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally,\nwe extend our framework through approximation to non-linear models, such as\nSVM. We show that, although using low-precision data induces bias, we can\nappropriately bound and control the bias. We find in practice 8-bit precision\nis often sufficient to converge to the correct solution. Interestingly,\nhowever, in practice we notice that our framework does not always outperform\nthe naive rounding approach. We discuss this negative result in detail.\n","negative":"  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n","id":227}
{"Unnamed: 0.1":11228,"Unnamed: 0":11228.0,"anchor":"Composing Music with Grammar Argumented Neural Networks and Note-Level\n  Encoding","positive":"  Creating aesthetically pleasing pieces of art, including music, has been a\nlong-term goal for artificial intelligence research. Despite recent successes\nof long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential\nlearning, LSTM neural networks have not, by themselves, been able to generate\nnatural-sounding music conforming to music theory. To transcend this\ninadequacy, we put forward a novel method for music composition that combines\nthe LSTM with Grammars motivated by music theory. The main tenets of music\ntheory are encoded as grammar argumented (GA) filters on the training data,\nsuch that the machine can be trained to generate music inheriting the\nnaturalness of human-composed pieces from the original dataset while adhering\nto the rules of music theory. Unlike previous approaches, pitches and durations\nare encoded as one semantic entity, which we refer to as note-level encoding.\nThis allows easy implementation of music theory grammars, as well as closer\nemulation of the thinking pattern of a musician. Although the GA rules are\napplied to the training data and never directly to the LSTM music generation,\nour machine still composes music that possess high incidences of diatonic scale\nnotes, small pitch intervals and chords, in deference to music theory.\n","negative":"  Lipreading is the task of decoding text from the movement of a speaker's\nmouth. Traditional approaches separated the problem into two stages: designing\nor learning visual features, and prediction. More recent deep lipreading\napproaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman,\n2016a). However, existing work on models trained end-to-end perform only word\nclassification, rather than sentence-level sequence prediction. Studies have\nshown that human lipreading performance increases for longer words (Easton &\nBasala, 1982), indicating the importance of features capturing temporal context\nin an ambiguous communication channel. Motivated by this observation, we\npresent LipNet, a model that maps a variable-length sequence of video frames to\ntext, making use of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. To the\nbest of our knowledge, LipNet is the first end-to-end sentence-level lipreading\nmodel that simultaneously learns spatiotemporal visual features and a sequence\nmodel. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level,\noverlapped speaker split task, outperforming experienced human lipreaders and\nthe previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).\n","id":228}
{"Unnamed: 0.1":11229,"Unnamed: 0":11229.0,"anchor":"Solving Cold-Start Problem in Large-scale Recommendation Engines: A Deep\n  Learning Approach","positive":"  Collaborative Filtering (CF) is widely used in large-scale recommendation\nengines because of its efficiency, accuracy and scalability. However, in\npractice, the fact that recommendation engines based on CF require interactions\nbetween users and items before making recommendations, make it inappropriate\nfor new items which haven't been exposed to the end users to interact with.\nThis is known as the cold-start problem. In this paper we introduce a novel\napproach which employs deep learning to tackle this problem in any CF based\nrecommendation engine. One of the most important features of the proposed\ntechnique is the fact that it can be applied on top of any existing CF based\nrecommendation engine without changing the CF core. We successfully applied\nthis technique to overcome the item cold-start problem in Careerbuilder's CF\nbased recommendation engine. Our experiments show that the proposed technique\nis very efficient to resolve the cold-start problem while maintaining high\naccuracy of the CF recommendations.\n","negative":"  Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement.\n","id":229}
{"Unnamed: 0.1":11230,"Unnamed: 0":11230.0,"anchor":"Algebraic multigrid support vector machines","positive":"  The support vector machine is a flexible optimization-based technique widely\nused for classification problems. In practice, its training part becomes\ncomputationally expensive on large-scale data sets because of such reasons as\nthe complexity and number of iterations in parameter fitting methods,\nunderlying optimization solvers, and nonlinearity of kernels. We introduce a\nfast multilevel framework for solving support vector machine models that is\ninspired by the algebraic multigrid. Significant improvement in the running has\nbeen achieved without any loss in the quality. The proposed technique is highly\nbeneficial on imbalanced sets. We demonstrate computational results on publicly\navailable and industrial data sets.\n","negative":"  We introduce an exceptionally simple gated recurrent neural network (RNN)\nthat achieves performance comparable to well-known gated architectures, such as\nLSTMs and GRUs, on the word-level language modeling task. We prove that our\nmodel has simple, predicable and non-chaotic dynamics. This stands in stark\ncontrast to more standard gated architectures, whose underlying dynamical\nsystems exhibit chaotic behavior.\n","id":230}
{"Unnamed: 0.1":11231,"Unnamed: 0":11231.0,"anchor":"Robust Hashing for Multi-View Data: Jointly Learning Low-Rank Kernelized\n  Similarity Consensus and Hash Functions","positive":"  Learning hash functions\/codes for similarity search over multi-view data is\nattracting increasing attention, where similar hash codes are assigned to the\ndata objects characterizing consistently neighborhood relationship across\nviews. Traditional methods in this category inherently suffer three\nlimitations: 1) they commonly adopt a two-stage scheme where similarity matrix\nis first constructed, followed by a subsequent hash function learning; 2) these\nmethods are commonly developed on the assumption that data samples with\nmultiple representations are noise-free,which is not practical in real-life\napplications; 3) they often incur cumbersome training model caused by the\nneighborhood graph construction using all $N$ points in the database ($O(N)$).\nIn this paper, we motivate the problem of jointly and efficiently training the\nrobust hash functions over data objects with multi-feature representations\nwhich may be noise corrupted. To achieve both the robustness and training\nefficiency, we propose an approach to effectively and efficiently learning\nlow-rank kernelized \\footnote{We use kernelized similarity rather than kernel,\nas it is not a squared symmetric matrix for data-landmark affinity matrix.}\nhash functions shared across views. Specifically, we utilize landmark graphs to\nconstruct tractable similarity matrices in multi-views to automatically\ndiscover neighborhood structure in the data. To learn robust hash functions, a\nlatent low-rank kernel function is used to construct hash functions in order to\naccommodate linearly inseparable data. In particular, a latent kernelized\nsimilarity matrix is recovered by rank minimization on multiple kernel-based\nsimilarity matrices. Extensive experiments on real-world multi-view datasets\nvalidate the efficacy of our method in the presence of error corruptions.\n","negative":"  Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart.\n","id":231}
{"Unnamed: 0.1":11232,"Unnamed: 0":11232.0,"anchor":"Automatic Node Selection for Deep Neural Networks using Group Lasso\n  Regularization","positive":"  We examine the effect of the Group Lasso (gLasso) regularizer in selecting\nthe salient nodes of Deep Neural Network (DNN) hidden layers by applying a\nDNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of\ngLasso regularization, one for outgoing weight vectors and another for incoming\nweight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096\nnodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment\nresults demonstrate that our DNN training, in which the gLasso regularizer was\nembedded, successfully selected the hidden layer nodes that are necessary and\nsufficient for achieving high classification power.\n","negative":"  Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain.\n","id":232}
{"Unnamed: 0.1":11233,"Unnamed: 0":11233.0,"anchor":"DelugeNets: Deep Networks with Efficient and Flexible Cross-layer\n  Information Inflows","positive":"  Deluge Networks (DelugeNets) are deep neural networks which efficiently\nfacilitate massive cross-layer information inflows from preceding layers to\nsucceeding layers. The connections between layers in DelugeNets are established\nthrough cross-layer depthwise convolutional layers with learnable filters,\nacting as a flexible yet efficient selection mechanism. DelugeNets can\npropagate information across many layers with greater flexibility and utilize\nnetwork parameters more effectively compared to ResNets, whilst being more\nefficient than DenseNets. Remarkably, a DelugeNet model with just model\ncomplexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve\nclassification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset\nrespectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on\nImageNet dataset, despite costing merely half of the computations needed by the\nlatter.\n","negative":"  Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.\n","id":233}
{"Unnamed: 0.1":11234,"Unnamed: 0":11234.0,"anchor":"Boosting Variational Inference","positive":"  Variational inference (VI) provides fast approximations of a Bayesian\nposterior in part because it formulates posterior approximation as an\noptimization problem: to find the closest distribution to the exact posterior\nover some family of distributions. For practical reasons, the family of\ndistributions in VI is usually constrained so that it does not include the\nexact posterior, even as a limit point. Thus, no matter how long VI is run, the\nresulting approximation will not approach the exact posterior. We propose to\ninstead consider a more flexible approximating family consisting of all\npossible finite mixtures of a parametric base distribution (e.g., Gaussian).\nFor efficient inference, we borrow ideas from gradient boosting to develop an\nalgorithm we call boosting variational inference (BVI). BVI iteratively\nimproves the current approximation by mixing it with a new component from the\nbase distribution family and thereby yields progressively more accurate\nposterior approximations as more computing time is spent. Unlike a number of\ncommon VI variants including mean-field VI, BVI is able to capture\nmultimodality, general posterior covariance, and nonstandard posterior shapes.\n","negative":"  Multivariate Pattern (MVP) classification can map different cognitive states\nto the brain tasks. One of the main challenges in MVP analysis is validating\nthe generated results across subjects. However, analyzing multi-subject fMRI\ndata requires accurate functional alignments between neuronal activities of\ndifferent subjects, which can rapidly increase the performance and robustness\nof the final results. Hyperalignment (HA) is one of the most effective\nfunctional alignment methods, which can be mathematically formulated by the\nCanonical Correlation Analysis (CCA) methods. Since HA mostly uses the\nunsupervised CCA techniques, its solution may not be optimized for MVP\nanalysis. By incorporating the idea of Local Discriminant Analysis (LDA) into\nCCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel\nsupervised HA method, which can provide better functional alignment for MVP\nanalysis. Indeed, the locality is defined based on the stimuli categories in\nthe train-set, where the correlation between all stimuli in the same category\nwill be maximized and the correlation between distinct categories of stimuli\napproaches to near zero. Experimental studies on multi-subject MVP analysis\nconfirm that the LDHA method achieves superior performance to other\nstate-of-the-art HA algorithms.\n","id":234}
{"Unnamed: 0.1":11235,"Unnamed: 0":11235.0,"anchor":"Optical Flow Requires Multiple Strategies (but only one network)","positive":"  We show that the matching problem that underlies optical flow requires\nmultiple strategies, depending on the amount of image motion and other factors.\nWe then study the implications of this observation on training a deep neural\nnetwork for representing image patches in the context of descriptor based\noptical flow. We propose a metric learning method, which selects suitable\nnegative samples based on the nature of the true match. This type of training\nproduces a network that displays multiple strategies depending on the input and\nleads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow\nbenchmarks.\n","negative":"  Multiple Sclerosis is a degenerative condition of the central nervous system\nthat affects nearly 2.5 million of individuals in terms of their physical,\ncognitive, psychological and social capabilities. Researchers are currently\ninvestigating on the use of patient reported outcome measures for the\nassessment of impact and evolution of the disease on the life of the patients.\nTo date, a clear understanding on the use of such measures to predict the\nevolution of the disease is still lacking. In this work we resort to\nregularized machine learning methods for binary classification and multiple\noutput regression. We propose a pipeline that can be used to predict the\ndisease progression from patient reported measures. The obtained model is\ntested on a data set collected from an ongoing clinical research project.\n","id":235}
{"Unnamed: 0.1":11236,"Unnamed: 0":11236.0,"anchor":"Inverting The Generator Of A Generative Adversarial Network","positive":"  Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning.\n","negative":"  Convolutional Neural Networks (CNNs) exhibit remarkable performance in\nvarious machine learning tasks. As sensor-equipped internet of things (IoT)\ndevices permeate into every aspect of modern life, it is increasingly important\nto run CNN inference, a computationally intensive application, on resource\nconstrained devices. We present a technique for fast and energy-efficient CNN\ninference on mobile SoC platforms, which are projected to be a major player in\nthe IoT space. We propose techniques for efficient parallelization of CNN\ninference targeting mobile GPUs, and explore the underlying tradeoffs.\nExperiments with running Squeezenet on three different mobile devices confirm\nthe effectiveness of our approach. For further study, please refer to the\nproject repository available on our GitHub page:\nhttps:\/\/github.com\/mtmd\/Mobile_ConvNet\n","id":236}
{"Unnamed: 0.1":11237,"Unnamed: 0":11237.0,"anchor":"Study on Feature Subspace of Archetypal Emotions for Speech Emotion\n  Recognition","positive":"  Feature subspace selection is an important part in speech emotion\nrecognition. Most of the studies are devoted to finding a feature subspace for\nrepresenting all emotions. However, some studies have indicated that the\nfeatures associated with different emotions are not exactly the same. Hence,\ntraditional methods may fail to distinguish some of the emotions with just one\nglobal feature subspace. In this work, we propose a new divide and conquer idea\nto solve the problem. First, the feature subspaces are constructed for all the\ncombinations of every two different emotions (emotion-pair). Bi-classifiers are\nthen trained on these feature subspaces respectively. The final emotion\nrecognition result is derived by the voting and competition method.\nExperimental results demonstrate that the proposed method can get better\nresults than the traditional multi-classification method.\n","negative":"  Random Forests (RFs) are strong machine learning tools for classification and\nregression. However, they remain supervised algorithms, and no extension of RFs\nto the one-class setting has been proposed, except for techniques based on\nsecond-class sampling. This work fills this gap by proposing a natural\nmethodology to extend standard splitting criteria to the one-class setting,\nstructurally generalizing RFs to one-class classification. An extensive\nbenchmark of seven state-of-the-art anomaly detection algorithms is also\npresented. This empirically demonstrates the relevance of our approach.\n","id":237}
{"Unnamed: 0.1":11238,"Unnamed: 0":11238.0,"anchor":"GENESIM: genetic extraction of a single, interpretable model","positive":"  Models obtained by decision tree induction techniques excel in being\ninterpretable.However, they can be prone to overfitting, which results in a low\npredictive performance. Ensemble techniques are able to achieve a higher\naccuracy. However, this comes at a cost of losing interpretability of the\nresulting model. This makes ensemble techniques impractical in applications\nwhere decision support, instead of decision making, is crucial.\n  To bridge this gap, we present the GENESIM algorithm that transforms an\nensemble of decision trees to a single decision tree with an enhanced\npredictive performance by using a genetic algorithm. We compared GENESIM to\nprevalent decision tree induction and ensemble techniques using twelve publicly\navailable data sets. The results show that GENESIM achieves a better predictive\nperformance on most of these data sets than decision tree induction techniques\nand a predictive performance in the same order of magnitude as the ensemble\ntechniques. Moreover, the resulting model of GENESIM has a very low complexity,\nmaking it very interpretable, in contrast to ensemble techniques.\n","negative":"  It is believed that hippocampus functions as a memory allocator in brain, the\nmechanism of which remains unrevealed. In Valiant's neuroidal model, the\nhippocampus was described as a randomly connected graph, the computation on\nwhich maps input to a set of activated neuroids with stable size. Valiant\nproposed three requirements for the hippocampal circuit to become a stable\nmemory allocator (SMA): stability, continuity and orthogonality. The\nfunctionality of SMA in hippocampus is essential in further computation within\ncortex, according to Valiant's model.\n  In this paper, we put these requirements for memorization functions into\nrigorous mathematical formulation and introduce the concept of capacity, based\non the probability of erroneous allocation. We prove fundamental limits for the\ncapacity and error probability of SMA, in both data-independent and\ndata-dependent settings. We also establish an example of stable memory\nallocator that can be implemented via neuroidal circuits. Both theoretical\nbounds and simulation results show that the neural SMA functions well.\n","id":238}
{"Unnamed: 0.1":11239,"Unnamed: 0":11239.0,"anchor":"Unimodal Thompson Sampling for Graph-Structured Arms","positive":"  We study, to the best of our knowledge, the first Bayesian algorithm for\nunimodal Multi-Armed Bandit (MAB) problems with graph structure. In this\nsetting, each arm corresponds to a node of a graph and each edge provides a\nrelationship, unknown to the learner, between two nodes in terms of expected\nreward. Furthermore, for any node of the graph there is a path leading to the\nunique node providing the maximum expected reward, along which the expected\nreward is monotonically increasing. Previous results on this setting describe\nthe behavior of frequentist MAB algorithms. In our paper, we design a Thompson\nSampling-based algorithm whose asymptotic pseudo-regret matches the lower bound\nfor the considered setting. We show that -as it happens in a wide number of\nscenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In\nparticular, we provide a thorough experimental evaluation of the performance of\nour and state-of-the-art algorithms as the properties of the graph vary.\n","negative":"  This paper proposes a method for domain adaptation that extends the maximum\nmargin domain transfer (MMDT) proposed by Hoffman et al., by introducing L2\ndistance constraints between samples of different domains; thus, our method is\ndenoted as MMDTL2. Motivated by the differences between the images taken by\nnarrow band imaging (NBI) endoscopic devices, we utilize different NBI devices\nas different domains and estimate the transformations between samples of\ndifferent domains, i.e., image samples taken by different NBI endoscope\nsystems. We first formulate the problem in the primal form, and then derive the\ndual form with much lesser computational costs as compared to the naive\napproach. From our experimental results using NBI image datasets from two\ndifferent NBI endoscopic devices, we find that MMDTL2 is better than MMDT and\nalso support vector machines without adaptation, especially when NBI image\nfeatures are high-dimensional and the per-class training samples are greater\nthan 20.\n","id":239}
{"Unnamed: 0.1":11240,"Unnamed: 0":11240.0,"anchor":"Relational Multi-Manifold Co-Clustering","positive":"  Co-clustering targets on grouping the samples (e.g., documents, users) and\nthe features (e.g., words, ratings) simultaneously. It employs the dual\nrelation and the bilateral information between the samples and features. In\nmany realworld applications, data usually reside on a submanifold of the\nambient Euclidean space, but it is nontrivial to estimate the intrinsic\nmanifold of the data space in a principled way. In this study, we focus on\nimproving the co-clustering performance via manifold ensemble learning, which\nis able to maximally approximate the intrinsic manifolds of both the sample and\nfeature spaces. To achieve this, we develop a novel co-clustering algorithm\ncalled Relational Multi-manifold Co-clustering (RMC) based on symmetric\nnonnegative matrix tri-factorization, which decomposes the relational data\nmatrix into three submatrices. This method considers the intertype relationship\nrevealed by the relational data matrix, and also the intra-type information\nreflected by the affinity matrices encoded on the sample and feature data\ndistributions. Specifically, we assume the intrinsic manifold of the sample or\nfeature space lies in a convex hull of some pre-defined candidate manifolds. We\nwant to learn a convex combination of them to maximally approach the desired\nintrinsic manifold. To optimize the objective function, the multiplicative\nrules are utilized to update the submatrices alternatively. Besides, both the\nentropic mirror descent algorithm and the coordinate descent algorithm are\nexploited to learn the manifold coefficient vector. Extensive experiments on\ndocuments, images and gene expression data sets have demonstrated the\nsuperiority of the proposed algorithm compared to other well-established\nmethods.\n","negative":"  Can humans impute missing data with similar proficiency as machines? This is\nthe question we aim to answer in this paper. We present a novel idea of\nconverting observations with missing data in to a survey questionnaire, which\nis presented to crowdworkers for completion. We replicate a multiple imputation\nframework by having multiple unique crowdworkers complete our questionnaire.\nExperimental results demonstrate that using our method, it is possible to\ngenerate valid imputations for qualitative and quantitative missing data, with\nresults comparable to imputations generated by complex statistical models.\n","id":240}
{"Unnamed: 0.1":11241,"Unnamed: 0":11241.0,"anchor":"A Multi-Modal Graph-Based Semi-Supervised Pipeline for Predicting Cancer\n  Survival","positive":"  Cancer survival prediction is an active area of research that can help\nprevent unnecessary therapies and improve patient's quality of life. Gene\nexpression profiling is being widely used in cancer studies to discover\ninformative biomarkers that aid predict different clinical endpoint prediction.\nWe use multiple modalities of data derived from RNA deep-sequencing (RNA-seq)\nto predict survival of cancer patients. Despite the wealth of information\navailable in expression profiles of cancer tumors, fulfilling the\naforementioned objective remains a big challenge, for the most part, due to the\npaucity of data samples compared to the high dimension of the expression\nprofiles. As such, analysis of transcriptomic data modalities calls for\nstate-of-the-art big-data analytics techniques that can maximally use all the\navailable data to discover the relevant information hidden within a significant\namount of noise. In this paper, we propose a pipeline that predicts cancer\npatients' survival by exploiting the structure of the input (manifold learning)\nand by leveraging the unlabeled samples using Laplacian support vector\nmachines, a graph-based semi supervised learning (GSSL) paradigm. We show that\nunder certain circumstances, no single modality per se will result in the best\naccuracy and by fusing different models together via a stacked generalization\nstrategy, we may boost the accuracy synergistically. We apply our approach to\ntwo cancer datasets and present promising results. We maintain that a similar\npipeline can be used for predictive tasks where labeled samples are expensive\nto acquire.\n","negative":"  The recently introduced Intelligent Trial-and-Error (IT&E) algorithm showed\nthat robots can adapt to damage in a matter of a few trials. The success of\nthis algorithm relies on two components: prior knowledge acquired through\nsimulation with an intact robot, and Bayesian optimization (BO) that operates\non-line, on the damaged robot. While IT&E leads to fast damage recovery, it\ndoes not incorporate any safety constraints that prevent the robot from\nattempting harmful behaviors. In this work, we address this limitation by\nreplacing the BO component with a constrained BO procedure. We evaluate our\napproach on a simulated damaged humanoid robot that needs to crawl as fast as\npossible, while performing as few unsafe trials as possible. We compare our new\n\"safety-aware IT&E\" algorithm to IT&E and a multi-objective version of IT&E in\nwhich the safety constraints are dealt as separate objectives. Our results show\nthat our algorithm outperforms the other approaches, both in crawling speed\nwithin the safe regions and number of unsafe trials.\n","id":241}
{"Unnamed: 0.1":11242,"Unnamed: 0":11242.0,"anchor":"Learning to reinforcement learn","positive":"  In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.\n","negative":"  Semi-supervised learning algorithms reduce the high cost of acquiring labeled\ntraining data by using both labeled and unlabeled data during learning. Deep\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\nand as such have been widely employed in the semi-supervised learning. In this\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\nprobabilistic generative model that models latent nuisance variation, and whose\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\nwe introduce a novel non-negativity constraint and a variational inference\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\nsemi-supervised setting represents latent nuisance variation using\nsynthetically rendered images. Taken together, our work provides a unified\nframework for supervised, unsupervised, and semi-supervised learning.\n","id":242}
{"Unnamed: 0.1":11243,"Unnamed: 0":11243.0,"anchor":"Gap Safe screening rules for sparsity enforcing penalties","positive":"  In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1\/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets.\n","negative":"  Learning representations of data, and in particular learning features for a\nsubsequent prediction task, has been a fruitful area of research delivering\nimpressive empirical results in recent years. However, relatively little is\nunderstood about what makes a representation `good'. We propose the idea of a\nrisk gap induced by representation learning for a given prediction context,\nwhich measures the difference in the risk of some learner using the learned\nfeatures as compared to the original inputs. We describe a set of sufficient\nconditions for unsupervised representation learning to provide a benefit, as\nmeasured by this risk gap. These conditions decompose the problem of when\nrepresentation learning works into its constituent parts, which can be\nseparately evaluated using an unlabeled sample, suitable domain-specific\nassumptions about the joint distribution, and analysis of the feature learner\nand subsequent supervised learner. We provide two examples of such conditions\nin the context of specific properties of the unlabeled distribution, namely\nwhen the data lies close to a low-dimensional manifold and when it forms\nclusters. We compare our approach to a recently proposed analysis of\nsemi-supervised learning.\n","id":243}
{"Unnamed: 0.1":11244,"Unnamed: 0":11244.0,"anchor":"Data Science in Service of Performing Arts: Applying Machine Learning to\n  Predicting Audience Preferences","positive":"  Performing arts organizations aim to enrich their communities through the\narts. To do this, they strive to match their performance offerings to the taste\nof those communities. Success relies on understanding audience preference and\npredicting their behavior. Similar to most e-commerce or digital entertainment\nfirms, arts presenters need to recommend the right performance to the right\ncustomer at the right time. As part of the Michigan Data Science Team (MDST),\nwe partnered with the University Musical Society (UMS), a non-profit performing\narts presenter housed in the University of Michigan, Ann Arbor. We are\nproviding UMS with analysis and business intelligence, utilizing historical\nindividual-level sales data. We built a recommendation system based on\ncollaborative filtering, gaining insights into the artistic preferences of\ncustomers, along with the similarities between performances. To better\nunderstand audience behavior, we used statistical methods from customer-base\nanalysis. We characterized customer heterogeneity via segmentation, and we\nmodeled customer cohorts to understand and predict ticket purchasing patterns.\nFinally, we combined statistical modeling with natural language processing\n(NLP) to explore the impact of wording in program descriptions. These ongoing\nefforts provide a platform to launch targeted marketing campaigns, helping UMS\ncarry out its mission by allocating its resources more efficiently. Celebrating\nits 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it\ncontinues to enrich communities by connecting world-renowned artists with\ndiverse audiences, especially students in their formative years. We aim to\ncontribute to that mission through data science and customer analytics.\n","negative":"  Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.\n","id":244}
{"Unnamed: 0.1":11245,"Unnamed: 0":11245.0,"anchor":"Nothing Else Matters: Model-Agnostic Explanations By Identifying\n  Prediction Invariance","positive":"  At the core of interpretable machine learning is the question of whether\nhumans are able to make accurate predictions about a model's behavior. Assumed\nin this question are three properties of the interpretable output: coverage,\nprecision, and effort. Coverage refers to how often humans think they can\npredict the model's behavior, precision to how accurate humans are in those\npredictions, and effort is either the up-front effort required in interpreting\nthe model, or the effort required to make predictions about a model's behavior.\n  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that\nproduces high-precision rule-based explanations for which the coverage\nboundaries are very clear. We compare aLIME to linear LIME with simulated\nexperiments, and demonstrate the flexibility of aLIME with qualitative examples\nfrom a variety of domains and tasks.\n","negative":"  In the scope of gestural action recognition, the size of the feature vector\nrepresenting movements is in general quite large especially when full body\nmovements are considered. Furthermore, this feature vector evolves during the\nmovement performance so that a complete movement is fully represented by a\nmatrix M of size DxT , whose element M i, j represents the value of feature i\nat timestamps j. Many studies have addressed dimensionality reduction\nconsidering only the size of the feature vector lying in R D to reduce both the\nvariability of gestural sequences expressed in the reduced space, and the\ncomputational complexity of their processing. In return, very few of these\nmethods have explicitly addressed the dimensionality reduction along the time\naxis. Yet this is a major issue when considering the use of elastic distances\nwhich are characterized by a quadratic complexity along the time axis. We\npresent in this paper an evaluation of straightforward approaches aiming at\nreducing the dimensionality of the matrix M for each movement, leading to\nconsider both the dimensionality reduction of the feature vector as well as its\nreduction along the time axis. The dimensionality reduction of the feature\nvector is achieved by selecting remarkable joints in the skeleton performing\nthe movement, basically the extremities of the articulatory chains composing\nthe skeleton. The temporal dimen-sionality reduction is achieved using either a\nregular or adaptive down-sampling that seeks to minimize the reconstruction\nerror of the movements. Elastic and Euclidean kernels are then compared through\nsupport vector machine learning. Two data sets 1 that are widely referenced in\nthe domain of human gesture recognition, and quite distinctive in terms of\nquality of motion capture, are used for the experimental assessment of the\nproposed approaches. On these data sets we experimentally show that it is\nfeasible, and possibly desirable, to significantly reduce simultaneously the\nsize of the feature vector and the number of skeleton frames to represent body\nmovements while maintaining a very good recognition rate. The method proves to\ngive satisfactory results at a level currently reached by state-of-the-art\nmethods on these data sets. We experimentally show that the computational\ncomplexity reduction that is obtained makes this approach eligible for\nreal-time applications.\n","id":245}
{"Unnamed: 0.1":11246,"Unnamed: 0":11246.0,"anchor":"Towards a Mathematical Understanding of the Difficulty in Learning with\n  Feedforward Neural Networks","positive":"  Training deep neural networks for solving machine learning problems is one\ngreat challenge in the field, mainly due to its associated optimisation problem\nbeing highly non-convex. Recent developments have suggested that many training\nalgorithms do not suffer from undesired local minima under certain scenario,\nand consequently led to great efforts in pursuing mathematical explanations for\nsuch observations. This work provides an alternative mathematical understanding\nof the challenge from a smooth optimisation perspective. By assuming exact\nlearning of finite samples, sufficient conditions are identified via a critical\npoint analysis to ensure any local minimum to be globally minimal as well.\nFurthermore, a state of the art algorithm, known as the Generalised\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\nNewton's algorithm, which shares the property of being locally quadratically\nconvergent to a global minimum under the condition of exact learning.\n","negative":"  Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.\n","id":246}
{"Unnamed: 0.1":11247,"Unnamed: 0":11247.0,"anchor":"Associative Memories to Accelerate Approximate Nearest Neighbor Search","positive":"  Nearest neighbor search is a very active field in machine learning for it\nappears in many application cases, including classification and object\nretrieval. In its canonical version, the complexity of the search is linear\nwith both the dimension and the cardinal of the collection of vectors the\nsearch is performed in. Recently many works have focused on reducing the\ndimension of vectors using quantization techniques or hashing, while providing\nan approximate result. In this paper we focus instead on tackling the cardinal\nof the collection of vectors. Namely, we introduce a technique that partitions\nthe collection of vectors and stores each part in its own associative memory.\nWhen a query vector is given to the system, associative memories are polled to\nidentify which one contain the closest match. Then an exhaustive search is\nconducted only on the part of vectors stored in the selected associative\nmemory. We study the effectiveness of the system when messages to store are\ngenerated from i.i.d. uniform $\\pm$1 random variables or 0-1 sparse i.i.d.\nrandom variables. We also conduct experiment on both synthetic data and real\ndata and show it is possible to achieve interesting trade-offs between\ncomplexity and accuracy.\n","negative":"  We present novel method for image-text multi-modal representation learning.\nIn our knowledge, this work is the first approach of applying adversarial\nlearning concept to multi-modal learning and not exploiting image-text pair\ninformation to learn multi-modal feature. We only use category information in\ncontrast with most previous methods using image-text pair information for\nmulti-modal embedding. In this paper, we show that multi-modal feature can be\nachieved without image-text pair information and our method makes more similar\ndistribution with image and text in multi-modal feature space than other\nmethods which use image-text pair information. And we show our multi-modal\nfeature has universal semantic information, even though it was trained for\ncategory prediction. Our model is end-to-end backpropagation, intuitive and\neasily extended to other multi-modal learning work.\n","id":247}
{"Unnamed: 0.1":11248,"Unnamed: 0":11248.0,"anchor":"\"Influence Sketching\": Finding Influential Samples In Large-Scale\n  Regressions","positive":"  There is an especially strong need in modern large-scale data analysis to\nprioritize samples for manual inspection. For example, the inspection could\ntarget important mislabeled samples or key vulnerabilities exploitable by an\nadversarial attack. In order to solve the \"needle in the haystack\" problem of\nwhich samples to inspect, we develop a new scalable version of Cook's distance,\na classical statistical technique for identifying samples which unusually\nstrongly impact the fit of a regression model (and its downstream predictions).\nIn order to scale this technique up to very large and high-dimensional\ndatasets, we introduce a new algorithm which we call \"influence sketching.\"\nInfluence sketching embeds random projections within the influence computation;\nin particular, the influence score is calculated using the randomly projected\npseudo-dataset from the post-convergence Generalized Linear Model (GLM). We\nvalidate that influence sketching can reliably and successfully discover\ninfluential samples by applying the technique to a malware detection dataset of\nover 2 million executable files, each represented with almost 100,000 features.\nFor example, we find that randomly deleting approximately 10% of training\nsamples reduces predictive accuracy only slightly from 99.47% to 99.45%,\nwhereas deleting the same number of samples with high influence sketch scores\nreduces predictive accuracy all the way down to 90.24%. Moreover, we find that\ninfluential samples are especially likely to be mislabeled. In the case study,\nwe manually inspect the most influential samples, and find that influence\nsketching pointed us to new, previously unidentified pieces of malware.\n","negative":"  We are interested in learning customers' video preferences from their\nhistoric viewing patterns and geographical location. We consider a Bayesian\nlatent factor modeling approach for this task. In order to tune the complexity\nof the model to best represent the data, we make use of Bayesian nonparameteric\ntechniques. We describe an inference technique that can scale to large\nreal-world data sets. Finally we show results obtained by applying the model to\na large internal Netflix data set, that illustrates that the model was able to\ncapture interesting relationships between viewing patterns and geographical\nlocation.\n","id":248}
{"Unnamed: 0.1":11249,"Unnamed: 0":11249.0,"anchor":"Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models","positive":"  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks, state of the art models in\nspeech recognition and translation. Our approach to increasing interpretability\nis by combining a long short-term memory (LSTM) model with a hidden Markov\nmodel (HMM), a simpler and more transparent model. We add the HMM state\nprobabilities to the output layer of the LSTM, and then train the HMM and LSTM\neither sequentially or jointly. The LSTM can make use of the information from\nthe HMM, and fill in the gaps when the HMM is not performing well. A small\nhybrid model usually performs better than a standalone LSTM of the same size,\nespecially on smaller data sets. We test the algorithms on text data and\nmedical time series data, and find that the LSTM and HMM learn complementary\ninformation about the features in the text.\n","negative":"  Several machine learning tasks require to represent the data using only a\nsparse set of interest points. An ideal detector is able to find the\ncorresponding interest points even if the data undergo a transformation typical\nfor a given domain. Since the task is of high practical interest in computer\nvision, many hand-crafted solutions were proposed. In this paper, we ask a\nfundamental question: can we learn such detectors from scratch? Since it is\noften unclear what points are \"interesting\", human labelling cannot be used to\nfind a truly unbiased solution. Therefore, the task requires an unsupervised\nformulation. We are the first to propose such a formulation: training a neural\nnetwork to rank points in a transformation-invariant manner. Interest points\nare then extracted from the top\/bottom quantiles of this ranking. We validate\nour approach on two tasks: standard RGB image interest point detection and\nchallenging cross-modal interest point detection between RGB and depth images.\nWe quantitatively show that our unsupervised method performs better or on-par\nwith baselines.\n","id":249}
{"Unnamed: 0.1":11250,"Unnamed: 0":11250.0,"anchor":"Analysis of a Design Pattern for Teaching with Features and Labels","positive":"  We study the task of teaching a machine to classify objects using features\nand labels. We introduce the Error-Driven-Featuring design pattern for teaching\nusing features and labels in which a teacher prefers to introduce features only\nif they are needed. We analyze the potential risks and benefits of this\nteaching pattern through the use of teaching protocols, illustrative examples,\nand by providing bounds on the effort required for an optimal machine teacher\nusing a linear learning algorithm, the most commonly used type of learners in\ninteractive machine learning systems. Our analysis provides a deeper\nunderstanding of potential trade-offs of using different learning algorithms\nand between the effort required for featuring (creating new features) and\nlabeling (providing labels for objects).\n","negative":"  Most neural network models for document classification on social media focus\non text infor-mation to the neglect of other information on these platforms. In\nthis paper, we classify post stance on social media channels and develop UTCNN,\na neural network model that incorporates user tastes, topic tastes, and user\ncomments on posts. UTCNN not only works on social media texts, but also\nanalyzes texts in forums and message boards. Experiments performed on Chinese\nFacebook data and English online debate forum data show that UTCNN achieves a\n0.755 macro-average f-score for supportive, neutral, and unsupportive stance\nclasses on Facebook data, which is significantly better than models in which\neither user, topic, or comment information is withheld. This model design\ngreatly mitigates the lack of data for the minor class without the use of\noversampling. In addition, UTCNN yields a 0.842 accuracy on English online\ndebate forum data, which also significantly outperforms results from previous\nwork as well as other deep learning models, showing that UTCNN performs well\nregardless of language or platform.\n","id":250}
{"Unnamed: 0.1":11251,"Unnamed: 0":11251.0,"anchor":"A Characterization of Prediction Errors","positive":"  Understanding prediction errors and determining how to fix them is critical\nto building effective predictive systems. In this paper, we delineate four\ntypes of prediction errors and demonstrate that these four types characterize\nall prediction errors. In addition, we describe potential remedies and tools\nthat can be used to reduce the uncertainty when trying to determine the source\nof a prediction error and when trying to take action to remove a prediction\nerrors.\n","negative":"  Automated fault localization is an important issue in model validation and\nverification. It helps the end users in analyzing the origin of failure. In\nthis work, we show the early experiments with probabilistic analysis approaches\nin fault localization. Inspired by the Kullback-Leibler Divergence from\nBayesian probabilistic theory, we propose a suspiciousness factor to compute\nthe fault contribution for the transitions in the reachability graph of model\nchecking, using which to rank the potential faulty transitions. To\nautomatically locate design faults in the simulation model of detailed design,\nwe propose to use the statistical model Hidden Markov Model (HMM), which\nprovides statistically identical information to component's real behavior. The\ncore of this method is a fault localization algorithm that gives out the set of\nsuspicious ranked faulty components and a backward algorithm that computes the\nmatching degree between the HMM and the simulation model to evaluate the\nconfidence degree of the localization conclusion.\n","id":251}
{"Unnamed: 0.1":11252,"Unnamed: 0":11252.0,"anchor":"Robust and Scalable Column\/Row Sampling from Corrupted Big Data","positive":"  Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data.\n","negative":"  We explore a recently proposed Variational Dropout technique that provided an\nelegant Bayesian interpretation to Gaussian Dropout. We extend Variational\nDropout to the case when dropout rates are unbounded, propose a way to reduce\nthe variance of the gradient estimator and report first experimental results\nwith individual dropout rates per weight. Interestingly, it leads to extremely\nsparse solutions both in fully-connected and convolutional layers. This effect\nis similar to automatic relevance determination effect in empirical Bayes but\nhas a number of advantages. We reduce the number of parameters up to 280 times\non LeNet architectures and up to 68 times on VGG-like networks with a\nnegligible decrease of accuracy.\n","id":252}
{"Unnamed: 0.1":11253,"Unnamed: 0":11253.0,"anchor":"Monte Carlo Tableau Proof Search","positive":"  We study Monte Carlo Tree Search to guide proof search in tableau calculi.\nThis includes proposing a number of proof-state evaluation heuristics, some of\nwhich are learnt from previous proofs. We present an implementation based on\nthe leanCoP prover. The system is trained and evaluated on a large suite of\nrelated problems coming from the Mizar proof assistant, showing that it is\ncapable to find new and different proofs.\n","negative":"  Near-sensor data analytics is a promising direction for IoT endpoints, as it\nminimizes energy spent on communication and reduces network load - but it also\nposes security concerns, as valuable data is stored or sent over the network at\nvarious stages of the analytics pipeline. Using encryption to protect sensitive\ndata at the boundary of the on-chip analytics engine is a way to address data\nsecurity issues. To cope with the combined workload of analytics and encryption\nin a tight power envelope, we propose Fulmine, a System-on-Chip based on a\ntightly-coupled multi-core cluster augmented with specialized blocks for\ncompute-intensive data processing and encryption functions, supporting software\nprogrammability for regular computing tasks. The Fulmine SoC, fabricated in\n65nm technology, consumes less than 20mW on average at 0.8V achieving an\nefficiency of up to 70pJ\/B in encryption, 50pJ\/px in convolution, or up to\n25MIPS\/mW in software. As a strong argument for real-life flexible application\nof our platform, we show experimental results for three secure analytics use\ncases: secure autonomous aerial surveillance with a state-of-the-art deep CNN\nconsuming 3.16pJ per equivalent RISC op; local CNN-based face detection with\nsecured remote recognition in 5.74pJ\/op; and seizure detection with encrypted\ndata collection from EEG within 12.7pJ\/op.\n","id":253}
{"Unnamed: 0.1":11254,"Unnamed: 0":11254.0,"anchor":"A Generalized Stochastic Variational Bayesian Hyperparameter Learning\n  Framework for Sparse Spectrum Gaussian Process Regression","positive":"  While much research effort has been dedicated to scaling up sparse Gaussian\nprocess (GP) models based on inducing variables for big data, little attention\nis afforded to the other less explored class of low-rank GP approximations that\nexploit the sparse spectral representation of a GP kernel. This paper presents\nsuch an effort to advance the state of the art of sparse spectrum GP models to\nachieve competitive predictive performance for massive datasets. Our\ngeneralized framework of stochastic variational Bayesian sparse spectrum GP\n(sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment\nof the spectral frequencies to avoid overfitting, modeling these frequencies\njointly in its variational distribution to enable their interaction a\nposteriori, and exploiting local data for boosting the predictive performance.\nHowever, such structural improvements result in a variational lower bound that\nis intractable to be optimized. To resolve this, we exploit a variational\nparameterization trick to make it amenable to stochastic optimization.\nInterestingly, the resulting stochastic gradient has a linearly decomposable\nstructure that can be exploited to refine our stochastic optimization method to\nincur constant time per iteration while preserving its property of being an\nunbiased estimator of the exact gradient of the variational lower bound.\nEmpirical evaluation on real-world datasets shows that sVBSSGP outperforms\nstate-of-the-art stochastic implementations of sparse GP models.\n","negative":"  We introduce a conditional generative model for learning to disentangle the\nhidden factors of variation within a set of labeled observations, and separate\nthem into complementary codes. One code summarizes the specified factors of\nvariation associated with the labels. The other summarizes the remaining\nunspecified variability. During training, the only available source of\nsupervision comes from our ability to distinguish among different observations\nbelonging to the same class. Examples of such observations include images of a\nset of labeled objects captured at different viewpoints, or recordings of set\nof speakers dictating multiple phrases. In both instances, the intra-class\ndiversity is the source of the unspecified factors of variation: each object is\nobserved at multiple viewpoints, and each speaker dictates multiple phrases.\nLearning to disentangle the specified factors from the unspecified ones becomes\neasier when strong supervision is possible. Suppose that during training, we\nhave access to pairs of images, where each pair shows two different objects\ncaptured from the same viewpoint. This source of alignment allows us to solve\nour task using existing methods. However, labels for the unspecified factors\nare usually unavailable in realistic scenarios where data acquisition is not\nstrictly controlled. We address the problem of disentanglement in this more\ngeneral setting by combining deep convolutional autoencoders with a form of\nadversarial training. Both factors of variation are implicitly captured in the\norganization of the learned embedding space, and can be used for solving\nsingle-image analogies. Experimental results on synthetic and real datasets\nshow that the proposed method is capable of generalizing to unseen classes and\nintra-class variabilities.\n","id":254}
{"Unnamed: 0.1":11255,"Unnamed: 0":11255.0,"anchor":"Faster variational inducing input Gaussian process classification","positive":"  Gaussian processes (GP) provide a prior over functions and allow finding\ncomplex regularities in data. Gaussian processes are successfully used for\nclassification\/regression problems and dimensionality reduction. In this work\nwe consider the classification problem only. The complexity of standard methods\nfor GP-classification scales cubically with the size of the training dataset.\nThis complexity makes them inapplicable to big data problems. Therefore, a\nvariety of methods were introduced to overcome this limitation. In the paper we\nfocus on methods based on so called inducing inputs. This approach is based on\nvariational inference and proposes a particular lower bound for marginal\nlikelihood (evidence). This bound is then maximized w.r.t. parameters of kernel\nfunction of the Gaussian process, thus fitting the model to data. The\ncomputational complexity of this method is $O(nm^2)$, where $m$ is the number\nof inducing inputs used by the model and is assumed to be substantially smaller\nthan the size of the dataset $n$. Recently, a new evidence lower bound for\nGP-classification problem was introduced. It allows using stochastic\noptimization, which makes it suitable for big data problems. However, the new\nlower bound depends on $O(m^2)$ variational parameter, which makes optimization\nchallenging in case of big m. In this work we develop a new approach for\ntraining inducing input GP models for classification problems. Here we use\nquadratic approximation of several terms in the aforementioned evidence lower\nbound, obtaining analytical expressions for optimal values of most of the\nparameters in the optimization, thus sufficiently reducing the dimension of\noptimization space. In our experiments we achieve as well or better results,\ncompared to the existing method. Moreover, our method doesn't require the user\nto manually set the learning rate, making it more practical, than the existing\nmethod.\n","negative":"  When humans learn a new concept, they might ignore examples that they cannot\nmake sense of at first, and only later focus on such examples, when they are\nmore useful for learning. We propose incorporating this idea of tunable\nsensitivity for hard examples in neural network learning, using a new\ngeneralization of the cross-entropy gradient step, which can be used in place\nof the gradient in any gradient-based training method. The generalized gradient\nis parameterized by a value that controls the sensitivity of the training\nprocess to harder training examples. We tested our method on several benchmark\ndatasets. We propose, and corroborate in our experiments, that the optimal\nlevel of sensitivity to hard example is positively correlated with the depth of\nthe network. Moreover, the test prediction error obtained by our method is\ngenerally lower than that of the vanilla cross-entropy gradient learner. We\ntherefore conclude that tunable sensitivity can be helpful for neural network\nlearning.\n","id":255}
{"Unnamed: 0.1":11256,"Unnamed: 0":11256.0,"anchor":"Compacting Neural Network Classifiers via Dropout Training","positive":"  We introduce dropout compaction, a novel method for training feed-forward\nneural networks which realizes the performance gains of training a large model\nwith dropout regularization, yet extracts a compact neural network for run-time\nefficiency. In the proposed method, we introduce a sparsity-inducing prior on\nthe per unit dropout retention probability so that the optimizer can\neffectively prune hidden units during training. By changing the prior\nhyperparameters, we can control the size of the resulting network. We performed\na systematic comparison of dropout compaction and competing methods on several\nreal-world speech recognition tasks and found that dropout compaction achieved\ncomparable accuracy with fewer than 50% of the hidden units, translating to a\n2.5x speedup in run-time.\n","negative":"  The combinatorial explosion that plagues planning and reinforcement learning\n(RL) algorithms can be moderated using state abstraction. Prohibitively large\ntask representations can be condensed such that essential information is\npreserved, and consequently, solutions are tractably computable. However, exact\nabstractions, which treat only fully-identical situations as equivalent, fail\nto present opportunities for abstraction in environments where no two\nsituations are exactly alike. In this work, we investigate approximate state\nabstractions, which treat nearly-identical situations as equivalent. We present\ntheoretical guarantees of the quality of behaviors derived from four types of\napproximate abstractions. Additionally, we empirically demonstrate that\napproximate abstractions lead to reduction in task complexity and bounded loss\nof optimality of behavior in a variety of environments.\n","id":256}
{"Unnamed: 0.1":11257,"Unnamed: 0":11257.0,"anchor":"Learning Interpretability for Visualizations using Adapted Cox Models\n  through a User Experiment","positive":"  In order to be useful, visualizations need to be interpretable. This paper\nuses a user-based approach to combine and assess quality measures in order to\nbetter model user preferences. Results show that cluster separability measures\nare outperformed by a neighborhood conservation measure, even though the former\nare usually considered as intuitively representative of user motives. Moreover,\ncombining measures, as opposed to using a single measure, further improves\nprediction performances.\n","negative":"  Mastering a video game requires skill, tactics and strategy. While these\nattributes may be acquired naturally by human players, teaching them to a\ncomputer program is a far more challenging task. In recent years, extensive\nresearch was carried out in the field of reinforcement learning and numerous\nalgorithms were introduced, aiming to learn how to perform human tasks such as\nplaying video games. As a result, the Arcade Learning Environment (ALE)\n(Bellemare et al., 2013) has become a commonly used benchmark environment\nallowing algorithms to train on various Atari 2600 games. In many games the\nstate-of-the-art algorithms outperform humans. In this paper we introduce a new\nlearning environment, the Retro Learning Environment --- RLE, that can run\ngames on the Super Nintendo Entertainment System (SNES), Sega Genesis and\nseveral other gaming consoles. The environment is expandable, allowing for more\nvideo games and consoles to be easily added to the environment, while\nmaintaining the same interface as ALE. Moreover, RLE is compatible with Python\nand Torch. SNES games pose a significant challenge to current algorithms due to\ntheir higher level of complexity and versatility.\n","id":257}
{"Unnamed: 0.1":11258,"Unnamed: 0":11258.0,"anchor":"Variable Computation in Recurrent Neural Networks","positive":"  Recurrent neural networks (RNNs) have been used extensively and with\nincreasing success to model various types of sequential data. Much of this\nprogress has been achieved through devising recurrent units and architectures\nwith the flexibility to capture complex statistics in the data, such as long\nrange dependency or localized attention phenomena. However, while many\nsequential data (such as video, speech or language) can have highly variable\ninformation flow, most recurrent models still consume input features at a\nconstant rate and perform a constant number of computations per time step,\nwhich can be detrimental to both speed and model capacity. In this paper, we\nexplore a modification to existing recurrent units which allows them to learn\nto vary the amount of computation they perform at each step, without prior\nknowledge of the sequence's time structure. We show experimentally that not\nonly do our models require fewer operations, they also lead to better\nperformance overall on evaluation tasks.\n","negative":"  Brain computer interfaces (BCI) enable direct communication with a computer,\nusing neural activity as the control signal. This neural signal is generally\nchosen from a variety of well-studied electroencephalogram (EEG) signals. For a\ngiven BCI paradigm, feature extractors and classifiers are tailored to the\ndistinct characteristics of its expected EEG control signal, limiting its\napplication to that specific signal. Convolutional Neural Networks (CNNs),\nwhich have been used in computer vision and speech recognition, have\nsuccessfully been applied to EEG-based BCIs; however, they have mainly been\napplied to single BCI paradigms and thus it remains unclear how these\narchitectures generalize to other paradigms. Here, we ask if we can design a\nsingle CNN architecture to accurately classify EEG signals from different BCI\nparadigms, while simultaneously being as compact as possible. In this work we\nintroduce EEGNet, a compact convolutional network for EEG-based BCIs. We\nintroduce the use of depthwise and separable convolutions to construct an\nEEG-specific model which encapsulates well-known EEG feature extraction\nconcepts for BCI. We compare EEGNet to current state-of-the-art approaches\nacross four BCI paradigms: P300 visual-evoked potentials, error-related\nnegativity responses (ERN), movement-related cortical potentials (MRCP), and\nsensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms\nbetter than the reference algorithms when only limited training data is\navailable. We demonstrate three different approaches to visualize the contents\nof a trained EEGNet model to enable interpretation of the learned features. Our\nresults suggest that EEGNet is robust enough to learn a wide variety of\ninterpretable features over a range of BCI tasks, suggesting that the observed\nperformances were not due to artifact or noise sources in the data.\n","id":258}
{"Unnamed: 0.1":11259,"Unnamed: 0":11259.0,"anchor":"Visualizing and Understanding Curriculum Learning for Long Short-Term\n  Memory Networks","positive":"  Curriculum Learning emphasizes the order of training instances in a\ncomputational learning setup. The core hypothesis is that simpler instances\nshould be learned early as building blocks to learn more complex ones. Despite\nits usefulness, it is still unknown how exactly the internal representation of\nmodels are affected by curriculum learning. In this paper, we study the effect\nof curriculum learning on Long Short-Term Memory (LSTM) networks, which have\nshown strong competency in many Natural Language Processing (NLP) problems. Our\nexperiments on sentiment analysis task and a synthetic task similar to sequence\nprediction tasks in NLP show that curriculum learning has a positive effect on\nthe LSTM's internal states by biasing the model towards building constructive\nrepresentations i.e. the internal representation at the previous timesteps are\nused as building blocks for the final prediction. We also find that smaller\nmodels significantly improves when they are trained with curriculum learning.\nLastly, we show that curriculum learning helps more when the amount of training\ndata is limited.\n","negative":"  In this paper, we study deep generative models for effective unsupervised\nlearning. We propose VGAN, which works by minimizing a variational lower bound\nof the negative log likelihood (NLL) of an energy based model (EBM), where the\nmodel density $p(\\mathbf{x})$ is approximated by a variational distribution\n$q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two\nstep procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize\nthe lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn\nfrom $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the\ngenerative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to\nthe discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with\nseveral notable differences. We hence name our model variational GANs (VGANs).\nVGAN provides a practical solution to training deep EBMs in high dimensional\nspace, by eliminating the need of MCMC sampling. From this view, we are also\nable to identify causes to the difficulty of training GANs and propose viable\nsolutions. \\footnote{Experimental code is available at\nhttps:\/\/github.com\/Shuangfei\/vgan}\n","id":259}
{"Unnamed: 0.1":11260,"Unnamed: 0":11260.0,"anchor":"GaDei: On Scale-up Training As A Service For Deep Learning","positive":"  Deep learning (DL) training-as-a-service (TaaS) is an important emerging\nindustrial workload. The unique challenge of TaaS is that it must satisfy a\nwide range of customers who have no experience and resources to tune DL\nhyper-parameters, and meticulous tuning for each user's dataset is\nprohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with\nvalues that are applicable to all users. IBM Watson Natural Language Classifier\n(NLC) service, the most popular IBM cognitive service used by thousands of\nenterprise-level clients around the globe, is a typical TaaS service. By\nevaluating the NLC workloads, we show that only the conservative\nhyper-parameter setup (e.g., small mini-batch size and small learning rate) can\nguarantee acceptable model accuracy for a wide range of customers. We further\njustify theoretically why such a setup guarantees better model convergence in\ngeneral. Unfortunately, the small mini-batch size causes a high volume of\ncommunication traffic in a parameter-server based system. We characterize the\nhigh communication bandwidth requirement of TaaS using representative\nindustrial deep learning workloads and demonstrate that none of the\nstate-of-the-art scale-up or scale-out solutions can satisfy such a\nrequirement. We then present GaDei, an optimized shared-memory based scale-up\nparameter server design. We prove that the designed protocol is deadlock-free\nand it processes each gradient exactly once. Our implementation is evaluated on\nboth commercial benchmarks and public benchmarks to demonstrate that it\nsignificantly outperforms the state-of-the-art parameter-server based\nimplementation while maintaining the required accuracy and our implementation\nreaches near the best possible runtime performance, constrained only by the\nhardware limitation. Furthermore, to the best of our knowledge, GaDei is the\nonly scale-up DL system that provides fault-tolerance.\n","negative":"  Regression under the \"small $n$, large $p$\" conditions, of small sample size\n$n$ and large number of features $p$ in the learning data set, is a recurring\nsetting in which learning from data is difficult. With prior knowledge about\nrelationships of the features, $p$ can effectively be reduced, but explicating\nsuch prior knowledge is difficult for experts. In this paper we introduce a new\nmethod for eliciting expert prior knowledge about the similarity of the roles\nof features in the prediction task. The key idea is to use an interactive\nmultidimensional-scaling (MDS) type scatterplot display of the features to\nelicit the similarity relationships, and then use the elicited relationships in\nthe prior distribution of prediction parameters. Specifically, for learning to\npredict a target variable with Bayesian linear regression, the feature\nrelationships are used to construct a Gaussian prior with a full covariance\nmatrix for the regression coefficients. Evaluation of our method in experiments\nwith simulated and real users on text data confirm that prior elicitation of\nfeature similarities improves prediction accuracy. Furthermore, elicitation\nwith an interactive scatterplot display outperforms straightforward elicitation\nwhere the users choose feature pairs from a feature list.\n","id":260}
{"Unnamed: 0.1":11261,"Unnamed: 0":11261.0,"anchor":"Foundations of Structural Causal Models with Cycles and Latent Variables","positive":"  Structural causal models (SCMs), also known as (nonparametric) structural\nequation models (SEMs), are widely used for causal modeling purposes. In\nparticular, acyclic SCMs, also known as recursive SEMs, form a well-studied\nsubclass of SCMs that generalize causal Bayesian networks to allow for latent\nconfounders. In this paper, we investigate SCMs in a more general setting,\nallowing for the presence of both latent confounders and cycles. We show that\nin the presence of cycles, many of the convenient properties of acyclic SCMs do\nnot hold in general: they do not always have a solution; they do not always\ninduce unique observational, interventional and counterfactual distributions; a\nmarginalization does not always exist, and if it exists the marginal model does\nnot always respect the latent projection; they do not always satisfy a Markov\nproperty; and their graphs are not always consistent with their causal\nsemantics. We prove that for SCMs in general each of these properties does hold\nunder certain solvability conditions. Our work generalizes results for SCMs\nwith cycles that were only known for certain special cases so far. We introduce\nthe class of simple SCMs that extends the class of acyclic SCMs to the cyclic\nsetting, while preserving many of the convenient properties of acyclic SCMs.\nWith this paper we aim to provide the foundations for a general theory of\nstatistical causal modeling with SCMs.\n","negative":"  We introduce and analyze a new technique for model reduction for deep neural\nnetworks. While large networks are theoretically capable of learning\narbitrarily complex models, overfitting and model redundancy negatively affects\nthe prediction accuracy and model variance. Our Net-Trim algorithm prunes\n(sparsifies) a trained network layer-wise, removing connections at each layer\nby solving a convex optimization program. This program seeks a sparse set of\nweights at each layer that keeps the layer inputs and outputs consistent with\nthe originally trained model. The algorithms and associated analysis are\napplicable to neural networks operating with the rectified linear unit (ReLU)\nas the nonlinear activation. We present both parallel and cascade versions of\nthe algorithm. While the latter can achieve slightly simpler models with the\nsame generalization performance, the former can be computed in a distributed\nmanner. In both cases, Net-Trim significantly reduces the number of connections\nin the network, while also providing enough regularization to slightly reduce\nthe generalization error. We also provide a mathematical analysis of the\nconsistency between the initial network and the retrained model. To analyze the\nmodel sample complexity, we derive the general sufficient conditions for the\nrecovery of a sparse transform matrix. For a single layer taking independent\nGaussian random vectors of length $N$ as inputs, we show that if the network\nresponse can be described using a maximum number of $s$ non-zero weights per\nnode, these weights can be learned from $\\mathcal{O}(s\\log N)$ samples.\n","id":261}
{"Unnamed: 0.1":11262,"Unnamed: 0":11262.0,"anchor":"Approximate Near Neighbors for General Symmetric Norms","positive":"  We show that every symmetric normed space admits an efficient nearest\nneighbor search data structure with doubly-logarithmic approximation.\nSpecifically, for every $n$, $d = n^{o(1)}$, and every $d$-dimensional\nsymmetric norm $\\|\\cdot\\|$, there exists a data structure for\n$\\mathrm{poly}(\\log \\log n)$-approximate nearest neighbor search over\n$\\|\\cdot\\|$ for $n$-point datasets achieving $n^{o(1)}$ query time and\n$n^{1+o(1)}$ space. The main technical ingredient of the algorithm is a\nlow-distortion embedding of a symmetric norm into a low-dimensional iterated\nproduct of top-$k$ norms.\n  We also show that our techniques cannot be extended to general norms.\n","negative":"  Today data mining techniques are exploited in medical science for diagnosing,\novercoming and treating diseases. Neural network is one of the techniques which\nare widely used for diagnosis in medical field. In this article efficiency of\nnine algorithms, which are basis of neural network learning in diagnosing\ncardiovascular diseases, will be assessed. Algorithms are assessed in terms of\naccuracy, sensitivity, transparency, AROC and convergence rate by means of 10\nfold cross validation. The results suggest that in training phase, Lonberg-M\nalgorithm has the best efficiency in terms of all metrics, algorithm OSS has\nmaximum accuracy in testing phase, algorithm SCG has the maximum transparency\nand algorithm CGB has the maximum sensitivity.\n","id":262}
{"Unnamed: 0.1":11263,"Unnamed: 0":11263.0,"anchor":"Using LSTM recurrent neural networks for monitoring the LHC\n  superconducting magnets","positive":"  The superconducting LHC magnets are coupled with an electronic monitoring\nsystem which records and analyses voltage time series reflecting their\nperformance. A currently used system is based on a range of preprogrammed\ntriggers which launches protection procedures when a misbehavior of the magnets\nis detected. All the procedures used in the protection equipment were designed\nand implemented according to known working scenarios of the system and are\nupdated and monitored by human operators.\n  This paper proposes a novel approach to monitoring and fault protection of\nthe Large Hadron Collider (LHC) superconducting magnets which employs\nstate-of-the-art Deep Learning algorithms. Consequently, the authors of the\npaper decided to examine the performance of LSTM recurrent neural networks for\nmodeling of voltage time series of the magnets. In order to address this\nchallenging task different network architectures and hyper-parameters were used\nto achieve the best possible performance of the solution. The regression\nresults were measured in terms of RMSE for different number of future steps and\nhistory length taken into account for the prediction. The best result of\nRMSE=0.00104 was obtained for a network of 128 LSTM cells within the internal\nlayer and 16 steps history buffer.\n","negative":"  A heuristic procedure based on novel recursive formulation of sinusoid (RFS)\nand on regression with predictive least-squares (LS) enables to decompose both\nuniformly and nonuniformly sampled 1-d signals into a sparse set of sinusoids\n(SSS). An optimal SSS is found by Levenberg-Marquardt (LM) optimization of RFS\nparameters of near-optimal sinusoids combined with common criteria for the\nestimation of the number of sinusoids embedded in noise. The procedure\nestimates both the cardinality and the parameters of SSS. The proposed\nalgorithm enables to identify the RFS parameters of a sinusoid from a data\nsequence containing only a fraction of its cycle. In extreme cases when the\nfrequency of a sinusoid approaches zero the algorithm is able to detect a\nlinear trend in data. Also, an irregular sampling pattern enables the algorithm\nto correctly reconstruct the under-sampled sinusoid. Parsimonious nature of the\nobtaining models opens the possibilities of using the proposed method in\nmachine learning and in expert and intelligent systems needing analysis and\nsimple representation of 1-d signals. The properties of the proposed algorithm\nare evaluated on examples of irregularly sampled artificial signals in noise\nand are compared with high accuracy frequency estimation algorithms based on\nlinear prediction (LP) approach, particularly with respect to Cramer-Rao Bound\n(CRB).\n","id":263}
{"Unnamed: 0.1":11264,"Unnamed: 0":11264.0,"anchor":"Spikes as regularizers","positive":"  We present a confidence-based single-layer feed-forward learning algorithm\nSPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of\nactivation spikes. We adaptively update a weight vector relying on confidence\nestimates and activation offsets relative to previous activity. We regularize\nupdates proportionally to item-level confidence and weight-specific support,\nloosely inspired by the observation from neurophysiology that high spike rates\nare sometimes accompanied by low temporal precision. Our experiments suggest\nthat the new learning algorithm SPIRAL is more robust and less prone to\noverfitting than both the averaged perceptron and AROW.\n","negative":"  In this work, we propose a training algorithm for an audio-visual automatic\nspeech recognition (AV-ASR) system using deep recurrent neural network\n(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal\nClassification (CTC) objective function. The frame labels obtained from the\nacoustic model are then used to perform a non-linear dimensionality reduction\nof the visual features using a deep bottleneck network. Audio and visual\nfeatures are fused and used to train a fusion RNN. The use of bottleneck\nfeatures for visual modality helps the model to converge properly during\ntraining. Our system is evaluated on GRID corpus. Our results show that\npresence of visual modality gives significant improvement in character error\nrate (CER) at various levels of noise even when the model is trained without\nnoisy data. We also provide a comparison of two fusion methods: feature fusion\nand decision fusion.\n","id":264}
{"Unnamed: 0.1":11265,"Unnamed: 0":11265.0,"anchor":"Reinforcement Learning through Asynchronous Advantage Actor-Critic on a\n  GPU","positive":"  We introduce a hybrid CPU\/GPU version of the Asynchronous Advantage\nActor-Critic (A3C) algorithm, currently the state-of-the-art method in\nreinforcement learning for various gaming tasks. We analyze its computational\ntraits and concentrate on aspects critical to leveraging the GPU's\ncomputational power. We introduce a system of queues and a dynamic scheduling\nstrategy, potentially helpful for other asynchronous algorithms as well. Our\nhybrid CPU\/GPU version of A3C, based on TensorFlow, achieves a significant\nspeed up compared to a CPU implementation; we make it publicly available to\nother researchers at https:\/\/github.com\/NVlabs\/GA3C .\n","negative":"  More than two thirds of mental health problems have their onset during\nchildhood or adolescence. Identifying children at risk for mental illness later\nin life and predicting the type of illness is not easy. We set out to develop a\nplatform to define subtypes of childhood social-emotional development using\nlongitudinal, multifactorial trait-based measures. Subtypes discovered through\nthis study could ultimately advance psychiatric knowledge of the early\nbehavioural signs of mental illness. To this extent we have examined two types\nof models: latent class mixture models and GP-based models. Our findings\nindicate that while GP models come close in accuracy of predicting future\ntrajectories, LCMMs predict the trajectories as well in a fraction of the time.\nUnfortunately, neither of the models are currently accurate enough to lead to\nimmediate clinical impact. The available data related to the development of\nchildhood mental health is often sparse with only a few time points measured\nand require novel methods with improved efficiency and accuracy.\n","id":265}
{"Unnamed: 0.1":11266,"Unnamed: 0":11266.0,"anchor":"Deep Clustering and Conventional Networks for Music Separation: Stronger\n  Together","positive":"  Deep clustering is the first method to handle general audio separation\nscenarios with multiple sources of the same type and an arbitrary number of\nsources, performing impressively in speaker-independent speech separation\ntasks. However, little is known about its effectiveness in other challenging\nsituations such as music source separation. Contrary to conventional networks\nthat directly estimate the source signals, deep clustering generates an\nembedding for each time-frequency bin, and separates sources by clustering the\nbins in the embedding space. We show that deep clustering outperforms\nconventional networks on a singing voice separation task, in both matched and\nmismatched conditions, even though conventional networks have the advantage of\nend-to-end training for best signal approximation, presumably because its more\nflexible objective engenders better regularization. Since the strengths of deep\nclustering and conventional network architectures appear complementary, we\nexplore combining them in a single hybrid network trained via an approach akin\nto multi-task learning. Remarkably, the combination significantly outperforms\neither of its components.\n","negative":"  Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.\n","id":266}
{"Unnamed: 0.1":11267,"Unnamed: 0":11267.0,"anchor":"Cross-model convolutional neural network for multiple modality data\n  representation","positive":"  A novel data representation method of convolutional neural net- work (CNN) is\nproposed in this paper to represent data of different modalities. We learn a\nCNN model for the data of each modality to map the data of differ- ent\nmodalities to a common space, and regularize the new representations in the\ncommon space by a cross-model relevance matrix. We further impose that the\nclass label of data points can also be predicted from the CNN representa- tions\nin the common space. The learning problem is modeled as a minimiza- tion\nproblem, which is solved by an augmented Lagrange method (ALM) with updating\nrules of Alternating direction method of multipliers (ADMM). The experiments\nover benchmark of sequence data of multiple modalities show its advantage.\n","negative":"  The Schatten-$p$ norm ($0<p<1$) has been widely used to replace the nuclear\nnorm for better approximating the rank function. However, existing methods are\neither 1) not scalable for large scale problems due to relying on singular\nvalue decomposition (SVD) in every iteration, or 2) specific to some $p$\nvalues, e.g., $1\/2$, and $2\/3$. In this paper, we show that for any $p$, $p_1$,\nand $p_2 >0$ satisfying $1\/p=1\/p_1+1\/p_2$, there is an equivalence between the\nSchatten-$p$ norm of one matrix and the Schatten-$p_1$ and the Schatten-$p_2$\nnorms of its two factor matrices. We further extend the equivalence to multiple\nfactor matrices and show that all the factor norms can be convex and smooth for\nany $p>0$. In contrast, the original Schatten-$p$ norm for $0<p<1$ is\nnon-convex and non-smooth. As an example we conduct experiments on matrix\ncompletion. To utilize the convexity of the factor matrix norms, we adopt the\naccelerated proximal alternating linearized minimization algorithm and\nestablish its sequence convergence. Experiments on both synthetic and real\ndatasets exhibit its superior performance over the state-of-the-art methods.\nIts speed is also highly competitive.\n","id":267}
{"Unnamed: 0.1":11268,"Unnamed: 0":11268.0,"anchor":"Local minima in training of neural networks","positive":"  There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and\/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space.\n","negative":"  We explore beyond existing work on learning from demonstration by asking the\nquestion: Can robots learn to teach?, that is, can a robot autonomously learn\nan instructional policy from expert demonstration and use it to instruct or\ncollaborate with humans in executing complex tasks in uncertain environments?\nIn this paper we pursue a solution to this problem by leveraging the idea that\nhumans often implicitly decompose a higher level task into several subgoals\nwhose execution brings the task closer to completion. We propose Dirichlet\nprocess based non-parametric Inverse Reinforcement Learning (DPMIRL) approach\nfor reward based unsupervised clustering of task space into subgoals. This\napproach is shown to capture the latent subgoals that a human teacher would\nhave utilized to train a novice. The notion of action primitive is introduced\nas the means to communicate instruction policy to humans in the least\ncomplicated manner, and as a computationally efficient tool to segment\ndemonstration data. We evaluate our approach through experiments on hydraulic\nactuated scaled model of an excavator and evaluate and compare different\nteaching strategies utilized by the robot.\n","id":268}
{"Unnamed: 0.1":11269,"Unnamed: 0":11269.0,"anchor":"Learning the Number of Neurons in Deep Networks","positive":"  Nowadays, the number of layers and of neurons in each layer of a deep network\nare typically set manually. While very deep and wide networks have proven\neffective in general, they come at a high memory and computation cost, thus\nmaking them impractical for constrained platforms. These networks, however, are\nknown to have many redundant parameters, and could thus, in principle, be\nreplaced by more compact architectures. In this paper, we introduce an approach\nto automatically determining the number of neurons in each layer of a deep\nnetwork during learning. To this end, we propose to make use of structured\nsparsity during learning. More precisely, we use a group sparsity regularizer\non the parameters of the network, where each group is defined to act on a\nsingle neuron. Starting from an overcomplete network, we show that our approach\ncan reduce the number of parameters by up to 80\\% while retaining or even\nimproving the network accuracy.\n","negative":"  Intrusion detection has attracted a considerable interest from researchers\nand industries. The community, after many years of research, still faces the\nproblem of building reliable and efficient IDS that are capable of handling\nlarge quantities of data, with changing patterns in real time situations. The\nwork presented in this manuscript classifies intrusion detection systems (IDS).\nMoreover, a taxonomy and survey of shallow and deep networks intrusion\ndetection systems is presented based on previous and current works. This\ntaxonomy and survey reviews machine learning techniques and their performance\nin detecting anomalies. Feature selection which influences the effectiveness of\nmachine learning (ML) IDS is discussed to explain the role of feature selection\nin the classification and training phase of ML IDS. Finally, a discussion of\nthe false and true positive alarm rates is presented to help researchers model\nreliable and efficient machine learning based intrusion detection systems.\n","id":269}
{"Unnamed: 0.1":11270,"Unnamed: 0":11270.0,"anchor":"Quantized neural network design under weight capacity constraint","positive":"  The complexity of deep neural network algorithms for hardware implementation\ncan be lowered either by scaling the number of units or reducing the\nword-length of weights. Both approaches, however, can accompany the performance\ndegradation although many types of research are conducted to relieve this\nproblem. Thus, it is an important question which one, between the network size\nscaling and the weight quantization, is more effective for hardware\noptimization. For this study, the performances of fully-connected deep neural\nnetworks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while\nchanging the network complexity and the word-length of weights. Based on these\nexperiments, we present the effective compression ratio (ECR) to guide the\ntrade-off between the network size and the precision of weights when the\nhardware resource is limited.\n","negative":"  Quantum annealing (QA) is a hardware-based heuristic optimization and\nsampling method applicable to discrete undirected graphical models. While\nsimilar to simulated annealing, QA relies on quantum, rather than thermal,\neffects to explore complex search spaces. For many classes of problems, QA is\nknown to offer computational advantages over simulated annealing. Here we\nreport on the ability of recent QA hardware to accelerate training of fully\nvisible Boltzmann machines. We characterize the sampling distribution of QA\nhardware, and show that in many cases, the quantum distributions differ\nsignificantly from classical Boltzmann distributions. In spite of this\ndifference, training (which seeks to match data and model statistics) using\nstandard classical gradient updates is still effective. We investigate the use\nof QA for seeding Markov chains as an alternative to contrastive divergence\n(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we\nshow that for problems with high-energy barriers between modes, QA-based seeds\ncan improve upon chains with CD and PCD initializations. For these hard\nproblems, QA gradient estimates are more accurate, and allow for faster\nlearning. Furthermore, and interestingly, even the case of raw QA samples (that\nis, $k=0$) achieved similar improvements. We argue that this relates to the\nfact that we are training a quantum rather than classical Boltzmann\ndistribution in this case. The learned parameters give rise to hardware QA\ndistributions closely approximating classical Boltzmann distributions that are\nhard to train with CD\/PCD.\n","id":270}
{"Unnamed: 0.1":11271,"Unnamed: 0":11271.0,"anchor":"Conservative Contextual Linear Bandits","positive":"  Safety is a desirable property that can immensely increase the applicability\nof learning algorithms in real-world decision-making problems. It is much\neasier for a company to deploy an algorithm that is safe, i.e., guaranteed to\nperform at least as well as a baseline. In this paper, we study the issue of\nsafety in contextual linear bandits that have application in many different\nfields including personalized ad recommendation in online marketing. We\nformulate a notion of safety for this class of algorithms. We develop a safe\ncontextual linear bandit algorithm, called conservative linear UCB (CLUCB),\nthat simultaneously minimizes its regret and satisfies the safety constraint,\ni.e., maintains its performance above a fixed percentage of the performance of\na baseline strategy, uniformly over time. We prove an upper-bound on the regret\nof CLUCB and show that it can be decomposed into two terms: 1) an upper-bound\nfor the regret of the standard linear UCB algorithm that grows with the time\nhorizon and 2) a constant (does not grow with the time horizon) term that\naccounts for the loss of being conservative in order to satisfy the safety\nconstraint. We empirically show that our algorithm is safe and validate our\ntheoretical analysis.\n","negative":"  In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance.\n","id":271}
{"Unnamed: 0.1":11272,"Unnamed: 0":11272.0,"anchor":"A Survey of Credit Card Fraud Detection Techniques: Data and Technique\n  Oriented Perspective","positive":"  Credit card plays a very important rule in today's economy. It becomes an\nunavoidable part of household, business and global activities. Although using\ncredit cards provides enormous benefits when used carefully and\nresponsibly,significant credit and financial damages may be caused by\nfraudulent activities. Many techniques have been proposed to confront the\ngrowth in credit card fraud. However, all of these techniques have the same\ngoal of avoiding the credit card fraud; each one has its own drawbacks,\nadvantages and characteristics. In this paper, after investigating difficulties\nof credit card fraud detection, we seek to review the state of the art in\ncredit card fraud detection techniques, data sets and evaluation criteria.The\nadvantages and disadvantages of fraud detection methods are enumerated and\ncompared.Furthermore, a classification of mentioned techniques into two main\nfraud detection approaches, namely, misuses (supervised) and anomaly detection\n(unsupervised) is presented. Again, a classification of techniques is proposed\nbased on capability to process the numerical and categorical data sets.\nDifferent data sets used in literature are then described and grouped into real\nand synthesized data and the effective and common attributes are extracted for\nfurther usage.Moreover, evaluation employed criterions in literature are\ncollected and discussed.Consequently, open issues for credit card fraud\ndetection are explained as guidelines for new researchers.\n","negative":"  We study information theoretic methods for ranking biomarkers. In clinical\ntrials there are two, closely related, types of biomarkers: predictive and\nprognostic, and disentangling them is a key challenge. Our first step is to\nphrase biomarker ranking in terms of optimizing an information theoretic\nquantity. This formalization of the problem will enable us to derive rankings\nof predictive\/prognostic biomarkers, by estimating different, high dimensional,\nconditional mutual information terms. To estimate these terms, we suggest\nefficient low dimensional approximations, and we derive an empirical Bayes\nestimator, which is suitable for small or sparse datasets. Finally, we\nintroduce a new visualisation tool that captures the prognostic and the\npredictive strength of a set of biomarkers. We believe this representation will\nprove to be a powerful tool in biomarker discovery.\n","id":272}
{"Unnamed: 0.1":11273,"Unnamed: 0":11273.0,"anchor":"Pruning Convolutional Neural Networks for Resource Efficient Inference","positive":"  We propose a new formulation for pruning convolutional kernels in neural\nnetworks to enable efficient inference. We interleave greedy criteria-based\npruning with fine-tuning by backpropagation - a computationally efficient\nprocedure that maintains good generalization in the pruned network. We propose\na new criterion based on Taylor expansion that approximates the change in the\ncost function induced by pruning network parameters. We focus on transfer\nlearning, where large pretrained networks are adapted to specialized tasks. The\nproposed criterion demonstrates superior performance compared to other\ncriteria, e.g. the norm of kernel weights or feature map activation, for\npruning large CNNs after adaptation to fine-grained classification tasks\n(Birds-200 and Flowers-102) relaying only on the first order gradient\ninformation. We also show that pruning can lead to more than 10x theoretical\n(5x practical) reduction in adapted 3D-convolutional filters with a small drop\nin accuracy in a recurrent gesture classifier. Finally, we show results for the\nlarge-scale ImageNet dataset to emphasize the flexibility of our approach.\n","negative":"  Creating aesthetically pleasing pieces of art, including music, has been a\nlong-term goal for artificial intelligence research. Despite recent successes\nof long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential\nlearning, LSTM neural networks have not, by themselves, been able to generate\nnatural-sounding music conforming to music theory. To transcend this\ninadequacy, we put forward a novel method for music composition that combines\nthe LSTM with Grammars motivated by music theory. The main tenets of music\ntheory are encoded as grammar argumented (GA) filters on the training data,\nsuch that the machine can be trained to generate music inheriting the\nnaturalness of human-composed pieces from the original dataset while adhering\nto the rules of music theory. Unlike previous approaches, pitches and durations\nare encoded as one semantic entity, which we refer to as note-level encoding.\nThis allows easy implementation of music theory grammars, as well as closer\nemulation of the thinking pattern of a musician. Although the GA rules are\napplied to the training data and never directly to the LSTM music generation,\nour machine still composes music that possess high incidences of diatonic scale\nnotes, small pitch intervals and chords, in deference to music theory.\n","id":273}
{"Unnamed: 0.1":11274,"Unnamed: 0":11274.0,"anchor":"Fast Video Classification via Adaptive Cascading of Deep Models","positive":"  Recent advances have enabled \"oracle\" classifiers that can classify across\nmany classes and input distributions with high accuracy without retraining.\nHowever, these classifiers are relatively heavyweight, so that applying them to\nclassify video is costly. We show that day-to-day video exhibits highly skewed\nclass distributions over the short term, and that these distributions can be\nclassified by much simpler models. We formulate the problem of detecting the\nshort-term skews online and exploiting models based on it as a new sequential\ndecision making problem dubbed the Online Bandit Problem, and present a new\nalgorithm to solve it. When applied to recognizing faces in TV shows and\nmovies, we realize end-to-end classification speedups of 2.4-7.8x\/2.6-11.2x (on\nGPU\/CPU) relative to a state-of-the-art convolutional neural network, at\ncompetitive accuracy.\n","negative":"  Using unitary (instead of general) matrices in artificial neural networks\n(ANNs) is a promising way to solve the gradient explosion\/vanishing problem, as\nwell as to enable ANNs to learn long-term correlations in the data. This\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\nIn this work, we present a new architecture for implementing an Efficient\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\nSecondly, the computational complexity for training an EUNN is merely\n$\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\narchitecture, in terms of the final performance and\/or the wall-clock training\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\nvariety of applications.\n","id":274}
{"Unnamed: 0.1":11275,"Unnamed: 0":11275.0,"anchor":"Time Series Classification from Scratch with Deep Neural Networks: A\n  Strong Baseline","positive":"  We propose a simple but strong baseline for time series classification from\nscratch with deep neural networks. Our proposed baseline models are pure\nend-to-end without any heavy preprocessing on the raw data or feature crafting.\nThe proposed Fully Convolutional Network (FCN) achieves premium performance to\nother state-of-the-art approaches and our exploration of the very deep neural\nnetworks with the ResNet structure is also competitive. The global average\npooling in our convolutional model enables the exploitation of the Class\nActivation Map (CAM) to find out the contributing region in the raw data for\nthe specific labels. Our models provides a simple choice for the real world\napplication and a good starting point for the future research. An overall\nanalysis is provided to discuss the generalization capability of our models,\nlearned features, network structures and the classification semantics.\n","negative":"  Recently deep neural networks have received considerable attention due to\ntheir ability to extract and represent high-level abstractions in data sets.\nDeep neural networks such as fully-connected and convolutional neural networks\nhave shown excellent performance on a wide range of recognition and\nclassification tasks. However, their hardware implementations currently suffer\nfrom large silicon area and high power consumption due to the their high degree\nof complexity. The power\/energy consumption of neural networks is dominated by\nmemory accesses, the majority of which occur in fully-connected networks. In\nfact, they contain most of the deep neural network parameters. In this paper,\nwe propose sparsely-connected networks, by showing that the number of\nconnections in fully-connected networks can be reduced by up to 90% while\nimproving the accuracy performance on three popular datasets (MNIST, CIFAR10\nand SVHN). We then propose an efficient hardware architecture based on\nlinear-feedback shift registers to reduce the memory requirements of the\nproposed sparsely-connected networks. The proposed architecture can save up to\n90% of memory compared to the conventional implementations of fully-connected\nneural networks. Moreover, implementation results show up to 84% reduction in\nthe energy consumption of a single neuron of the proposed sparsely-connected\nnetworks compared to a single neuron of fully-connected neural networks.\n","id":275}
{"Unnamed: 0.1":11276,"Unnamed: 0":11276.0,"anchor":"Dealing with Range Anxiety in Mean Estimation via Statistical Queries","positive":"  We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions.\n","negative":"  Video recommendation has become an essential way of helping people explore\nthe massive videos and discover the ones that may be of interest to them. In\nthe existing video recommender systems, the models make the recommendations\nbased on the user-video interactions and single specific content features. When\nthe specific content features are unavailable, the performance of the existing\nmodels will seriously deteriorate. Inspired by the fact that rich contents\n(e.g., text, audio, motion, and so on) exist in videos, in this paper, we\nexplore how to use these rich contents to overcome the limitations caused by\nthe unavailability of the specific ones. Specifically, we propose a novel\ngeneral framework that incorporates arbitrary single content feature with\nuser-video interactions, named as collaborative embedding regression (CER)\nmodel, to make effective video recommendation in both in-matrix and\nout-of-matrix scenarios. Our extensive experiments on two real-world\nlarge-scale datasets show that CER beats the existing recommender models with\nany single content feature and is more time efficient. In addition, we propose\na priority-based late fusion (PRI) method to gain the benefit brought by the\nintegrating the multiple content features. The corresponding experiment shows\nthat PRI brings real performance improvement to the baseline and outperforms\nthe existing fusion methods.\n","id":276}
{"Unnamed: 0.1":11277,"Unnamed: 0":11277.0,"anchor":"Prototypical Recurrent Unit","positive":"  Despite the great successes of deep learning, the effectiveness of deep\nneural networks has not been understood at any theoretical depth. This work is\nmotivated by the thrust of developing a deeper understanding of recurrent\nneural networks, particularly LSTM\/GRU-like networks. As the highly complex\nstructure of the recurrent unit in LSTM and GRU networks makes them difficult\nto analyze, our methodology in this research theme is to construct an\nalternative recurrent unit that is as simple as possible and yet also captures\nthe key components of LSTM\/GRU recurrent units. Such a unit can then be used\nfor the study of recurrent networks and its structural simplicity may allow\neasier analysis. Towards that goal, we take a system-theoretic perspective to\ndesign a new recurrent unit, which we call the prototypical recurrent unit\n(PRU). Not only having minimal complexity, PRU is demonstrated experimentally\nto have comparable performance to GRU and LSTM unit. This establishes PRU\nnetworks as a prototype for future study of LSTM\/GRU-like recurrent networks.\nThis paper also studies the memorization abilities of LSTM, GRU and PRU\nnetworks, motivated by the folk belief that such networks possess long-term\nmemory. For this purpose, we design a simple and controllable task, called\n``memorization problem'', where the networks are trained to memorize certain\ntargeted information. We show that the memorization performance of all three\nnetworks depends on the amount of targeted information, the amount of\n``interfering\" information, and the state space dimension of the recurrent\nunit. Experiments are also performed for another controllable task, the adding\nproblem, and similar conclusions are obtained.\n","negative":"  Reinforcement learning (RL) in Markov decision processes (MDPs) with large\nstate spaces is a challenging problem. The performance of standard RL\nalgorithms degrades drastically with the dimensionality of state space.\nHowever, in practice, these large MDPs typically incorporate a latent or hidden\nlow-dimensional structure. In this paper, we study the setting of\nrich-observation Markov decision processes (ROMDP), where there are a small\nnumber of hidden states which possess an injective mapping to the observation\nstates. In other words, every observation state is generated through a single\nhidden state, and this mapping is unknown a priori. We introduce a spectral\ndecomposition method that consistently learns this mapping, and more\nimportantly, achieves it with low regret. The estimated mapping is integrated\ninto an optimistic RL algorithm (UCRL), which operates on the estimated hidden\nspace. We derive finite-time regret bounds for our algorithm with a weak\ndependence on the dimensionality of the observed space. In fact, our algorithm\nasymptotically achieves the same average regret as the oracle UCRL algorithm,\nwhich has the knowledge of the mapping from hidden to observed spaces. Thus, we\nderive an efficient spectral RL algorithm for ROMDPs.\n","id":277}
{"Unnamed: 0.1":11278,"Unnamed: 0":11278.0,"anchor":"Linear Thompson Sampling Revisited","positive":"  We derive an alternative proof for the regret of Thompson sampling (\\ts) in\nthe stochastic linear bandit setting. While we obtain a regret bound of order\n$\\widetilde{O}(d^{3\/2}\\sqrt{T})$ as in previous results, the proof sheds new\nlight on the functioning of the \\ts. We leverage on the structure of the\nproblem to show how the regret is related to the sensitivity (i.e., the\ngradient) of the objective function and how selecting optimal arms associated\nto \\textit{optimistic} parameters does control it. Thus we show that \\ts can be\nseen as a generic randomized algorithm where the sampling distribution is\ndesigned to have a fixed probability of being optimistic, at the cost of an\nadditional $\\sqrt{d}$ regret factor compared to a UCB-like approach.\nFurthermore, we show that our proof can be readily applied to regularized\nlinear optimization and generalized linear model problems.\n","negative":"  We propose a neural network based approach for learning topics from text and\nimage datasets. The model makes no assumptions about the conditional\ndistribution of the observed features given the latent topics. This allows us\nto perform topic modelling efficiently using sentences of documents and patches\nof images as observed features, rather than limiting ourselves to words.\nMoreover, the proposed approach is online, and hence can be used for streaming\ndata. Furthermore, since the approach utilizes neural networks, it can be\nimplemented on GPU with ease, and hence it is very scalable.\n","id":278}
{"Unnamed: 0.1":11279,"Unnamed: 0":11279.0,"anchor":"Efficient Stochastic Inference of Bitwise Deep Neural Networks","positive":"  Recently published methods enable training of bitwise neural networks which\nallow reduced representation of down to a single bit per weight. We present a\nmethod that exploits ensemble decisions based on multiple stochastically\nsampled network models to increase performance figures of bitwise neural\nnetworks in terms of classification accuracy at inference. Our experiments with\nthe CIFAR-10 and GTSRB datasets show that the performance of such network\nensembles surpasses the performance of the high-precision base model. With this\ntechnique we achieve 5.81% best classification error on CIFAR-10 test set using\nbitwise networks. Concerning inference on embedded systems we evaluate these\nbitwise networks using a hardware efficient stochastic rounding procedure. Our\nwork contributes to efficient embedded bitwise neural networks.\n","negative":"  Learning a natural language interface for database tables is a challenging\ntask that involves deep language understanding and multi-step reasoning. The\ntask is often approached by mapping natural language queries to logical forms\nor programs that provide the desired response when executed on the database. To\nour knowledge, this paper presents the first weakly supervised, end-to-end\nneural network model to induce such programs on a real-world dataset. We\nenhance the objective function of Neural Programmer, a neural network with\nbuilt-in discrete operations, and apply it on WikiTableQuestions, a natural\nlanguage question-answering dataset. The model is trained end-to-end with weak\nsupervision of question-answer pairs, and does not require domain-specific\ngrammars, rules, or annotations that are key elements in previous approaches to\nprogram induction. The main experimental result in this paper is that a single\nNeural Programmer model achieves 34.2% accuracy using only 10,000 examples with\nweak supervision. An ensemble of 15 models, with a trivial combination\ntechnique, achieves 37.7% accuracy, which is competitive to the current\nstate-of-the-art accuracy of 37.1% obtained by a traditional natural language\nsemantic parser.\n","id":279}
{"Unnamed: 0.1":11280,"Unnamed: 0":11280.0,"anchor":"Variational Boosting: Iteratively Refining Posterior Approximations","positive":"  We propose a black-box variational inference method to approximate\nintractable distributions with an increasingly rich approximating class. Our\nmethod, termed variational boosting, iteratively refines an existing\nvariational approximation by solving a sequence of optimization problems,\nallowing the practitioner to trade computation time for accuracy. We show how\nto expand the variational approximating class by incorporating additional\ncovariance structure and by introducing new components to form a mixture. We\napply variational boosting to synthetic and real statistical models, and show\nthat resulting posterior inferences compare favorably to existing posterior\napproximation algorithms in both accuracy and efficiency.\n","negative":"  Reinforcement learning is concerned with identifying reward-maximizing\nbehaviour policies in environments that are initially unknown. State-of-the-art\nreinforcement learning approaches, such as deep Q-networks, are model-free and\nlearn to act effectively across a wide range of environments such as Atari\ngames, but require huge amounts of data. Model-based techniques are more\ndata-efficient, but need to acquire explicit knowledge about the environment.\n  In this paper, we take a step towards using model-based techniques in\nenvironments with a high-dimensional visual state space by demonstrating that\nit is possible to learn system dynamics and the reward structure jointly. Our\ncontribution is to extend a recently developed deep neural network for video\nframe prediction in Atari games to enable reward prediction as well. To this\nend, we phrase a joint optimization problem for minimizing both video frame and\nreward reconstruction loss, and adapt network parameters accordingly. Empirical\nevaluations on five Atari games demonstrate accurate cumulative reward\nprediction of up to 200 frames. We consider these results as opening up\nimportant directions for model-based reinforcement learning in complex,\ninitially unknown environments.\n","id":280}
{"Unnamed: 0.1":11281,"Unnamed: 0":11281.0,"anchor":"Temporal Generative Adversarial Nets with Singular Value Clipping","positive":"  In this paper, we propose a generative model, Temporal Generative Adversarial\nNets (TGAN), which can learn a semantic representation of unlabeled videos, and\nis capable of generating videos. Unlike existing Generative Adversarial Nets\n(GAN)-based methods that generate videos with a single generator consisting of\n3D deconvolutional layers, our model exploits two different types of\ngenerators: a temporal generator and an image generator. The temporal generator\ntakes a single latent variable as input and outputs a set of latent variables,\neach of which corresponds to an image frame in a video. The image generator\ntransforms a set of such latent variables into a video. To deal with\ninstability in training of GAN with such advanced networks, we adopt a recently\nproposed model, Wasserstein GAN, and propose a novel method to train it stably\nin an end-to-end manner. The experimental results demonstrate the effectiveness\nof our methods.\n","negative":"  Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans.\n","id":281}
{"Unnamed: 0.1":11282,"Unnamed: 0":11282.0,"anchor":"Deep Learning for the Classification of Lung Nodules","positive":"  Deep learning, as a promising new area of machine learning, has attracted a\nrapidly increasing attention in the field of medical imaging. Compared to the\nconventional machine learning methods, deep learning requires no hand-tuned\nfeature extractor, and has shown a superior performance in many visual object\nrecognition applications. In this study, we develop a deep convolutional neural\nnetwork (CNN) and apply it to thoracic CT images for the classification of lung\nnodules. We present the CNN architecture and classification accuracy for the\noriginal images of lung nodules. In order to understand the features of lung\nnodules, we further construct new datasets, based on the combination of\nartificial geometric nodules and some transformations of the original images,\nas well as a stochastic nodule shape model. It is found that simplistic\ngeometric nodules cannot capture the important features of lung nodules.\n","negative":"  Exploration has been a crucial part of reinforcement learning, yet several\nimportant questions concerning exploration efficiency are still not answered\nsatisfactorily by existing analytical frameworks. These questions include\nexploration parameter setting, situation analysis, and hardness of MDPs, all of\nwhich are unavoidable for practitioners. To bridge the gap between the theory\nand practice, we propose a new analytical framework called the success\nprobability of exploration. We show that those important questions of\nexploration above can all be answered under our framework, and the answers\nprovided by our framework meet the needs of practitioners better than the\nexisting ones. More importantly, we introduce a concrete and practical approach\nto evaluating the success probabilities in certain MDPs without the need of\nactually running the learning algorithm. We then provide empirical results to\nverify our approach, and demonstrate how the success probability of exploration\ncan be used to analyse and predict the behaviours and possible outcomes of\nexploration, which are the keys to the answer of the important questions of\nexploration.\n","id":282}
{"Unnamed: 0.1":11283,"Unnamed: 0":11283.0,"anchor":"Scalable Adaptive Stochastic Optimization Using Random Projections","positive":"  Adaptive stochastic gradient methods such as AdaGrad have gained popularity\nin particular for training deep neural networks. The most commonly used and\nstudied variant maintains a diagonal matrix approximation to second order\ninformation by accumulating past gradients which are used to tune the step size\nadaptively. In certain situations the full-matrix variant of AdaGrad is\nexpected to attain better performance, however in high dimensions it is\ncomputationally impractical. We present Ada-LR and RadaGrad two computationally\nefficient approximations to full-matrix AdaGrad based on randomized\ndimensionality reduction. They are able to capture dependencies between\nfeatures and achieve similar performance to full-matrix AdaGrad but at a much\nsmaller computational cost. We show that the regret of Ada-LR is close to the\nregret of full-matrix AdaGrad which can have an up-to exponentially smaller\ndependence on the dimension than the diagonal variant. Empirically, we show\nthat Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task\nof training convolutional neural networks as well as recurrent neural networks,\nRadaGrad achieves faster convergence than diagonal AdaGrad.\n","negative":"  In this paper, prediction for linear systems with missing information is\ninvestigated. New methods are introduced to improve the Mean Squared Error\n(MSE) on the test set in comparison to state-of-the-art methods, through\nappropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft\nWeighted Prediction (SWP) algorithm and its efficacy are depicted and compared\nto previous works for non-missing scenarios. The algorithm is then modified and\noptimized for missing scenarios. It is shown that controlled over-fitting by\nsuggested algorithms will improve prediction accuracy in various cases.\nSimulation results approve our heuristics in enhancing the prediction accuracy.\n","id":283}
{"Unnamed: 0.1":11284,"Unnamed: 0":11284.0,"anchor":"Error analysis of regularized least-square regression with Fredholm\n  kernel","positive":"  Learning with Fredholm kernel has attracted increasing attention recently\nsince it can effectively utilize the data information to improve the prediction\nperformance. Despite rapid progress on theoretical and experimental\nevaluations, its generalization analysis has not been explored in learning\ntheory literature. In this paper, we establish the generalization bound of\nleast square regularized regression with Fredholm kernel, which implies that\nthe fast learning rate O(l^{-1}) can be reached under mild capacity conditions.\nSimulated examples show that this Fredholm regression algorithm can achieve the\nsatisfactory prediction performance.\n","negative":"  Building a good generative model for image has long been an important topic\nin computer vision and machine learning. Restricted Boltzmann machine (RBM) is\none of such models that is simple but powerful. However, its restricted form\nalso has placed heavy constraints on the models representation power and\nscalability. Many extensions have been invented based on RBM in order to\nproduce deeper architectures with greater power. The most famous ones among\nthem are deep belief network, which stacks multiple layer-wise pretrained RBMs\nto form a hybrid model, and deep Boltzmann machine, which allows connections\nbetween hidden units to form a multi-layer structure. In this paper, we present\na new method to compose RBMs to form a multi-layer network style architecture\nand a training method that trains all layers jointly. We call the resulted\nstructure deep restricted Boltzmann network. We further explore the combination\nof convolutional RBM with the normal fully connected RBM, which is made trivial\nunder our composition framework. Experiments show that our model can generate\ndescent images and outperform the normal RBM significantly in terms of image\nquality and feature quality, without losing much efficiency for training.\n","id":284}
{"Unnamed: 0.1":11285,"Unnamed: 0":11285.0,"anchor":"Probabilistic Duality for Parallel Gibbs Sampling without Graph Coloring","positive":"  We present a new notion of probabilistic duality for random variables\ninvolving mixture distributions. Using this notion, we show how to implement a\nhighly-parallelizable Gibbs sampler for weakly coupled discrete pairwise\ngraphical models with strictly positive factors that requires almost no\npreprocessing and is easy to implement. Moreover, we show how our method can be\ncombined with blocking to improve mixing. Even though our method leads to\ninferior mixing times compared to a sequential Gibbs sampler, we argue that our\nmethod is still very useful for large dynamic networks, where factors are added\nand removed on a continuous basis, as it is hard to maintain a graph coloring\nin this setup. Similarly, our method is useful for parallelizing Gibbs sampling\nin graphical models that do not allow for graph colorings with a small number\nof colors such as densely connected graphs.\n","negative":"  Foreshock events provide valuable insight to predict imminent major\nearthquakes. However, it is difficult to identify them in real time. In this\npaper, I propose an algorithm based on deep learning to instantaneously\nclassify a seismic waveform as a foreshock, mainshock or an aftershock event\nachieving a high accuracy of 99% in classification. As a result, this is by far\nthe most reliable method to predict major earthquakes that are preceded by\nforeshocks. In addition, I discuss methods to create an earthquake dataset that\nis compatible with deep networks.\n","id":285}
{"Unnamed: 0.1":11286,"Unnamed: 0":11286.0,"anchor":"Training Sparse Neural Networks","positive":"  Deep neural networks with lots of parameters are typically used for\nlarge-scale computer vision tasks such as image classification. This is a\nresult of using dense matrix multiplications and convolutions. However, sparse\ncomputations are known to be much more efficient. In this work, we train and\nbuild neural networks which implicitly use sparse computations. We introduce\nadditional gate variables to perform parameter selection and show that this is\nequivalent to using a spike-and-slab prior. We experimentally validate our\nmethod on both small and large networks and achieve state-of-the-art\ncompression results for sparse neural network models.\n","negative":"  In computer security, designing a robust intrusion detection system is one of\nthe most fundamental and important problems. In this paper, we propose a\nsystem-call language-modeling approach for designing anomaly-based host\nintrusion detection systems. To remedy the issue of high false-alarm rates\ncommonly arising in conventional methods, we employ a novel ensemble method\nthat blends multiple thresholding classifiers into a single one, making it\npossible to accumulate 'highly normal' sequences. The proposed system-call\nlanguage model has various advantages leveraged by the fact that it can learn\nthe semantic meaning and interactions of each system call that existing methods\ncannot effectively consider. Through diverse experiments on public benchmark\ndatasets, we demonstrate the validity and effectiveness of the proposed method.\nMoreover, we show that our model possesses high portability, which is one of\nthe key aspects of realizing successful intrusion detection systems.\n","id":286}
{"Unnamed: 0.1":11287,"Unnamed: 0":11287.0,"anchor":"On the convergence of gradient-like flows with noisy gradient input","positive":"  In view of solving convex optimization problems with noisy gradient input, we\nanalyze the asymptotic behavior of gradient-like flows under stochastic\ndisturbances. Specifically, we focus on the widely studied class of mirror\ndescent schemes for convex programs with compact feasible regions, and we\nexamine the dynamics' convergence and concentration properties in the presence\nof noise. In the vanishing noise limit, we show that the dynamics converge to\nthe solution set of the underlying problem (a.s.). Otherwise, when the noise is\npersistent, we show that the dynamics are concentrated around interior\nsolutions in the long run, and they converge to boundary solutions that are\nsufficiently \"sharp\". Finally, we show that a suitably rectified variant of the\nmethod converges irrespective of the magnitude of the noise (or the structure\nof the underlying convex program), and we derive an explicit estimate for its\nrate of convergence.\n","negative":"  Generalized linear model with $L_1$ and $L_2$ regularization is a widely used\ntechnique for solving classification, class probability estimation and\nregression problems. With the numbers of both features and examples growing\nrapidly in the fields like text mining and clickstream data analysis\nparallelization and the use of cluster architectures becomes important. We\npresent a novel algorithm for fitting regularized generalized linear models in\nthe distributed environment. The algorithm splits data between nodes by\nfeatures, uses coordinate descent on each node and line search to merge results\nglobally. Convergence proof is provided. A modifications of the algorithm\naddresses slow node problem. For an important particular case of logistic\nregression we empirically compare our program with several state-of-the art\napproaches that rely on different algorithmic and data spitting methods.\nExperiments demonstrate that our approach is scalable and superior when\ntraining on large and sparse datasets.\n","id":287}
{"Unnamed: 0.1":11288,"Unnamed: 0":11288.0,"anchor":"Emergence of Compositional Representations in Restricted Boltzmann\n  Machines","positive":"  Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST.\n","negative":"  Motivated by applications in computational advertising and systems biology,\nwe consider the problem of identifying the best out of several possible soft\ninterventions at a source node $V$ in an acyclic causal directed graph, to\nmaximize the expected value of a target node $Y$ (located downstream of $V$).\nOur setting imposes a fixed total budget for sampling under various\ninterventions, along with cost constraints on different types of interventions.\nWe pose this as a best arm identification bandit problem with $K$ arms where\neach arm is a soft intervention at $V,$ and leverage the information leakage\namong the arms to provide the first gap dependent error and simple regret\nbounds for this problem. Our results are a significant improvement over the\ntraditional best arm identification results. We empirically show that our\nalgorithms outperform the state of the art in the Flow Cytometry data-set, and\nalso apply our algorithm for model interpretation of the Inception-v3 deep net\nthat classifies images.\n","id":288}
{"Unnamed: 0.1":11289,"Unnamed: 0":11289.0,"anchor":"Effective Deterministic Initialization for $k$-Means-Like Methods via\n  Local Density Peaks Searching","positive":"  The $k$-means clustering algorithm is popular but has the following main\ndrawbacks: 1) the number of clusters, $k$, needs to be provided by the user in\nadvance, 2) it can easily reach local minima with randomly selected initial\ncenters, 3) it is sensitive to outliers, and 4) it can only deal with well\nseparated hyperspherical clusters. In this paper, we propose a Local Density\nPeaks Searching (LDPS) initialization framework to address these issues. The\nLDPS framework includes two basic components: one of them is the local density\nthat characterizes the density distribution of a data set, and the other is the\nlocal distinctiveness index (LDI) which we introduce to characterize how\ndistinctive a data point is compared with its neighbors. Based on these two\ncomponents, we search for the local density peaks which are characterized with\nhigh local densities and high LDIs to deal with 1) and 2). Moreover, we detect\noutliers characterized with low local densities but high LDIs, and exclude them\nout before clustering begins. Finally, we apply the LDPS initialization\nframework to $k$-medoids, which is a variant of $k$-means and chooses data\nsamples as centers, with diverse similarity measures other than the Euclidean\ndistance to fix the last drawback of $k$-means. Combining the LDPS\ninitialization framework with $k$-means and $k$-medoids, we obtain two novel\nclustering methods called LDPS-means and LDPS-medoids, respectively.\nExperiments on synthetic data sets verify the effectiveness of the proposed\nmethods, especially when the ground truth of the cluster number $k$ is large.\nFurther, experiments on several real world data sets, Handwritten Pendigits,\nCoil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give\na superior performance than the analogous approaches on both estimating $k$ and\nunsupervised object categorization.\n","negative":"  Neural networks are a revolutionary but immature technique that is fast\nevolving and heavily relies on data. To benefit from the newest development and\nnewly available data, we want the gap between research and production as small\nas possibly. On the other hand, differing from traditional machine learning\nmodels, neural network is not just yet another statistic model, but a model for\nthe natural processing engine --- the brain. In this work, we describe a neural\nnetwork library named {\\texttt akid}. It provides higher level of abstraction\nfor entities (abstracted as blocks) in nature upon the abstraction done on\nsignals (abstracted as tensors) by Tensorflow, characterizing the dataism\nobservation that all entities in nature processes input and emit out in some\nways. It includes a full stack of software that provides abstraction to let\nresearchers focus on research instead of implementation, while at the same time\nthe developed program can also be put into production seamlessly in a\ndistributed environment, and be production ready. At the top application stack,\nit provides out-of-box tools for neural network applications. Lower down, akid\nprovides a programming paradigm that lets user easily build customized models.\nThe distributed computing stack handles the concurrency and communication, thus\nletting models be trained or deployed to a single GPU, multiple GPUs, or a\ndistributed environment without affecting how a model is specified in the\nprogramming paradigm stack. Lastly, the distributed deployment stack handles\nhow the distributed computing is deployed, thus decoupling the research\nprototype environment with the actual production environment, and is able to\ndynamically allocate computing resources, so development (Devs) and operations\n(Ops) could be separated. Please refer to http:\/\/akid.readthedocs.io\/en\/latest\/\nfor documentation.\n","id":289}
{"Unnamed: 0.1":11290,"Unnamed: 0":11290.0,"anchor":"Generalized Dropout","positive":"  Deep Neural Networks often require good regularizers to generalize well.\nDropout is one such regularizer that is widely used among Deep Learning\npractitioners. Recent work has shown that Dropout can also be viewed as\nperforming Approximate Bayesian Inference over the network parameters. In this\nwork, we generalize this notion and introduce a rich family of regularizers\nwhich we call Generalized Dropout. One set of methods in this family, called\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\nemerges as a special case of this method. Another member of this family selects\nthe width of neural network layers. Experiments show that these methods help in\nimproving generalization performance over Dropout.\n","negative":"  In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).\n","id":290}
{"Unnamed: 0.1":11291,"Unnamed: 0":11291.0,"anchor":"Options Discovery with Budgeted Reinforcement Learning","positive":"  We consider the problem of learning hierarchical policies for Reinforcement\nLearning able to discover options, an option corresponding to a sub-policy over\na set of primitive actions. Different models have been proposed during the last\ndecade that usually rely on a predefined set of options. We specifically\naddress the problem of automatically discovering options in decision processes.\nWe describe a new learning model called Budgeted Option Neural Network (BONN)\nable to discover options based on a budgeted learning objective. The BONN model\nis evaluated on different classical RL problems, demonstrating both\nquantitative and qualitative interesting results.\n","negative":"  We propose a black-box variational inference method to approximate\nintractable distributions with an increasingly rich approximating class. Our\nmethod, termed variational boosting, iteratively refines an existing\nvariational approximation by solving a sequence of optimization problems,\nallowing the practitioner to trade computation time for accuracy. We show how\nto expand the variational approximating class by incorporating additional\ncovariance structure and by introducing new components to form a mixture. We\napply variational boosting to synthetic and real statistical models, and show\nthat resulting posterior inferences compare favorably to existing posterior\napproximation algorithms in both accuracy and efficiency.\n","id":291}
{"Unnamed: 0.1":11292,"Unnamed: 0":11292.0,"anchor":"Probabilistic structure discovery in time series data","positive":"  Existing methods for structure discovery in time series data construct\ninterpretable, compositional kernels for Gaussian process regression models.\nWhile the learned Gaussian process model provides posterior mean and variance\nestimates, typically the structure is learned via a greedy optimization\nprocedure. This restricts the space of possible solutions and leads to\nover-confident uncertainty estimates. We introduce a fully Bayesian approach,\ninferring a full posterior over structures, which more reliably captures the\nuncertainty of the model.\n","negative":"  There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities.\n","id":292}
{"Unnamed: 0.1":11293,"Unnamed: 0":11293.0,"anchor":"Learning From Graph Neighborhoods Using LSTMs","positive":"  Many prediction problems can be phrased as inferences over local\nneighborhoods of graphs. The graph represents the interaction between entities,\nand the neighborhood of each entity contains information that allows the\ninferences or predictions. We present an approach for applying machine learning\ndirectly to such graph neighborhoods, yielding predicitons for graph nodes on\nthe basis of the structure of their local neighborhood and the features of the\nnodes in it. Our approach allows predictions to be learned directly from\nexamples, bypassing the step of creating and tuning an inference model or\nsummarizing the neighborhoods via a fixed set of hand-crafted features. The\napproach is based on a multi-level architecture built from Long Short-Term\nMemory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood\nfrom data. We demonstrate the effectiveness of the proposed technique on a\nsynthetic example and on real-world data related to crowdsourced grading,\nBitcoin transactions, and Wikipedia edit reversions.\n","negative":"  In reinforcement learning, the standard criterion to evaluate policies in a\nstate is the expectation of (discounted) sum of rewards. However, this\ncriterion may not always be suitable, we consider an alternative criterion\nbased on the notion of quantiles. In the case of episodic reinforcement\nlearning problems, we propose an algorithm based on stochastic approximation\nwith two timescales. We evaluate our proposition on a simple model of the TV\nshow, Who wants to be a millionaire.\n","id":293}
{"Unnamed: 0.1":11294,"Unnamed: 0":11294.0,"anchor":"Unsupervised Learning for Lexicon-Based Classification","positive":"  In lexicon-based classification, documents are assigned labels by comparing\nthe number of words that appear from two opposed lexicons, such as positive and\nnegative sentiment. Creating such words lists is often easier than labeling\ninstances, and they can be debugged by non-experts if classification\nperformance is unsatisfactory. However, there is little analysis or\njustification of this classification heuristic. This paper describes a set of\nassumptions that can be used to derive a probabilistic justification for\nlexicon-based classification, as well as an analysis of its expected accuracy.\nOne key assumption behind lexicon-based classification is that all words in\neach lexicon are equally predictive. This is rarely true in practice, which is\nwhy lexicon-based approaches are usually outperformed by supervised classifiers\nthat learn distinct weights on each word from labeled instances. This paper\nshows that it is possible to learn such weights without labeled data, by\nleveraging co-occurrence statistics across the lexicons. This offers the best\nof both worlds: light supervision in the form of lexicons, and data-driven\nclassification with higher accuracy than traditional word-counting heuristics.\n","negative":"  Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions.\n","id":294}
{"Unnamed: 0.1":11295,"Unnamed: 0":11295.0,"anchor":"Statistical Learning for OCR Text Correction","positive":"  The accuracy of Optical Character Recognition (OCR) is crucial to the success\nof subsequent applications used in text analyzing pipeline. Recent models of\nOCR post-processing significantly improve the quality of OCR-generated text,\nbut are still prone to suggest correction candidates from limited observations\nwhile insufficiently accounting for the characteristics of OCR errors. In this\npaper, we show how to enlarge candidate suggestion space by using external\ncorpus and integrating OCR-specific features in a regression approach to\ncorrect OCR-generated errors. The evaluation results show that our model can\ncorrect 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of\nthe OCR-errors (considering the top 3 suggestions), for cases where the\ntheoretical correction upper-bound is 78%.\n","negative":"  Time Series Clustering is an important subroutine in many higher-level data\nmining analyses, including data editing for classifiers, summarization, and\noutlier detection. It is well known that for similarity search the superiority\nof Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as\nwe consider ever larger datasets. However, as we shall show, the same is not\ntrue for clustering. Clustering time series under DTW remains a computationally\nexpensive operation. In this work, we address this issue in two ways. We\npropose a novel pruning strategy that exploits both the upper and lower bounds\nto prune off a very large fraction of the expensive distance calculations. This\npruning strategy is admissible and gives us provably identical results to the\nbrute force algorithm, but is at least an order of magnitude faster. For\ndatasets where even this level of speedup is inadequate, we show that we can\nuse a simple heuristic to order the unavoidable calculations in a\nmost-useful-first ordering, thus casting the clustering into an anytime\nframework. We demonstrate the utility of our ideas with both single and\nmultidimensional case studies in the domains of astronomy, speech physiology,\nmedicine and entomology. In addition, we show the generality of our clustering\nframework to other domains by efficiently obtaining semantically significant\nclusters in protein sequences using the Edit Distance, the discrete data\nanalogue of DTW.\n","id":295}
{"Unnamed: 0.1":11296,"Unnamed: 0":11296.0,"anchor":"Associative Adversarial Networks","positive":"  We propose a higher-level associative memory for learning adversarial\nnetworks. Generative adversarial network (GAN) framework has a discriminator\nand a generator network. The generator (G) maps white noise (z) to data samples\nwhile the discriminator (D) maps data samples to a single scalar. To do so, G\nlearns how to map from high-level representation space to data space, and D\nlearns to do the opposite. We argue that higher-level representation spaces\nneed not necessarily follow a uniform probability distribution. In this work,\nwe use Restricted Boltzmann Machines (RBMs) as a higher-level associative\nmemory and learn the probability distribution for the high-level features\ngenerated by D. The associative memory samples its underlying probability\ndistribution and G learns how to map these samples to data space. The proposed\nassociative adversarial networks (AANs) are generative models in the\nhigher-levels of the learning, and use adversarial non-stochastic models D and\nG for learning the mapping between data and higher-level representation spaces.\nExperiments show the potential of the proposed networks.\n","negative":"  Emerging workloads, such as graph processing and machine learning are\napproximate because of the scale of data involved and the stochastic nature of\nthe underlying algorithms. These algorithms are often distributed over multiple\nmachines using bulk-synchronous processing (BSP) or other synchronous\nprocessing paradigms such as map-reduce. However, data parallel processing\nprimitives such as repeated barrier and reduce operations introduce high\nsynchronization overheads. Hence, many existing data-processing platforms use\nasynchrony and staleness to improve data-parallel job performance. Often, these\nsystems simply change the synchronous communication to asynchronous between the\nworker nodes in the cluster. This improves the throughput of data processing\nbut results in poor accuracy of the final output since different workers may\nprogress at different speeds and process inconsistent intermediate outputs.\n  In this paper, we present ASAP, a model that provides asynchronous and\napproximate processing semantics for data-parallel computation. ASAP provides\nfine-grained worker synchronization using NOTIFY-ACK semantics that allows\nindependent workers to run asynchronously. ASAP also provides stochastic reduce\nthat provides approximate but guaranteed convergence to the same result as an\naggregated all-reduce. In our results, we show that ASAP can reduce\nsynchronization costs and provides 2-10X speedups in convergence and up to 10X\nsavings in network costs for distributed machine learning applications and\nprovides strong convergence guarantees.\n","id":296}
{"Unnamed: 0.1":11297,"Unnamed: 0":11297.0,"anchor":"Measuring Sample Quality with Diffusions","positive":"  Stein's method for measuring convergence to a continuous target distribution\nrelies on an operator characterizing the target and Stein factor bounds on the\nsolutions of an associated differential equation. While such operators and\nbounds are readily available for a diversity of univariate targets, few\nmultivariate targets have been analyzed. We introduce a new class of\ncharacterizing operators based on Ito diffusions and develop explicit\nmultivariate Stein factor bounds for any target with a fast-coupling Ito\ndiffusion. As example applications, we develop computable and\nconvergence-determining diffusion Stein discrepancies for log-concave,\nheavy-tailed, and multimodal targets and use these quality measures to select\nthe hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare\nrandom and deterministic quadrature rules, and quantify bias-variance tradeoffs\nin approximate MCMC. Our results establish a near-linear relationship between\ndiffusion Stein discrepancies and Wasserstein distances, improving upon past\nwork even for strongly log-concave targets. The exposed relationship between\nStein factors and Markov process coupling may be of independent interest.\n","negative":"  In this paper we describe an algorithm for estimating the provenance of hacks\non websites. That is, given properties of sites and the temporal occurrence of\nattacks, we are able to attribute individual attacks to joint causes and\nvulnerabilities, as well as estimating the evolution of these vulnerabilities\nover time. Specifically, we use hazard regression with a time-varying additive\nhazard function parameterized in a generalized linear form. The activation\ncoefficients on each feature are continuous-time functions over time. We\nformulate the problem of learning these functions as a constrained variational\nmaximum likelihood estimation problem with total variation penalty and show\nthat the optimal solution is a 0th order spline (a piecewise constant function)\nwith a finite number of known knots. This allows the inference problem to be\nsolved efficiently and at scale by solving a finite dimensional optimization\nproblem. Extensive experiments on real data sets show that our method\nsignificantly outperforms Cox's proportional hazard model. We also conduct a\ncase study and verify that the fitted functions are indeed recovering\nvulnerable features and real-life events such as the release of code to exploit\nthese features in hacker blogs.\n","id":297}
{"Unnamed: 0.1":11298,"Unnamed: 0":11298.0,"anchor":"Robust end-to-end deep audiovisual speech recognition","positive":"  Speech is one of the most effective ways of communication among humans. Even\nthough audio is the most common way of transmitting speech, very important\ninformation can be found in other modalities, such as vision. Vision is\nparticularly useful when the acoustic signal is corrupted. Multi-modal speech\nrecognition however has not yet found wide-spread use, mostly because the\ntemporal alignment and fusion of the different information sources is\nchallenging.\n  This paper presents an end-to-end audiovisual speech recognizer (AVSR), based\non recurrent neural networks (RNN) with a connectionist temporal classification\n(CTC) loss function. CTC creates sparse \"peaky\" output activations, and we\nanalyze the differences in the alignments of output targets (phonemes or\nvisemes) between audio-only, video-only, and audio-visual feature\nrepresentations. We present the first such experiments on the large vocabulary\nIBM ViaVoice database, which outperform previously published approaches on\nphone accuracy in clean and noisy conditions.\n","negative":"  Structural correspondence learning (SCL) is an effective method for\ncross-lingual sentiment classification. This approach uses unlabeled documents\nalong with a word translation oracle to automatically induce task specific,\ncross-lingual correspondences. It transfers knowledge through identifying\nimportant features, i.e., pivot features. For simplicity, however, it assumes\nthat the word translation oracle maps each pivot feature in source language to\nexactly only one word in target language. This one-to-one mapping between words\nin different languages is too strict. Also the context is not considered at\nall. In this paper, we propose a cross-lingual SCL based on distributed\nrepresentation of words; it can learn meaningful one-to-many mappings for pivot\nwords using large amounts of monolingual data and a small dictionary. We\nconduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset,\nemploying English as source language, and Chinese as target language. Our\nmethod does not rely on the parallel corpora and the experimental results show\nthat our approach is more competitive than the state-of-the-art methods in\ncross-lingual sentiment classification.\n","id":298}
{"Unnamed: 0.1":11299,"Unnamed: 0":11299.0,"anchor":"Spatial contrasting for deep unsupervised learning","positive":"  Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.\n","negative":"  The \"fire together, wire together\" Hebbian model is a central principle for\nlearning in neuroscience, but surprisingly, it has found limited applicability\nin modern machine learning. In this paper, we take a first step towards\nbridging this gap, by developing flavors of competitive Hebbian learning which\nproduce sparse, distributed neural codes using online adaptation with minimal\ntuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning\n(AHL). We illustrate the distributed nature of the learned representations via\noutput entropy computations for synthetic data, and demonstrate superior\nperformance, compared to standard alternatives such as autoencoders, in\ntraining a deep convolutional net on standard image datasets.\n","id":299}
{"Unnamed: 0.1":11300,"Unnamed: 0":11300.0,"anchor":"GRAM: Graph-based Attention Model for Healthcare Representation Learning","positive":"  Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.\n","negative":"  Supervised learning with a deep convolutional neural network is used to\nidentify the QCD equation of state (EoS) employed in relativistic hydrodynamic\nsimulations of heavy-ion collisions from the simulated final-state particle\nspectra $\\rho(p_T,\\Phi)$. High-level correlations of $\\rho(p_T,\\Phi)$ learned\nby the neural network act as an effective \"EoS-meter\" in detecting the nature\nof the QCD transition. The EoS-meter is model independent and insensitive to\nother simulation inputs, especially the initial conditions. Thus it provides a\npowerful direct-connection of heavy-ion collision observables with the bulk\nproperties of QCD.\n","id":300}
{"Unnamed: 0.1":11301,"Unnamed: 0":11301.0,"anchor":"An Efficient Training Algorithm for Kernel Survival Support Vector\n  Machines","positive":"  Survival analysis is a fundamental tool in medical research to identify\npredictors of adverse events and develop systems for clinical decision support.\nIn order to leverage large amounts of patient data, efficient optimisation\nroutines are paramount. We propose an efficient training algorithm for the\nkernel survival support vector machine (SSVM). We directly optimise the primal\nobjective function and employ truncated Newton optimisation and order statistic\ntrees to significantly lower computational costs compared to previous training\nalgorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with\n$n$ samples and $p$ features. Our results demonstrate that our proposed\noptimisation scheme allows analysing data of a much larger scale with no loss\nin prediction performance. Experiments on synthetic and 5 real-world datasets\nshow that our technique outperforms existing kernel SSVM formulations if the\namount of right censoring is high ($\\geq85\\%$), and performs comparably\notherwise.\n","negative":"  We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case.\n","id":301}
{"Unnamed: 0.1":11302,"Unnamed: 0":11302.0,"anchor":"The Recycling Gibbs Sampler for Efficient Learning","positive":"  Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression.\n","negative":"  We show that the square Hellinger distance between two Bayesian networks on\nthe same directed graph, $G$, is subadditive with respect to the neighborhoods\nof $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two\nBayesian networks on the same DAG, our inequality states that the square\nHellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the\nsum, $\\sum_v H^2(P_{\\{v\\} \\cup \\Pi_v}, Q_{\\{v\\} \\cup \\Pi_v})$, of the square\nHellinger distances between the marginals of $P$ and $Q$ on every node $v$ and\nits parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the\nconditionals but the marginals of $P$ and $Q$. We derive a similar inequality\nfor more general Markov Random Fields.\n  As an application of our inequality, we show that distinguishing whether two\nBayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy\n$P=Q$ vs $d_{\\rm TV}(P,Q)>\\epsilon$ can be performed from\n$\\tilde{O}(|\\Sigma|^{3\/4(d+1)} \\cdot n\/\\epsilon^2)$ samples, where $d$ is the\nmaximum in-degree of the DAG and $\\Sigma$ the domain of each variable of the\nBayesian networks. If $P$ and $Q$ are defined on potentially different and\npotentially unknown trees, the sample complexity becomes\n$\\tilde{O}(|\\Sigma|^{4.5} n\/\\epsilon^2)$, whose dependence on $n, \\epsilon$ is\noptimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product\ndistributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes\n$O(\\sqrt{n}\/\\epsilon^2)$, which is optimal up to constant factors.\n","id":302}
{"Unnamed: 0.1":11303,"Unnamed: 0":11303.0,"anchor":"A Deep Learning Approach for Joint Video Frame and Reward Prediction in\n  Atari Games","positive":"  Reinforcement learning is concerned with identifying reward-maximizing\nbehaviour policies in environments that are initially unknown. State-of-the-art\nreinforcement learning approaches, such as deep Q-networks, are model-free and\nlearn to act effectively across a wide range of environments such as Atari\ngames, but require huge amounts of data. Model-based techniques are more\ndata-efficient, but need to acquire explicit knowledge about the environment.\n  In this paper, we take a step towards using model-based techniques in\nenvironments with a high-dimensional visual state space by demonstrating that\nit is possible to learn system dynamics and the reward structure jointly. Our\ncontribution is to extend a recently developed deep neural network for video\nframe prediction in Atari games to enable reward prediction as well. To this\nend, we phrase a joint optimization problem for minimizing both video frame and\nreward reconstruction loss, and adapt network parameters accordingly. Empirical\nevaluations on five Atari games demonstrate accurate cumulative reward\nprediction of up to 200 frames. We consider these results as opening up\nimportant directions for model-based reinforcement learning in complex,\ninitially unknown environments.\n","negative":"  Max-cut, clustering, and many other partitioning problems that are of\nsignificant importance to machine learning and other scientific fields are\nNP-hard, a reality that has motivated researchers to develop a wealth of\napproximation algorithms and heuristics. Although the best algorithm to use\ntypically depends on the specific application domain, a worst-case analysis is\noften used to compare algorithms. This may be misleading if worst-case\ninstances occur infrequently, and thus there is a demand for optimization\nmethods which return the algorithm configuration best suited for the given\napplication's typical inputs. We address this problem for clustering, max-cut,\nand other partitioning problems, such as integer quadratic programming, by\ndesigning computationally efficient and sample efficient learning algorithms\nwhich receive samples from an application-specific distribution over problem\ninstances and learn a partitioning algorithm with high expected performance.\nOur algorithms learn over common integer quadratic programming and clustering\nalgorithm families: SDP rounding algorithms and agglomerative clustering\nalgorithms with dynamic programming. For our sample complexity analysis, we\nprovide tight bounds on the pseudodimension of these algorithm classes, and\nshow that surprisingly, even for classes of algorithms parameterized by a\nsingle parameter, the pseudo-dimension is superconstant. In this way, our work\nboth contributes to the foundations of algorithm configuration and pushes the\nboundaries of learning theory, since the algorithm classes we analyze consist\nof multi-stage optimization procedures and are significantly more complex than\nclasses typically studied in learning theory.\n","id":303}
{"Unnamed: 0.1":11304,"Unnamed: 0":11304.0,"anchor":"Structured Prediction by Conditional Risk Minimization","positive":"  We propose a general approach for supervised learning with structured output\nspaces, such as combinatorial and polyhedral sets, that is based on minimizing\nestimated conditional risk functions. Given a loss function defined over pairs\nof output labels, we first estimate the conditional risk function by solving a\n(possibly infinite) collection of regularized least squares problems. A\nprediction is made by solving an inference problem that minimizes the estimated\nconditional risk function over the output space. We show that this approach\nenables, in some cases, efficient training and inference without explicitly\nintroducing a convex surrogate for the original loss function, even when it is\ndiscontinuous. Empirical evaluations on real-world and synthetic data sets\ndemonstrate the effectiveness of our method in adapting to a variety of loss\nfunctions.\n","negative":"  Weather affects our mood and behaviors, and many aspects of our life. When it\nis sunny, most people become happier; but when it rains, some people get\ndepressed. Despite this evidence and the abundance of data, weather has mostly\nbeen overlooked in the machine learning and data science research. This work\npresents a causal analysis of how weather affects TV watching patterns. We show\nthat some weather attributes, such as pressure and precipitation, cause major\nchanges in TV watching patterns. To the best of our knowledge, this is the\nfirst large-scale causal study of the impact of weather on TV watching\npatterns.\n","id":304}
{"Unnamed: 0.1":11305,"Unnamed: 0":11305.0,"anchor":"Risk-Sensitive Learning and Pricing for Demand Response","positive":"  We consider the setting in which an electric power utility seeks to curtail\nits peak electricity demand by offering a fixed group of customers a uniform\nprice for reductions in consumption relative to their predetermined baselines.\nThe underlying demand curve, which describes the aggregate reduction in\nconsumption in response to the offered price, is assumed to be affine and\nsubject to unobservable random shocks. Assuming that both the parameters of the\ndemand curve and the distribution of the random shocks are initially unknown to\nthe utility, we investigate the extent to which the utility might dynamically\nadjust its offered prices to maximize its cumulative risk-sensitive payoff over\na finite number of $T$ days. In order to do so effectively, the utility must\ndesign its pricing policy to balance the tradeoff between the need to learn the\nunknown demand model (exploration) and maximize its payoff (exploitation) over\ntime. In this paper, we propose such a pricing policy, which is shown to\nexhibit an expected payoff loss over $T$ days that is at most\n$O(\\sqrt{T}\\log(T))$, relative to an oracle pricing policy that knows the\nunderlying demand model. Moreover, the proposed pricing policy is shown to\nyield a sequence of prices that converge to the oracle optimal prices in the\nmean square sense.\n","negative":"  Complex nonlinear models such as deep neural network (DNNs) have become an\nimportant tool for image classification, speech recognition, natural language\nprocessing, and many other fields of application. These models however lack\ntransparency due to their complex nonlinear structure and to the complex data\ndistributions to which they typically apply. As a result, it is difficult to\nfully characterize what makes these models reach a particular decision for a\ngiven input. This lack of transparency can be a drawback, especially in the\ncontext of sensitive applications such as medical analysis or security. In this\nshort paper, we summarize a recent technique introduced by Bach et al. [1] that\nexplains predictions by decomposing the classification decision of DNN models\nin terms of input variables.\n","id":305}
{"Unnamed: 0.1":11306,"Unnamed: 0":11306.0,"anchor":"Tree Space Prototypes: Another Look at Making Tree Ensembles\n  Interpretable","positive":"  Ensembles of decision trees perform well on many problems, but are not\ninterpretable. In contrast to existing approaches in interpretability that\nfocus on explaining relationships between features and predictions, we propose\nan alternative approach to interpret tree ensemble classifiers by surfacing\nrepresentative points for each class -- prototypes. We introduce a new distance\nfor Gradient Boosted Tree models, and propose new, adaptive prototype selection\nmethods with theoretical guarantees, with the flexibility to choose a different\nnumber of prototypes in each class. We demonstrate our methods on random\nforests and gradient boosted trees, showing that the prototypes can perform as\nwell as or even better than the original tree ensemble when used as a\nnearest-prototype classifier. In a user study, humans were better at predicting\nthe output of a tree ensemble classifier when using prototypes than when using\nShapley values, a popular feature attribution method. Hence, prototypes present\na viable alternative to feature-based explanations for tree ensembles.\n","negative":"  In this paper, we discuss the approaches we took and trade-offs involved in\nmaking a paper on a conceptual topic in pattern recognition research fully\nreproducible. We discuss our definition of reproducibility, the tools used, how\nthe analysis was set up, show some examples of alternative analyses the code\nenables and discuss our views on reproducibility.\n","id":306}
{"Unnamed: 0.1":11307,"Unnamed: 0":11307.0,"anchor":"Max-Margin Deep Generative Models for (Semi-)Supervised Learning","positive":"  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks.\n","negative":"  We consider the design of computationally efficient online learning\nalgorithms in an adversarial setting in which the learner has access to an\noffline optimization oracle. We present an algorithm called Generalized\nFollow-the-Perturbed-Leader and provide conditions under which it is\noracle-efficient while achieving vanishing regret. Our results make significant\nprogress on an open problem raised by Hazan and Koren, who showed that\noracle-efficient algorithms do not exist in general and asked whether one can\nidentify properties under which oracle-efficient online learning may be\npossible.\n  Our auction-design framework considers an auctioneer learning an optimal\nauction for a sequence of adversarially selected valuations with the goal of\nachieving revenue that is almost as good as the optimal auction in hindsight,\namong a class of auctions. We give oracle-efficient learning results for: (1)\nVCG auctions with bidder-specific reserves in single-parameter settings, (2)\nenvy-free item pricing in multi-item auctions, and (3) s-level auctions of\nMorgenstern and Roughgarden for single-item settings. The last result leads to\nan approximation of the overall optimal Myerson auction when bidders'\nvaluations are drawn according to a fast-mixing Markov process, extending prior\nwork that only gave such guarantees for the i.i.d. setting.\n  Finally, we derive various extensions, including: (1) oracle-efficient\nalgorithms for the contextual learning setting in which the learner has access\nto side information (such as bidder demographics), (2) learning with\napproximate oracles such as those based on Maximal-in-Range algorithms, and (3)\nno-regret bidding in simultaneous auctions, resolving an open problem of\nDaskalakis and Syrgkanis.\n","id":307}
{"Unnamed: 0.1":11308,"Unnamed: 0":11308.0,"anchor":"Fast and Energy-Efficient CNN Inference on IoT Devices","positive":"  Convolutional Neural Networks (CNNs) exhibit remarkable performance in\nvarious machine learning tasks. As sensor-equipped internet of things (IoT)\ndevices permeate into every aspect of modern life, it is increasingly important\nto run CNN inference, a computationally intensive application, on resource\nconstrained devices. We present a technique for fast and energy-efficient CNN\ninference on mobile SoC platforms, which are projected to be a major player in\nthe IoT space. We propose techniques for efficient parallelization of CNN\ninference targeting mobile GPUs, and explore the underlying tradeoffs.\nExperiments with running Squeezenet on three different mobile devices confirm\nthe effectiveness of our approach. For further study, please refer to the\nproject repository available on our GitHub page:\nhttps:\/\/github.com\/mtmd\/Mobile_ConvNet\n","negative":"  A large amount of information exists in reviews written by users. This source\nof information has been ignored by most of the current recommender systems\nwhile it can potentially alleviate the sparsity problem and improve the quality\nof recommendations. In this paper, we present a deep model to learn item\nproperties and user behaviors jointly from review text. The proposed model,\nnamed Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel\nneural networks coupled in the last layers. One of the networks focuses on\nlearning user behaviors exploiting reviews written by the user, and the other\none learns item properties from the reviews written for the item. A shared\nlayer is introduced on the top to couple these two networks together. The\nshared layer enables latent factors learned for users and items to interact\nwith each other in a manner similar to factorization machine techniques.\nExperimental results demonstrate that DeepCoNN significantly outperforms all\nbaseline recommender systems on a variety of datasets.\n","id":308}
{"Unnamed: 0.1":11309,"Unnamed: 0":11309.0,"anchor":"Deep Recurrent Convolutional Neural Network: Improving Performance For\n  Speech Recognition","positive":"  A deep learning approach has been widely applied in sequence modeling\nproblems. In terms of automatic speech recognition (ASR), its performance has\nsignificantly been improved by increasing large speech corpus and deeper neural\nnetwork. Especially, recurrent neural network and deep convolutional neural\nnetwork have been applied in ASR successfully. Given the arising problem of\ntraining speed, we build a novel deep recurrent convolutional network for\nacoustic modeling and then apply deep residual learning to it. Our experiments\nshow that it has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network. In the\nexperiments, we compare the convergence speed of our novel deep recurrent\nconvolutional networks and traditional deep convolutional recurrent networks.\nWith faster convergence speed, our novel deep recurrent convolutional networks\ncan reach the comparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel deep recurret\nconvolutional networks. Finally, we evaluate all our experimental networks by\nphoneme error rate (PER) with our proposed bidirectional statistical n-gram\nlanguage model. Our evaluation results show that our newly proposed deep\nrecurrent convolutional network applied with deep residual learning can reach\nthe best PER of 17.33\\% with the fastest convergence speed on TIMIT database.\nThe outstanding performance of our novel deep recurrent convolutional neural\nnetwork with deep residual learning indicates that it can be potentially\nadopted in other sequential problems.\n","negative":"  Learning algorithms that learn linear models often have high representation\nbias on real-world problems. In this paper, we show that this representation\nbias can be greatly reduced by discretization. Discretization is a common\nprocedure in machine learning that is used to convert a quantitative attribute\ninto a qualitative one. It is often motivated by the limitation of some\nlearners to qualitative data. Discretization loses information, as fewer\ndistinctions between instances are possible using discretized data relative to\nundiscretized data. In consequence, where discretization is not essential, it\nmight appear desirable to avoid it. However, it has been shown that\ndiscretization often substantially reduces the error of the linear generative\nBayesian classifier naive Bayes. This motivates a systematic study of the\neffectiveness of discretizing quantitative attributes for other linear\nclassifiers. In this work, we study the effect of discretization on the\nperformance of linear classifiers optimizing three distinct discriminative\nobjective functions --- logistic regression (optimizing negative\nlog-likelihood), support vector classifiers (optimizing hinge loss) and a\nzero-hidden layer artificial neural network (optimizing mean-square-error). We\nshow that discretization can greatly increase the accuracy of these linear\ndiscriminative learners by reducing their representation bias, especially on\nbig datasets. We substantiate our claims with an empirical study on $42$\nbenchmark datasets.\n","id":309}
{"Unnamed: 0.1":11310,"Unnamed: 0":11310.0,"anchor":"Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery","positive":"  Recurrent neural networks (RNNs) are powerful and effective for processing\nsequential data. However, RNNs are usually considered \"black box\" models whose\ninternal structure and learned parameters are not interpretable. In this paper,\nwe propose an interpretable RNN based on the sequential iterative\nsoft-thresholding algorithm (SISTA) for solving the sequential sparse recovery\nproblem, which models a sequence of correlated observations with a sequence of\nsparse latent vectors. The architecture of the resulting SISTA-RNN is\nimplicitly defined by the computational structure of SISTA, which results in a\nnovel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are\nperfectly interpretable as the parameters of a principled statistical model,\nwhich in this case include a sparsifying dictionary, iterative step size, and\nregularization parameters. In addition, on a particular sequential compressive\nsensing task, the SISTA-RNN trains faster and achieves better performance than\nconventional state-of-the-art black box RNNs, including long-short term memory\n(LSTM) RNNs.\n","negative":"  The classification of MRI images according to the anatomical field of view is\na necessary task to solve when faced with the increasing quantity of medical\nimages. In parallel, advances in deep learning makes it a suitable tool for\ncomputer vision problems. Using a common architecture (such as AlexNet)\nprovides quite good results, but not sufficient for clinical use. Improving the\nmodel is not an easy task, due to the large number of hyper-parameters\ngoverning both the architecture and the training of the network, and to the\nlimited understanding of their relevance. Since an exhaustive search is not\ntractable, we propose to optimize the network first by random search, and then\nby an adaptive search based on Gaussian Processes and Probability of\nImprovement. Applying this method on a large and varied MRI dataset, we show a\nsubstantial improvement between the baseline network and the final one (up to\n20\\% for the most difficult classes).\n","id":310}
{"Unnamed: 0.1":11311,"Unnamed: 0":11311.0,"anchor":"Investigating the influence of noise and distractors on the\n  interpretation of neural networks","positive":"  Understanding neural networks is becoming increasingly important. Over the\nlast few years different types of visualisation and explanation methods have\nbeen proposed. However, none of them explicitly considered the behaviour in the\npresence of noise and distracting elements. In this work, we will show how\nnoise and distracting dimensions can influence the result of an explanation\nmodel. This gives a new theoretical insights to aid selection of the most\nappropriate explanation model within the deep-Taylor decomposition framework.\n","negative":"  The Wisdom of Crowds (WOC), as a theory in the social science, gets a new\nparadigm in computer science. The WOC theory explains that the aggregate\ndecision made by a group is often better than those of its individual members\nif specific conditions are satisfied. This paper presents a novel framework for\nunsupervised and semi-supervised cluster ensemble by exploiting the WOC theory.\nWe employ four conditions in the WOC theory, i.e., diversity, independency,\ndecentralization and aggregation, to guide both the constructing of individual\nclustering results and the final combination for clustering ensemble. Firstly,\nindependency criterion, as a novel mapping system on the raw data set, removes\nthe correlation between features on our proposed method. Then, decentralization\nas a novel mechanism generates high-quality individual clustering results.\nNext, uniformity as a new diversity metric evaluates the generated clustering\nresults. Further, weighted evidence accumulation clustering method is proposed\nfor the final aggregation without using thresholding procedure. Experimental\nstudy on varied data sets demonstrates that the proposed approach achieves\nsuperior performance to state-of-the-art methods.\n","id":311}
{"Unnamed: 0.1":11312,"Unnamed: 0":11312.0,"anchor":"Correlation Clustering with Low-Rank Matrices","positive":"  Correlation clustering is a technique for aggregating data based on\nqualitative information about which pairs of objects are labeled 'similar' or\n'dissimilar.' Because the optimization problem is NP-hard, much of the previous\nliterature focuses on finding approximation algorithms. In this paper we\nexplore how to solve the correlation clustering objective exactly when the data\nto be clustered can be represented by a low-rank matrix. We prove in particular\nthat correlation clustering can be solved in polynomial time when the\nunderlying matrix is positive semidefinite with small constant rank, but that\nthe task remains NP-hard in the presence of even one negative eigenvalue. Based\non our theoretical results, we develop an algorithm for efficiently \"solving\"\nlow-rank positive semidefinite correlation clustering by employing a procedure\nfor zonotope vertex enumeration. We demonstrate the effectiveness and speed of\nour algorithm by using it to solve several clustering problems on both\nsynthetic and real-world data.\n","negative":"  Surgical Site Infection (SSI) is a national priority in healthcare research.\nMuch research attention has been attracted to develop better SSI risk\nprediction models. However, most of the existing SSI risk prediction models are\nbuilt on static risk factors such as comorbidities and operative factors. In\nthis paper, we investigate the use of the dynamic wound data for SSI risk\nprediction. There have been emerging mobile health (mHealth) tools that can\nclosely monitor the patients and generate continuous measurements of many\nwound-related variables and other evolving clinical variables. Since existing\nprediction models of SSI have quite limited capacity to utilize the evolving\nclinical data, we develop the corresponding solution to equip these mHealth\ntools with decision-making capabilities for SSI prediction with a seamless\nassembly of several machine learning models to tackle the analytic challenges\narising from the spatial-temporal data. The basic idea is to exploit the\nlow-rank property of the spatial-temporal data via the bilinear formulation,\nand further enhance it with automatic missing data imputation by the matrix\ncompletion technique. We derive efficient optimization algorithms to implement\nthese models and demonstrate the superior performances of our new predictive\nmodel on a real-world dataset of SSI, compared to a range of state-of-the-art\nmethods.\n","id":312}
{"Unnamed: 0.1":11313,"Unnamed: 0":11313.0,"anchor":"Variational Graph Auto-Encoders","positive":"  We introduce the variational graph auto-encoder (VGAE), a framework for\nunsupervised learning on graph-structured data based on the variational\nauto-encoder (VAE). This model makes use of latent variables and is capable of\nlearning interpretable latent representations for undirected graphs. We\ndemonstrate this model using a graph convolutional network (GCN) encoder and a\nsimple inner product decoder. Our model achieves competitive results on a link\nprediction task in citation networks. In contrast to most existing models for\nunsupervised learning on graph-structured data and link prediction, our model\ncan naturally incorporate node features, which significantly improves\npredictive performance on a number of benchmark datasets.\n","negative":"  We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors.\n","id":313}
{"Unnamed: 0.1":11314,"Unnamed: 0":11314.0,"anchor":"Limbo: A Fast and Flexible Library for Bayesian Optimization","positive":"  Limbo is an open-source C++11 library for Bayesian optimization which is\ndesigned to be both highly flexible and very fast. It can be used to optimize\nfunctions for which the gradient is unknown, evaluations are expensive, and\nruntime cost matters (e.g., on embedded systems or robots). Benchmarks on\nstandard functions show that Limbo is about 2 times faster than BayesOpt\n(another C++ library) for a similar accuracy.\n","negative":"  In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the \"Fast\nSDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.\n","id":314}
{"Unnamed: 0.1":11315,"Unnamed: 0":11315.0,"anchor":"Deep Learning Approximation for Stochastic Control Problems","positive":"  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n","negative":"  In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.\n","id":315}
{"Unnamed: 0.1":11316,"Unnamed: 0":11316.0,"anchor":"TreeView: Peeking into Deep Neural Networks Via Feature-Space\n  Partitioning","positive":"  With the advent of highly predictive but opaque deep learning models, it has\nbecome more important than ever to understand and explain the predictions of\nsuch models. Existing approaches define interpretability as the inverse of\ncomplexity and achieve interpretability at the cost of accuracy. This\nintroduces a risk of producing interpretable but misleading explanations. As\nhumans, we are prone to engage in this kind of behavior \\cite{mythos}. In this\npaper, we take a step in the direction of tackling the problem of\ninterpretability without compromising the model accuracy. We propose to build a\nTreeview representation of the complex model via hierarchical partitioning of\nthe feature space, which reveals the iterative rejection of unlikely class\nlabels until the correct association is predicted.\n","negative":"  Low-rank learning has attracted much attention recently due to its efficacy\nin a rich variety of real-world tasks, e.g., subspace segmentation and image\ncategorization. Most low-rank methods are incapable of capturing\nlow-dimensional subspace for supervised learning tasks, e.g., classification\nand regression. This paper aims to learn both the discriminant low-rank\nrepresentation (LRR) and the robust projecting subspace in a supervised manner.\nTo achieve this goal, we cast the problem into a constrained rank minimization\nframework by adopting the least squares regularization. Naturally, the data\nlabel structure tends to resemble that of the corresponding low-dimensional\nrepresentation, which is derived from the robust subspace projection of clean\ndata by low-rank learning. Moreover, the low-dimensional representation of\noriginal data can be paired with some informative structure by imposing an\nappropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a\nnovel constrained LRR method. The objective function is formulated as a\nconstrained nuclear norm minimization problem, which can be solved by the\ninexact augmented Lagrange multiplier algorithm. Extensive experiments on image\nclassification, human pose estimation, and robust face recovery have confirmed\nthe superiority of our method.\n","id":316}
{"Unnamed: 0.1":11317,"Unnamed: 0":11317.0,"anchor":"Achieving non-discrimination in data release","positive":"  Discrimination discovery and prevention\/removal are increasingly important\ntasks in data mining. Discrimination discovery aims to unveil discriminatory\npractices on the protected attribute (e.g., gender) by analyzing the dataset of\nhistorical decision records, and discrimination prevention aims to remove\ndiscrimination by modifying the biased data before conducting predictive\nanalysis. In this paper, we show that the key to discrimination discovery and\nprevention is to find the meaningful partitions that can be used to provide\nquantitative evidences for the judgment of discrimination. With the support of\nthe causal graph, we present a graphical condition for identifying a meaningful\npartition. Based on that, we develop a simple criterion for the claim of\nnon-discrimination, and propose discrimination removal algorithms which\naccurately remove discrimination while retaining good data utility. Experiments\nusing real datasets show the effectiveness of our approaches.\n","negative":"  We present a Deep Convolutional Neural Network architecture which serves as a\ngeneric image-to-image regressor that can be trained end-to-end without any\nfurther machinery. Our proposed architecture: the Recursively Branched\nDeconvolutional Network (RBDN) develops a cheap multi-context image\nrepresentation very early on using an efficient recursive branching scheme with\nextensive parameter sharing and learnable upsampling. This multi-context\nrepresentation is subjected to a highly non-linear locality preserving\ntransformation by the remainder of our network comprising of a series of\nconvolutions\/deconvolutions without any spatial downsampling. The RBDN\narchitecture is fully convolutional and can handle variable sized images during\ninference. We provide qualitative\/quantitative results on $3$ diverse tasks:\nrelighting, denoising and colorization and show that our proposed RBDN\narchitecture obtains comparable results to the state-of-the-art on each of\nthese tasks when used off-the-shelf without any post processing or\ntask-specific architectural modifications.\n","id":317}
{"Unnamed: 0.1":11318,"Unnamed: 0":11318.0,"anchor":"Grad-CAM: Why did you say that?","positive":"  We propose a technique for making Convolutional Neural Network (CNN)-based\nmodels more transparent by visualizing input regions that are 'important' for\npredictions -- or visual explanations. Our approach, called Gradient-weighted\nClass Activation Mapping (Grad-CAM), uses class-specific gradient information\nto localize important regions. These localizations are combined with existing\npixel-space visualizations to create a novel high-resolution and\nclass-discriminative visualization called Guided Grad-CAM. These methods help\nbetter understand CNN-based models, including image captioning and visual\nquestion answering (VQA) models. We evaluate our visual explanations by\nmeasuring their ability to discriminate between classes, to inspire trust in\nhumans, and their correlation with occlusion maps. Grad-CAM provides a new way\nto understand CNN-based models.\n  We have released code, an online demo hosted on CloudCV, and a full version\nof this extended abstract.\n","negative":"  The presence of bacteria or fungi in the bloodstream of patients is abnormal\nand can lead to life-threatening conditions. A computational model based on a\nbidirectional long short-term memory artificial neural network, is explored to\nassist doctors in the intensive care unit to predict whether examination of\nblood cultures of patients will return positive. As input it uses nine\nmonitored clinical parameters, presented as time series data, collected from\n2177 ICU admissions at the Ghent University Hospital. Our main goal is to\ndetermine if general machine learning methods and more specific, temporal\nmodels, can be used to create an early detection system. This preliminary\nresearch obtains an area of 71.95% under the precision recall curve, proving\nthe potential of temporal neural networks in this context.\n","id":318}
{"Unnamed: 0.1":11319,"Unnamed: 0":11319.0,"anchor":"Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond","positive":"  We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata.\n","negative":"  Performing arts organizations aim to enrich their communities through the\narts. To do this, they strive to match their performance offerings to the taste\nof those communities. Success relies on understanding audience preference and\npredicting their behavior. Similar to most e-commerce or digital entertainment\nfirms, arts presenters need to recommend the right performance to the right\ncustomer at the right time. As part of the Michigan Data Science Team (MDST),\nwe partnered with the University Musical Society (UMS), a non-profit performing\narts presenter housed in the University of Michigan, Ann Arbor. We are\nproviding UMS with analysis and business intelligence, utilizing historical\nindividual-level sales data. We built a recommendation system based on\ncollaborative filtering, gaining insights into the artistic preferences of\ncustomers, along with the similarities between performances. To better\nunderstand audience behavior, we used statistical methods from customer-base\nanalysis. We characterized customer heterogeneity via segmentation, and we\nmodeled customer cohorts to understand and predict ticket purchasing patterns.\nFinally, we combined statistical modeling with natural language processing\n(NLP) to explore the impact of wording in program descriptions. These ongoing\nefforts provide a platform to launch targeted marketing campaigns, helping UMS\ncarry out its mission by allocating its resources more efficiently. Celebrating\nits 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it\ncontinues to enrich communities by connecting world-renowned artists with\ndiverse audiences, especially students in their formative years. We aim to\ncontribute to that mission through data science and customer analytics.\n","id":319}
{"Unnamed: 0.1":11320,"Unnamed: 0":11320.0,"anchor":"Can Co-robots Learn to Teach?","positive":"  We explore beyond existing work on learning from demonstration by asking the\nquestion: Can robots learn to teach?, that is, can a robot autonomously learn\nan instructional policy from expert demonstration and use it to instruct or\ncollaborate with humans in executing complex tasks in uncertain environments?\nIn this paper we pursue a solution to this problem by leveraging the idea that\nhumans often implicitly decompose a higher level task into several subgoals\nwhose execution brings the task closer to completion. We propose Dirichlet\nprocess based non-parametric Inverse Reinforcement Learning (DPMIRL) approach\nfor reward based unsupervised clustering of task space into subgoals. This\napproach is shown to capture the latent subgoals that a human teacher would\nhave utilized to train a novice. The notion of action primitive is introduced\nas the means to communicate instruction policy to humans in the least\ncomplicated manner, and as a computationally efficient tool to segment\ndemonstration data. We evaluate our approach through experiments on hydraulic\nactuated scaled model of an excavator and evaluate and compare different\nteaching strategies utilized by the robot.\n","negative":"  Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint).\n","id":320}
{"Unnamed: 0.1":11321,"Unnamed: 0":11321.0,"anchor":"Inducing Interpretable Representations with Variational Autoencoders","positive":"  We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes.\n","negative":"  Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.\n","id":321}
{"Unnamed: 0.1":11322,"Unnamed: 0":11322.0,"anchor":"Variational Intrinsic Control","positive":"  In this paper we introduce a new unsupervised reinforcement learning method\nfor discovering the set of intrinsic options available to an agent. This set is\nlearned by maximizing the number of different states an agent can reliably\nreach, as measured by the mutual information between the set of options and\noption termination states. To this end, we instantiate two policy gradient\nbased algorithms, one that creates an explicit embedding space of options and\none that represents options implicitly. The algorithms also provide an explicit\nmeasure of empowerment in a given state that can be used by an empowerment\nmaximizing agent. The algorithm scales well with function approximation and we\ndemonstrate the applicability of the algorithm on a range of tasks.\n","negative":"  Predicting business process behaviour is an important aspect of business\nprocess management. Motivated by research in natural language processing, this\npaper describes an application of deep learning with recurrent neural networks\nto the problem of predicting the next event in a business process. This is both\na novel method in process prediction, which has largely relied on explicit\nprocess models, and also a novel application of deep learning methods. The\napproach is evaluated on two real datasets and our results surpass the\nstate-of-the-art in prediction precision.\n","id":322}
{"Unnamed: 0.1":11323,"Unnamed: 0":11323.0,"anchor":"A causal framework for discovering and removing direct and indirect\n  discrimination","positive":"  Anti-discrimination is an increasingly important task in data science. In\nthis paper, we investigate the problem of discovering both direct and indirect\ndiscrimination from the historical data, and removing the discriminatory\neffects before the data is used for predictive analysis (e.g., building\nclassifiers). We make use of the causal network to capture the causal structure\nof the data. Then we model direct and indirect discrimination as the\npath-specific effects, which explicitly distinguish the two types of\ndiscrimination as the causal effects transmitted along different paths in the\nnetwork. Based on that, we propose an effective algorithm for discovering\ndirect and indirect discrimination, as well as an algorithm for precisely\nremoving both types of discrimination while retaining good data utility.\nDifferent from previous works, our approaches can ensure that the predictive\nmodels built from the modified data will not incur discrimination in decision\nmaking. Experiments using real datasets show the effectiveness of our\napproaches.\n","negative":"  Machine learning is making substantial progress in diverse applications. The\nsuccess is mostly due to advances in deep learning. However, deep learning can\nmake mistakes and its generalization abilities to new tasks are questionable.\nWe ask when and how one can combine network outputs, when (i) details of the\nobservations are evaluated by learned deep components and (ii) facts and\nconfirmation rules are available in knowledge based systems. We show that in\nlimited contexts the required number of training samples can be low and\nself-improvement of pre-trained networks in more general context is possible.\nWe argue that the combination of sparse outlier detection with deep components\nthat can support each other diminish the fragility of deep methods, an\nimportant requirement for engineering applications. We argue that supervised\nlearning of labels may be fully eliminated under certain conditions: a\ncomponent based architecture together with a knowledge based system can train\nitself and provide high quality answers. We demonstrate these concepts on the\nState Farm Distracted Driver Detection benchmark. We argue that the view of the\nStudy Panel (2016) may overestimate the requirements on `years of focused\nresearch' and `careful, unique construction' for `AI systems'.\n","id":323}
{"Unnamed: 0.1":11324,"Unnamed: 0":11324.0,"anchor":"Feature Importance Measure for Non-linear Learning Algorithms","positive":"  Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint).\n","negative":"  Many prediction problems can be phrased as inferences over local\nneighborhoods of graphs. The graph represents the interaction between entities,\nand the neighborhood of each entity contains information that allows the\ninferences or predictions. We present an approach for applying machine learning\ndirectly to such graph neighborhoods, yielding predicitons for graph nodes on\nthe basis of the structure of their local neighborhood and the features of the\nnodes in it. Our approach allows predictions to be learned directly from\nexamples, bypassing the step of creating and tuning an inference model or\nsummarizing the neighborhoods via a fixed set of hand-crafted features. The\napproach is based on a multi-level architecture built from Long Short-Term\nMemory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood\nfrom data. We demonstrate the effectiveness of the proposed technique on a\nsynthetic example and on real-world data related to crowdsourced grading,\nBitcoin transactions, and Wikipedia edit reversions.\n","id":324}
{"Unnamed: 0.1":11325,"Unnamed: 0":11325.0,"anchor":"Quad-networks: unsupervised learning to rank for interest point\n  detection","positive":"  Several machine learning tasks require to represent the data using only a\nsparse set of interest points. An ideal detector is able to find the\ncorresponding interest points even if the data undergo a transformation typical\nfor a given domain. Since the task is of high practical interest in computer\nvision, many hand-crafted solutions were proposed. In this paper, we ask a\nfundamental question: can we learn such detectors from scratch? Since it is\noften unclear what points are \"interesting\", human labelling cannot be used to\nfind a truly unbiased solution. Therefore, the task requires an unsupervised\nformulation. We are the first to propose such a formulation: training a neural\nnetwork to rank points in a transformation-invariant manner. Interest points\nare then extracted from the top\/bottom quantiles of this ranking. We validate\nour approach on two tasks: standard RGB image interest point detection and\nchallenging cross-modal interest point detection between RGB and depth images.\nWe quantitatively show that our unsupervised method performs better or on-par\nwith baselines.\n","negative":"  Feature selection problems arise in a variety of applications, such as\nmicroarray analysis, clinical prediction, text categorization, image\nclassification and face recognition, multi-label learning, and classification\nof internet traffic. Among the various classes of methods, forward feature\nselection methods based on mutual information have become very popular and are\nwidely used in practice. However, comparative evaluations of these methods have\nbeen limited by being based on specific datasets and classifiers. In this\npaper, we develop a theoretical framework that allows evaluating the methods\nbased on their theoretical properties. Our framework is grounded on the\nproperties of the target objective function that the methods try to\napproximate, and on a novel categorization of features, according to their\ncontribution to the explanation of the class; we derive upper and lower bounds\nfor the target objective function and relate these bounds with the feature\ntypes. Then, we characterize the types of approximations taken by the methods,\nand analyze how these approximations cope with the good properties of the\ntarget objective function. Additionally, we develop a distributional setting\ndesigned to illustrate the various deficiencies of the methods, and provide\nseveral examples of wrong feature selections. Based on our work, we identify\nclearly the methods that should be avoided, and the methods that currently have\nthe best performance.\n","id":325}
{"Unnamed: 0.1":11326,"Unnamed: 0":11326.0,"anchor":"Programs as Black-Box Explanations","positive":"  Recent work in model-agnostic explanations of black-box machine learning has\ndemonstrated that interpretability of complex models does not have to come at\nthe cost of accuracy or model flexibility. However, it is not clear what kind\nof explanations, such as linear models, decision trees, and rule lists, are the\nappropriate family to consider, and different tasks and models may benefit from\ndifferent kinds of explanations. Instead of picking a single family of\nrepresentations, in this work we propose to use \"programs\" as model-agnostic\nexplanations. We show that small programs can be expressive yet intuitive as\nexplanations, and generalize over a number of existing interpretable families.\nWe propose a prototype program induction method based on simulated annealing\nthat approximates the local behavior of black-box classifiers around a specific\nprediction using random perturbations. Finally, we present preliminary\napplication on small datasets and show that the generated explanations are\nintuitive and accurate for a number of classifiers.\n","negative":"  Reducing bit-widths of weights, activations, and gradients of a Neural\nNetwork can shrink its storage size and memory usage, and also allow for faster\ntraining and inference by exploiting bitwise operations. However, previous\nattempts for quantization of RNNs show considerable performance degradation\nwhen using low bit-width weights and activations. In this paper, we propose\nmethods to quantize the structure of gates and interlinks in LSTM and GRU\ncells. In addition, we propose balanced quantization methods for weights to\nfurther reduce performance degradation. Experiments on PTB and IMDB datasets\nconfirm effectiveness of our methods as performances of our models match or\nsurpass the previous state-of-the-art of quantized RNN.\n","id":326}
{"Unnamed: 0.1":11327,"Unnamed: 0":11327.0,"anchor":"A Neural Network Model to Classify Liver Cancer Patients Using Data\n  Expansion and Compression","positive":"  We develop a neural network model to classify liver cancer patients into\nhigh-risk and low-risk groups using genomic data. Our approach provides a novel\ntechnique to classify big data sets using neural network models. We preprocess\nthe data before training the neural network models. We first expand the data\nusing wavelet analysis. We then compress the wavelet coefficients by mapping\nthem onto a new scaled orthonormal coordinate system. Then the data is used to\ntrain a neural network model that enables us to classify cancer patients into\ntwo different classes of high-risk and low-risk patients. We use the\nleave-one-out approach to build a neural network model. This neural network\nmodel enables us to classify a patient using genomic data as a high-risk or\nlow-risk patient without any information about the survival time of the\npatient. The results from genomic data analysis are compared with survival time\nanalysis. It is shown that the expansion and compression of data using wavelet\nanalysis and singular value decomposition (SVD) is essential to train the\nneural network model.\n","negative":"  Inference in expressive probabilistic models is generally intractable, which\nmakes them difficult to learn and limits their applicability. Sum-product\nnetworks are a class of deep models where, surprisingly, inference remains\ntractable even when an arbitrary number of hidden layers are present. In this\npaper, we generalize this result to a much broader set of learning problems:\nall those where inference consists of summing a function over a semiring. This\nincludes satisfiability, constraint satisfaction, optimization, integration,\nand others. In any semiring, for summation to be tractable it suffices that the\nfactors of every product have disjoint scopes. This unifies and extends many\nprevious results in the literature. Enforcing this condition at learning time\nthus ensures that the learned models are tractable. We illustrate the power and\ngenerality of this approach by applying it to a new type of structured\nprediction problem: learning a nonconvex function that can be globally\noptimized in polynomial time. We show empirically that this greatly outperforms\nthe standard approach of learning without regard to the cost of optimization.\n","id":327}
{"Unnamed: 0.1":11328,"Unnamed: 0":11328.0,"anchor":"SyGuS-Comp 2016: Results and Analysis","positive":"  Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an\nimplementation f that meets both a semantic constraint given by a logical\nformula $\\varphi$ in a background theory T, and a syntactic constraint given by\na grammar G, which specifies the allowed set of candidate implementations. Such\na synthesis problem can be formally defined in SyGuS-IF, a language that is\nbuilt on top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to\nfacilitate, bring together and accelerate research and development of efficient\nsolvers for SyGuS by providing a platform for evaluating different synthesis\ntechniques on a comprehensive set of benchmarks. In this year's competition we\nadded a new track devoted to programming by examples. This track consisted of\ntwo categories, one using the theory of bit-vectors and one using the theory of\nstrings. This paper presents and analyses the results of SyGuS-Comp'16.\n","negative":"  Smart phone apps that enable users to easily track their diets have become\nwidespread in the last decade. This has created an opportunity to discover new\ninsights into obesity and weight loss by analyzing the eating habits of the\nusers of such apps. In this paper, we present diet2vec: an approach to modeling\nlatent structure in a massive database of electronic diet journals. Through an\niterative contract-and-expand process, our model learns real-valued embeddings\nof users' diets, as well as embeddings for individual foods and meals. We\ndemonstrate the effectiveness of our approach on a real dataset of 55K users of\nthe popular diet-tracking app LoseIt\\footnote{http:\/\/www.loseit.com\/}. To the\nbest of our knowledge, this is the largest fine-grained diet tracking study in\nthe history of nutrition and obesity research. Our results suggest that\ndiet2vec finds interpretable results at all levels, discovering intuitive\nrepresentations of foods, meals, and diets.\n","id":328}
{"Unnamed: 0.1":11329,"Unnamed: 0":11329.0,"anchor":"Interpretation of Prediction Models Using the Input Gradient","positive":"  State of the art machine learning algorithms are highly optimized to provide\nthe optimal prediction possible, naturally resulting in complex models. While\nthese models often outperform simpler more interpretable models by order of\nmagnitudes, in terms of understanding the way the model functions, we are often\nfacing a \"black box\".\n  In this paper we suggest a simple method to interpret the behavior of any\npredictive model, both for regression and classification. Given a particular\nmodel, the information required to interpret it can be obtained by studying the\npartial derivatives of the model with respect to the input. We exemplify this\ninsight by interpreting convolutional and multi-layer neural networks in the\nfield of natural language processing.\n","negative":"  Logistic regression is by far the most widely used classifier in real-world\napplications. In this paper, we benchmark the state-of-the-art active learning\nmethods for logistic regression and discuss and illustrate their underlying\ncharacteristics. Experiments are carried out on three synthetic datasets and 44\nreal-world datasets, providing insight into the behaviors of these active\nlearning methods with respect to the area of the learning curve (which plots\nclassification accuracy as a function of the number of queried examples) and\ntheir computational costs. Surprisingly, one of the earliest and simplest\nsuggested active learning methods, i.e., uncertainty sampling, performs\nexceptionally well overall. Another remarkable finding is that random sampling,\nwhich is the rudimentary baseline to improve upon, is not overwhelmed by\nindividual active learning techniques in many cases.\n","id":329}
{"Unnamed: 0.1":11330,"Unnamed: 0":11330.0,"anchor":"Improving Efficiency of SVM k-fold Cross-validation by Alpha Seeding","positive":"  The k-fold cross-validation is commonly used to evaluate the effectiveness of\nSVMs with the selected hyper-parameters. It is known that the SVM k-fold\ncross-validation is expensive, since it requires training k SVMs. However,\nlittle work has explored reusing the h-th SVM for training the (h+1)-th SVM for\nimproving the efficiency of k-fold cross-validation. In this paper, we propose\nthree algorithms that reuse the h-th SVM for improving the efficiency of\ntraining the (h+1)-th SVM. Our key idea is to efficiently identify the support\nvectors and to accurately estimate their associated weights (also called alpha\nvalues) of the next SVM by using the previous SVM. Our experimental results\nshow that our algorithms are several times faster than the k-fold\ncross-validation which does not make use of the previously trained SVM.\nMoreover, our algorithms produce the same results (hence same accuracy) as the\nk-fold cross-validation which does not make use of the previously trained SVM.\n","negative":"  Deep clustering is the first method to handle general audio separation\nscenarios with multiple sources of the same type and an arbitrary number of\nsources, performing impressively in speaker-independent speech separation\ntasks. However, little is known about its effectiveness in other challenging\nsituations such as music source separation. Contrary to conventional networks\nthat directly estimate the source signals, deep clustering generates an\nembedding for each time-frequency bin, and separates sources by clustering the\nbins in the embedding space. We show that deep clustering outperforms\nconventional networks on a singing voice separation task, in both matched and\nmismatched conditions, even though conventional networks have the advantage of\nend-to-end training for best signal approximation, presumably because its more\nflexible objective engenders better regularization. Since the strengths of deep\nclustering and conventional network architectures appear complementary, we\nexplore combining them in a single hybrid network trained via an approach akin\nto multi-task learning. Remarkably, the combination significantly outperforms\neither of its components.\n","id":330}
{"Unnamed: 0.1":11331,"Unnamed: 0":11331.0,"anchor":"Multigrid Neural Architectures","positive":"  We propose a multigrid extension of convolutional neural networks (CNNs).\nRather than manipulating representations living on a single spatial grid, our\nnetwork layers operate across scale space, on a pyramid of grids. They consume\nmultigrid inputs and produce multigrid outputs; convolutional filters\nthemselves have both within-scale and cross-scale extent. This aspect is\ndistinct from simple multiscale designs, which only process the input at\ndifferent scales. Viewed in terms of information flow, a multigrid network\npasses messages across a spatial pyramid. As a consequence, receptive field\nsize grows exponentially with depth, facilitating rapid integration of context.\nMost critically, multigrid structure enables networks to learn internal\nattention and dynamic routing mechanisms, and use them to accomplish tasks on\nwhich modern CNNs fail.\n  Experiments demonstrate wide-ranging performance advantages of multigrid. On\nCIFAR and ImageNet classification tasks, flipping from a single grid to\nmultigrid within the standard CNN paradigm improves accuracy, while being\ncompute and parameter efficient. Multigrid is independent of other\narchitectural choices; we show synergy in combination with residual\nconnections. Multigrid yields dramatic improvement on a synthetic semantic\nsegmentation dataset. Most strikingly, relatively shallow multigrid networks\ncan learn to directly perform spatial transformation tasks, where, in contrast,\ncurrent CNNs fail. Together, our results suggest that continuous evolution of\nfeatures on a multigrid pyramid is a more powerful alternative to existing CNN\ndesigns on a flat grid.\n","negative":"  Training of one-vs.-rest SVMs can be parallelized over the number of classes\nin a straight forward way. Given enough computational resources, one-vs.-rest\nSVMs can thus be trained on data involving a large number of classes. The same\ncannot be stated, however, for the so-called all-in-one SVMs, which require\nsolving a quadratic program of size quadratically in the number of classes. We\ndevelop distributed algorithms for two all-in-one SVM formulations (Lee et al.\nand Weston and Watkins) that parallelize the computation evenly over the number\nof classes. This allows us to compare these models to one-vs.-rest SVMs on\nunprecedented scale. The results indicate superior accuracy on text\nclassification data.\n","id":331}
{"Unnamed: 0.1":11332,"Unnamed: 0":11332.0,"anchor":"iCaRL: Incremental Classifier and Representation Learning","positive":"  A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.\n","negative":"  The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem.\n","id":332}
{"Unnamed: 0.1":11333,"Unnamed: 0":11333.0,"anchor":"Tunable Sensitivity to Large Errors in Neural Network Training","positive":"  When humans learn a new concept, they might ignore examples that they cannot\nmake sense of at first, and only later focus on such examples, when they are\nmore useful for learning. We propose incorporating this idea of tunable\nsensitivity for hard examples in neural network learning, using a new\ngeneralization of the cross-entropy gradient step, which can be used in place\nof the gradient in any gradient-based training method. The generalized gradient\nis parameterized by a value that controls the sensitivity of the training\nprocess to harder training examples. We tested our method on several benchmark\ndatasets. We propose, and corroborate in our experiments, that the optimal\nlevel of sensitivity to hard example is positively correlated with the depth of\nthe network. Moreover, the test prediction error obtained by our method is\ngenerally lower than that of the vanilla cross-entropy gradient learner. We\ntherefore conclude that tunable sensitivity can be helpful for neural network\nlearning.\n","negative":"  In this paper, a Wide Learning architecture is proposed that attempts to\nautomate the feature engineering portion of the machine learning (ML) pipeline.\nFeature engineering is widely considered as the most time consuming and expert\nknowledge demanding portion of any ML task. The proposed feature recommendation\napproach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016\ndataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure\nclassification dataset of photoplethysmogram (PPG) signals and c) an emotion\nclassification dataset of PPG signals. While the proposed method beats the\nstate of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the\naccuracy level of the winner of PhysioNet Challenge 2016. In all cases, the\neffort to reach a satisfactory performance was drastically less (a few days)\nthan manual feature engineering.\n","id":333}
{"Unnamed: 0.1":11334,"Unnamed: 0":11334.0,"anchor":"Adaptive Down-Sampling and Dimension Reduction in Time Elastic Kernel\n  Machines for Efficient Recognition of Isolated Gestures","positive":"  In the scope of gestural action recognition, the size of the feature vector\nrepresenting movements is in general quite large especially when full body\nmovements are considered. Furthermore, this feature vector evolves during the\nmovement performance so that a complete movement is fully represented by a\nmatrix M of size DxT , whose element M i, j represents the value of feature i\nat timestamps j. Many studies have addressed dimensionality reduction\nconsidering only the size of the feature vector lying in R D to reduce both the\nvariability of gestural sequences expressed in the reduced space, and the\ncomputational complexity of their processing. In return, very few of these\nmethods have explicitly addressed the dimensionality reduction along the time\naxis. Yet this is a major issue when considering the use of elastic distances\nwhich are characterized by a quadratic complexity along the time axis. We\npresent in this paper an evaluation of straightforward approaches aiming at\nreducing the dimensionality of the matrix M for each movement, leading to\nconsider both the dimensionality reduction of the feature vector as well as its\nreduction along the time axis. The dimensionality reduction of the feature\nvector is achieved by selecting remarkable joints in the skeleton performing\nthe movement, basically the extremities of the articulatory chains composing\nthe skeleton. The temporal dimen-sionality reduction is achieved using either a\nregular or adaptive down-sampling that seeks to minimize the reconstruction\nerror of the movements. Elastic and Euclidean kernels are then compared through\nsupport vector machine learning. Two data sets 1 that are widely referenced in\nthe domain of human gesture recognition, and quite distinctive in terms of\nquality of motion capture, are used for the experimental assessment of the\nproposed approaches. On these data sets we experimentally show that it is\nfeasible, and possibly desirable, to significantly reduce simultaneously the\nsize of the feature vector and the number of skeleton frames to represent body\nmovements while maintaining a very good recognition rate. The method proves to\ngive satisfactory results at a level currently reached by state-of-the-art\nmethods on these data sets. We experimentally show that the computational\ncomplexity reduction that is obtained makes this approach eligible for\nreal-time applications.\n","negative":"  Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers.\n","id":334}
{"Unnamed: 0.1":11335,"Unnamed: 0":11335.0,"anchor":"Infinite Variational Autoencoder for Semi-Supervised Learning","positive":"  This paper presents an infinite variational autoencoder (VAE) whose capacity\nadapts to suit the input data. This is achieved using a mixture model where the\nmixing coefficients are modeled by a Dirichlet process, allowing us to\nintegrate over the coefficients when performing inference. Critically, this\nthen allows us to automatically vary the number of autoencoders in the mixture\nbased on the data. Experiments show the flexibility of our method, particularly\nfor semi-supervised learning, where only a small number of training samples are\navailable.\n","negative":"  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n","id":335}
{"Unnamed: 0.1":11336,"Unnamed: 0":11336.0,"anchor":"Learning Generic Sentence Representations Using Convolutional Neural\n  Networks","positive":"  We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.\n","negative":"  We study Monte Carlo Tree Search to guide proof search in tableau calculi.\nThis includes proposing a number of proof-state evaluation heuristics, some of\nwhich are learnt from previous proofs. We present an implementation based on\nthe leanCoP prover. The system is trained and evaluated on a large suite of\nrelated problems coming from the Mizar proof assistant, showing that it is\ncapable to find new and different proofs.\n","id":336}
{"Unnamed: 0.1":11337,"Unnamed: 0":11337.0,"anchor":"Deep Restricted Boltzmann Networks","positive":"  Building a good generative model for image has long been an important topic\nin computer vision and machine learning. Restricted Boltzmann machine (RBM) is\none of such models that is simple but powerful. However, its restricted form\nalso has placed heavy constraints on the models representation power and\nscalability. Many extensions have been invented based on RBM in order to\nproduce deeper architectures with greater power. The most famous ones among\nthem are deep belief network, which stacks multiple layer-wise pretrained RBMs\nto form a hybrid model, and deep Boltzmann machine, which allows connections\nbetween hidden units to form a multi-layer structure. In this paper, we present\na new method to compose RBMs to form a multi-layer network style architecture\nand a training method that trains all layers jointly. We call the resulted\nstructure deep restricted Boltzmann network. We further explore the combination\nof convolutional RBM with the normal fully connected RBM, which is made trivial\nunder our composition framework. Experiments show that our model can generate\ndescent images and outperform the normal RBM significantly in terms of image\nquality and feature quality, without losing much efficiency for training.\n","negative":"  Nowadays Big Data are becoming more and more important. Many sectors of our\neconomy are now guided by data-driven decision processes. Big Data and business\nintelligence applications are facilitated by the MapReduce programming model\nwhile, at infrastructural layer, cloud computing provides flexible and cost\neffective solutions for allocating on demand large clusters. In such systems,\ncapacity allocation, which is the ability to optimally size minimal resources\nfor achieve a certain level of performance, is a key challenge to enhance\nperformance for MapReduce jobs and minimize cloud resource costs. In order to\ndo so, one of the biggest challenge is to build an accurate performance model\nto estimate job execution time of MapReduce systems. Previous works applied\nsimulation based models for modeling such systems. Although this approach can\naccurately describe the behavior of Big Data clusters, it is too\ncomputationally expensive and does not scale to large system. We try to\novercome these issues by applying machine learning techniques. More precisely\nwe focus on Support Vector Regression (SVR) which is intrinsically more robust\nw.r.t other techniques, like, e.g., neural networks, and less sensitive to\noutliers in the training set. To better investigate these benefits, we compare\nSVR to linear regression.\n","id":337}
{"Unnamed: 0.1":11338,"Unnamed: 0":11338.0,"anchor":"Semantic Compositional Networks for Visual Captioning","positive":"  A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics.\n","negative":"  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different\n\"contexts\". Bayesian optimization approaches to contextual policy search (CPS)\noffer data-efficient policy learning that generalize over a context space. We\npropose to improve data-efficiency by factoring typically considered contexts\ninto two components: target-type contexts that correspond to a desired outcome\nof the learned behavior, e.g. target position for throwing a ball; and\nenvironment type contexts that correspond to some state of the environment,\ne.g. initial ball position or wind speed. Our key observation is that\nexperience can be directly generalized over target-type contexts. Based on that\nwe introduce Factored Contextual Policy Search with Bayesian Optimization for\nboth passive and active learning settings. Preliminary results show faster\npolicy generalization on a simulated toy problem. A full paper extension is\navailable at arXiv:1904.11761\n","id":338}
{"Unnamed: 0.1":11339,"Unnamed: 0":11339.0,"anchor":"EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer\n  Interfaces","positive":"  Brain computer interfaces (BCI) enable direct communication with a computer,\nusing neural activity as the control signal. This neural signal is generally\nchosen from a variety of well-studied electroencephalogram (EEG) signals. For a\ngiven BCI paradigm, feature extractors and classifiers are tailored to the\ndistinct characteristics of its expected EEG control signal, limiting its\napplication to that specific signal. Convolutional Neural Networks (CNNs),\nwhich have been used in computer vision and speech recognition, have\nsuccessfully been applied to EEG-based BCIs; however, they have mainly been\napplied to single BCI paradigms and thus it remains unclear how these\narchitectures generalize to other paradigms. Here, we ask if we can design a\nsingle CNN architecture to accurately classify EEG signals from different BCI\nparadigms, while simultaneously being as compact as possible. In this work we\nintroduce EEGNet, a compact convolutional network for EEG-based BCIs. We\nintroduce the use of depthwise and separable convolutions to construct an\nEEG-specific model which encapsulates well-known EEG feature extraction\nconcepts for BCI. We compare EEGNet to current state-of-the-art approaches\nacross four BCI paradigms: P300 visual-evoked potentials, error-related\nnegativity responses (ERN), movement-related cortical potentials (MRCP), and\nsensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms\nbetter than the reference algorithms when only limited training data is\navailable. We demonstrate three different approaches to visualize the contents\nof a trained EEGNet model to enable interpretation of the learned features. Our\nresults suggest that EEGNet is robust enough to learn a wide variety of\ninterpretable features over a range of BCI tasks, suggesting that the observed\nperformances were not due to artifact or noise sources in the data.\n","negative":"  We introduce a new framework for training deep generative models for\nhigh-dimensional conditional density estimation. The Bottleneck Conditional\nDensity Estimator (BCDE) is a variant of the conditional variational\nautoencoder (CVAE) that employs layer(s) of stochastic variables as the\nbottleneck between the input $x$ and target $y$, where both are\nhigh-dimensional. Crucially, we propose a new hybrid training method that\nblends the conditional generative model with a joint generative model. Hybrid\nblending is the key to effective training of the BCDE, which avoids overfitting\nand provides a novel mechanism for leveraging unlabeled data. We show that our\nhybrid training procedure enables models to achieve competitive results in the\nMNIST quadrant prediction task in the fully-supervised setting, and sets new\nbenchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.\n","id":339}
{"Unnamed: 0.1":11340,"Unnamed: 0":11340.0,"anchor":"Scalable Bayesian Learning of Recurrent Neural Networks for Language\n  Modeling","positive":"  Recurrent neural networks (RNNs) have shown promising performance for\nlanguage modeling. However, traditional training of RNNs using back-propagation\nthrough time often suffers from overfitting. One reason for this is that\nstochastic optimization (used for large training sets) does not provide good\nestimates of model uncertainty. This paper leverages recent advances in\nstochastic gradient Markov Chain Monte Carlo (also appropriate for large\ntraining sets) to learn weight uncertainty in RNNs. It yields a principled\nBayesian learning algorithm, adding gradient noise during training (enhancing\nexploration of the model-parameter space) and model averaging when testing.\nExtensive experiments on various RNN models and across a broad range of\napplications demonstrate the superiority of the proposed approach over\nstochastic optimization.\n","negative":"  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks.\n","id":340}
{"Unnamed: 0.1":11341,"Unnamed: 0":11341.0,"anchor":"Multiscale Inverse Reinforcement Learning using Diffusion Wavelets","positive":"  This work presents a multiscale framework to solve an inverse reinforcement\nlearning (IRL) problem for continuous-time\/state stochastic systems. We take\nadvantage of a diffusion wavelet representation of the associated Markov chain\nto abstract the state space. This not only allows for effectively handling the\nlarge (and geometrically complex) decision space but also provides more\ninterpretable representations of the demonstrated state trajectories and also\nof the resulting policy of IRL. In the proposed framework, the problem is\ndivided into the global and local IRL, where the global approximation of the\noptimal value functions are obtained using coarse features and the local\ndetails are quantified using fine local features. An illustrative numerical\nexample on robot path control in a complex environment is presented to verify\nthe proposed method.\n","negative":"  We study the problem of learning influence functions under incomplete\nobservations of node activations. Incomplete observations are a major concern\nas most (online and real-world) social networks are not fully observable. We\nestablish both proper and improper PAC learnability of influence functions\nunder randomly missing observations. Proper PAC learnability under the\nDiscrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade\n(DIC) models is established by reducing incomplete observations to complete\nobservations in a modified graph. Our improper PAC learnability result applies\nfor the DLT and DIC models as well as the Continuous-Time Independent Cascade\n(CIC) model. It is based on a parametrization in terms of reachability\nfeatures, and also gives rise to an efficient and practical heuristic.\nExperiments on synthetic and real-world datasets demonstrate the ability of our\nmethod to compensate even for a fairly large fraction of missing observations.\n","id":341}
{"Unnamed: 0.1":11342,"Unnamed: 0":11342.0,"anchor":"Survey of Expressivity in Deep Neural Networks","positive":"  We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity.\n","negative":"  The advent of the attention mechanism in neural machine translation models\nhas improved the performance of machine translation systems by enabling\nselective lookup into the source sentence. In this paper, the efficiencies of\ntranslation using bidirectional encoder attention decoder models were studied\nwith respect to translation involving morphologically rich languages. The\nEnglish - Tamil language pair was selected for this analysis. First, the use of\nWord2Vec embedding for both the English and Tamil words improved the\ntranslation results by 0.73 BLEU points over the baseline RNNSearch model with\n4.84 BLEU score. The use of morphological segmentation before word\nvectorization to split the morphologically rich Tamil words into their\nrespective morphemes before the translation, caused a reduction in the target\nvocabulary size by a factor of 8. Also, this model (RNNMorph) improved the\nperformance of neural machine translation by 7.05 BLEU points over the\nRNNSearch model used over the same corpus. Since the BLEU evaluation of the\nRNNMorph model might be unreliable due to an increase in the number of matching\ntokens per sentence, the performances of the translations were also compared by\nmeans of human evaluation metrics of adequacy, fluency and relative ranking.\nFurther, the use of morphological segmentation also improved the efficacy of\nthe attention mechanism.\n","id":342}
{"Unnamed: 0.1":11343,"Unnamed: 0":11343.0,"anchor":"Dynamic Key-Value Memory Networks for Knowledge Tracing","positive":"  Knowledge Tracing (KT) is a task of tracing evolving knowledge state of\nstudents with respect to one or more concepts as they engage in a sequence of\nlearning activities. One important purpose of KT is to personalize the practice\nsequence to help students learn knowledge concepts efficiently. However,\nexisting methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing\neither model knowledge state for each predefined concept separately or fail to\npinpoint exactly which concepts a student is good at or unfamiliar with. To\nsolve these problems, this work introduces a new model called Dynamic Key-Value\nMemory Networks (DKVMN) that can exploit the relationships between underlying\nconcepts and directly output a student's mastery level of each concept. Unlike\nstandard memory-augmented neural networks that facilitate a single memory\nmatrix or two static memory matrices, our model has one static matrix called\nkey, which stores the knowledge concepts and the other dynamic matrix called\nvalue, which stores and updates the mastery levels of corresponding concepts.\nExperiments show that our model consistently outperforms the state-of-the-art\nmodel in a range of KT datasets. Moreover, the DKVMN model can automatically\ndiscover underlying concepts of exercises typically performed by human\nannotations and depict the changing knowledge state of a student.\n","negative":"  We propose a scheme for training a computerized agent to perform complex\nhuman tasks such as highway steering. The scheme is designed to follow a\nnatural learning process whereby a human instructor teaches a computerized\ntrainee. The learning process consists of five elements: (i) unsupervised\nfeature learning; (ii) supervised imitation learning; (iii) supervised reward\ninduction; (iv) supervised safety module construction; and (v) reinforcement\nlearning. We implemented the last four elements of the scheme using deep\nconvolutional networks and applied it to successfully create a computerized\nagent capable of autonomous highway steering over the well-known racing game\nAssetto Corsa. We demonstrate that the use of the last four elements is\nessential to effectively carry out the steering task using vision alone,\nwithout access to a driving simulator internals, and operating in wall-clock\ntime. This is made possible also through the introduction of a safety network,\na novel way for preventing the agent from performing catastrophic mistakes\nduring the reinforcement learning stage.\n","id":343}
{"Unnamed: 0.1":11344,"Unnamed: 0":11344.0,"anchor":"Interpreting the Predictions of Complex ML Models by Layer-wise\n  Relevance Propagation","positive":"  Complex nonlinear models such as deep neural network (DNNs) have become an\nimportant tool for image classification, speech recognition, natural language\nprocessing, and many other fields of application. These models however lack\ntransparency due to their complex nonlinear structure and to the complex data\ndistributions to which they typically apply. As a result, it is difficult to\nfully characterize what makes these models reach a particular decision for a\ngiven input. This lack of transparency can be a drawback, especially in the\ncontext of sensitive applications such as medical analysis or security. In this\nshort paper, we summarize a recent technique introduced by Bach et al. [1] that\nexplains predictions by decomposing the classification decision of DNN models\nin terms of input variables.\n","negative":"  For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).\n","id":344}
{"Unnamed: 0.1":11345,"Unnamed: 0":11345.0,"anchor":"Fast Orthonormal Sparsifying Transforms Based on Householder Reflectors","positive":"  Dictionary learning is the task of determining a data-dependent transform\nthat yields a sparse representation of some observed data. The dictionary\nlearning problem is non-convex, and usually solved via computationally complex\niterative algorithms. Furthermore, the resulting transforms obtained generally\nlack structure that permits their fast application to data. To address this\nissue, this paper develops a framework for learning orthonormal dictionaries\nwhich are built from products of a few Householder reflectors. Two algorithms\nare proposed to learn the reflector coefficients: one that considers a\nsequential update of the reflectors and one with a simultaneous update of all\nreflectors that imposes an additional internal orthogonal constraint. The\nproposed methods have low computational complexity and are shown to converge to\nlocal minimum points which can be described in terms of the spectral properties\nof the matrices involved. The resulting dictionaries balance between the\ncomputational complexity and the quality of the sparse representations by\ncontrolling the number of Householder reflectors in their product. Simulations\nof the proposed algorithms are shown in the image processing setting where\nwell-known fast transforms are available for comparisons. The proposed\nalgorithms have favorable reconstruction error and the advantage of a fast\nimplementation relative to the classical, unstructured, dictionaries.\n","negative":"  Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer.\n","id":345}
{"Unnamed: 0.1":11346,"Unnamed: 0":11346.0,"anchor":"Learning Fast Sparsifying Transforms","positive":"  Given a dataset, the task of learning a transform that allows sparse\nrepresentations of the data bears the name of dictionary learning. In many\napplications, these learned dictionaries represent the data much better than\nthe static well-known transforms (Fourier, Hadamard etc.). The main downside of\nlearned transforms is that they lack structure and therefore they are not\ncomputationally efficient, unlike their classical counterparts. These posse\nseveral difficulties especially when using power limited hardware such as\nmobile devices, therefore discouraging the application of sparsity techniques\nin such scenarios. In this paper we construct orthogonal and non-orthogonal\ndictionaries that are factorized as a product of a few basic transformations.\nIn the orthogonal case, we solve exactly the dictionary update problem for one\nbasic transformation, which can be viewed as a generalized Givens rotation, and\nthen propose to construct orthogonal dictionaries that are a product of these\ntransformations, guaranteeing their fast manipulation. We also propose a method\nto construct fast square but non-orthogonal dictionaries that are factorized as\na product of few transforms that can be viewed as a further generalization of\nGivens rotations to the non-orthogonal setting. We show how the proposed\ntransforms can balance very well data representation performance and\ncomputational complexity. We also compare with classical fast and learned\ngeneral and orthogonal transforms.\n","negative":"  Representation learning seeks to expose certain aspects of observed data in a\nlearned representation that's amenable to downstream tasks like classification.\nFor instance, a good representation for 2D images might be one that describes\nonly global structure and discards information about detailed texture. In this\npaper, we present a simple but principled method to learn such global\nrepresentations by combining Variational Autoencoder (VAE) with neural\nautoregressive models such as RNN, MADE and PixelRNN\/CNN. Our proposed VAE\nmodel allows us to have control over what the global latent code can learn and\n, by designing the architecture accordingly, we can force the global latent\ncode to discard irrelevant information such as texture in 2D images, and hence\nthe VAE only \"autoencodes\" data in a lossy fashion. In addition, by leveraging\nautoregressive models as both prior distribution $p(z)$ and decoding\ndistribution $p(x|z)$, we can greatly improve generative modeling performance\nof VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and\nCaltech-101 Silhouettes density estimation tasks.\n","id":346}
{"Unnamed: 0.1":11347,"Unnamed: 0":11347.0,"anchor":"Identifying Significant Predictive Bias in Classifiers","positive":"  We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data.\n","negative":"  Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.\n","id":347}
{"Unnamed: 0.1":11348,"Unnamed: 0":11348.0,"anchor":"On Human Intellect and Machine Failures: Troubleshooting Integrative\n  Machine Learning Systems","positive":"  We study the problem of troubleshooting machine learning systems that rely on\nanalytical pipelines of distinct components. Understanding and fixing errors\nthat arise in such integrative systems is difficult as failures can occur at\nmultiple points in the execution workflow. Moreover, errors can propagate,\nbecome amplified or be suppressed, making blame assignment difficult. We\npropose a human-in-the-loop methodology which leverages human intellect for\ntroubleshooting system failures. The approach simulates potential component\nfixes through human computation tasks and measures the expected improvements in\nthe holistic behavior of the system. The method provides guidance to designers\nabout how they can best improve the system. We demonstrate the effectiveness of\nthe approach on an automated image captioning system that has been pressed into\nreal-world use.\n","negative":"  Data science models, although successful in a number of commercial domains,\nhave had limited applicability in scientific problems involving complex\nphysical phenomena. Theory-guided data science (TGDS) is an emerging paradigm\nthat aims to leverage the wealth of scientific knowledge for improving the\neffectiveness of data science models in enabling scientific discovery. The\noverarching vision of TGDS is to introduce scientific consistency as an\nessential component for learning generalizable models. Further, by producing\nscientifically interpretable models, TGDS aims to advance our scientific\nunderstanding by discovering novel domain insights. Indeed, the paradigm of\nTGDS has started to gain prominence in a number of scientific disciplines such\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\nscience, bio-marker discovery, climate science, and hydrology. In this paper,\nwe formally conceptualize the paradigm of TGDS and present a taxonomy of\nresearch themes in TGDS. We describe several approaches for integrating domain\nknowledge in different research themes using illustrative examples from\ndifferent disciplines. We also highlight some of the promising avenues of novel\nresearch for realizing the full potential of theory-guided data science.\n","id":348}
{"Unnamed: 0.1":11349,"Unnamed: 0":11349.0,"anchor":"Training and Evaluating Multimodal Word Embeddings with Large-scale Web\n  Annotated Images","positive":"  In this paper, we focus on training and evaluating effective word embeddings\nwith both text and visual information. More specifically, we introduce a\nlarge-scale dataset with 300 million sentences describing over 40 million\nimages crawled and downloaded from publicly available Pins (i.e. an image with\nsentence descriptions uploaded by users) on Pinterest. This dataset is more\nthan 200 times larger than MS COCO, the standard large-scale image dataset with\nsentence descriptions. In addition, we construct an evaluation dataset to\ndirectly assess the effectiveness of word embeddings in terms of finding\nsemantically similar or related words and phrases. The word\/phrase pairs in\nthis evaluation dataset are collected from the click data with millions of\nusers in an image search system, thus contain rich semantic relationships.\nBased on these datasets, we propose and compare several Recurrent Neural\nNetworks (RNNs) based multimodal (text and image) models. Experiments show that\nour model benefits from incorporating the visual information into the word\nembeddings, and a weight sharing strategy is crucial for learning such\nmultimodal embeddings. The project page is:\nhttp:\/\/www.stat.ucla.edu\/~junhua.mao\/multimodal_embedding.html\n","negative":"  A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1\/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a\/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability.\n","id":349}
{"Unnamed: 0.1":11350,"Unnamed: 0":11350.0,"anchor":"An Overview on Data Representation Learning: From Traditional Feature\n  Learning to Recent Deep Learning","positive":"  Since about 100 years ago, to learn the intrinsic structure of data, many\nrepresentation learning approaches have been proposed, including both linear\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\ndeep architectures are widely applied for representation learning in recent\nyears, and have delivered top results in many tasks, such as image\nclassification, object detection and speech recognition. In this paper, we\nreview the development of data representation learning methods. Specifically,\nwe investigate both traditional feature learning algorithms and\nstate-of-the-art deep learning models. The history of data representation\nlearning is introduced, while available resources (e.g. online course, tutorial\nand book information) and toolboxes are provided. Finally, we conclude this\npaper with remarks and some interesting research directions on data\nrepresentation learning.\n","negative":"  In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory.\n","id":350}
{"Unnamed: 0.1":11351,"Unnamed: 0":11351.0,"anchor":"Local Discriminant Hyperalignment for multi-subject fMRI data alignment","positive":"  Multivariate Pattern (MVP) classification can map different cognitive states\nto the brain tasks. One of the main challenges in MVP analysis is validating\nthe generated results across subjects. However, analyzing multi-subject fMRI\ndata requires accurate functional alignments between neuronal activities of\ndifferent subjects, which can rapidly increase the performance and robustness\nof the final results. Hyperalignment (HA) is one of the most effective\nfunctional alignment methods, which can be mathematically formulated by the\nCanonical Correlation Analysis (CCA) methods. Since HA mostly uses the\nunsupervised CCA techniques, its solution may not be optimized for MVP\nanalysis. By incorporating the idea of Local Discriminant Analysis (LDA) into\nCCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel\nsupervised HA method, which can provide better functional alignment for MVP\nanalysis. Indeed, the locality is defined based on the stimuli categories in\nthe train-set, where the correlation between all stimuli in the same category\nwill be maximized and the correlation between distinct categories of stimuli\napproaches to near zero. Experimental studies on multi-subject MVP analysis\nconfirm that the LDHA method achieves superior performance to other\nstate-of-the-art HA algorithms.\n","negative":"  We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and\/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.\n","id":351}
{"Unnamed: 0.1":11352,"Unnamed: 0":11352.0,"anchor":"A Unified Convex Surrogate for the Schatten-$p$ Norm","positive":"  The Schatten-$p$ norm ($0<p<1$) has been widely used to replace the nuclear\nnorm for better approximating the rank function. However, existing methods are\neither 1) not scalable for large scale problems due to relying on singular\nvalue decomposition (SVD) in every iteration, or 2) specific to some $p$\nvalues, e.g., $1\/2$, and $2\/3$. In this paper, we show that for any $p$, $p_1$,\nand $p_2 >0$ satisfying $1\/p=1\/p_1+1\/p_2$, there is an equivalence between the\nSchatten-$p$ norm of one matrix and the Schatten-$p_1$ and the Schatten-$p_2$\nnorms of its two factor matrices. We further extend the equivalence to multiple\nfactor matrices and show that all the factor norms can be convex and smooth for\nany $p>0$. In contrast, the original Schatten-$p$ norm for $0<p<1$ is\nnon-convex and non-smooth. As an example we conduct experiments on matrix\ncompletion. To utilize the convexity of the factor matrix norms, we adopt the\naccelerated proximal alternating linearized minimization algorithm and\nestablish its sequence convergence. Experiments on both synthetic and real\ndatasets exhibit its superior performance over the state-of-the-art methods.\nIts speed is also highly competitive.\n","negative":"  The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods.\n","id":352}
{"Unnamed: 0.1":11353,"Unnamed: 0":11353.0,"anchor":"Bidirectional LSTM-CRF for Clinical Concept Extraction","positive":"  Automated extraction of concepts from patient clinical records is an\nessential facilitator of clinical research. For this reason, the 2010 i2b2\/VA\nNatural Language Processing Challenges for Clinical Records introduced a\nconcept extraction task aimed at identifying and classifying concepts into\npredefined categories (i.e., treatments, tests and problems). State-of-the-art\nconcept extraction approaches heavily rely on handcrafted features and\ndomain-specific resources which are hard to collect and define. For this\nreason, this paper proposes an alternative, streamlined approach: a recurrent\nneural network (the bidirectional LSTM with CRF decoding) initialized with\ngeneral-purpose, off-the-shelf word embeddings. The experimental results\nachieved on the 2010 i2b2\/VA reference corpora using the proposed framework\noutperform all recent methods and ranks closely to the best submission from the\noriginal 2010 i2b2\/VA challenge.\n","negative":"  Composition and parameterization of multicomponent predictive systems (MCPSs)\nconsisting of chains of data transformation steps are a challenging task.\nAuto-WEKA is a tool to automate the combined algorithm selection and\nhyperparameter (CASH) optimization problem. In this paper, we extend the CASH\nproblem and Auto-WEKA to support the MCPS, including preprocessing steps for\nboth classification and regression tasks. We define the optimization problem in\nwhich the search space consists of suitably parameterized Petri nets forming\nthe sought MCPS solutions. In the experimental analysis, we focus on examining\nthe impact of considerably extending the search space (from approximately\n22,000 to 812 billion possible combinations of methods and categorical\nhyperparameters). In a range of extensive experiments, three different\noptimization strategies are used to automatically compose MCPSs for 21 publicly\navailable data sets. The diversity of the composed MCPSs found is an indication\nthat fully and automatically exploiting different combinations of data cleaning\nand preprocessing techniques is possible and highly beneficial for different\npredictive models. We also present the results on seven data sets from real\nchemical production processes. Our findings can have a major impact on the\ndevelopment of high-quality predictive models as well as their maintenance and\nscalability aspects needed in modern applications and deployment scenarios.\n","id":353}
{"Unnamed: 0.1":11354,"Unnamed: 0":11354.0,"anchor":"Distributed Optimization of Multi-Class SVMs","positive":"  Training of one-vs.-rest SVMs can be parallelized over the number of classes\nin a straight forward way. Given enough computational resources, one-vs.-rest\nSVMs can thus be trained on data involving a large number of classes. The same\ncannot be stated, however, for the so-called all-in-one SVMs, which require\nsolving a quadratic program of size quadratically in the number of classes. We\ndevelop distributed algorithms for two all-in-one SVM formulations (Lee et al.\nand Weston and Watkins) that parallelize the computation evenly over the number\nof classes. This allows us to compare these models to one-vs.-rest SVMs on\nunprecedented scale. The results indicate superior accuracy on text\nclassification data.\n","negative":"  Modern technologies are producing datasets with complex intrinsic structures,\nand they can be naturally represented as matrices instead of vectors. To\npreserve the latent data structures during processing, modern regression\napproaches incorporate the low-rank property to the model and achieve\nsatisfactory performance for certain applications. These approaches all assume\nthat both predictors and labels for each pair of data within the training set\nare accurate. However, in real-world applications, it is common to see the\ntraining data contaminated by noises, which can affect the robustness of these\nmatrix regression methods. In this paper, we address this issue by introducing\na novel robust matrix regression method. We also derive efficient proximal\nalgorithms for model training. To evaluate the performance of our methods, we\napply it to real world applications with comparative studies. Our method\nachieves the state-of-the-art performance, which shows the effectiveness and\nthe practical value of our method.\n","id":354}
{"Unnamed: 0.1":11355,"Unnamed: 0":11355.0,"anchor":"On the Exponentially Weighted Aggregate with the Laplace Prior","positive":"  In this paper, we study the statistical behaviour of the Exponentially\nWeighted Aggregate (EWA) in the problem of high-dimensional regression with\nfixed design. Under the assumption that the underlying regression vector is\nsparse, it is reasonable to use the Laplace distribution as a prior. The\nresulting estimator and, specifically, a particular instance of it referred to\nas the Bayesian lasso, was already used in the statistical literature because\nof its computational convenience, even though no thorough mathematical analysis\nof its statistical properties was carried out. The present work fills this gap\nby establishing sharp oracle inequalities for the EWA with the Laplace prior.\nThese inequalities show that if the temperature parameter is small, the EWA\nwith the Laplace prior satisfies the same type of oracle inequality as the\nlasso estimator does, as long as the quality of estimation is measured by the\nprediction loss. Extensions of the proposed methodology to the problem of\nprediction with low-rank matrices are considered.\n","negative":"  Direct contextual policy search methods learn to improve policy parameters\nand simultaneously generalize these parameters to different context or task\nvariables. However, learning from high-dimensional context variables, such as\ncamera images, is still a prominent problem in many real-world tasks. A naive\napplication of unsupervised dimensionality reduction methods to the context\nvariables, such as principal component analysis, is insufficient as\ntask-relevant input may be ignored. In this paper, we propose a contextual\npolicy search method in the model-based relative entropy stochastic search\nframework with integrated dimensionality reduction. We learn a model of the\nreward that is locally quadratic in both the policy parameters and the context\nvariables. Furthermore, we perform supervised linear dimensionality reduction\non the context variables by nuclear norm regularization. The experimental\nresults show that the proposed method outperforms naive dimensionality\nreduction via principal component analysis and a state-of-the-art contextual\npolicy search method.\n","id":355}
{"Unnamed: 0.1":11356,"Unnamed: 0":11356.0,"anchor":"Bottleneck Conditional Density Estimation","positive":"  We introduce a new framework for training deep generative models for\nhigh-dimensional conditional density estimation. The Bottleneck Conditional\nDensity Estimator (BCDE) is a variant of the conditional variational\nautoencoder (CVAE) that employs layer(s) of stochastic variables as the\nbottleneck between the input $x$ and target $y$, where both are\nhigh-dimensional. Crucially, we propose a new hybrid training method that\nblends the conditional generative model with a joint generative model. Hybrid\nblending is the key to effective training of the BCDE, which avoids overfitting\nand provides a novel mechanism for leveraging unlabeled data. We show that our\nhybrid training procedure enables models to achieve competitive results in the\nMNIST quadrant prediction task in the fully-supervised setting, and sets new\nbenchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.\n","negative":"  Deep learning techniques lie at the heart of several significant AI advances\nin recent years including object recognition and detection, image captioning,\nmachine translation, speech recognition and synthesis, and playing the game of\nGo. Automated first-order theorem provers can aid in the formalization and\nverification of mathematical theorems and play a crucial role in program\nanalysis, theory reasoning, security, interpolation, and system verification.\nHere we suggest deep learning based guidance in the proof search of the theorem\nprover E. We train and compare several deep neural network models on the traces\nof existing ATP proofs of Mizar statements and use them to select processed\nclauses during proof search. We give experimental evidence that with a hybrid,\ntwo-phase approach, deep learning based guidance can significantly reduce the\naverage number of proof search steps while increasing the number of theorems\nproved. Using a few proof guidance strategies that leverage deep neural\nnetworks, we have found first-order proofs of 7.36% of the first-order logic\ntranslations of the Mizar Mathematical Library theorems that did not previously\nhave ATP generated proofs. This increases the ratio of statements in the corpus\nwith ATP generated proofs from 56% to 59%.\n","id":356}
{"Unnamed: 0.1":11357,"Unnamed: 0":11357.0,"anchor":"A Benchmark and Comparison of Active Learning for Logistic Regression","positive":"  Logistic regression is by far the most widely used classifier in real-world\napplications. In this paper, we benchmark the state-of-the-art active learning\nmethods for logistic regression and discuss and illustrate their underlying\ncharacteristics. Experiments are carried out on three synthetic datasets and 44\nreal-world datasets, providing insight into the behaviors of these active\nlearning methods with respect to the area of the learning curve (which plots\nclassification accuracy as a function of the number of queried examples) and\ntheir computational costs. Surprisingly, one of the earliest and simplest\nsuggested active learning methods, i.e., uncertainty sampling, performs\nexceptionally well overall. Another remarkable finding is that random sampling,\nwhich is the rudimentary baseline to improve upon, is not overwhelmed by\nindividual active learning techniques in many cases.\n","negative":"  This paper presents a systematic evaluation of Neural Network (NN) for\nclassification of real-world data. In the field of machine learning, it is\noften seen that a single parameter that is 'predictive accuracy' is being used\nfor evaluating the performance of a classifier model. However, this parameter\nmight not be considered reliable given a dataset with very high level of\nskewness. To demonstrate such behavior, seven different types of datasets have\nbeen used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different\nparameters which include micro- and macro-level estimation. In the present\nstudy, the most common problem of prediction called 'multiclass' classification\nhas been considered. The results that are obtained for different parameters for\neach of the dataset could demonstrate interesting findings to support the\nusability of these set of performance evaluation parameters.\n","id":357}
{"Unnamed: 0.1":11358,"Unnamed: 0":11358.0,"anchor":"Patient-Driven Privacy Control through Generalized Distillation","positive":"  The introduction of data analytics into medicine has changed the nature of\npatient treatment. In this, patients are asked to disclose personal information\nsuch as genetic markers, lifestyle habits, and clinical history. This data is\nthen used by statistical models to predict personalized treatments. However,\ndue to privacy concerns, patients often desire to withhold sensitive\ninformation. This self-censorship can impede proper diagnosis and treatment,\nwhich may lead to serious health complications and even death over time. In\nthis paper, we present privacy distillation, a mechanism which allows patients\nto control the type and amount of information they wish to disclose to the\nhealthcare providers for use in statistical models. Meanwhile, it retains the\naccuracy of models that have access to all patient data under a sufficient but\nnot full set of privacy-relevant information. We validate privacy distillation\nusing a corpus of patients prescribed to warfarin for a personalized dosage. We\nuse a deep neural network to implement privacy distillation for training and\nmaking dose predictions. We find that privacy distillation with sufficient\nprivacy-relevant information i) retains accuracy almost as good as having all\npatient data (only 3\\% worse), and ii) is effective at preventing errors that\nintroduce health-related risks (only 3.9\\% worse under- or over-prescriptions).\n","negative":"  This paper presents a novel multitask multiple kernel learning framework that\nefficiently learns the kernel weights leveraging the relationship across\nmultiple tasks. The idea is to automatically infer this task relationship in\nthe \\textit{RKHS} space corresponding to the given base kernels. The problem is\nformulated as a regularization-based approach called \\textit{Multi-Task\nMultiple Kernel Relationship Learning} (\\textit{MK-MTRL}), which models the\ntask relationship matrix from the weights learned from latent feature spaces of\ntask-specific base kernels. Unlike in previous work, the proposed formulation\nallows one to incorporate prior knowledge for simultaneously learning several\nrelated tasks. We propose an alternating minimization algorithm to learn the\nmodel parameters, kernel weights and task relationship matrix. In order to\ntackle large-scale problems, we further propose a two-stage \\textit{MK-MTRL}\nonline learning algorithm and show that it significantly reduces the\ncomputational time, and also achieves performance comparable to that of the\njoint learning framework. Experimental results on benchmark datasets show that\nthe proposed formulations outperform several state-of-the-art multitask\nlearning methods.\n","id":358}
{"Unnamed: 0.1":11359,"Unnamed: 0":11359.0,"anchor":"A Deep Neural Network to identify foreshocks in real time","positive":"  Foreshock events provide valuable insight to predict imminent major\nearthquakes. However, it is difficult to identify them in real time. In this\npaper, I propose an algorithm based on deep learning to instantaneously\nclassify a seismic waveform as a foreshock, mainshock or an aftershock event\nachieving a high accuracy of 99% in classification. As a result, this is by far\nthe most reliable method to predict major earthquakes that are preceded by\nforeshocks. In addition, I discuss methods to create an earthquake dataset that\nis compatible with deep networks.\n","negative":"  Most existing word embedding approaches do not distinguish the same words in\ndifferent contexts, therefore ignoring their contextual meanings. As a result,\nthe learned embeddings of these words are usually a mixture of multiple\nmeanings. In this paper, we acknowledge multiple identities of the same word in\ndifferent contexts and learn the \\textbf{identity-sensitive} word embeddings.\nBased on an identity-labeled text corpora, a heterogeneous network of words and\nword identities is constructed to model different-levels of word\nco-occurrences. The heterogeneous network is further embedded into a\nlow-dimensional space through a principled network embedding approach, through\nwhich we are able to obtain the embeddings of words and the embeddings of word\nidentities. We study three different types of word identities including topics,\nsentiments and categories. Experimental results on real-world data sets show\nthat the identity-sensitive word embeddings learned by our approach indeed\ncapture different meanings of words and outperforms competitive methods on\ntasks including text classification and word similarity computation.\n","id":359}
{"Unnamed: 0.1":11360,"Unnamed: 0":11360.0,"anchor":"Training an Interactive Humanoid Robot Using Multimodal Deep\n  Reinforcement Learning","positive":"  Training robots to perceive, act and communicate using multiple modalities\nstill represents a challenging problem, particularly if robots are expected to\nlearn efficiently from small sets of example interactions. We describe a\nlearning approach as a step in this direction, where we teach a humanoid robot\nhow to play the game of noughts and crosses. Given that multiple multimodal\nskills can be trained to play this game, we focus our attention to training the\nrobot to perceive the game, and to interact in this game. Our multimodal deep\nreinforcement learning agent perceives multimodal features and exhibits verbal\nand non-verbal actions while playing. Experimental results using simulations\nshow that the robot can learn to win or draw up to 98% of the games. A pilot\ntest of the proposed multimodal system for the targeted game---integrating\nspeech, vision and gestures---reports that reasonable and fluent interactions\ncan be achieved using the proposed approach.\n","negative":"  Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression.\n","id":360}
{"Unnamed: 0.1":11361,"Unnamed: 0":11361.0,"anchor":"Visual Dialog","positive":"  We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps:\/\/visualdialog.org\n","negative":"  Transcriptional profiling on microarrays to obtain gene expressions has been\nused to facilitate cancer diagnosis. We propose a deep generative machine\nlearning architecture (called DeepCancer) that learn features from unlabeled\nmicroarray data. These models have been used in conjunction with conventional\nclassifiers that perform classification of the tissue samples as either being\ncancerous or non-cancerous. The proposed model has been tested on two different\nclinical datasets. The evaluation demonstrates that DeepCancer model achieves a\nvery high precision score, while significantly controlling the false positive\nand false negative scores.\n","id":361}
{"Unnamed: 0.1":11362,"Unnamed: 0":11362.0,"anchor":"Deep Reinforcement Learning for Multi-Domain Dialogue Systems","positive":"  Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.\n","negative":"  We propose a new framework for manifold denoising based on processing in the\ngraph Fourier frequency domain, derived from the spectral decomposition of the\ndiscrete graph Laplacian. Our approach uses the Spectral Graph Wavelet\ntransform in order to per- form non-iterative denoising directly in the graph\nfrequency domain, an approach inspired by conventional wavelet-based signal\ndenoising methods. We theoretically justify our approach, based on the fact\nthat for smooth manifolds the coordinate information energy is localized in the\nlow spectral graph wavelet sub-bands, while the noise affects all frequency\nbands in a similar way. Experimental results show that our proposed manifold\nfrequency denoising (MFD) approach significantly outperforms the state of the\nart denoising meth- ods, and is robust to a wide range of parameter selections,\ne.g., the choice of k nearest neighbor connectivity of the graph.\n","id":362}
{"Unnamed: 0.1":11363,"Unnamed: 0":11363.0,"anchor":"Machine Learning on Human Connectome Data from MRI","positive":"  Functional MRI (fMRI) and diffusion MRI (dMRI) are non-invasive imaging\nmodalities that allow in-vivo analysis of a patient's brain network (known as a\nconnectome). Use of these technologies has enabled faster and better diagnoses\nand treatments of neurological disorders and a deeper understanding of the\nhuman brain. Recently, researchers have been exploring the application of\nmachine learning models to connectome data in order to predict clinical\noutcomes and analyze the importance of subnetworks in the brain. Connectome\ndata has unique properties, which present both special challenges and\nopportunities when used for machine learning. The purpose of this work is to\nreview the literature on the topic of applying machine learning models to\nMRI-based connectome data. This field is growing rapidly and now encompasses a\nlarge body of research. To summarize the research done to date, we provide a\ncomparative, structured summary of 77 relevant works, tabulated according to\ndifferent criteria, that represent the majority of the literature on this\ntopic. (We also published a living version of this table online at\nhttp:\/\/connectomelearning.cs.sfu.ca that the community can continue to\ncontribute to.) After giving an overview of how connectomes are constructed\nfrom dMRI and fMRI data, we discuss the variety of machine learning tasks that\nhave been explored with connectome data. We then compare the advantages and\ndrawbacks of different machine learning approaches that have been employed,\ndiscussing different feature selection and feature extraction schemes, as well\nas the learning models and regularization penalties themselves. Throughout this\ndiscussion, we focus particularly on how the methods are adapted to the unique\nnature of graphical connectome data. Finally, we conclude by summarizing the\ncurrent state of the art and by outlining what we believe are strategic\ndirections for future research.\n","negative":"  We propose a simple but strong baseline for time series classification from\nscratch with deep neural networks. Our proposed baseline models are pure\nend-to-end without any heavy preprocessing on the raw data or feature crafting.\nThe proposed Fully Convolutional Network (FCN) achieves premium performance to\nother state-of-the-art approaches and our exploration of the very deep neural\nnetworks with the ResNet structure is also competitive. The global average\npooling in our convolutional model enables the exploitation of the Class\nActivation Map (CAM) to find out the contributing region in the raw data for\nthe specific labels. Our models provides a simple choice for the real world\napplication and a good starting point for the future research. An overall\nanalysis is provided to discuss the generalization capability of our models,\nlearned features, network structures and the classification semantics.\n","id":363}
{"Unnamed: 0.1":11364,"Unnamed: 0":11364.0,"anchor":"BliStrTune: Hierarchical Invention of Theorem Proving Strategies","positive":"  Inventing targeted proof search strategies for specific problem sets is a\ndifficult task. State-of-the-art automated theorem provers (ATPs) such as E\nallow a large number of user-specified proof search strategies described in a\nrich domain specific language. Several machine learning methods that invent\nstrategies automatically for ATPs were proposed previously. One of them is the\nBlind Strategymaker (BliStr), a system for automated invention of ATP\nstrategies.\n  In this paper we introduce BliStrTune -- a hierarchical extension of BliStr.\nBliStrTune allows exploring much larger space of E strategies by interleaving\nsearch for high-level parameters with their fine-tuning. We use BliStrTune to\ninvent new strategies based also on new clause weight functions targeted at\nproblems from large ITP libraries. We show that the new strategies\nsignificantly improve E's performance in solving problems from the Mizar\nMathematical Library.\n","negative":"  In this paper, we improve the previously best known regret bound to achieve\n$\\epsilon$-differential privacy in oblivious adversarial bandits from\n$\\mathcal{O}{(T^{2\/3}\/\\epsilon)}$ to $\\mathcal{O}{(\\sqrt{T} \\ln T \/\\epsilon)}$.\nThis is achieved by combining a Laplace Mechanism with EXP3. We show that\nthough EXP3 is already differentially private, it leaks a linear amount of\ninformation in $T$. However, we can improve this privacy by relying on its\nintrinsic exponential mechanism for selecting actions. This allows us to reach\n$\\mathcal{O}{(\\sqrt{\\ln T})}$-DP, with a regret of $\\mathcal{O}{(T^{2\/3})}$\nthat holds against an adaptive adversary, an improvement from the best known of\n$\\mathcal{O}{(T^{3\/4})}$. This is done by using an algorithm that run EXP3 in a\nmini-batch loop. Finally, we run experiments that clearly demonstrate the\nvalidity of our theoretical analysis.\n","id":364}
{"Unnamed: 0.1":11365,"Unnamed: 0":11365.0,"anchor":"Structural Correspondence Learning for Cross-lingual Sentiment\n  Classification with One-to-many Mappings","positive":"  Structural correspondence learning (SCL) is an effective method for\ncross-lingual sentiment classification. This approach uses unlabeled documents\nalong with a word translation oracle to automatically induce task specific,\ncross-lingual correspondences. It transfers knowledge through identifying\nimportant features, i.e., pivot features. For simplicity, however, it assumes\nthat the word translation oracle maps each pivot feature in source language to\nexactly only one word in target language. This one-to-one mapping between words\nin different languages is too strict. Also the context is not considered at\nall. In this paper, we propose a cross-lingual SCL based on distributed\nrepresentation of words; it can learn meaningful one-to-many mappings for pivot\nwords using large amounts of monolingual data and a small dictionary. We\nconduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset,\nemploying English as source language, and Chinese as target language. Our\nmethod does not rely on the parallel corpora and the experimental results show\nthat our approach is more competitive than the state-of-the-art methods in\ncross-lingual sentiment classification.\n","negative":"  With the introduction of the Electric Health Records, large amounts of\ndigital data become available for analysis and decision support. When\nphysicians are prescribing treatments to a patient, they need to consider a\nlarge range of data variety and volume, making decisions increasingly complex.\nMachine learning based Clinical Decision Support systems can be a solution to\nthe data challenges. In this work we focus on a class of decision support in\nwhich the physicians' decision is directly predicted. Concretely, the model\nwould assign higher probabilities to decisions that it presumes the physician\nare more likely to make. Thus the CDS system can provide physicians with\nrational recommendations. We also address the problem of correlation in target\nfeatures: Often a physician is required to make multiple (sub-)decisions in a\nblock, and that these decisions are mutually dependent. We propose a solution\nto the target correlation problem using a tensor factorization model. In order\nto handle the patients' historical information as sequential data, we apply the\nso-called Encoder-Decoder-Framework which is based on Recurrent Neural Networks\n(RNN) as encoders and a tensor factorization model as a decoder, a combination\nwhich is novel in machine learning. With experiments with real-world datasets\nwe show that the proposed model does achieve better prediction performances.\n","id":365}
{"Unnamed: 0.1":11366,"Unnamed: 0":11366.0,"anchor":"What Can Be Predicted from Six Seconds of Driver Glances?","positive":"  We consider a large dataset of real-world, on-road driving from a 100-car\nnaturalistic study to explore the predictive power of driver glances and,\nspecifically, to answer the following question: what can be predicted about the\nstate of the driver and the state of the driving environment from a 6-second\nsequence of macro-glances? The context-based nature of such glances allows for\napplication of supervised learning to the problem of vision-based gaze\nestimation, making it robust, accurate, and reliable in messy, real-world\nconditions. So, it's valuable to ask whether such macro-glances can be used to\ninfer behavioral, environmental, and demographic variables? We analyze 27\nbinary classification problems based on these variables. The takeaway is that\nglance can be used as part of a multi-sensor real-time system to predict\nradio-tuning, fatigue state, failure to signal, talking, and several\nenvironment variables.\n","negative":"  Techniques known as Nonlinear Set Membership prediction, Lipschitz\nInterpolation or Kinky Inference are approaches to machine learning that\nutilise presupposed Lipschitz properties to compute inferences over unobserved\nfunction values. Provided a bound on the true best Lipschitz constant of the\ntarget function is known a priori they offer convergence guarantees as well as\nbounds around the predictions. Considering a more general setting that builds\non Hoelder continuity relative to pseudo-metrics, we propose an online method\nfor estimating the Hoelder constant online from function value observations\nthat possibly are corrupted by bounded observational errors. Utilising this to\ncompute adaptive parameters within a kinky inference rule gives rise to a\nnonparametric machine learning method, for which we establish strong universal\napproximation guarantees. That is, we show that our prediction rule can learn\nany continuous function in the limit of increasingly dense data to within a\nworst-case error bound that depends on the level of observational uncertainty.\nWe apply our method in the context of nonparametric model-reference adaptive\ncontrol (MRAC). Across a range of simulated aircraft roll-dynamics and\nperformance metrics our approach outperforms recently proposed alternatives\nthat were based on Gaussian processes and RBF-neural networks. For\ndiscrete-time systems, we provide guarantees on the tracking success of our\nlearning-based controllers both for the batch and the online learning setting.\n","id":366}
{"Unnamed: 0.1":11367,"Unnamed: 0":11367.0,"anchor":"Should I use TensorFlow","positive":"  Google's Machine Learning framework TensorFlow was open-sourced in November\n2015 [1] and has since built a growing community around it. TensorFlow is\nsupposed to be flexible for research purposes while also allowing its models to\nbe deployed productively. This work is aimed towards people with experience in\nMachine Learning considering whether they should use TensorFlow in their\nenvironment. Several aspects of the framework important for such a decision are\nexamined, such as the heterogenity, extensibility and its computation graph. A\npure Python implementation of linear classification is compared with an\nimplementation utilizing TensorFlow. I also contrast TensorFlow to other\npopular frameworks with respect to modeling capability, deployment and\nperformance and give a brief description of the current adaption of the\nframework.\n","negative":"  Feature selection is a process of choosing a subset of relevant features so\nthat the quality of prediction models can be improved. An extensive body of\nwork exists on information-theoretic feature selection, based on maximizing\nMutual Information (MI) between subsets of features and class labels. The prior\nmethods use a lower order approximation, by treating the joint entropy as a\nsummation of several single variable entropies. This leads to locally optimal\nselections and misses multi-way feature combinations. We present a higher order\nMI based approximation technique called Higher Order Feature Selection (HOFS).\nInstead of producing a single list of features, our method produces a ranked\ncollection of feature subsets that maximizes MI, giving better comprehension\n(feature ranking) as to which features work best together when selected, due to\ntheir underlying interdependent structure. Our experiments demonstrate that the\nproposed method performs better than existing feature selection approaches\nwhile keeping similar running times and computational complexity.\n","id":367}
{"Unnamed: 0.1":11368,"Unnamed: 0":11368.0,"anchor":"Deep attractor network for single-microphone speaker separation","positive":"  Despite the overwhelming success of deep learning in various speech\nprocessing tasks, the problem of separating simultaneous speakers in a mixture\nremains challenging. Two major difficulties in such systems are the arbitrary\nsource permutation and unknown number of sources in the mixture. We propose a\nnovel deep learning framework for single channel speech separation by creating\nattractor points in high dimensional embedding space of the acoustic signals\nwhich pull together the time-frequency bins corresponding to each source.\nAttractor points in this study are created by finding the centroids of the\nsources in the embedding space, which are subsequently used to determine the\nsimilarity of each bin in the mixture to each source. The network is then\ntrained to minimize the reconstruction error of each source by optimizing the\nembeddings. The proposed model is different from prior works in that it\nimplements an end-to-end training, and it does not depend on the number of\nsources in the mixture. Two strategies are explored in the test time, K-means\nand fixed attractor points, where the latter requires no post-processing and\ncan be implemented in real-time. We evaluated our system on Wall Street Journal\ndataset and show 5.49\\% improvement over the previous state-of-the-art methods.\n","negative":"  Decision trees have been a very popular class of predictive models for\ndecades due to their interpretability and good performance on categorical\nfeatures. However, they are not always robust and tend to overfit the data.\nAdditionally, if allowed to grow large, they lose interpretability. In this\npaper, we present a mixed integer programming formulation to construct optimal\ndecision trees of a prespecified size. We take the special structure of\ncategorical features into account and allow combinatorial decisions (based on\nsubsets of values of features) at each node. Our approach can also handle\nnumerical features via thresholding. We show that very good accuracy can be\nachieved with small trees using moderately-sized training sets. The\noptimization problems we solve are tractable with modern solvers.\n","id":368}
{"Unnamed: 0.1":11369,"Unnamed: 0":11369.0,"anchor":"Learning a Natural Language Interface with Neural Programmer","positive":"  Learning a natural language interface for database tables is a challenging\ntask that involves deep language understanding and multi-step reasoning. The\ntask is often approached by mapping natural language queries to logical forms\nor programs that provide the desired response when executed on the database. To\nour knowledge, this paper presents the first weakly supervised, end-to-end\nneural network model to induce such programs on a real-world dataset. We\nenhance the objective function of Neural Programmer, a neural network with\nbuilt-in discrete operations, and apply it on WikiTableQuestions, a natural\nlanguage question-answering dataset. The model is trained end-to-end with weak\nsupervision of question-answer pairs, and does not require domain-specific\ngrammars, rules, or annotations that are key elements in previous approaches to\nprogram induction. The main experimental result in this paper is that a single\nNeural Programmer model achieves 34.2% accuracy using only 10,000 examples with\nweak supervision. An ensemble of 15 models, with a trivial combination\ntechnique, achieves 37.7% accuracy, which is competitive to the current\nstate-of-the-art accuracy of 37.1% obtained by a traditional natural language\nsemantic parser.\n","negative":"  Inventing targeted proof search strategies for specific problem sets is a\ndifficult task. State-of-the-art automated theorem provers (ATPs) such as E\nallow a large number of user-specified proof search strategies described in a\nrich domain specific language. Several machine learning methods that invent\nstrategies automatically for ATPs were proposed previously. One of them is the\nBlind Strategymaker (BliStr), a system for automated invention of ATP\nstrategies.\n  In this paper we introduce BliStrTune -- a hierarchical extension of BliStr.\nBliStrTune allows exploring much larger space of E strategies by interleaving\nsearch for high-level parameters with their fine-tuning. We use BliStrTune to\ninvent new strategies based also on new clause weight functions targeted at\nproblems from large ITP libraries. We show that the new strategies\nsignificantly improve E's performance in solving problems from the Mizar\nMathematical Library.\n","id":369}
{"Unnamed: 0.1":11370,"Unnamed: 0":11370.0,"anchor":"DeepSetNet: Predicting Sets with Deep Neural Networks","positive":"  This paper addresses the task of set prediction using deep learning. This is\nimportant because the output of many computer vision tasks, including image\ntagging and object detection, are naturally expressed as sets of entities\nrather than vectors. As opposed to a vector, the size of a set is not fixed in\nadvance, and it is invariant to the ordering of entities within it. We define a\nlikelihood for a set distribution and learn its parameters using a deep neural\nnetwork. We also derive a loss for predicting a discrete distribution\ncorresponding to set cardinality. Set prediction is demonstrated on the problem\nof multi-class image classification. Moreover, we show that the proposed\ncardinality loss can also trivially be applied to the tasks of object counting\nand pedestrian detection. Our approach outperforms existing methods in all\nthree cases on standard datasets.\n","negative":"  Heretofore, automatic checkpointing at procedure-call boundaries, to reduce\nthe space complexity of reverse mode, has been provided by systems like\nTapenade. However, binomial checkpointing, or treeverse, has only been provided\nin Automatic Differentiation (AD) systems in special cases, e.g., through\nuser-provided pragmas on DO loops in Tapenade, or as the nested taping\nmechanism in adol-c for time integration processes, which requires that user\ncode be refactored. We present a framework for applying binomial checkpointing\nto arbitrary code with no special annotation or refactoring required. This is\naccomplished by applying binomial checkpointing directly to a program trace.\nThis trace is produced by a general-purpose checkpointing mechanism that is\northogonal to AD.\n","id":370}
{"Unnamed: 0.1":11371,"Unnamed: 0":11371.0,"anchor":"Image Based Appraisal of Real Estate Properties","positive":"  Real estate appraisal, which is the process of estimating the price for real\nestate properties, is crucial for both buys and sellers as the basis for\nnegotiation and transaction. Traditionally, the repeat sales model has been\nwidely adopted to estimate real estate price. However, it depends the design\nand calculation of a complex economic related index, which is challenging to\nestimate accurately. Today, real estate brokers provide easy access to detailed\nonline information on real estate properties to their clients. We are\ninterested in estimating the real estate price from these large amounts of\neasily accessed data. In particular, we analyze the prediction power of online\nhouse pictures, which is one of the key factors for online users to make a\npotential visiting decision. The development of robust computer vision\nalgorithms makes the analysis of visual content possible. In this work, we\nemploy a Recurrent Neural Network (RNN) to predict real estate price using the\nstate-of-the-art visual features. The experimental results indicate that our\nmodel outperforms several of other state-of-the-art baseline algorithms in\nterms of both mean absolute error (MAE) and mean absolute percentage error\n(MAPE).\n","negative":"  We introduce a new dynamical system for sequentially observed multivariate\ncount data. This model is based on the gamma--Poisson construction---a natural\nchoice for count data---and relies on a novel Bayesian nonparametric prior that\nties and shrinks the model parameters, thus avoiding overfitting. We present an\nefficient MCMC inference algorithm that advances recent work on augmentation\nschemes for inference in negative binomial models. Finally, we demonstrate the\nmodel's inductive bias using a variety of real-world data sets, showing that it\nexhibits superior predictive performance over other models and infers highly\ninterpretable latent structure.\n","id":371}
{"Unnamed: 0.1":11372,"Unnamed: 0":11372.0,"anchor":"Times series averaging and denoising from a probabilistic perspective on\n  time-elastic kernels","positive":"  In the light of regularized dynamic time warping kernels, this paper\nre-considers the concept of time elastic centroid for a setof time series. We\nderive a new algorithm based on a probabilistic interpretation of kernel\nalignment matrices. This algorithm expressesthe averaging process in terms of a\nstochastic alignment automata. It uses an iterative agglomerative heuristic\nmethod for averagingthe aligned samples, while also averaging the times of\noccurrence of the aligned samples. By comparing classification accuracies for45\nheterogeneous time series datasets obtained by first nearest centroid\/medoid\nclassifiers we show that: i) centroid-basedapproaches significantly outperform\nmedoid-based approaches, ii) for the considered datasets, our algorithm that\ncombines averagingin the sample space and along the time axes, emerges as the\nmost significantly robust model for time-elastic averaging with apromising\nnoise reduction capability. We also demonstrate its benefit in an isolated\ngesture recognition experiment and its ability tosignificantly reduce the size\nof training instance sets. Finally we highlight its denoising capability using\ndemonstrative synthetic data:we show that it is possible to retrieve, from few\nnoisy instances, a signal whose components are scattered in a wide spectral\nband.\n","negative":"  The problem of anomaly detection has been studied for a long time. In short,\nanomalies are abnormal or unlikely things. In financial networks, thieves and\nillegal activities are often anomalous in nature. Members of a network want to\ndetect anomalies as soon as possible to prevent them from harming the network's\ncommunity and integrity. Many Machine Learning techniques have been proposed to\ndeal with this problem; some results appear to be quite promising but there is\nno obvious superior method. In this paper, we consider anomaly detection\nparticular to the Bitcoin transaction network. Our goal is to detect which\nusers and transactions are the most suspicious; in this case, anomalous\nbehavior is a proxy for suspicious behavior. To this end, we use three\nunsupervised learning methods including k-means clustering, Mahalanobis\ndistance, and Unsupervised Support Vector Machine (SVM) on two graphs generated\nby the Bitcoin transaction network: one graph has users as nodes, and the other\nhas transactions as nodes.\n","id":372}
{"Unnamed: 0.1":11373,"Unnamed: 0":11373.0,"anchor":"AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech","positive":"  Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop.\n","negative":"  We propose a novel and efficient algorithm for the collaborative preference\ncompletion problem, which involves jointly estimating individualized rankings\nfor a set of entities over a shared set of items, based on a limited number of\nobserved affinity values. Our approach exploits the observation that while\npreferences are often recorded as numerical scores, the predictive quantity of\ninterest is the underlying rankings. Thus, attempts to closely match the\nrecorded scores may lead to overfitting and impair generalization performance.\nInstead, we propose an estimator that directly fits the underlying preference\norder, combined with nuclear norm constraints to encourage low--rank\nparameters. Besides (approximate) correctness of the ranking order, the\nproposed estimator makes no generative assumption on the numerical scores of\nthe observations. One consequence is that the proposed estimator can fit any\nconsistent partial ranking over a subset of the items represented as a directed\nacyclic graph (DAG), generalizing standard techniques that can only fit\npreference scores. Despite this generality, for supervision representing total\nor blockwise total orders, the computational complexity of our algorithm is\nwithin a $\\log$ factor of the standard algorithms for nuclear norm\nregularization based estimates for matrix completion. We further show promising\nempirical results for a novel and challenging application of collaboratively\nranking of the associations between brain--regions and cognitive neuroscience\nterms.\n","id":373}
{"Unnamed: 0.1":11374,"Unnamed: 0":11374.0,"anchor":"Robust Variational Inference","positive":"  Variational inference is a powerful tool for approximate inference. However,\nit mainly focuses on the evidence lower bound as variational objective and the\ndevelopment of other measures for variational inference is a promising area of\nresearch. This paper proposes a robust modification of evidence and a lower\nbound for the evidence, which is applicable when the majority of the training\nset samples are random noise objects. We provide experiments for variational\nautoencoders to show advantage of the objective over the evidence lower bound\non synthetic datasets obtained by adding uninformative noise objects to MNIST\nand OMNIGLOT. Additionally, for the original MNIST and OMNIGLOT datasets we\nobserve a small improvement over the non-robust evidence lower bound.\n","negative":"  Collaborative Filtering (CF) is widely used in large-scale recommendation\nengines because of its efficiency, accuracy and scalability. However, in\npractice, the fact that recommendation engines based on CF require interactions\nbetween users and items before making recommendations, make it inappropriate\nfor new items which haven't been exposed to the end users to interact with.\nThis is known as the cold-start problem. In this paper we introduce a novel\napproach which employs deep learning to tackle this problem in any CF based\nrecommendation engine. One of the most important features of the proposed\ntechnique is the fact that it can be applied on top of any existing CF based\nrecommendation engine without changing the CF core. We successfully applied\nthis technique to overcome the item cold-start problem in Careerbuilder's CF\nbased recommendation engine. Our experiments show that the proposed technique\nis very efficient to resolve the cold-start problem while maintaining high\naccuracy of the CF recommendations.\n","id":374}
{"Unnamed: 0.1":11375,"Unnamed: 0":11375.0,"anchor":"Efficient Convolutional Auto-Encoding via Random Convexification and\n  Frequency-Domain Minimization","positive":"  The omnipresence of deep learning architectures such as deep convolutional\nneural networks (CNN)s is fueled by the synergistic combination of\never-increasing labeled datasets and specialized hardware. Despite the\nindisputable success, the reliance on huge amounts of labeled data and\nspecialized hardware can be a limiting factor when approaching new\napplications. To help alleviating these limitations, we propose an efficient\nlearning strategy for layer-wise unsupervised training of deep CNNs on\nconventional hardware in acceptable time. Our proposed strategy consists of\nrandomly convexifying the reconstruction contractive auto-encoding (RCAE)\nlearning objective and solving the resulting large-scale convex minimization\nproblem in the frequency domain via coordinate descent (CD). The main\nadvantages of our proposed learning strategy are: (1) single tunable\noptimization parameter; (2) fast and guaranteed convergence; (3) possibilities\nfor full parallelization. Numerical experiments show that our proposed learning\nstrategy scales (in the worst case) linearly with image size, number of filters\nand filter size.\n","negative":"  Deep neural networks have been developed drawing inspiration from the brain\nvisual pathway, implementing an end-to-end approach: from image data to video\nobject classes. However building an fMRI decoder with the typical structure of\nConvolutional Neural Network (CNN), i.e. learning multiple level of\nrepresentations, seems impractical due to lack of brain data. As a possible\nsolution, this work presents the first hybrid fMRI and deep features decoding\napproach: collected fMRI and deep learnt representations of video object\nclasses are linked together by means of Kernel Canonical Correlation Analysis.\nIn decoding, this allows exploiting the discriminatory power of CNN by relating\nthe fMRI representation to the last layer of CNN (fc7). We show the\neffectiveness of embedding fMRI data onto a subspace related to deep features\nin distinguishing semantic visual categories based solely on brain imaging\ndata.\n","id":375}
{"Unnamed: 0.1":11376,"Unnamed: 0":11376.0,"anchor":"Dense Prediction on Sequences with Time-Dilated Convolutions for Speech\n  Recognition","positive":"  In computer vision pixelwise dense prediction is the task of predicting a\nlabel for each pixel in the image. Convolutional neural networks achieve good\nperformance on this task, while being computationally efficient. In this paper\nwe carry these ideas over to the problem of assigning a sequence of labels to a\nset of speech frames, a task commonly known as framewise classification. We\nshow that dense prediction view of framewise classification offers several\nadvantages and insights, including computational efficiency and the ability to\napply batch normalization. When doing dense prediction we pay specific\nattention to strided pooling in time and introduce an asymmetric dilated\nconvolution, called time-dilated convolution, that allows for efficient and\nelegant implementation of pooling in time. We show results using time-dilated\nconvolutions in a very deep VGG-style CNN with batch normalization on the Hub5\nSwitchboard-2000 benchmark task. With a big n-gram language model, we achieve\n7.7% WER which is the best single model single-pass performance reported so\nfar.\n","negative":"  Linear predictors are especially useful when the data is high-dimensional and\nsparse. One of the standard techniques used to train a linear predictor is the\nAveraged Stochastic Gradient Descent (ASGD) algorithm. We present an efficient\nimplementation of ASGD that avoids dense vector operations. We also describe a\ntranslation invariant extension called Centered Averaged Stochastic Gradient\nDescent (CASGD).\n","id":376}
{"Unnamed: 0.1":11377,"Unnamed: 0":11377.0,"anchor":"Improving Policy Gradient by Exploring Under-appreciated Rewards","positive":"  This paper presents a novel form of policy gradient for model-free\nreinforcement learning (RL) with improved exploration properties. Current\npolicy-based methods use entropy regularization to encourage undirected\nexploration of the reward landscape, which is ineffective in high dimensional\nspaces with sparse rewards. We propose a more directed exploration strategy\nthat promotes exploration of under-appreciated reward regions. An action\nsequence is considered under-appreciated if its log-probability under the\ncurrent policy under-estimates its resulting reward. The proposed exploration\nstrategy is easy to implement, requiring small modifications to an\nimplementation of the REINFORCE algorithm. We evaluate the approach on a set of\nalgorithmic tasks that have long challenged RL methods. Our approach reduces\nhyper-parameter sensitivity and demonstrates significant improvements over\nbaseline methods. Our algorithm successfully solves a benchmark multi-digit\naddition task and generalizes to long sequences. This is, to our knowledge, the\nfirst time that a pure RL method has solved addition using only reward\nfeedback.\n","negative":"  The success of deep neural networks hinges on our ability to accurately and\nefficiently optimize high-dimensional, non-convex functions. In this paper, we\nempirically investigate the loss functions of state-of-the-art networks, and\nhow commonly-used stochastic gradient descent variants optimize these loss\nfunctions. To do this, we visualize the loss function by projecting them down\nto low-dimensional spaces chosen based on the convergence points of different\noptimization algorithms. Our observations suggest that optimization algorithms\nencounter and choose different descent directions at many saddle points to find\ndifferent final weights. Based on consistency we observe across re-runs of the\nsame stochastic optimization algorithm, we hypothesize that each optimization\nalgorithm makes characteristic choices at these saddle points.\n","id":377}
{"Unnamed: 0.1":11378,"Unnamed: 0":11378.0,"anchor":"Accelerated Gradient Temporal Difference Learning","positive":"  The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain.\n","negative":"  Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is\nused as the building block in energy-based deep generative models. Due to\nnumerical stability and quantifiability of the likelihood, RBM is commonly used\nwith Bernoulli units. Here, we consider an alternative member of exponential\nfamily RBM with leaky rectified linear units -- called leaky RBM. We first\nstudy the joint and marginal distributions of leaky RBM under different\nleakiness, which provides us important insights by connecting the leaky RBM\nmodel and truncated Gaussian distributions. The connection leads us to a simple\nyet efficient method for sampling from this model, where the basic idea is to\nanneal the leakiness rather than the energy; -- i.e., start from a fully\nGaussian\/Linear unit and gradually decrease the leakiness over iterations. This\nserves as an alternative to the annealing of the temperature parameter and\nenables numerical estimation of the likelihood that are more efficient and more\naccurate than the commonly used annealed importance sampling (AIS). We further\ndemonstrate that the proposed sampling algorithm enjoys faster mixing property\nthan contrastive divergence algorithm, which benefits the training without any\nadditional computational cost.\n","id":378}
{"Unnamed: 0.1":11379,"Unnamed: 0":11379.0,"anchor":"Dictionary Learning with Equiprobable Matching Pursuit","positive":"  Sparse signal representations based on linear combinations of learned atoms\nhave been used to obtain state-of-the-art results in several practical signal\nprocessing applications. Approximation methods are needed to process\nhigh-dimensional signals in this way because the problem to calculate optimal\natoms for sparse coding is NP-hard. Here we study greedy algorithms for\nunsupervised learning of dictionaries of shift-invariant atoms and propose a\nnew method where each atom is selected with the same probability on average,\nwhich corresponds to the homeostatic regulation of a recurrent convolutional\nneural network. Equiprobable selection can be used with several greedy\nalgorithms for dictionary learning to ensure that all atoms adapt during\ntraining and that no particular atom is more likely to take part in the linear\ncombination on average. We demonstrate via simulation experiments that\ndictionary learning with equiprobable selection results in higher entropy of\nthe sparse representation and lower reconstruction and denoising errors, both\nin the case of ordinary matching pursuit and orthogonal matching pursuit with\nshift-invariant dictionaries. Furthermore, we show that the computational costs\nof the matching pursuits are lower with equiprobable selection, leading to\nfaster and more accurate dictionary learning algorithms.\n","negative":"  Understanding neural networks is becoming increasingly important. Over the\nlast few years different types of visualisation and explanation methods have\nbeen proposed. However, none of them explicitly considered the behaviour in the\npresence of noise and distracting elements. In this work, we will show how\nnoise and distracting dimensions can influence the result of an explanation\nmodel. This gives a new theoretical insights to aid selection of the most\nappropriate explanation model within the deep-Taylor decomposition framework.\n","id":379}
{"Unnamed: 0.1":11380,"Unnamed: 0":11380.0,"anchor":"Diet Networks: Thin Parameters for Fat Genomics","positive":"  Learning tasks such as those involving genomic data often poses a serious\nchallenge: the number of input features can be orders of magnitude larger than\nthe number of training examples, making it difficult to avoid overfitting, even\nwhen using the known regularization techniques. We focus here on tasks in which\nthe input is a description of the genetic variation specific to a patient, the\nsingle nucleotide polymorphisms (SNPs), yielding millions of ternary inputs.\nImproving the ability of deep learning to handle such datasets could have an\nimportant impact in precision medicine, where high-dimensional data regarding a\nparticular patient is used to make predictions of interest. Even though the\namount of data for such tasks is increasing, this mismatch between the number\nof examples and the number of inputs remains a concern. Naive implementations\nof classifier neural networks involve a huge number of free parameters in their\nfirst layer: each input feature is associated with as many parameters as there\nare hidden units. We propose a novel neural network parametrization which\nconsiderably reduces the number of free parameters. It is based on the idea\nthat we can first learn or provide a distributed representation for each input\nfeature (e.g. for each position in the genome where variations are observed),\nand then learn (with another neural network called the parameter prediction\nnetwork) how to map a feature's distributed representation to the vector of\nparameters specific to that feature in the classifier neural network (the\nweights which link the value of the feature to each of the hidden units). We\nshow experimentally on a population stratification task of interest to medical\nstudies that the proposed approach can significantly reduce both the number of\nparameters and the error rate of the classifier.\n","negative":"  This paper proposes a deep learning architecture based on Residual Network\nthat dynamically adjusts the number of executed layers for the regions of the\nimage. This architecture is end-to-end trainable, deterministic and\nproblem-agnostic. It is therefore applicable without any modifications to a\nwide range of computer vision problems such as image classification, object\ndetection and image segmentation. We present experimental results showing that\nthis model improves the computational efficiency of Residual Networks on the\nchallenging ImageNet classification and COCO object detection datasets.\nAdditionally, we evaluate the computation time maps on the visual saliency\ndataset cat2000 and find that they correlate surprisingly well with human eye\nfixation positions.\n","id":380}
{"Unnamed: 0.1":11381,"Unnamed: 0":11381.0,"anchor":"Unifying Multi-Domain Multi-Task Learning: Tensor and Neural Network\n  Perspectives","positive":"  Multi-domain learning aims to benefit from simultaneously learning across\nseveral different but related domains. In this chapter, we propose a single\nframework that unifies multi-domain learning (MDL) and the related but better\nstudied area of multi-task learning (MTL). By exploiting the concept of a\n\\emph{semantic descriptor} we show how our framework encompasses various\nclassic and recent MDL\/MTL algorithms as special cases with different semantic\ndescriptor encodings. As a second contribution, we present a higher order\ngeneralisation of this framework, capable of simultaneous\nmulti-task-multi-domain learning. This generalisation has two mathematically\nequivalent views in multi-linear algebra and gated neural networks\nrespectively. Moreover, by exploiting the semantic descriptor, it provides\nneural networks the capability of zero-shot learning (ZSL), where a classifier\nis generated for an unseen class without any training data; as well as\nzero-shot domain adaptation (ZSDA), where a model is generated for an unseen\ndomain without any training data. In practice, this framework provides a\npowerful yet easy to implement method that can be flexibly applied to MTL, MDL,\nZSL and ZSDA.\n","negative":"  In machine learning research, the proximal gradient methods are popular for\nsolving various optimization problems with non-smooth regularization. Inexact\nproximal gradient methods are extremely important when exactly solving the\nproximal operator is time-consuming, or the proximal operator does not have an\nanalytic solution. However, existing inexact proximal gradient methods only\nconsider convex problems. The knowledge of inexact proximal gradient methods in\nthe non-convex setting is very limited. % Moreover, for some machine learning\nmodels, there is still no proposed solver for exactly solving the proximal\noperator. To address this challenge, in this paper, we first propose three\ninexact proximal gradient algorithms, including the basic version and\nNesterov's accelerated version. After that, we provide the theoretical analysis\nto the basic and Nesterov's accelerated versions. The theoretical results show\nthat our inexact proximal gradient algorithms can have the same convergence\nrates as the ones of exact proximal gradient algorithms in the non-convex\nsetting.\n  Finally, we show the applications of our inexact proximal gradient algorithms\non three representative non-convex learning problems. All experimental results\nconfirm the superiority of our new inexact proximal gradient algorithms.\n","id":381}
{"Unnamed: 0.1":11382,"Unnamed: 0":11382.0,"anchor":"The Emergence of Organizing Structure in Conceptual Representation","positive":"  Both scientists and children make important structural discoveries, yet their\ncomputational underpinnings are not well understood. Structure discovery has\npreviously been formalized as probabilistic inference about the right\nstructural form --- where form could be a tree, ring, chain, grid, etc. [Kemp &\nTenenbaum (2008). The discovery of structural form. PNAS, 105(3), 10687-10692].\nWhile this approach can learn intuitive organizations, including a tree for\nanimals and a ring for the color circle, it assumes a strong inductive bias\nthat considers only these particular forms, and each form is explicitly\nprovided as initial knowledge. Here we introduce a new computational model of\nhow organizing structure can be discovered, utilizing a broad hypothesis space\nwith a preference for sparse connectivity. Given that the inductive bias is\nmore general, the model's initial knowledge shows little qualitative\nresemblance to some of the discoveries it supports. As a consequence, the model\ncan also learn complex structures for domains that lack intuitive description,\nas well as predict human property induction judgments without explicit\nstructural forms. By allowing form to emerge from sparsity, our approach\nclarifies how both the richness and flexibility of human conceptual\norganization can coexist.\n","negative":"  In this paper we introduce a new unsupervised reinforcement learning method\nfor discovering the set of intrinsic options available to an agent. This set is\nlearned by maximizing the number of different states an agent can reliably\nreach, as measured by the mutual information between the set of options and\noption termination states. To this end, we instantiate two policy gradient\nbased algorithms, one that creates an explicit embedding space of options and\none that represents options implicitly. The algorithms also provide an explicit\nmeasure of empowerment in a given state that can be used by an empowerment\nmaximizing agent. The algorithm scales well with function approximation and we\ndemonstrate the applicability of the algorithm on a range of tasks.\n","id":382}
{"Unnamed: 0.1":11383,"Unnamed: 0":11383.0,"anchor":"Safety-Aware Robot Damage Recovery Using Constrained Bayesian\n  Optimization and Simulated Priors","positive":"  The recently introduced Intelligent Trial-and-Error (IT&E) algorithm showed\nthat robots can adapt to damage in a matter of a few trials. The success of\nthis algorithm relies on two components: prior knowledge acquired through\nsimulation with an intact robot, and Bayesian optimization (BO) that operates\non-line, on the damaged robot. While IT&E leads to fast damage recovery, it\ndoes not incorporate any safety constraints that prevent the robot from\nattempting harmful behaviors. In this work, we address this limitation by\nreplacing the BO component with a constrained BO procedure. We evaluate our\napproach on a simulated damaged humanoid robot that needs to crawl as fast as\npossible, while performing as few unsafe trials as possible. We compare our new\n\"safety-aware IT&E\" algorithm to IT&E and a multi-objective version of IT&E in\nwhich the safety constraints are dealt as separate objectives. Our results show\nthat our algorithm outperforms the other approaches, both in crawling speed\nwithin the safe regions and number of unsafe trials.\n","negative":"  We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward\ndeep neural network that allows selective execution. Given an input, only a\nsubset of D2NN neurons are executed, and the particular subset is determined by\nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs\nprovide a way to improve computational efficiency. To achieve dynamic selective\nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic\ngraph of differentiable modules) with controller modules. Each controller\nmodule is a sub-network whose output is a decision that controls whether other\nmodules can execute. A D2NN is trained end to end. Both regular and controller\nmodules in a D2NN are learnable and are jointly trained to optimize both\naccuracy and efficiency. Such training is achieved by integrating\nbackpropagation with reinforcement learning. With extensive experiments of\nvarious D2NN architectures on image classification tasks, we demonstrate that\nD2NNs are general and flexible, and can effectively optimize\naccuracy-efficiency trade-offs.\n","id":383}
{"Unnamed: 0.1":11384,"Unnamed: 0":11384.0,"anchor":"Emergence of foveal image sampling from learning to attend in visual\n  scenes","positive":"  We describe a neural attention model with a learnable retinal sampling\nlattice. The model is trained on a visual search task requiring the\nclassification of an object embedded in a visual scene amidst background\ndistractors using the smallest number of fixations. We explore the tiling\nproperties that emerge in the model's retinal sampling lattice after training.\nSpecifically, we show that this lattice resembles the eccentricity dependent\nsampling lattice of the primate retina, with a high resolution region in the\nfovea surrounded by a low resolution periphery. Furthermore, we find conditions\nwhere these emergent properties are amplified or eliminated providing clues to\ntheir function.\n","negative":"  Accurately predicting drug responses to cancer is an important problem\nhindering oncologists' efforts to find the most effective drugs to treat\ncancer, which is a core goal in precision medicine. The scientific community\nhas focused on improving this prediction based on genomic, epigenomic, and\nproteomic datasets measured in human cancer cell lines. Real-world cancer cell\nlines contain noise, which degrades the performance of machine learning\nalgorithms. This problem is rarely addressed in the existing approaches. In\nthis paper, we present a noise-filtering approach that integrates techniques\nfrom numerical linear algebra and information retrieval targeted at filtering\nout noisy cancer cell lines. By filtering out noisy cancer cell lines, we can\ntrain machine learning algorithms on better quality cancer cell lines. We\nevaluate the performance of our approach and compare it with an existing\napproach using the Area Under the ROC Curve (AUC) on clinical trial data. The\nexperimental results show that our proposed approach is stable and also yields\nthe highest AUC at a statistically significant level.\n","id":384}
{"Unnamed: 0.1":11385,"Unnamed: 0":11385.0,"anchor":"Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability","positive":"  There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities.\n","negative":"  Deep learning research over the past years has shown that by increasing the\nscope or difficulty of the learning problem over time, increasingly complex\nlearning problems can be addressed. We study incremental learning in the\ncontext of sequence learning, using generative RNNs in the form of multi-layer\nrecurrent Mixture Density Networks. While the potential of incremental or\ncurriculum learning to enhance learning is known, indiscriminate application of\nthe principle does not necessarily lead to improvement, and it is essential\ntherefore to know which forms of incremental or curriculum learning have a\npositive effect. This research contributes to that aim by comparing three\ninstantiations of incremental or curriculum learning.\n  We introduce Incremental Sequence Learning, a simple incremental approach to\nsequence learning. Incremental Sequence Learning starts out by using only the\nfirst few steps of each sequence as training data. Each time a performance\ncriterion has been reached, the length of the parts of the sequences used for\ntraining is increased.\n  We introduce and make available a novel sequence learning task and data set:\npredicting and classifying MNIST pen stroke sequences. We find that Incremental\nSequence Learning greatly speeds up sequence learning and reaches the best test\nperformance level of regular sequence learning 20 times faster, reduces the\ntest error by 74%, and in general performs more robustly; it displays lower\nvariance and achieves sustained progress after all three comparison methods\nhave stopped improving. The other instantiations of curriculum learning do not\nresult in any noticeable improvement. A trained sequence prediction model is\nalso used in transfer learning to the task of sequence classification, where it\nis found that transfer learning realizes improved classification performance\ncompared to methods that learn to classify from scratch.\n","id":385}
{"Unnamed: 0.1":11386,"Unnamed: 0":11386.0,"anchor":"The empirical size of trained neural networks","positive":"  ReLU neural networks define piecewise linear functions of their inputs.\nHowever, initializing and training a neural network is very different from\nfitting a linear spline. In this paper, we expand empirically upon previous\ntheoretical work to demonstrate features of trained neural networks. Standard\nnetwork initialization and training produce networks vastly simpler than a\nnaive parameter count would suggest and can impart odd features to the trained\nnetwork. However, we also show the forced simplicity is beneficial and, indeed,\ncritical for the wide success of these networks.\n","negative":"  Sparsity-based subspace clustering algorithms have attracted significant\nattention thanks to their excellent performance in practical applications. A\nprominent example is the sparse subspace clustering (SSC) algorithm by\nElhamifar and Vidal, which performs spectral clustering based on an adjacency\nmatrix obtained by sparsely representing each data point in terms of all the\nother data points via the Lasso. When the number of data points is large or the\ndimension of the ambient space is high, the computational complexity of SSC\nquickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by\nreplacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm\nresults in significantly lower computational complexity, while often yielding\ncomparable performance. The central goal of this paper is an analytical\nperformance characterization of SSC-OMP for noisy data. Moreover, we introduce\nand analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu\nof OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces\nintersect and when the data points are contaminated by severe noise. The\nclustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for\nSSC and for the thresholding-based subspace clustering (TSC) algorithm due to\nHeckel and B\\\"olcskei. Analytical results in combination with numerical results\nindicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion\nautomatically detect the dimensions of the subspaces underlying the data.\nMoreover, experiments on synthetic and on real data show that SSC-MP compares\nvery favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor\nalgorithm, both in terms of clustering performance and running time. In\naddition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is\nvery robust with respect to the choice of parameters in the stopping criteria.\n","id":386}
{"Unnamed: 0.1":11387,"Unnamed: 0":11387.0,"anchor":"The Upper Bound on Knots in Neural Networks","positive":"  Neural networks with rectified linear unit activations are essentially\nmultivariate linear splines. As such, one of many ways to measure the\n\"complexity\" or \"expressivity\" of a neural network is to count the number of\nknots in the spline model. We study the number of knots in fully-connected\nfeedforward neural networks with rectified linear unit activation functions. We\nintentionally keep the neural networks very simple, so as to make theoretical\nanalyses more approachable. An induction on the number of layers $l$ reveals a\ntight upper bound on the number of knots in $\\mathbb{R} \\to \\mathbb{R}^p$ deep\nneural networks. With $n_i \\gg 1$ neurons in layer $i = 1, \\dots, l$, the upper\nbound is approximately $n_1 \\dots n_l$. We then show that the exact upper bound\nis tight, and we demonstrate the upper bound with an example. The purpose of\nthese analyses is to pave a path for understanding the behavior of general\n$\\mathbb{R}^q \\to \\mathbb{R}^p$ neural networks.\n","negative":"  Imputing incomplete medical tests and predicting patient outcomes are crucial\nfor guiding the decision making for therapy, such as after an Achilles Tendon\nRupture (ATR). We formulate the problem of data imputation and prediction for\nATR relevant medical measurements into a recommender system framework. By\napplying MatchBox, which is a collaborative filtering approach, on a real\ndataset collected from 374 ATR patients, we aim at offering personalized\nmedical data imputation and prediction. In this work, we show the feasibility\nof this approach and discuss potential research directions by conducting\ninitial qualitative evaluations.\n","id":387}
{"Unnamed: 0.1":11388,"Unnamed: 0":11388.0,"anchor":"Cost-Sensitive Reference Pair Encoding for Multi-Label Learning","positive":"  Label space expansion for multi-label classification (MLC) is a methodology\nthat encodes the original label vectors to higher dimensional codes before\ntraining and decodes the predicted codes back to the label vectors during\ntesting. The methodology has been demonstrated to improve the performance of\nMLC algorithms when coupled with off-the-shelf error-correcting codes for\nencoding and decoding. Nevertheless, such a coding scheme can be complicated to\nimplement, and cannot easily satisfy a common application need of\ncost-sensitive MLC---adapting to different evaluation criteria of interest. In\nthis work, we show that a simpler coding scheme based on the concept of a\nreference pair of label vectors achieves cost-sensitivity more naturally. In\nparticular, our proposed cost-sensitive reference pair encoding (CSRPE)\nalgorithm contains cluster-based encoding, weight-based training and\nvoting-based decoding steps, all utilizing the cost information. Furthermore,\nwe leverage the cost information embedded in the code space of CSRPE to propose\na novel active learning algorithm for cost-sensitive MLC. Extensive\nexperimental results verify that CSRPE performs better than state-of-the-art\nalgorithms across different MLC criteria. The results also demonstrate that the\nCSRPE-backed active learning algorithm is superior to existing algorithms for\nactive MLC, and further justify the usefulness of CSRPE.\n","negative":"  Co-adaptation is a special form of on-line learning where an algorithm\n$\\mathcal{A}$ must assist an unknown algorithm $\\mathcal{B}$ to perform some\ntask. This is a general framework and has applications in recommendation\nsystems, search, education, and much more. Today, the most common use of\nco-adaptive algorithms is in brain-computer interfacing (BCI), where algorithms\nhelp patients gain and maintain control over prosthetic devices. While previous\nstudies have shown strong empirical results Kowalski et al. (2013); Orsborn et\nal. (2014) or have been studied in specific examples Merel et al. (2013, 2015),\nthere is no general analysis of the co-adaptive learning problem. Here we will\nstudy the co-adaptive learning problem in the online, closed-loop setting. We\nwill prove that, with high probability, co-adaptive learning is guaranteed to\noutperform learning with a fixed decoder as long as a particular condition is\nmet.\n","id":388}
{"Unnamed: 0.1":11389,"Unnamed: 0":11389.0,"anchor":"Fast Wavenet Generation Algorithm","positive":"  This paper presents an efficient implementation of the Wavenet generation\nprocess called Fast Wavenet. Compared to a naive implementation that has\ncomplexity O(2^L) (L denotes the number of layers in the network), our proposed\napproach removes redundant convolution operations by caching previous\ncalculations, thereby reducing the complexity to O(L) time. Timing experiments\nshow significant advantages of our fast implementation over a naive one. While\nthis method is presented for Wavenet, the same scheme can be applied anytime\none wants to perform autoregressive generation or online prediction using a\nmodel with dilated convolution layers. The code for our method is publicly\navailable.\n","negative":"  This work presents a fast and scalable algorithm for incremental learning of\nGaussian mixture models. By performing rank-one updates on its precision\nmatrices and determinants, its asymptotic time complexity is of \\BigO{NKD^2}\nfor $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting\nalgorithm can be applied to high dimensional tasks, and this is confirmed by\napplying it to the classification datasets MNIST and CIFAR-10. Additionally, in\norder to show the algorithm's applicability to function approximation and\ncontrol tasks, it is applied to three reinforcement learning tasks and its\ndata-efficiency is evaluated.\n","id":389}
{"Unnamed: 0.1":11390,"Unnamed: 0":11390.0,"anchor":"Graph-Based Manifold Frequency Analysis for Denoising","positive":"  We propose a new framework for manifold denoising based on processing in the\ngraph Fourier frequency domain, derived from the spectral decomposition of the\ndiscrete graph Laplacian. Our approach uses the Spectral Graph Wavelet\ntransform in order to per- form non-iterative denoising directly in the graph\nfrequency domain, an approach inspired by conventional wavelet-based signal\ndenoising methods. We theoretically justify our approach, based on the fact\nthat for smooth manifolds the coordinate information energy is localized in the\nlow spectral graph wavelet sub-bands, while the noise affects all frequency\nbands in a similar way. Experimental results show that our proposed manifold\nfrequency denoising (MFD) approach significantly outperforms the state of the\nart denoising meth- ods, and is robust to a wide range of parameter selections,\ne.g., the choice of k nearest neighbor connectivity of the graph.\n","negative":"  We analyzed the performance of a biologically inspired algorithm called the\nCorrected Projections Algorithm (CPA) when a sparseness constraint is required\nto unambiguously reconstruct an observed signal using atoms from an\novercomplete dictionary. By changing the geometry of the estimation problem,\nCPA gives an analytical expression for a binary variable that indicates the\npresence or absence of a dictionary atom using an L2 regularizer. The\nregularized solution can be implemented using an efficient real-time\nKalman-filter type of algorithm. The smoother L2 regularization of CPA makes it\nvery robust to noise, and CPA outperforms other methods in identifying known\natoms in the presence of strong novel atoms in the signal.\n","id":390}
{"Unnamed: 0.1":11391,"Unnamed: 0":11391.0,"anchor":"Associative Memory using Dictionary Learning and Expander Decoding","positive":"  An associative memory is a framework of content-addressable memory that\nstores a collection of message vectors (or a dataset) over a neural network\nwhile enabling a neurally feasible mechanism to recover any message in the\ndataset from its noisy version. Designing an associative memory requires\naddressing two main tasks: 1) learning phase: given a dataset, learn a concise\nrepresentation of the dataset in the form of a graphical model (or a neural\nnetwork), 2) recall phase: given a noisy version of a message vector from the\ndataset, output the correct message vector via a neurally feasible algorithm\nover the network learnt during the learning phase. This paper studies the\nproblem of designing a class of neural associative memories which learns a\nnetwork representation for a large dataset that ensures correction against a\nlarge number of adversarial errors during the recall phase. Specifically, the\nassociative memories designed in this paper can store dataset containing\n$\\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can\ntolerate $\\Omega(\\frac{n}{{\\rm polylog} n})$ adversarial errors. This paper\ncarries out this memory design by mapping the learning phase and recall phase\nto the tasks of dictionary learning with a square dictionary and iterative\nerror correction in an expander code, respectively.\n","negative":"  This paper presents an alternative approach to p-values in regression\nsettings. This approach, whose origins can be traced to machine learning, is\nbased on the leave-one-out bootstrap for prediction error. In machine learning\nthis is called the out-of-bag (OOB) error. To obtain the OOB error for a model,\none draws a bootstrap sample and fits the model to the in-sample data. The\nout-of-sample prediction error for the model is obtained by calculating the\nprediction error for the model using the out-of-sample data. Repeating and\naveraging yields the OOB error, which represents a robust cross-validated\nestimate of the accuracy of the underlying model. By a simple modification to\nthe bootstrap data involving \"noising up\" a variable, the OOB method yields a\nvariable importance (VIMP) index, which directly measures how much a specific\nvariable contributes to the prediction precision of a model. VIMP provides a\nscientifically interpretable measure of the effect size of a variable, we call\nthe \"predictive effect size\", that holds whether the researcher's model is\ncorrect or not, unlike the p-value whose calculation is based on the assumed\ncorrectness of the model. We also discuss a marginal VIMP index, also easily\ncalculated, which measures the marginal effect of a variable, or what we call\n\"the discovery effect\". The OOB procedure can be applied to both parametric and\nnonparametric regression models and requires only that the researcher can\nrepeatedly fit their model to bootstrap and modified bootstrap data. We\nillustrate this approach on a survival data set involving patients with\nsystolic heart failure and to a simulated survival data set where the model is\nincorrectly specified to illustrate its robustness to model misspecification.\n","id":391}
{"Unnamed: 0.1":11392,"Unnamed: 0":11392.0,"anchor":"Improving Variational Auto-Encoders using Householder Flow","positive":"  Variational auto-encoders (VAE) are scalable and powerful generative models.\nHowever, the choice of the variational posterior determines tractability and\nflexibility of the VAE. Commonly, latent variables are modeled using the normal\ndistribution with a diagonal covariance matrix. This results in computational\nefficiency but typically it is not flexible enough to match the true posterior\ndistribution. One fashion of enriching the variational posterior distribution\nis application of normalizing flows, i.e., a series of invertible\ntransformations to latent variables with a simple posterior. In this paper, we\nfollow this line of thinking and propose a volume-preserving flow that uses a\nseries of Householder transformations. We show empirically on MNIST dataset and\nhistopathology data that the proposed flow allows to obtain more flexible\nvariational posterior and competitive results comparing to other normalizing\nflows.\n","negative":"  This paper corrects the proof of the Theorem 2 from the Gower's paper\n\\cite[page 5]{Gower:1982} as well as corrects the Theorem 7 from Gower's paper\n\\cite{Gower:1986}. The first correction is needed in order to establish the\nexistence of the kernel function used commonly in the kernel trick e.g. for\n$k$-means clustering algorithm, on the grounds of distance matrix. The\ncorrection encompasses the missing if-part proof and dropping unnecessary\nconditions. The second correction deals with transformation of the kernel\nmatrix into a one embeddable in Euclidean space.\n","id":392}
{"Unnamed: 0.1":11393,"Unnamed: 0":11393.0,"anchor":"Gossip training for deep learning","positive":"  We address the issue of speeding up the training of convolutional networks.\nHere we study a distributed method adapted to stochastic gradient descent\n(SGD). The parallel optimization setup uses several threads, each applying\nindividual gradient descents on a local variable. We propose a new way to share\ninformation between different threads inspired by gossip algorithms and showing\ngood consensus convergence properties. Our method called GoSGD has the\nadvantage to be fully asynchronous and decentralized. We compared our method to\nthe recent EASGD in \\cite{elastic} on CIFAR-10 show encouraging results.\n","negative":"  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n","id":393}
{"Unnamed: 0.1":11394,"Unnamed: 0":11394.0,"anchor":"Co-adaptive learning over a countable space","positive":"  Co-adaptation is a special form of on-line learning where an algorithm\n$\\mathcal{A}$ must assist an unknown algorithm $\\mathcal{B}$ to perform some\ntask. This is a general framework and has applications in recommendation\nsystems, search, education, and much more. Today, the most common use of\nco-adaptive algorithms is in brain-computer interfacing (BCI), where algorithms\nhelp patients gain and maintain control over prosthetic devices. While previous\nstudies have shown strong empirical results Kowalski et al. (2013); Orsborn et\nal. (2014) or have been studied in specific examples Merel et al. (2013, 2015),\nthere is no general analysis of the co-adaptive learning problem. Here we will\nstudy the co-adaptive learning problem in the online, closed-loop setting. We\nwill prove that, with high probability, co-adaptive learning is guaranteed to\noutperform learning with a fixed decoder as long as a particular condition is\nmet.\n","negative":"  Artificial neural networks have gone through a recent rise in popularity,\nachieving state-of-the-art results in various fields, including image\nclassification, speech recognition, and automated control. Both the performance\nand computational complexity of such models are heavily dependant on the design\nof characteristic hyper-parameters (e.g., number of hidden layers, nodes per\nlayer, or choice of activation functions), which have traditionally been\noptimized manually. With machine learning penetrating low-power mobile and\nembedded areas, the need to optimize not only for performance (accuracy), but\nalso for implementation complexity, becomes paramount. In this work, we present\na multi-objective design space exploration method that reduces the number of\nsolution networks trained and evaluated through response surface modelling.\nGiven spaces which can easily exceed 1020 solutions, manually designing a\nnear-optimal architecture is unlikely as opportunities to reduce network\ncomplexity, while maintaining performance, may be overlooked. This problem is\nexacerbated by the fact that hyper-parameters which perform well on specific\ndatasets may yield sub-par results on others, and must therefore be designed on\na per-application basis. In our work, machine learning is leveraged by training\nan artificial neural network to predict the performance of future candidate\nnetworks. The method is evaluated on the MNIST and CIFAR-10 image datasets,\noptimizing for both recognition accuracy and computational complexity.\nExperimental results demonstrate that the proposed method can closely\napproximate the Pareto-optimal front, while only exploring a small fraction of\nthe design space.\n","id":394}
{"Unnamed: 0.1":11395,"Unnamed: 0":11395.0,"anchor":"Measuring and modeling the perception of natural and unconstrained gaze\n  in humans and machines","positive":"  Humans are remarkably adept at interpreting the gaze direction of other\nindividuals in their surroundings. This skill is at the core of the ability to\nengage in joint visual attention, which is essential for establishing social\ninteractions. How accurate are humans in determining the gaze direction of\nothers in lifelike scenes, when they can move their heads and eyes freely, and\nwhat are the sources of information for the underlying perceptual processes?\nThese questions pose a challenge from both empirical and computational\nperspectives, due to the complexity of the visual input in real-life\nsituations. Here we measure empirically human accuracy in perceiving the gaze\ndirection of others in lifelike scenes, and study computationally the sources\nof information and representations underlying this cognitive capacity. We show\nthat humans perform better in face-to-face conditions compared with recorded\nconditions, and that this advantage is not due to the availability of input\ndynamics. We further show that humans are still performing well when only the\neyes-region is visible, rather than the whole face. We develop a computational\nmodel, which replicates the pattern of human performance, including the finding\nthat the eyes-region contains on its own, the required information for\nestimating both head orientation and direction of gaze. Consistent with\nneurophysiological findings on task-specific face regions in the brain, the\nlearned computational representations reproduce perceptual effects such as the\nWollaston illusion, when trained to estimate direction of gaze, but not when\ntrained to recognize objects or faces.\n","negative":"  It has long been recognized that the invariance and equivariance properties\nof a representation are critically important for success in many vision tasks.\nIn this paper we present Steerable Convolutional Neural Networks, an efficient\nand flexible class of equivariant convolutional networks. We show that\nsteerable CNNs achieve state of the art results on the CIFAR image\nclassification benchmark. The mathematical theory of steerable representations\nreveals a type system in which any steerable representation is a composition of\nelementary feature types, each one associated with a particular kind of\nsymmetry. We show how the parameter cost of a steerable filter bank depends on\nthe types of the input and output features, and show how to use this knowledge\nto construct CNNs that utilize parameters effectively.\n","id":395}
{"Unnamed: 0.1":11396,"Unnamed: 0":11396.0,"anchor":"Learning Features of Music from Scratch","positive":"  This paper introduces a new large-scale music dataset, MusicNet, to serve as\na source of supervision and evaluation of machine learning methods for music\nresearch. MusicNet consists of hundreds of freely-licensed classical music\nrecordings by 10 composers, written for 11 instruments, together with\ninstrument\/note annotations resulting in over 1 million temporal labels on 34\nhours of chamber music performances under various studio and microphone\nconditions.\n  The paper defines a multi-label classification task to predict notes in\nmusical recordings, along with an evaluation protocol, and benchmarks several\nmachine learning architectures for this task: i) learning from spectrogram\nfeatures; ii) end-to-end learning with a neural net; iii) end-to-end learning\nwith a convolutional neural net. These experiments show that end-to-end models\ntrained for note prediction learn frequency selective filters as a low-level\nrepresentation of audio.\n","negative":"  We derive an alternative proof for the regret of Thompson sampling (\\ts) in\nthe stochastic linear bandit setting. While we obtain a regret bound of order\n$\\widetilde{O}(d^{3\/2}\\sqrt{T})$ as in previous results, the proof sheds new\nlight on the functioning of the \\ts. We leverage on the structure of the\nproblem to show how the regret is related to the sensitivity (i.e., the\ngradient) of the objective function and how selecting optimal arms associated\nto \\textit{optimistic} parameters does control it. Thus we show that \\ts can be\nseen as a generic randomized algorithm where the sampling distribution is\ndesigned to have a fixed probability of being optimistic, at the cost of an\nadditional $\\sqrt{d}$ regret factor compared to a UCB-like approach.\nFurthermore, we show that our proof can be readily applied to regularized\nlinear optimization and generalized linear model problems.\n","id":396}
{"Unnamed: 0.1":11397,"Unnamed: 0":11397.0,"anchor":"Identity-sensitive Word Embedding through Heterogeneous Networks","positive":"  Most existing word embedding approaches do not distinguish the same words in\ndifferent contexts, therefore ignoring their contextual meanings. As a result,\nthe learned embeddings of these words are usually a mixture of multiple\nmeanings. In this paper, we acknowledge multiple identities of the same word in\ndifferent contexts and learn the \\textbf{identity-sensitive} word embeddings.\nBased on an identity-labeled text corpora, a heterogeneous network of words and\nword identities is constructed to model different-levels of word\nco-occurrences. The heterogeneous network is further embedded into a\nlow-dimensional space through a principled network embedding approach, through\nwhich we are able to obtain the embeddings of words and the embeddings of word\nidentities. We study three different types of word identities including topics,\nsentiments and categories. Experimental results on real-world data sets show\nthat the identity-sensitive word embeddings learned by our approach indeed\ncapture different meanings of words and outperforms competitive methods on\ntasks including text classification and word similarity computation.\n","negative":"  We present a method for performing hierarchical object detection in images\nguided by a deep reinforcement learning agent. The key idea is to focus on\nthose parts of the image that contain richer information and zoom on them. We\ntrain an intelligent agent that, given an image window, is capable of deciding\nwhere to focus the attention among five different predefined region candidates\n(smaller windows). This procedure is iterated providing a hierarchical image\nanalysis.We compare two different candidate proposal strategies to guide the\nobject search: with and without overlap. Moreover, our work compares two\ndifferent strategies to extract features from a convolutional neural network\nfor each region proposal: a first one that computes new feature maps for each\nregion proposal, and a second one that computes the feature maps for the whole\nimage to later generate crops for each region proposal. Experiments indicate\nbetter results for the overlapping candidate proposal strategy and a loss of\nperformance for the cropped image features due to the loss of spatial\nresolution. We argue that, while this loss seems unavoidable when working with\nlarge amounts of object candidates, the much more reduced amount of region\nproposals generated by our reinforcement learning agent allows considering to\nextract features for each location without sharing convolutional computation\namong regions.\n","id":397}
{"Unnamed: 0.1":11398,"Unnamed: 0":11398.0,"anchor":"Exploration for Multi-task Reinforcement Learning with Deep Generative\n  Models","positive":"  Exploration in multi-task reinforcement learning is critical in training\nagents to deduce the underlying MDP. Many of the existing exploration\nframeworks such as $E^3$, $R_{max}$, Thompson sampling assume a single\nstationary MDP and are not suitable for system identification in the multi-task\nsetting. We present a novel method to facilitate exploration in multi-task\nreinforcement learning using deep generative models. We supplement our method\nwith a low dimensional energy model to learn the underlying MDP distribution\nand provide a resilient and adaptive exploration signal to the agent. We\nevaluate our method on a new set of environments and provide intuitive\ninterpretation of our results.\n","negative":"  A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.\n","id":398}
{"Unnamed: 0.1":11399,"Unnamed: 0":11399.0,"anchor":"Autism Spectrum Disorder Classification using Graph Kernels on\n  Multidimensional Time Series","positive":"  We present an approach to model time series data from resting state fMRI for\nautism spectrum disorder (ASD) severity classification. We propose to adopt\nkernel machines and employ graph kernels that define a kernel dot product\nbetween two graphs. This enables us to take advantage of spatio-temporal\ninformation to capture the dynamics of the brain network, as opposed to\naggregating them in the spatial or temporal dimension. In addition to the\nconventional similarity graphs, we explore the use of L1 graph using sparse\ncoding, and the persistent homology of time delay embeddings, in the proposed\npipeline for ASD classification. In our experiments on two datasets from the\nABIDE collection, we demonstrate a consistent and significant advantage in\nusing graph kernels over traditional linear or non linear kernels for a variety\nof time series features.\n","negative":"  Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures.\n","id":399}
{"Unnamed: 0.1":11400,"Unnamed: 0":11400.0,"anchor":"C-RNN-GAN: Continuous recurrent neural networks with adversarial\n  training","positive":"  Generative adversarial networks have been proposed as a way of efficiently\ntraining deep generative neural networks. We propose a generative adversarial\nmodel that works on continuous sequential data, and apply it by training it on\na collection of classical music. We conclude that it generates music that\nsounds better and better as the model is trained, report statistics on\ngenerated music, and let the reader judge the quality by downloading the\ngenerated songs.\n","negative":"  Statistical topic models efficiently facilitate the exploration of\nlarge-scale data sets. Many models have been developed and broadly used to\nsummarize the semantic structure in news, science, social media, and digital\nhumanities. However, a common and practical objective in data exploration tasks\nis not to enumerate all existing topics, but to quickly extract representative\nones that broadly cover the content of the corpus, i.e., a few topics that\nserve as a good summary of the data. Most existing topic models fit exactly the\nsame number of topics as a user specifies, which have imposed an unnecessary\nburden to the users who have limited prior knowledge. We instead propose new\nmodels that are able to learn fewer but more representative topics for the\npurpose of data summarization. We propose a reinforced random walk that allows\nprominent topics to absorb tokens from similar and smaller topics, thus\nenhances the diversity among the top topics extracted. With this reinforced\nrandom walk as a general process embedded in classical topic models, we obtain\n\\textit{diverse topic models} that are able to extract the most prominent and\ndiverse topics from data. The inference procedures of these diverse topic\nmodels remain as simple and efficient as the classical models. Experimental\nresults demonstrate that the diverse topic models not only discover topics that\nbetter summarize the data, but also require minimal prior knowledge of the\nusers.\n","id":400}
{"Unnamed: 0.1":11401,"Unnamed: 0":11401.0,"anchor":"Capacity and Trainability in Recurrent Neural Networks","positive":"  Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures.\n","negative":"  This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep\nlearning model able to predict structured sequences of data. Precisely, GCRN is\na generalization of classical recurrent neural networks (RNN) to data\nstructured by an arbitrary graph. Such structured sequences can represent\nseries of frames in videos, spatio-temporal measurements on a network of\nsensors, or random walks on a vocabulary graph for natural language modeling.\nThe proposed model combines convolutional neural networks (CNN) on graphs to\nidentify spatial structures and RNN to find dynamic patterns. We study two\npossible architectures of GCRN, and apply the models to two practical problems:\npredicting moving MNIST data, and modeling natural language with the Penn\nTreebank dataset. Experiments show that exploiting simultaneously graph spatial\nand dynamic information about data can improve both precision and learning\nspeed.\n","id":401}
{"Unnamed: 0.1":11402,"Unnamed: 0":11402.0,"anchor":"Less is More: Learning Prominent and Diverse Topics for Data\n  Summarization","positive":"  Statistical topic models efficiently facilitate the exploration of\nlarge-scale data sets. Many models have been developed and broadly used to\nsummarize the semantic structure in news, science, social media, and digital\nhumanities. However, a common and practical objective in data exploration tasks\nis not to enumerate all existing topics, but to quickly extract representative\nones that broadly cover the content of the corpus, i.e., a few topics that\nserve as a good summary of the data. Most existing topic models fit exactly the\nsame number of topics as a user specifies, which have imposed an unnecessary\nburden to the users who have limited prior knowledge. We instead propose new\nmodels that are able to learn fewer but more representative topics for the\npurpose of data summarization. We propose a reinforced random walk that allows\nprominent topics to absorb tokens from similar and smaller topics, thus\nenhances the diversity among the top topics extracted. With this reinforced\nrandom walk as a general process embedded in classical topic models, we obtain\n\\textit{diverse topic models} that are able to extract the most prominent and\ndiverse topics from data. The inference procedures of these diverse topic\nmodels remain as simple and efficient as the classical models. Experimental\nresults demonstrate that the diverse topic models not only discover topics that\nbetter summarize the data, but also require minimal prior knowledge of the\nusers.\n","negative":"  Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments.\n","id":402}
{"Unnamed: 0.1":11403,"Unnamed: 0":11403.0,"anchor":"Neural Combinatorial Optimization with Reinforcement Learning","positive":"  This paper presents a framework to tackle combinatorial optimization problems\nusing neural networks and reinforcement learning. We focus on the traveling\nsalesman problem (TSP) and train a recurrent network that, given a set of city\ncoordinates, predicts a distribution over different city permutations. Using\nnegative tour length as the reward signal, we optimize the parameters of the\nrecurrent network using a policy gradient method. We compare learning the\nnetwork parameters on a set of training graphs against learning them on\nindividual test graphs. Despite the computational expense, without much\nengineering and heuristic designing, Neural Combinatorial Optimization achieves\nclose to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied\nto the KnapSack, another NP-hard problem, the same method obtains optimal\nsolutions for instances with up to 200 items.\n","negative":"  Mass segmentation is an important task in mammogram analysis, providing\neffective morphological features and regions of interest (ROI) for mass\ndetection and classification. Inspired by the success of using deep\nconvolutional features for natural image analysis and conditional random fields\n(CRF) for structural learning, we propose an end-to-end network for\nmammographic mass segmentation. The network employs a fully convolutional\nnetwork (FCN) to model potential function, followed by a CRF to perform\nstructural learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with position priori for the task. Due to the\nsmall size of mammogram datasets, we use adversarial training to control\nover-fitting. Four models with different convolutional kernels are further\nfused to improve the segmentation results. Experimental results on two public\ndatasets, INbreast and DDSM-BCRP, show that our end-to-end network combined\nwith adversarial training achieves the-state-of-the-art results.\n","id":403}
{"Unnamed: 0.1":11404,"Unnamed: 0":11404.0,"anchor":"Low-dimensional Data Embedding via Robust Ranking","positive":"  We describe a new method called t-ETE for finding a low-dimensional embedding\nof a set of objects in Euclidean space. We formulate the embedding problem as a\njoint ranking problem over a set of triplets, where each triplet captures the\nrelative similarities between three objects in the set. By exploiting recent\nadvances in robust ranking, t-ETE produces high-quality embeddings even in the\npresence of a significant amount of noise and better preserves local scale than\nknown methods, such as t-STE and t-SNE. In particular, our method produces\nsignificantly better results than t-SNE on signature datasets while also being\nfaster to compute.\n","negative":"  Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge.\n","id":404}
{"Unnamed: 0.1":11405,"Unnamed: 0":11405.0,"anchor":"Machine Learning for Dental Image Analysis","positive":"  In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).\n","negative":"  Semi-supervised wrapper methods are concerned with building effective\nsupervised classifiers from partially labeled data. Though previous works have\nsucceeded in some fields, it is still difficult to apply semi-supervised\nwrapper methods to practice because the assumptions those methods rely on tend\nto be unrealistic in practice. For practical use, this paper proposes a novel\nsemi-supervised wrapper method, Dual Teaching, whose assumptions are easy to\nset up. Dual Teaching adopts two external classifiers to estimate the false\npositives and false negatives of the base learner. Only if the recall of every\nexternal classifier is greater than zero and the sum of the precision is\ngreater than one, Dual Teaching will train a base learner from partially\nlabeled data as effectively as the fully-labeled-data-trained classifier. The\neffectiveness of Dual Teaching is proved in both theory and practice.\n","id":405}
{"Unnamed: 0.1":11406,"Unnamed: 0":11406.0,"anchor":"Fast Supervised Discrete Hashing and its Analysis","positive":"  In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the \"Fast\nSDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.\n","negative":"  Gradients have been used to quantify feature importance in machine learning\nmodels. Unfortunately, in nonlinear deep networks, not only individual neurons\nbut also the whole network can saturate, and as a result an important input\nfeature can have a tiny gradient. We study various networks, and observe that\nthis phenomena is indeed widespread, across many inputs.\n  We propose to examine interior gradients, which are gradients of\ncounterfactual inputs constructed by scaling down the original input. We apply\nour method to the GoogleNet architecture for object recognition in images, as\nwell as a ligand-based virtual screening network with categorical features and\nan LSTM based language model for the Penn Treebank dataset. We visualize how\ninterior gradients better capture feature importance. Furthermore, interior\ngradients are applicable to a wide variety of deep networks, and have the\nattribution property that the feature importance scores sum to the the\nprediction score.\n  Best of all, interior gradients can be computed just as easily as gradients.\nIn contrast, previous methods are complex to implement, which hinders practical\nadoption.\n","id":406}
{"Unnamed: 0.1":11407,"Unnamed: 0":11407.0,"anchor":"Active Deep Learning for Classification of Hyperspectral Images","positive":"  Active deep learning classification of hyperspectral images is considered in\nthis paper. Deep learning has achieved success in many applications, but\ngood-quality labeled samples are needed to construct a deep learning network.\nIt is expensive getting good labeled samples in hyperspectral images for remote\nsensing applications. An active learning algorithm based on a weighted\nincremental dictionary learning is proposed for such applications. The proposed\nalgorithm selects training samples that maximize two selection criteria, namely\nrepresentative and uncertainty. This algorithm trains a deep network\nefficiently by actively selecting training samples at each iteration. The\nproposed algorithm is applied for the classification of hyperspectral images,\nand compared with other classification algorithms employing active learning. It\nis shown that the proposed algorithm is efficient and effective in classifying\nhyperspectral images.\n","negative":"  Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model.\n","id":407}
{"Unnamed: 0.1":11408,"Unnamed: 0":11408.0,"anchor":"Subsampled online matrix factorization with convergence guarantees","positive":"  We present a matrix factorization algorithm that scales to input matrices\nthat are large in both dimensions (i.e., that contains morethan 1TB of data).\nThe algorithm streams the matrix columns while subsampling them, resulting in\nlow complexity per iteration andreasonable memory footprint. In contrast to\nprevious online matrix factorization methods, our approach relies on\nlow-dimensional statistics from past iterates to control the extra variance\nintroduced by subsampling. We present a convergence analysis that guarantees us\nto reach a stationary point of the problem. Large speed-ups can be obtained\ncompared to previous online algorithms that do not perform subsampling, thanks\nto the feature redundancy that often exists in high-dimensional settings.\n","negative":"  Scene understanding and object recognition is a difficult to achieve yet\ncrucial skill for robots. Recently, Convolutional Neural Networks (CNN), have\nshown success in this task. However, there is still a gap between their\nperformance on image datasets and real-world robotics scenarios. We present a\nnovel paradigm for incrementally improving a robot's visual perception through\nactive human interaction. In this paradigm, the user introduces novel objects\nto the robot by means of pointing and voice commands. Given this information,\nthe robot visually explores the object and adds images from it to re-train the\nperception module. Our base perception module is based on recent development in\nobject detection and recognition using deep learning. Our method leverages\nstate of the art CNNs from off-line batch learning, human guidance, robot\nexploration and incremental on-line learning.\n","id":408}
{"Unnamed: 0.1":11409,"Unnamed: 0":11409.0,"anchor":"Performance Tuning of Hadoop MapReduce: A Noisy Gradient Approach","positive":"  Hadoop MapReduce is a framework for distributed storage and processing of\nlarge datasets that is quite popular in big data analytics. It has various\nconfiguration parameters (knobs) which play an important role in deciding the\nperformance i.e., the execution time of a given big data processing job.\nDefault values of these parameters do not always result in good performance and\nhence it is important to tune them. However, there is inherent difficulty in\ntuning the parameters due to two important reasons - firstly, the parameter\nsearch space is large and secondly, there are cross-parameter interactions.\nHence, there is a need for a dimensionality-free method which can automatically\ntune the configuration parameters by taking into account the cross-parameter\ndependencies. In this paper, we propose a novel Hadoop parameter tuning\nmethodology, based on a noisy gradient algorithm known as the simultaneous\nperturbation stochastic approximation (SPSA). The SPSA algorithm tunes the\nparameters by directly observing the performance of the Hadoop MapReduce\nsystem. The approach followed is independent of parameter dimensions and\nrequires only $2$ observations per iteration while tuning. We demonstrate the\neffectiveness of our methodology in achieving good performance on popular\nHadoop benchmarks namely \\emph{Grep}, \\emph{Bigram}, \\emph{Inverted Index},\n\\emph{Word Co-occurrence} and \\emph{Terasort}. Our method, when tested on a 25\nnode Hadoop cluster shows 66\\% decrease in execution time of Hadoop jobs on an\naverage, when compared to the default configuration. Further, we also observe a\nreduction of 45\\% in execution times, when compared to prior methods.\n","negative":"  Since about 100 years ago, to learn the intrinsic structure of data, many\nrepresentation learning approaches have been proposed, including both linear\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\ndeep architectures are widely applied for representation learning in recent\nyears, and have delivered top results in many tasks, such as image\nclassification, object detection and speech recognition. In this paper, we\nreview the development of data representation learning methods. Specifically,\nwe investigate both traditional feature learning algorithms and\nstate-of-the-art deep learning models. The history of data representation\nlearning is introduced, while available resources (e.g. online course, tutorial\nand book information) and toolboxes are provided. Finally, we conclude this\npaper with remarks and some interesting research directions on data\nrepresentation learning.\n","id":409}
{"Unnamed: 0.1":11410,"Unnamed: 0":11410.0,"anchor":"Effective Quantization Methods for Recurrent Neural Networks","positive":"  Reducing bit-widths of weights, activations, and gradients of a Neural\nNetwork can shrink its storage size and memory usage, and also allow for faster\ntraining and inference by exploiting bitwise operations. However, previous\nattempts for quantization of RNNs show considerable performance degradation\nwhen using low bit-width weights and activations. In this paper, we propose\nmethods to quantize the structure of gates and interlinks in LSTM and GRU\ncells. In addition, we propose balanced quantization methods for weights to\nfurther reduce performance degradation. Experiments on PTB and IMDB datasets\nconfirm effectiveness of our methods as performances of our models match or\nsurpass the previous state-of-the-art of quantized RNN.\n","negative":"  Service level agreement (SLA) is an essential part of cloud systems to ensure\nmaximum availability of services for customers. With a violation of SLA, the\nprovider has to pay penalties. In this paper, we explore two machine learning\nmodels: Naive Bayes and Random Forest Classifiers to predict SLA violations.\nSince SLA violations are a rare event in the real world (~0.2 %), the\nclassification task becomes more challenging. In order to overcome these\nchallenges, we use several re-sampling methods. We find that random forests\nwith SMOTE-ENN re-sampling have the best performance among other methods with\nthe accuracy of 99.88 % and F_1 score of 0.9980.\n","id":410}
{"Unnamed: 0.1":11411,"Unnamed: 0":11411.0,"anchor":"Unit Commitment using Nearest Neighbor as a Short-Term Proxy","positive":"  We devise the Unit Commitment Nearest Neighbor (UCNN) algorithm to be used as\na proxy for quickly approximating outcomes of short-term decisions, to make\ntractable hierarchical long-term assessment and planning for large power\nsystems. Experimental results on updated versions of IEEE-RTS79 and IEEE-RTS96\nshow high accuracy measured on operational cost, achieved in runtimes that are\nlower in several orders of magnitude than the traditional approach.\n","negative":"  Suicide is an important but often misunderstood problem, one that researchers\nare now seeking to better understand through social media. Due in large part to\nthe fuzzy nature of what constitutes suicidal risks, most supervised approaches\nfor learning to automatically detect suicide-related activity in social media\nrequire a great deal of human labor to train. However, humans themselves have\ndiverse or conflicting views on what constitutes suicidal thoughts. So how to\nobtain reliable gold standard labels is fundamentally challenging and, we\nhypothesize, depends largely on what is asked of the annotators and what slice\nof the data they label. We conducted multiple rounds of data labeling and\ncollected annotations from crowdsourcing workers and domain experts. We\naggregated the resulting labels in various ways to train a series of supervised\nmodels. Our preliminary evaluations show that using unanimously agreed labels\nfrom multiple annotators is helpful to achieve robust machine models.\n","id":411}
{"Unnamed: 0.1":11412,"Unnamed: 0":11412.0,"anchor":"Behavior-Based Machine-Learning: A Hybrid Approach for Predicting Human\n  Decision Making","positive":"  A large body of work in behavioral fields attempts to develop models that\ndescribe the way people, as opposed to rational agents, make decisions. A\nrecent Choice Prediction Competition (2015) challenged researchers to suggest a\nmodel that captures 14 classic choice biases and can predict human decisions\nunder risk and ambiguity. The competition focused on simple decision problems,\nin which human subjects were asked to repeatedly choose between two gamble\noptions.\n  In this paper we present our approach for predicting human decision behavior:\nwe suggest to use machine learning algorithms with features that are based on\nwell-established behavioral theories. The basic idea is that these\npsychological features are essential for the representation of the data and are\nimportant for the success of the learning process. We implement a vanilla model\nin which we train SVM models using behavioral features that rely on the\npsychological properties underlying the competition baseline model. We show\nthat this basic model captures the 14 choice biases and outperforms all the\nother learning-based models in the competition. The preliminary results suggest\nthat such hybrid models can significantly improve the prediction of human\ndecision making, and are a promising direction for future research.\n","negative":"  Recent work in model-agnostic explanations of black-box machine learning has\ndemonstrated that interpretability of complex models does not have to come at\nthe cost of accuracy or model flexibility. However, it is not clear what kind\nof explanations, such as linear models, decision trees, and rule lists, are the\nappropriate family to consider, and different tasks and models may benefit from\ndifferent kinds of explanations. Instead of picking a single family of\nrepresentations, in this work we propose to use \"programs\" as model-agnostic\nexplanations. We show that small programs can be expressive yet intuitive as\nexplanations, and generalize over a number of existing interpretable families.\nWe propose a prototype program induction method based on simulated annealing\nthat approximates the local behavior of black-box classifiers around a specific\nprediction using random perturbations. Finally, we present preliminary\napplication on small datasets and show that the generated explanations are\nintuitive and accurate for a number of classifiers.\n","id":412}
{"Unnamed: 0.1":11413,"Unnamed: 0":11413.0,"anchor":"SeDMiD for Confusion Detection: Uncovering Mind State from Time Series\n  Brain Wave Data","positive":"  Understanding how brain functions has been an intriguing topic for years.\nWith the recent progress on collecting massive data and developing advanced\ntechnology, people have become interested in addressing the challenge of\ndecoding brain wave data into meaningful mind states, with many machine\nlearning models and algorithms being revisited and developed, especially the\nones that handle time series data because of the nature of brain waves.\nHowever, many of these time series models, like HMM with hidden state in\ndiscrete space or State Space Model with hidden state in continuous space, only\nwork with one source of data and cannot handle different sources of information\nsimultaneously. In this paper, we propose an extension of State Space Model to\nwork with different sources of information together with its learning and\ninference algorithms. We apply this model to decode the mind state of students\nduring lectures based on their brain waves and reach a significant better\nresults compared to traditional methods.\n","negative":"  We demonstrate the possibility of classifying causal systems into kinds that\nshare a common structure without first constructing an explicit dynamical model\nor using prior knowledge of the system dynamics. The algorithmic ability to\ndetermine whether arbitrary systems are governed by causal relations of the\nsame form offers significant practical applications in the development and\nvalidation of dynamical models. It is also of theoretical interest as an\nessential stage in the scientific inference of laws from empirical data. The\nalgorithm presented is based on the dynamical symmetry approach to dynamical\nkinds. A dynamical symmetry with respect to time is an intervention on one or\nmore variables of a system that commutes with the time evolution of the system.\nA dynamical kind is a class of systems sharing a set of dynamical symmetries.\nThe algorithm presented classifies deterministic, time-dependent causal systems\nby directly comparing their exhibited symmetries. Using simulated, noisy data\nfrom a variety of nonlinear systems, we show that this algorithm correctly\nsorts systems into dynamical kinds. It is robust under significant sampling\nerror, is immune to violations of normality in sampling error, and fails\ngracefully with increasing dynamical similarity. The algorithm we demonstrate\nis the first to address this aspect of automated scientific discovery.\n","id":413}
{"Unnamed: 0.1":11414,"Unnamed: 0":11414.0,"anchor":"Reliably Learning the ReLU in Polynomial Time","positive":"  We give the first dimension-efficient algorithms for learning Rectified\nLinear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto\n\\max(0, \\mathbf{w} \\cdot \\mathbf{x})$ with $\\mathbf{w} \\in \\mathbb{S}^{n-1}$.\nOur algorithm works in the challenging Reliable Agnostic learning model of\nKalai, Kanade, and Mansour (2009) where the learner is given access to a\ndistribution $\\cal{D}$ on labeled examples but the labeling may be arbitrary.\nWe construct a hypothesis that simultaneously minimizes the false-positive rate\nand the loss on inputs given positive labels by $\\cal{D}$, for any convex,\nbounded, and Lipschitz loss function.\n  The algorithm runs in polynomial-time (in $n$) with respect to any\ndistribution on $\\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for\nany error parameter $\\epsilon = \\Omega(1\/\\log n)$ (this yields a PTAS for a\nquestion raised by F. Bach on the complexity of maximizing ReLUs). These\nresults are in contrast to known efficient algorithms for reliably learning\nlinear threshold functions, where $\\epsilon$ must be $\\Omega(1)$ and strong\nassumptions are required on the marginal distribution. We can compose our\nresults to obtain the first set of efficient algorithms for learning\nconstant-depth networks of ReLUs.\n  Our techniques combine kernel methods and polynomial approximations with a\n\"dual-loss\" approach to convex programming. As a byproduct we obtain a number\nof applications including the first set of efficient algorithms for \"convex\npiecewise-linear fitting\" and the first efficient algorithms for noisy\npolynomial reconstruction of low-weight polynomials on the unit sphere.\n","negative":"  We propose a higher-level associative memory for learning adversarial\nnetworks. Generative adversarial network (GAN) framework has a discriminator\nand a generator network. The generator (G) maps white noise (z) to data samples\nwhile the discriminator (D) maps data samples to a single scalar. To do so, G\nlearns how to map from high-level representation space to data space, and D\nlearns to do the opposite. We argue that higher-level representation spaces\nneed not necessarily follow a uniform probability distribution. In this work,\nwe use Restricted Boltzmann Machines (RBMs) as a higher-level associative\nmemory and learn the probability distribution for the high-level features\ngenerated by D. The associative memory samples its underlying probability\ndistribution and G learns how to map these samples to data space. The proposed\nassociative adversarial networks (AANs) are generative models in the\nhigher-levels of the learning, and use adversarial non-stochastic models D and\nG for learning the mapping between data and higher-level representation spaces.\nExperiments show the potential of the proposed networks.\n","id":414}
{"Unnamed: 0.1":11415,"Unnamed: 0":11415.0,"anchor":"Weighted bandits or: How bandits learn distorted values that are not\n  expected","positive":"  Motivated by models of human decision making proposed to explain commonly\nobserved deviations from conventional expected value preferences, we formulate\ntwo stochastic multi-armed bandit problems with distorted probabilities on the\ncost distributions: the classic $K$-armed bandit and the linearly parameterized\nbandit. In both settings, we propose algorithms that are inspired by Upper\nConfidence Bound (UCB), incorporate cost distortions, and exhibit sublinear\nregret assuming \\holder continuous weight distortion functions. For the\n$K$-armed setting, we show that the algorithm, called W-UCB, achieves\nproblem-dependent regret $O(L^2 M^2 \\log n\/ \\Delta^{\\frac{2}{\\alpha}-1})$,\nwhere $n$ is the number of plays, $\\Delta$ is the gap in distorted expected\nvalue between the best and next best arm, $L$ and $\\alpha$ are the H\\\"{o}lder\nconstants for the distortion function, and $M$ is an upper bound on costs, and\na problem-independent regret bound of\n$O((KL^2M^2)^{\\alpha\/2}n^{(2-\\alpha)\/2})$. We also present a matching lower\nbound on the regret, showing that the regret of W-UCB is essentially\nunimprovable over the class of H\\\"{o}lder-continuous weight distortions. For\nthe linearly parameterized setting, we develop a new algorithm, a variant of\nthe Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm called\nWOFUL (Weight-distorted OFUL), and show that it has regret $O(d\\sqrt{n} \\;\n\\mbox{polylog}(n))$ with high probability, for sub-Gaussian cost distributions.\nFinally, numerical examples demonstrate the advantages resulting from using\ndistortion-aware learning algorithms.\n","negative":"  The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.\n","id":415}
{"Unnamed: 0.1":11416,"Unnamed: 0":11416.0,"anchor":"Influential Node Detection in Implicit Social Networks using Multi-task\n  Gaussian Copula Models","positive":"  Influential node detection is a central research topic in social network\nanalysis. Many existing methods rely on the assumption that the network\nstructure is completely known \\textit{a priori}. However, in many applications,\nnetwork structure is unavailable to explain the underlying information\ndiffusion phenomenon. To address the challenge of information diffusion\nanalysis with incomplete knowledge of network structure, we develop a\nmulti-task low rank linear influence model. By exploiting the relationships\nbetween contagions, our approach can simultaneously predict the volume (i.e.\ntime series prediction) for each contagion (or topic) and automatically\nidentify the most influential nodes for each contagion. The proposed model is\nvalidated using synthetic data and an ISIS twitter dataset. In addition to\nimproving the volume prediction performance significantly, we show that the\nproposed approach can reliably infer the most influential users for specific\ncontagions.\n","negative":"  Deep learning (DL) training-as-a-service (TaaS) is an important emerging\nindustrial workload. The unique challenge of TaaS is that it must satisfy a\nwide range of customers who have no experience and resources to tune DL\nhyper-parameters, and meticulous tuning for each user's dataset is\nprohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with\nvalues that are applicable to all users. IBM Watson Natural Language Classifier\n(NLC) service, the most popular IBM cognitive service used by thousands of\nenterprise-level clients around the globe, is a typical TaaS service. By\nevaluating the NLC workloads, we show that only the conservative\nhyper-parameter setup (e.g., small mini-batch size and small learning rate) can\nguarantee acceptable model accuracy for a wide range of customers. We further\njustify theoretically why such a setup guarantees better model convergence in\ngeneral. Unfortunately, the small mini-batch size causes a high volume of\ncommunication traffic in a parameter-server based system. We characterize the\nhigh communication bandwidth requirement of TaaS using representative\nindustrial deep learning workloads and demonstrate that none of the\nstate-of-the-art scale-up or scale-out solutions can satisfy such a\nrequirement. We then present GaDei, an optimized shared-memory based scale-up\nparameter server design. We prove that the designed protocol is deadlock-free\nand it processes each gradient exactly once. Our implementation is evaluated on\nboth commercial benchmarks and public benchmarks to demonstrate that it\nsignificantly outperforms the state-of-the-art parameter-server based\nimplementation while maintaining the required accuracy and our implementation\nreaches near the best possible runtime performance, constrained only by the\nhardware limitation. Furthermore, to the best of our knowledge, GaDei is the\nonly scale-up DL system that provides fault-tolerance.\n","id":416}
{"Unnamed: 0.1":11417,"Unnamed: 0":11417.0,"anchor":"The observer-assisted method for adjusting hyper-parameters in deep\n  learning algorithms","positive":"  This paper presents a concept of a novel method for adjusting\nhyper-parameters in Deep Learning (DL) algorithms. An external agent-observer\nmonitors a performance of a selected Deep Learning algorithm. The observer\nlearns to model the DL algorithm using a series of random experiments.\nConsequently, it may be used for predicting a response of the DL algorithm in\nterms of a selected quality measurement to a set of hyper-parameters. This\nallows to construct an ensemble composed of a series of evaluators which\nconstitute an observer-assisted architecture. The architecture may be used to\ngradually iterate towards to the best achievable quality score in tiny steps\ngoverned by a unit of progress. The algorithm is stopped when the maximum\nnumber of steps is reached or no further progress is made.\n","negative":"  This work presents a multiscale framework to solve an inverse reinforcement\nlearning (IRL) problem for continuous-time\/state stochastic systems. We take\nadvantage of a diffusion wavelet representation of the associated Markov chain\nto abstract the state space. This not only allows for effectively handling the\nlarge (and geometrically complex) decision space but also provides more\ninterpretable representations of the demonstrated state trajectories and also\nof the resulting policy of IRL. In the proposed framework, the problem is\ndivided into the global and local IRL, where the global approximation of the\noptimal value functions are obtained using coarse features and the local\ndetails are quantified using fine local features. An illustrative numerical\nexample on robot path control in a complex environment is presented to verify\nthe proposed method.\n","id":417}
{"Unnamed: 0.1":11418,"Unnamed: 0":11418.0,"anchor":"SLA Violation Prediction In Cloud Computing: A Machine Learning\n  Perspective","positive":"  Service level agreement (SLA) is an essential part of cloud systems to ensure\nmaximum availability of services for customers. With a violation of SLA, the\nprovider has to pay penalties. In this paper, we explore two machine learning\nmodels: Naive Bayes and Random Forest Classifiers to predict SLA violations.\nSince SLA violations are a rare event in the real world (~0.2 %), the\nclassification task becomes more challenging. In order to overcome these\nchallenges, we use several re-sampling methods. We find that random forests\nwith SMOTE-ENN re-sampling have the best performance among other methods with\nthe accuracy of 99.88 % and F_1 score of 0.9980.\n","negative":"  In this paper, we study the statistical behaviour of the Exponentially\nWeighted Aggregate (EWA) in the problem of high-dimensional regression with\nfixed design. Under the assumption that the underlying regression vector is\nsparse, it is reasonable to use the Laplace distribution as a prior. The\nresulting estimator and, specifically, a particular instance of it referred to\nas the Bayesian lasso, was already used in the statistical literature because\nof its computational convenience, even though no thorough mathematical analysis\nof its statistical properties was carried out. The present work fills this gap\nby establishing sharp oracle inequalities for the EWA with the Laplace prior.\nThese inequalities show that if the temperature parameter is small, the EWA\nwith the Laplace prior satisfies the same type of oracle inequality as the\nlasso estimator does, as long as the quality of estimation is measured by the\nprediction loss. Extensions of the proposed methodology to the problem of\nprediction with low-rank matrices are considered.\n","id":418}
{"Unnamed: 0.1":11419,"Unnamed: 0":11419.0,"anchor":"Joint Causal Inference from Multiple Contexts","positive":"  The gold standard for discovering causal relations is by means of\nexperimentation. Over the last decades, alternative methods have been proposed\nthat can infer causal relations between variables from certain statistical\npatterns in purely observational data. We introduce Joint Causal Inference\n(JCI), a novel approach to causal discovery from multiple data sets from\ndifferent contexts that elegantly unifies both approaches. JCI is a causal\nmodeling framework rather than a specific algorithm, and it can be implemented\nusing any causal discovery algorithm that can take into account certain\nbackground knowledge. JCI can deal with different types of interventions (e.g.,\nperfect, imperfect, stochastic, etc.) in a unified fashion, and does not\nrequire knowledge of intervention targets or types in case of interventional\ndata. We explain how several well-known causal discovery algorithms can be seen\nas addressing special cases of the JCI framework, and we also propose novel\nimplementations that extend existing causal discovery methods for purely\nobservational data to the JCI setting. We evaluate different JCI\nimplementations on synthetic data and on flow cytometry protein expression data\nand conclude that JCI implementations can considerably outperform\nstate-of-the-art causal discovery algorithms.\n","negative":"  In rare disease physician targeting, a major challenge is how to identify\nphysicians who are treating diagnosed or underdiagnosed rare diseases patients.\nRare diseases have extremely low incidence rate. For a specified rare disease,\nonly a small number of patients are affected and a fractional of physicians are\ninvolved. The existing targeting methodologies, such as segmentation and\nprofiling, are developed under mass market assumption. They are not suitable\nfor rare disease market where the target classes are extremely imbalanced. The\nauthors propose a graphical model approach to predict targets by jointly\nmodeling physician and patient features from different data spaces and\nutilizing the extra relational information. Through an empirical example with\nmedical claim and prescription data, the proposed approach demonstrates better\naccuracy in finding target physicians. The graph representation also provides\nvisual interpretability of relationship among physicians and patients. The\nmodel can be extended to incorporate more complex dependency structures. This\narticle contributes to the literature of exploring the benefit of utilizing\nrelational dependencies among entities in healthcare industry.\n","id":419}
{"Unnamed: 0.1":11420,"Unnamed: 0":11420.0,"anchor":"Semi-supervised Kernel Metric Learning Using Relative Comparisons","positive":"  We consider the problem of metric learning subject to a set of constraints on\nrelative-distance comparisons between the data items. Such constraints are\nmeant to reflect side-information that is not expressed directly in the feature\nvectors of the data items. The relative-distance constraints used in this work\nare particularly effective in expressing structures at finer level of detail\nthan must-link (ML) and cannot-link (CL) constraints, which are most commonly\nused for semi-supervised clustering. Relative-distance constraints are thus\nuseful in settings where providing an ML or a CL constraint is difficult\nbecause the granularity of the true clustering is unknown.\n  Our main contribution is an efficient algorithm for learning a kernel matrix\nusing the log determinant divergence --- a variant of the Bregman divergence\n--- subject to a set of relative-distance constraints. The learned kernel\nmatrix can then be employed by many different kernel methods in a wide range of\napplications. In our experimental evaluations, we consider a semi-supervised\nclustering setting and show empirically that kernels found by our algorithm\nyield clusterings of higher quality than existing approaches that either use\nML\/CL constraints or a different means to implement the supervision using\nrelative comparisons.\n","negative":"  At the core of interpretable machine learning is the question of whether\nhumans are able to make accurate predictions about a model's behavior. Assumed\nin this question are three properties of the interpretable output: coverage,\nprecision, and effort. Coverage refers to how often humans think they can\npredict the model's behavior, precision to how accurate humans are in those\npredictions, and effort is either the up-front effort required in interpreting\nthe model, or the effort required to make predictions about a model's behavior.\n  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that\nproduces high-precision rule-based explanations for which the coverage\nboundaries are very clear. We compare aLIME to linear LIME with simulated\nexperiments, and demonstrate the flexibility of aLIME with qualitative examples\nfrom a variety of domains and tasks.\n","id":420}
{"Unnamed: 0.1":11421,"Unnamed: 0":11421.0,"anchor":"Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling","positive":"  We study the problem of recovering an incomplete $m\\times n$ matrix of rank\n$r$ with columns arriving online over time. This is known as the problem of\nlife-long matrix completion, and is widely applied to recommendation system,\ncomputer vision, system identification, etc. The challenge is to design\nprovable algorithms tolerant to a large amount of noises, with small sample\ncomplexity. In this work, we give algorithms achieving strong guarantee under\ntwo realistic noise models. In bounded deterministic noise, an adversary can\nadd any bounded yet unstructured noise to each column. For this problem, we\npresent an algorithm that returns a matrix of a small error, with sample\ncomplexity almost as small as the best prior results in the noiseless case. For\nsparse random noise, where the corrupted columns are sparse and drawn randomly,\nwe give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by\nprobability at least $1-\\delta$ with sample complexity as small as\n$O\\left(\\mu_0rn\\log (r\/\\delta)\\right)$. This result advances the\nstate-of-the-art work and matches the lower bound in a worst case. We also\nstudy the scenario where the hidden matrix lies on a mixture of subspaces and\nshow that the sample complexity can be even smaller. Our proposed algorithms\nperform well experimentally in both synthetic and real-world datasets.\n","negative":"  Learning with Fredholm kernel has attracted increasing attention recently\nsince it can effectively utilize the data information to improve the prediction\nperformance. Despite rapid progress on theoretical and experimental\nevaluations, its generalization analysis has not been explored in learning\ntheory literature. In this paper, we establish the generalization bound of\nleast square regularized regression with Fredholm kernel, which implies that\nthe fast learning rate O(l^{-1}) can be reached under mild capacity conditions.\nSimulated examples show that this Fredholm regression algorithm can achieve the\nsatisfactory prediction performance.\n","id":421}
{"Unnamed: 0.1":11422,"Unnamed: 0":11422.0,"anchor":"When to Reset Your Keys: Optimal Timing of Security Updates via Learning","positive":"  Cybersecurity is increasingly threatened by advanced and persistent attacks.\nAs these attacks are often designed to disable a system (or a critical\nresource, e.g., a user account) repeatedly, it is crucial for the defender to\nkeep updating its security measures to strike a balance between the risk of\nbeing compromised and the cost of security updates. Moreover, these decisions\noften need to be made with limited and delayed feedback due to the stealthy\nnature of advanced attacks. In addition to targeted attacks, such an optimal\ntiming policy under incomplete information has broad applications in\ncybersecurity. Examples include key rotation, password change, application of\npatches, and virtual machine refreshing. However, rigorous studies of optimal\ntiming are rare. Further, existing solutions typically rely on a pre-defined\nattack model that is known to the defender, which is often not the case in\npractice. In this work, we make an initial effort towards achieving optimal\ntiming of security updates in the face of unknown stealthy attacks. We consider\na variant of the influential FlipIt game model with asymmetric feedback and\nunknown attack time distribution, which provides a general model to consecutive\nsecurity updates. The defender's problem is then modeled as a time associative\nbandit problem with dependent arms. We derive upper confidence bound based\nlearning policies that achieve low regret compared with optimal periodic\ndefense strategies that can only be derived when attack time distributions are\nknown.\n","negative":"  We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS.\n","id":422}
{"Unnamed: 0.1":11423,"Unnamed: 0":11423.0,"anchor":"A New Method for Classification of Datasets for Data Mining","positive":"  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification\/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n","negative":"  Our ability to synthesize sensory data that preserves specific statistical\nproperties of the real data has had tremendous implications on data privacy and\nbig data analytics. The synthetic data can be used as a substitute for\nselective real data segments,that are sensitive to the user, thus protecting\nprivacy and resulting in improved analytics.However, increasingly adversarial\nroles taken by data recipients such as mobile apps, or other cloud-based\nanalytics services, mandate that the synthetic data, in addition to preserving\nstatistical properties, should also be difficult to distinguish from the real\ndata. Typically, visual inspection has been used as a test to distinguish\nbetween datasets. But more recently, sophisticated classifier models\n(discriminators), corresponding to a set of events, have also been employed to\ndistinguish between synthesized and real data. The model operates on both\ndatasets and the respective event outputs are compared for consistency. In this\npaper, we take a step towards generating sensory data that can pass a deep\nlearning based discriminator model test, and make two specific contributions:\nfirst, we present a deep learning based architecture for synthesizing sensory\ndata. This architecture comprises of a generator model, which is a stack of\nmultiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network.\nsecond, we use another LSTM network based discriminator model for\ndistinguishing between the true and the synthesized data. Using a dataset of\naccelerometer traces, collected using smartphones of users doing their daily\nactivities, we show that the deep learning based discriminator model can only\ndistinguish between the real and synthesized traces with an accuracy in the\nneighborhood of 50%.\n","id":423}
{"Unnamed: 0.1":11424,"Unnamed: 0":11424.0,"anchor":"Adversarial Images for Variational Autoencoders","positive":"  We investigate adversarial attacks for autoencoders. We propose a procedure\nthat distorts the input image to mislead the autoencoder in reconstructing a\ncompletely different target image. We attack the internal latent\nrepresentations, attempting to make the adversarial input produce an internal\nrepresentation as similar as possible as the target's. We find that\nautoencoders are much more robust to the attack than classifiers: while some\nexamples have tolerably small input distortion, and reasonable similarity to\nthe target image, there is a quasi-linear trade-off between those aims. We\nreport results on MNIST and SVHN datasets, and also test regular deterministic\nautoencoders, reaching similar conclusions in all cases. Finally, we show that\nthe usual adversarial attack for classifiers, while being much easier, also\npresents a direct proportion between distortion on the input, and misdirection\non the output. That proportionality however is hidden by the normalization of\nthe output, which maps a linear layer into non-linear probabilities.\n","negative":"  Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA.\n","id":424}
{"Unnamed: 0.1":11425,"Unnamed: 0":11425.0,"anchor":"Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using\n  Householder Reflections","positive":"  The problem of learning long-term dependencies in sequences using Recurrent\nNeural Networks (RNNs) is still a major challenge. Recent methods have been\nsuggested to solve this problem by constraining the transition matrix to be\nunitary during training which ensures that its norm is equal to one and\nprevents exploding gradients. These methods either have limited expressiveness\nor scale poorly with the size of the network when compared with the simple RNN\ncase, especially when using stochastic gradient descent with a small mini-batch\nsize. Our contributions are as follows; we first show that constraining the\ntransition matrix to be unitary is a special case of an orthogonal constraint.\nThen we present a new parametrisation of the transition matrix which allows\nefficient training of an RNN while ensuring that the matrix is always\northogonal. Our results show that the orthogonal constraint on the transition\nmatrix applied through our parametrisation gives similar benefits to the\nunitary constraint, without the time complexity limitations.\n","negative":"  The use of deep reinforcement learning allows for high-dimensional state\ndescriptors, but little is known about how the choice of action representation\nimpacts the learning difficulty and the resulting performance. We compare the\nimpact of four different action parameterizations (torques, muscle-activations,\ntarget joint angles, and target joint-angle velocities) in terms of learning\ntime, policy robustness, motion quality, and policy query rates. Our results\nare evaluated on a gait-cycle imitation task for multiple planar articulated\nfigures and multiple gaits. We demonstrate that the local feedback provided by\nhigher-level action parameterizations can significantly impact the learning,\nrobustness, and quality of the resulting policies.\n","id":425}
{"Unnamed: 0.1":11426,"Unnamed: 0":11426.0,"anchor":"Learning molecular energies using localized graph kernels","positive":"  Recent machine learning methods make it possible to model potential energy of\natomic configurations with chemical-level accuracy (as calculated from\nab-initio calculations) and at speeds suitable for molecular dynam- ics\nsimulation. Best performance is achieved when the known physical constraints\nare encoded in the machine learning models. For example, the atomic energy is\ninvariant under global translations and rotations, it is also invariant to\npermutations of same-species atoms. Although simple to state, these symmetries\nare complicated to encode into machine learning algorithms. In this paper, we\npresent a machine learning approach based on graph theory that naturally\nincorporates translation, rotation, and permutation symmetries. Specifically,\nwe use a random walk graph kernel to measure the similarity of two adjacency\nmatrices, each of which represents a local atomic environment. This Graph\nApproximated Energy (GRAPE) approach is flexible and admits many possible\nextensions. We benchmark a simple version of GRAPE by predicting atomization\nenergies on a standard dataset of organic molecules.\n","negative":"  While much research effort has been dedicated to scaling up sparse Gaussian\nprocess (GP) models based on inducing variables for big data, little attention\nis afforded to the other less explored class of low-rank GP approximations that\nexploit the sparse spectral representation of a GP kernel. This paper presents\nsuch an effort to advance the state of the art of sparse spectrum GP models to\nachieve competitive predictive performance for massive datasets. Our\ngeneralized framework of stochastic variational Bayesian sparse spectrum GP\n(sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment\nof the spectral frequencies to avoid overfitting, modeling these frequencies\njointly in its variational distribution to enable their interaction a\nposteriori, and exploiting local data for boosting the predictive performance.\nHowever, such structural improvements result in a variational lower bound that\nis intractable to be optimized. To resolve this, we exploit a variational\nparameterization trick to make it amenable to stochastic optimization.\nInterestingly, the resulting stochastic gradient has a linearly decomposable\nstructure that can be exploited to refine our stochastic optimization method to\nincur constant time per iteration while preserving its property of being an\nunbiased estimator of the exact gradient of the variational lower bound.\nEmpirical evaluation on real-world datasets shows that sVBSSGP outperforms\nstate-of-the-art stochastic implementations of sparse GP models.\n","id":426}
{"Unnamed: 0.1":11427,"Unnamed: 0":11427.0,"anchor":"Training Bit Fully Convolutional Network for Fast Semantic Segmentation","positive":"  Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart.\n","negative":"  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word\/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n","id":427}
{"Unnamed: 0.1":11428,"Unnamed: 0":11428.0,"anchor":"The Coconut Model with Heterogeneous Strategies and Learning","positive":"  In this paper, we develop an agent-based version of the Diamond search\nequilibrium model - also called Coconut Model. In this model, agents are faced\nwith production decisions that have to be evaluated based on their expectations\nabout the future utility of the produced entity which in turn depends on the\nglobal production level via a trading mechanism. While the original dynamical\nsystems formulation assumes an infinite number of homogeneously adapting agents\nobeying strong rationality conditions, the agent-based setting allows to\ndiscuss the effects of heterogeneous and adaptive expectations and enables the\nanalysis of non-equilibrium trajectories. Starting from a baseline\nimplementation that matches the asymptotic behavior of the original model, we\nshow how agent heterogeneity can be accounted for in the aggregate dynamical\nequations. We then show that when agents adapt their strategies by a simple\ntemporal difference learning scheme, the system converges to one of the fixed\npoints of the original system. Systematic simulations reveal that this is the\nonly stable equilibrium solution.\n","negative":"  Credit card plays a very important rule in today's economy. It becomes an\nunavoidable part of household, business and global activities. Although using\ncredit cards provides enormous benefits when used carefully and\nresponsibly,significant credit and financial damages may be caused by\nfraudulent activities. Many techniques have been proposed to confront the\ngrowth in credit card fraud. However, all of these techniques have the same\ngoal of avoiding the credit card fraud; each one has its own drawbacks,\nadvantages and characteristics. In this paper, after investigating difficulties\nof credit card fraud detection, we seek to review the state of the art in\ncredit card fraud detection techniques, data sets and evaluation criteria.The\nadvantages and disadvantages of fraud detection methods are enumerated and\ncompared.Furthermore, a classification of mentioned techniques into two main\nfraud detection approaches, namely, misuses (supervised) and anomaly detection\n(unsupervised) is presented. Again, a classification of techniques is proposed\nbased on capability to process the numerical and categorical data sets.\nDifferent data sets used in literature are then described and grouped into real\nand synthesized data and the effective and common attributes are extracted for\nfurther usage.Moreover, evaluation employed criterions in literature are\ncollected and discussed.Consequently, open issues for credit card fraud\ndetection are explained as guidelines for new researchers.\n","id":428}
{"Unnamed: 0.1":11429,"Unnamed: 0":11429.0,"anchor":"Interaction Networks for Learning about Objects, Relations and Physics","positive":"  Reasoning about objects, relations, and physics is central to human\nintelligence, and a key goal of artificial intelligence. Here we introduce the\ninteraction network, a model which can reason about how objects in complex\nsystems interact, supporting dynamical predictions, as well as inferences about\nthe abstract properties of the system. Our model takes graphs as input,\nperforms object- and relation-centric reasoning in a way that is analogous to a\nsimulation, and is implemented using deep neural networks. We evaluate its\nability to reason about several challenging physical domains: n-body problems,\nrigid-body collision, and non-rigid dynamics. Our results show it can be\ntrained to accurately simulate the physical trajectories of dozens of objects\nover thousands of time steps, estimate abstract quantities such as energy, and\ngeneralize automatically to systems with different numbers and configurations\nof objects and relations. Our interaction network implementation is the first\ngeneral-purpose, learnable physics engine, and a powerful general framework for\nreasoning about object and relations in a wide variety of complex real-world\ndomains.\n","negative":"  We propose a technique for making Convolutional Neural Network (CNN)-based\nmodels more transparent by visualizing input regions that are 'important' for\npredictions -- or visual explanations. Our approach, called Gradient-weighted\nClass Activation Mapping (Grad-CAM), uses class-specific gradient information\nto localize important regions. These localizations are combined with existing\npixel-space visualizations to create a novel high-resolution and\nclass-discriminative visualization called Guided Grad-CAM. These methods help\nbetter understand CNN-based models, including image captioning and visual\nquestion answering (VQA) models. We evaluate our visual explanations by\nmeasuring their ability to discriminate between classes, to inspire trust in\nhumans, and their correlation with occlusion maps. Grad-CAM provides a new way\nto understand CNN-based models.\n  We have released code, an online demo hosted on CloudCV, and a full version\nof this extended abstract.\n","id":429}
{"Unnamed: 0.1":11430,"Unnamed: 0":11430.0,"anchor":"A Theoretical Framework for Robustness of (Deep) Classifiers against\n  Adversarial Examples","positive":"  Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.\n","negative":"  AUC (Area under the ROC curve) is an important performance measure for\napplications where the data is highly imbalanced. Learning to maximize AUC\nperformance is thus an important research problem. Using a max-margin based\nsurrogate loss function, AUC optimization problem can be approximated as a\npairwise rankSVM learning problem. Batch learning methods for solving the\nkernelized version of this problem suffer from scalability and may not result\nin sparse classifiers. Recent years have witnessed an increased interest in the\ndevelopment of online or single-pass online learning algorithms that design a\nclassifier by maximizing the AUC performance. The AUC performance of nonlinear\nclassifiers, designed using online methods, is not comparable with that of\nnonlinear classifiers designed using batch learning algorithms on many\nreal-world datasets. Motivated by these observations, we design a scalable\nalgorithm for maximizing AUC performance by greedily adding the required number\nof basis functions into the classifier model. The resulting sparse classifiers\nperform faster inference. Our experimental results show that the level of\nsparsity achievable can be order of magnitude smaller than the Kernel RankSVM\nmodel without affecting the AUC performance much.\n","id":430}
{"Unnamed: 0.1":11431,"Unnamed: 0":11431.0,"anchor":"A Compositional Object-Based Approach to Learning Physical Dynamics","positive":"  We present the Neural Physics Engine (NPE), a framework for learning\nsimulators of intuitive physics that naturally generalize across variable\nobject count and different scene configurations. We propose a factorization of\na physical scene into composable object-based representations and a neural\nnetwork architecture whose compositional structure factorizes object dynamics\ninto pairwise interactions. Like a symbolic physics engine, the NPE is endowed\nwith generic notions of objects and their interactions; realized as a neural\nnetwork, it can be trained via stochastic gradient descent to adapt to specific\nobject properties and dynamics of different worlds. We evaluate the efficacy of\nour approach on simple rigid body dynamics in two-dimensional worlds. By\ncomparing to less structured architectures, we show that the NPE's\ncompositional representation of the structure in physical interactions improves\nits ability to predict movement, generalize across variable object count and\ndifferent scene configurations, and infer latent properties of objects such as\nmass.\n","negative":"  Representation of human actions as a sequence of human body movements or\naction attributes enables the development of models for human activity\nrecognition and summarization. We present an extension of the low-rank\nrepresentation (LRR) model, termed the clustering-aware structure-constrained\nlow-rank representation (CS-LRR) model, for unsupervised learning of human\naction attributes from video data. Our model is based on the union-of-subspaces\n(UoS) framework, and integrates spectral clustering into the LRR optimization\nproblem for better subspace clustering results. We lay out an efficient linear\nalternating direction method to solve the CS-LRR optimization problem. We also\nintroduce a hierarchical subspace clustering approach, termed hierarchical\nCS-LRR, to learn the attributes without the need for a priori specification of\ntheir number. By visualizing and labeling these action attributes, the\nhierarchical model can be used to semantically summarize long video sequences\nof human actions at multiple resolutions. A human action or activity can also\nbe uniquely represented as a sequence of transitions from one action attribute\nto another, which can then be used for human action recognition. We demonstrate\nthe effectiveness of the proposed model for semantic summarization and action\nrecognition through comprehensive experiments on five real-world human action\ndatasets.\n","id":431}
{"Unnamed: 0.1":11432,"Unnamed: 0":11432.0,"anchor":"Large-scale Validation of Counterfactual Learning Methods: A Test-Bed","positive":"  The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set.\n","negative":"  Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification.\n","id":432}
{"Unnamed: 0.1":11433,"Unnamed: 0":11433.0,"anchor":"Spatial Decompositions for Large Scale SVMs","positive":"  Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017).\n","negative":"  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n","id":433}
{"Unnamed: 0.1":11434,"Unnamed: 0":11434.0,"anchor":"Piecewise Latent Variables for Neural Variational Text Processing","positive":"  Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.\n","negative":"  Discovering causal models from observational and interventional data is an\nimportant first step preceding what-if analysis or counterfactual reasoning. As\nhas been shown before, the direction of pairwise causal relations can, under\ncertain conditions, be inferred from observational data via standard\ngradient-boosted classifiers (GBC) using carefully engineered statistical\nfeatures. In this paper we apply deep convolutional neural networks (CNNs) to\nthis problem by plotting attribute pairs as 2-D scatter plots that are fed to\nthe CNN as images. We evaluate our approach on the 'Cause- Effect Pairs' NIPS\n2013 Data Challenge. We observe that a weighted ensemble of CNN with the\nearlier GBC approach yields significant improvement. Further, we observe that\nwhen less training data is available, our approach performs better than the GBC\nbased approach suggesting that CNN models pre-trained to determine the\ndirection of pairwise causal direction could have wider applicability in causal\ndiscovery and enabling what-if or counterfactual analysis.\n","id":434}
{"Unnamed: 0.1":11435,"Unnamed: 0":11435.0,"anchor":"Tuning the Scheduling of Distributed Stochastic Gradient Descent with\n  Bayesian Optimization","positive":"  We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X.\n","negative":"  We present probabilistic neural programs, a framework for program induction\nthat permits flexible specification of both a computational model and inference\nalgorithm while simultaneously enabling the use of deep neural networks.\nProbabilistic neural programs combine a computation graph for specifying a\nneural network with an operator for weighted nondeterministic choice. Thus, a\nprogram describes both a collection of decisions as well as the neural network\narchitecture used to make each one. We evaluate our approach on a challenging\ndiagram question answering task where probabilistic neural programs correctly\nexecute nearly twice as many programs as a baseline model.\n","id":435}
{"Unnamed: 0.1":11436,"Unnamed: 0":11436.0,"anchor":"Diet2Vec: Multi-scale analysis of massive dietary data","positive":"  Smart phone apps that enable users to easily track their diets have become\nwidespread in the last decade. This has created an opportunity to discover new\ninsights into obesity and weight loss by analyzing the eating habits of the\nusers of such apps. In this paper, we present diet2vec: an approach to modeling\nlatent structure in a massive database of electronic diet journals. Through an\niterative contract-and-expand process, our model learns real-valued embeddings\nof users' diets, as well as embeddings for individual foods and meals. We\ndemonstrate the effectiveness of our approach on a real dataset of 55K users of\nthe popular diet-tracking app LoseIt\\footnote{http:\/\/www.loseit.com\/}. To the\nbest of our knowledge, this is the largest fine-grained diet tracking study in\nthe history of nutrition and obesity research. Our results suggest that\ndiet2vec finds interpretable results at all levels, discovering intuitive\nrepresentations of foods, meals, and diets.\n","negative":"  Many real-world systems can be studied in terms of pattern recognition tasks,\nso that proper use (and understanding) of machine learning methods in practical\napplications becomes essential. While a myriad of classification methods have\nbeen proposed, there is no consensus on which methods are more suitable for a\ngiven dataset. As a consequence, it is important to comprehensively compare\nmethods in many possible scenarios. In this context, we performed a systematic\ncomparison of 7 well-known clustering methods available in the R language. In\norder to account for the many possible variations of data, we considered\nartificial datasets with several tunable properties (number of classes,\nseparation between classes, etc). In addition, we also evaluated the\nsensitivity of the clustering methods with regard to their parameters\nconfiguration. The results revealed that, when considering the default\nconfigurations of the adopted methods, the spectral approach usually\noutperformed the other clustering algorithms. We also found that the default\nconfiguration of the adopted implementations was not accurate. In these cases,\na simple approach based on random selection of parameters values proved to be a\ngood alternative to improve the performance. All in all, the reported approach\nprovides subsidies guiding the choice of clustering algorithms.\n","id":436}
{"Unnamed: 0.1":11437,"Unnamed: 0":11437.0,"anchor":"Hypervolume-based Multi-objective Bayesian Optimization with Student-t\n  Processes","positive":"  Student-$t$ processes have recently been proposed as an appealing alternative\nnon-parameteric function prior. They feature enhanced flexibility and\npredictive variance. In this work the use of Student-$t$ processes are explored\nfor multi-objective Bayesian optimization. In particular, an analytical\nexpression for the hypervolume-based probability of improvement is developed\nfor independent Student-$t$ process priors of the objectives. Its effectiveness\nis shown on a multi-objective optimization problem which is known to be\ndifficult with traditional Gaussian processes.\n","negative":"  We analyze a new approach to Machine Learning coming from a modification of\nclassical regularization networks by casting the process in the time dimension,\nleading to a sort of collapse of dimensionality in the problem of learning the\nmodel parameters. This approach allows the definition of a online learning\nalgorithm that progressively accumulates the knowledge provided in the input\ntrajectory. The regularization principle leads to a solution based on a\ndynamical system that is paired with a procedure to develop a graph structure\nthat stores the input regularities acquired from the temporal evolution. We\nreport an extensive experimental exploration on the behavior of the parameter\nof the proposed model and an evaluation on artificial dataset.\n","id":437}
{"Unnamed: 0.1":11438,"Unnamed: 0":11438.0,"anchor":"Deep Variational Information Bottleneck","positive":"  We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.\n","negative":"  In this paper we introduce a fully end-to-end approach for visual tracking in\nvideos that learns to predict the bounding box locations of a target object at\nevery frame. An important insight is that the tracking problem can be\nconsidered as a sequential decision-making process and historical semantics\nencode highly relevant information for future decisions. Based on this\nintuition, we formulate our model as a recurrent convolutional neural network\nagent that interacts with a video overtime, and our model can be trained with\nreinforcement learning (RL) algorithms to learn good tracking policies that pay\nattention to continuous, inter-frame correlation and maximize tracking\nperformance in the long run. The proposed tracking algorithm achieves\nstate-of-the-art performance in an existing tracking benchmark and operates at\nframe-rates faster than real-time. To the best of our knowledge, our tracker is\nthe first neural-network tracker that combines convolutional and recurrent\nnetworks with RL algorithms.\n","id":438}
{"Unnamed: 0.1":11439,"Unnamed: 0":11439.0,"anchor":"Generalizing Skills with Semi-Supervised Reinforcement Learning","positive":"  Deep reinforcement learning (RL) can acquire complex behaviors from low-level\ninputs, such as images. However, real-world applications of such methods\nrequire generalizing to the vast variability of the real world. Deep networks\nare known to achieve remarkable generalization when provided with massive\namounts of labeled data, but can we provide this breadth of experience to an RL\nagent, such as a robot? The robot might continuously learn as it explores the\nworld around it, even while deployed. However, this learning requires access to\na reward function, which is often hard to measure in real-world domains, where\nthe reward could depend on, for example, unknown positions of objects or the\nemotional state of the user. Conversely, it is often quite practical to provide\nthe agent with reward functions in a limited set of situations, such as when a\nhuman supervisor is present or in a controlled setting. Can we make use of this\nlimited supervision, and still benefit from the breadth of experience an agent\nmight collect on its own? In this paper, we formalize this problem as\nsemisupervised reinforcement learning, where the reward function can only be\nevaluated in a set of \"labeled\" MDPs, and the agent must generalize its\nbehavior to the wide range of states it might encounter in a set of \"unlabeled\"\nMDPs, by using experience from both settings. Our proposed method infers the\ntask objective in the unlabeled MDPs through an algorithm that resembles\ninverse RL, using the agent's own prior experience in the labeled MDPs as a\nkind of demonstration of optimal behavior. We evaluate our method on\nchallenging tasks that require control directly from images, and show that our\napproach can improve the generalization of a learned deep neural network policy\nby using experience for which no reward function is available. We also show\nthat our method outperforms direct supervised learning of the reward.\n","negative":"  In this paper we consider the problem of robot navigation in simple maze-like\nenvironments where the robot has to rely on its onboard sensors to perform the\nnavigation task. In particular, we are interested in solutions to this problem\nthat do not require localization, mapping or planning. Additionally, we require\nthat our solution can quickly adapt to new situations (e.g., changing\nnavigation goals and environments). To meet these criteria we frame this\nproblem as a sequence of related reinforcement learning tasks. We propose a\nsuccessor feature based deep reinforcement learning algorithm that can learn to\ntransfer knowledge from previously mastered navigation tasks to new problem\ninstances. Our algorithm substantially decreases the required learning time\nafter the first task instance has been solved, which makes it easily adaptable\nto changing environments. We validate our method in both simulated and real\nrobot experiments with a Robotino and compare it to a set of baseline methods\nincluding classical planning-based navigation.\n","id":439}
{"Unnamed: 0.1":11440,"Unnamed: 0":11440.0,"anchor":"Transfer Learning Across Patient Variations with Hidden Parameter Markov\n  Decision Processes","positive":"  Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment.\n","negative":"  Co-clustering targets on grouping the samples (e.g., documents, users) and\nthe features (e.g., words, ratings) simultaneously. It employs the dual\nrelation and the bilateral information between the samples and features. In\nmany realworld applications, data usually reside on a submanifold of the\nambient Euclidean space, but it is nontrivial to estimate the intrinsic\nmanifold of the data space in a principled way. In this study, we focus on\nimproving the co-clustering performance via manifold ensemble learning, which\nis able to maximally approximate the intrinsic manifolds of both the sample and\nfeature spaces. To achieve this, we develop a novel co-clustering algorithm\ncalled Relational Multi-manifold Co-clustering (RMC) based on symmetric\nnonnegative matrix tri-factorization, which decomposes the relational data\nmatrix into three submatrices. This method considers the intertype relationship\nrevealed by the relational data matrix, and also the intra-type information\nreflected by the affinity matrices encoded on the sample and feature data\ndistributions. Specifically, we assume the intrinsic manifold of the sample or\nfeature space lies in a convex hull of some pre-defined candidate manifolds. We\nwant to learn a convex combination of them to maximally approach the desired\nintrinsic manifold. To optimize the objective function, the multiplicative\nrules are utilized to update the submatrices alternatively. Besides, both the\nentropic mirror descent algorithm and the coordinate descent algorithm are\nexploited to learn the manifold coefficient vector. Extensive experiments on\ndocuments, images and gene expression data sets have demonstrated the\nsuperiority of the proposed algorithm compared to other well-established\nmethods.\n","id":440}
{"Unnamed: 0.1":11441,"Unnamed: 0":11441.0,"anchor":"Canonical Correlation Analysis for Analyzing Sequences of Medical\n  Billing Codes","positive":"  We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare.\n","negative":"  We study the $\\ell_1$-low rank approximation problem, where for a given $n\n\\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to\noutput a rank-$k$ matrix $\\widehat{A}$ for which\n  $$\\|A-\\widehat{A}\\|_1 \\leq \\alpha \\cdot \\min_{\\textrm{rank-}k\\textrm{\nmatrices}~A'}\\|A-A'\\|_1,$$ where for an $n \\times d$ matrix $C$, we let\n$\\|C\\|_1 = \\sum_{i=1}^n \\sum_{j=1}^d |C_{i,j}|$. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is\nindicated in models where Gaussian assumptions on the noise may not apply. The\nproblem was shown to be NP-hard by Gillis and Vavasis and a number of\nheuristics have been proposed. It was asked in multiple places if there are any\napproximation algorithms.\n  We give the first provable approximation algorithms for $\\ell_1$-low rank\napproximation, showing that it is possible to achieve approximation factor\n$\\alpha = (\\log d) \\cdot \\mathrm{poly}(k)$ in $\\mathrm{nnz}(A) + (n+d)\n\\mathrm{poly}(k)$ time, where $\\mathrm{nnz}(A)$ denotes the number of non-zero\nentries of $A$. If $k$ is constant, we further improve the approximation ratio\nto $O(1)$ with a $\\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time\nHypothesis, we show there is no $\\mathrm{poly}(nd)$-time algorithm achieving a\n$(1+\\frac{1}{\\log^{1+\\gamma}(nd)})$-approximation, for $\\gamma > 0$ an\narbitrarily small constant, even when $k = 1$.\n  We give a number of additional results for $\\ell_1$-low rank approximation:\nnearly tight upper and lower bounds for column subset selection, CUR\ndecompositions, extensions to low rank approximation with respect to\n$\\ell_p$-norms for $1 \\leq p < 2$ and earthmover distance, low-communication\ndistributed protocols and low-memory streaming algorithms, algorithms with\nlimited randomness, and bicriteria algorithms. We also give a preliminary\nempirical evaluation.\n","id":441}
{"Unnamed: 0.1":11442,"Unnamed: 0":11442.0,"anchor":"A Noise-Filtering Approach for Cancer Drug Sensitivity Prediction","positive":"  Accurately predicting drug responses to cancer is an important problem\nhindering oncologists' efforts to find the most effective drugs to treat\ncancer, which is a core goal in precision medicine. The scientific community\nhas focused on improving this prediction based on genomic, epigenomic, and\nproteomic datasets measured in human cancer cell lines. Real-world cancer cell\nlines contain noise, which degrades the performance of machine learning\nalgorithms. This problem is rarely addressed in the existing approaches. In\nthis paper, we present a noise-filtering approach that integrates techniques\nfrom numerical linear algebra and information retrieval targeted at filtering\nout noisy cancer cell lines. By filtering out noisy cancer cell lines, we can\ntrain machine learning algorithms on better quality cancer cell lines. We\nevaluate the performance of our approach and compare it with an existing\napproach using the Area Under the ROC Curve (AUC) on clinical trial data. The\nexperimental results show that our proposed approach is stable and also yields\nthe highest AUC at a statistically significant level.\n","negative":"  We present a new class of decentralized first-order methods for nonsmooth and\nstochastic optimization problems defined over multiagent networks. Considering\nthat communication is a major bottleneck in decentralized optimization, our\nmain goal in this paper is to develop algorithmic frameworks which can\nsignificantly reduce the number of inter-node communications. We first propose\na decentralized primal-dual method which can find an $\\epsilon$-solution both\nin terms of functional optimality gap and feasibility residual in\n$O(1\/\\epsilon)$ inter-node communication rounds when the objective functions\nare convex and the local primal subproblems are solved exactly. Our major\ncontribution is to present a new class of decentralized primal-dual type\nalgorithms, namely the decentralized communication sliding (DCS) methods, which\ncan skip the inter-node communications while agents solve the primal\nsubproblems iteratively through linearizations of their local objective\nfunctions. By employing DCS, agents can still find an $\\epsilon$-solution in\n$O(1\/\\epsilon)$ (resp., $O(1\/\\sqrt{\\epsilon})$) communication rounds for\ngeneral convex functions (resp., strongly convex functions), while maintaining\nthe $O(1\/\\epsilon^2)$ (resp., $O(1\/\\epsilon)$) bound on the total number of\nintra-node subgradient evaluations. We also present a stochastic counterpart\nfor these algorithms, denoted by SDCS, for solving stochastic optimization\nproblems whose objective function cannot be evaluated exactly. In comparison\nwith existing results for decentralized nonsmooth and stochastic optimization,\nwe can reduce the total number of inter-node communication rounds by orders of\nmagnitude while still maintaining the optimal complexity bounds on intra-node\nstochastic subgradient evaluations. The bounds on the subgradient evaluations\nare actually comparable to those required for centralized nonsmooth and\nstochastic optimization.\n","id":442}
{"Unnamed: 0.1":11443,"Unnamed: 0":11443.0,"anchor":"Breast Mass Classification from Mammograms using Deep Convolutional\n  Neural Networks","positive":"  Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model.\n","negative":"  A main focus of machine learning research has been improving the\ngeneralization accuracy and efficiency of prediction models. Many models such\nas SVM, random forest, and deep neural nets have been proposed and achieved\ngreat success. However, what emerges as missing in many applications is\nactionability, i.e., the ability to turn prediction results into actions. For\nexample, in applications such as customer relationship management, clinical\nprediction, and advertisement, the users need not only accurate prediction, but\nalso actionable instructions which can transfer an input to a desirable goal\n(e.g., higher profit repays, lower morbidity rates, higher ads hit rates).\nExisting effort in deriving such actionable knowledge is few and limited to\nsimple action models which restricted to only change one attribute for each\naction. The dilemma is that in many real applications those action models are\noften more complex and harder to extract an optimal solution.\n  In this paper, we propose a novel approach that achieves actionability by\ncombining learning with planning, two core areas of AI. In particular, we\npropose a framework to extract actionable knowledge from random forest, one of\nthe most widely used and best off-the-shelf classifiers. We formulate the\nactionability problem to a sub-optimal action planning (SOAP) problem, which is\nto find a plan to alter certain features of a given input so that the random\nforest would yield a desirable output, while minimizing the total costs of\nactions. Technically, the SOAP problem is formulated in the SAS+ planning\nformalism, and solved using a Max-SAT based approach. Our experimental results\ndemonstrate the effectiveness and efficiency of the proposed approach on a\npersonal credit dataset and other benchmarks. Our work represents a new\napplication of automated planning on an emerging and challenging machine\nlearning paradigm.\n","id":443}
{"Unnamed: 0.1":11444,"Unnamed: 0":11444.0,"anchor":"Higher Order Mutual Information Approximation for Feature Selection","positive":"  Feature selection is a process of choosing a subset of relevant features so\nthat the quality of prediction models can be improved. An extensive body of\nwork exists on information-theoretic feature selection, based on maximizing\nMutual Information (MI) between subsets of features and class labels. The prior\nmethods use a lower order approximation, by treating the joint entropy as a\nsummation of several single variable entropies. This leads to locally optimal\nselections and misses multi-way feature combinations. We present a higher order\nMI based approximation technique called Higher Order Feature Selection (HOFS).\nInstead of producing a single list of features, our method produces a ranked\ncollection of feature subsets that maximizes MI, giving better comprehension\n(feature ranking) as to which features work best together when selected, due to\ntheir underlying interdependent structure. Our experiments demonstrate that the\nproposed method performs better than existing feature selection approaches\nwhile keeping similar running times and computational complexity.\n","negative":"  Restricted Boltzmann machines (RBMs) and their variants are usually trained\nby contrastive divergence (CD) learning, but the training procedure is an\nunsupervised learning approach, without any guidances of the background\nknowledge. To enhance the expression ability of traditional RBMs, in this\npaper, we propose pairwise constraints restricted Boltzmann machine with\nGaussian visible units (pcGRBM) model, in which the learning procedure is\nguided by pairwise constraints and the process of encoding is conducted under\nthese guidances. The pairwise constraints are encoded in hidden layer features\nof pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and\nanother part of them are separated by the guidances. In order to deal with\nreal-valued data, the binary visible units are replaced by linear units with\nGausian noise in the pcGRBM model. In the learning process of pcGRBM, the\npairwise constraints are iterated transitions between visible and hidden units\nduring CD learning procedure. Then, the proposed model is inferred by\napproximative gradient descent method and the corresponding learning algorithm\nis designed in this paper. In order to compare the availability of pcGRBM and\ntraditional RBMs with Gaussian visible units, the features of the pcGRBM and\nRBMs hidden layer are used as input 'data' for K-means, spectral clustering\n(SP) and affinity propagation (AP) algorithms, respectively. A thorough\nexperimental evaluation is performed with sixteen image datasets of Microsoft\nResearch Asia Multimedia (MSRA-MM). The experimental results show that the\nclustering performance of K-means, SP and AP algorithms based on pcGRBM model\nare significantly better than traditional RBMs. In addition, the pcGRBM model\nfor clustering task shows better performance than some semi-supervised\nclustering algorithms.\n","id":444}
{"Unnamed: 0.1":11445,"Unnamed: 0":11445.0,"anchor":"Self-critical Sequence Training for Image Captioning","positive":"  Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.\n","negative":"  Organ transplants can improve the life expectancy and quality of life for the\nrecipient but carries the risk of serious post-operative complications, such as\nseptic shock and organ rejection. The probability of a successful transplant\ndepends in a very subtle fashion on compatibility between the donor and the\nrecipient but current medical practice is short of domain knowledge regarding\nthe complex nature of recipient-donor compatibility. Hence a data-driven\napproach for learning compatibility has the potential for significant\nimprovements in match quality. This paper proposes a novel system\n(ConfidentMatch) that is trained using data from electronic health records.\nConfidentMatch predicts the success of an organ transplant (in terms of the 3\nyear survival rates) on the basis of clinical and demographic traits of the\ndonor and recipient. ConfidentMatch captures the heterogeneity of the donor and\nrecipient traits by optimally dividing the feature space into clusters and\nconstructing different optimal predictive models to each cluster. The system\ncontrols the complexity of the learned predictive model in a way that allows\nfor assuring more granular and confident predictions for a larger number of\npotential recipient-donor pairs, thereby ensuring that predictions are\n\"personalized\" and tailored to individual characteristics to the finest\npossible granularity. Experiments conducted on the UNOS heart transplant\ndataset show the superiority of the prognostic value of ConfidentMatch to other\ncompeting benchmarks; ConfidentMatch can provide predictions of success with\n95% confidence for 5,489 patients of a total population of 9,620 patients,\nwhich corresponds to 410 more patients than the most competitive benchmark\nalgorithm (DeepBoost).\n","id":445}
{"Unnamed: 0.1":11446,"Unnamed: 0":11446.0,"anchor":"Active Search for Sparse Signals with Region Sensing","positive":"  Autonomous systems can be used to search for sparse signals in a large space;\ne.g., aerial robots can be deployed to localize threats, detect gas leaks, or\nrespond to distress calls. Intuitively, search algorithms may increase\nefficiency by collecting aggregate measurements summarizing large contiguous\nregions. However, most existing search methods either ignore the possibility of\nsuch region observations (e.g., Bayesian optimization and multi-armed bandits)\nor make strong assumptions about the sensing mechanism that allow each\nmeasurement to arbitrarily encode all signals in the entire environment (e.g.,\ncompressive sensing). We propose an algorithm that actively collects data to\nsearch for sparse signals using only noisy measurements of the average values\non rectangular regions (including single points), based on the greedy\nmaximization of information gain. We analyze our algorithm in 1d and show that\nit requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$\nsignal locations with small Bayes error, where $\\mu$ and $n$ are the signal\nstrength and the size of the search space, respectively. We also show that\nactive designs can be fundamentally more efficient than passive designs with\nregion sensing, contrasting with the results of Arias-Castro, Candes, and\nDavenport (2013). We demonstrate the empirical performance of our algorithm on\na search problem using satellite image data and in high dimensions.\n","negative":"  Gaussian Process bandit optimization has emerged as a powerful tool for\noptimizing noisy black box functions. One example in machine learning is\nhyper-parameter optimization where each evaluation of the target function\nrequires training a model which may involve days or even weeks of computation.\nMost methods for this so-called \"Bayesian optimization\" only allow sequential\nexploration of the parameter space. However, it is often desirable to propose\nbatches or sets of parameter values to explore simultaneously, especially when\nthere are large parallel processing facilities at our disposal. Batch methods\nrequire modeling the interaction between the different evaluations in the\nbatch, which can be expensive in complex scenarios. In this paper, we propose a\nnew approach for parallelizing Bayesian optimization by modeling the diversity\nof a batch via Determinantal point processes (DPPs) whose kernels are learned\nautomatically. This allows us to generalize a previous result as well as prove\nbetter regret bounds based on DPP sampling. Our experiments on a variety of\nsynthetic and real-world robotics and hyper-parameter optimization tasks\nindicate that our DPP-based methods, especially those based on DPP sampling,\noutperform state-of-the-art methods.\n","id":446}
{"Unnamed: 0.1":11447,"Unnamed: 0":11447.0,"anchor":"Development of a hybrid learning system based on SVM, ANFIS and domain\n  knowledge: DKFIS","positive":"  This paper presents the development of a hybrid learning system based on\nSupport Vector Machines (SVM), Adaptive Neuro-Fuzzy Inference System (ANFIS)\nand domain knowledge to solve prediction problem. The proposed two-stage Domain\nKnowledge based Fuzzy Information System (DKFIS) improves the prediction\naccuracy attained by ANFIS alone. The proposed framework has been implemented\non a noisy and incomplete dataset acquired from a hydrocarbon field located at\nwestern part of India. Here, oil saturation has been predicted from four\ndifferent well logs i.e. gamma ray, resistivity, density, and clay volume. In\nthe first stage, depending on zero or near zero and non-zero oil saturation\nlevels the input vector is classified into two classes (Class 0 and Class 1)\nusing SVM. The classification results have been further fine-tuned applying\nexpert knowledge based on the relationship among predictor variables i.e. well\nlogs and target variable - oil saturation. Second, an ANFIS is designed to\npredict non-zero (Class 1) oil saturation values from predictor logs. The\npredicted output has been further refined based on expert knowledge. It is\napparent from the experimental results that the expert intervention with\nqualitative judgment at each stage has rendered the prediction into the\nfeasible and realistic ranges. The performance analysis of the prediction in\nterms of four performance metrics such as correlation coefficient (CC), root\nmean square error (RMSE), and absolute error mean (AEM), scatter index (SI) has\nestablished DKFIS as a useful tool for reservoir characterization.\n","negative":"  Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.\n","id":447}
{"Unnamed: 0.1":11448,"Unnamed: 0":11448.0,"anchor":"Communication Lower Bounds for Distributed Convex Optimization:\n  Partition Data on Features","positive":"  Recently, there has been an increasing interest in designing distributed\nconvex optimization algorithms under the setting where the data matrix is\npartitioned on features. Algorithms under this setting sometimes have many\nadvantages over those under the setting where data is partitioned on samples,\nespecially when the number of features is huge. Therefore, it is important to\nunderstand the inherent limitations of these optimization problems. In this\npaper, with certain restrictions on the communication allowed in the\nprocedures, we develop tight lower bounds on communication rounds for a broad\nclass of non-incremental algorithms under this setting. We also provide a lower\nbound on communication rounds for a class of (randomized) incremental\nalgorithms.\n","negative":"  There is an especially strong need in modern large-scale data analysis to\nprioritize samples for manual inspection. For example, the inspection could\ntarget important mislabeled samples or key vulnerabilities exploitable by an\nadversarial attack. In order to solve the \"needle in the haystack\" problem of\nwhich samples to inspect, we develop a new scalable version of Cook's distance,\na classical statistical technique for identifying samples which unusually\nstrongly impact the fit of a regression model (and its downstream predictions).\nIn order to scale this technique up to very large and high-dimensional\ndatasets, we introduce a new algorithm which we call \"influence sketching.\"\nInfluence sketching embeds random projections within the influence computation;\nin particular, the influence score is calculated using the randomly projected\npseudo-dataset from the post-convergence Generalized Linear Model (GLM). We\nvalidate that influence sketching can reliably and successfully discover\ninfluential samples by applying the technique to a malware detection dataset of\nover 2 million executable files, each represented with almost 100,000 features.\nFor example, we find that randomly deleting approximately 10% of training\nsamples reduces predictive accuracy only slightly from 99.47% to 99.45%,\nwhereas deleting the same number of samples with high influence sketch scores\nreduces predictive accuracy all the way down to 90.24%. Moreover, we find that\ninfluential samples are especially likely to be mislabeled. In the case study,\nwe manually inspect the most influential samples, and find that influence\nsketching pointed us to new, previously unidentified pieces of malware.\n","id":448}
{"Unnamed: 0.1":11449,"Unnamed: 0":11449.0,"anchor":"Predictive Clinical Decision Support System with RNN Encoding and Tensor\n  Decoding","positive":"  With the introduction of the Electric Health Records, large amounts of\ndigital data become available for analysis and decision support. When\nphysicians are prescribing treatments to a patient, they need to consider a\nlarge range of data variety and volume, making decisions increasingly complex.\nMachine learning based Clinical Decision Support systems can be a solution to\nthe data challenges. In this work we focus on a class of decision support in\nwhich the physicians' decision is directly predicted. Concretely, the model\nwould assign higher probabilities to decisions that it presumes the physician\nare more likely to make. Thus the CDS system can provide physicians with\nrational recommendations. We also address the problem of correlation in target\nfeatures: Often a physician is required to make multiple (sub-)decisions in a\nblock, and that these decisions are mutually dependent. We propose a solution\nto the target correlation problem using a tensor factorization model. In order\nto handle the patients' historical information as sequential data, we apply the\nso-called Encoder-Decoder-Framework which is based on Recurrent Neural Networks\n(RNN) as encoders and a tensor factorization model as a decoder, a combination\nwhich is novel in machine learning. With experiments with real-world datasets\nwe show that the proposed model does achieve better prediction performances.\n","negative":"  This work proposes a novel method for semi-supervised learning from partially\nlabeled massive network-structured datasets, i.e., big data over networks. We\nmodel the underlying hypothesis, which relates data points to labels, as a\ngraph signal, defined over some graph (network) structure intrinsic to the\ndataset. Following the key principle of supervised learning, i.e., similar\ninputs yield similar outputs, we require the graph signals induced by labels to\nhave small total variation. Accordingly, we formulate the problem of learning\nthe labels of data points as a non-smooth convex optimization problem which\namounts to balancing between the empirical loss, i.e., the discrepancy with\nsome partially available label information, and the smoothness quantified by\nthe total variation of the learned graph signal. We solve this optimization\nproblem by appealing to a recently proposed preconditioned variant of the\npopular primal-dual method by Pock and Chambolle, which results in a sparse\nlabel propagation algorithm. This learning algorithm allows for a highly\nscalable implementation as message passing over the underlying data graph. By\napplying concepts of compressed sensing to the learning problem, we are also\nable to provide a transparent sufficient condition on the underlying network\nstructure such that accurate learning of the labels is possible. We also\npresent an implementation of the message passing formulation allows for a\nhighly scalable implementation in big data frameworks.\n","id":449}
{"Unnamed: 0.1":11450,"Unnamed: 0":11450.0,"anchor":"A temporal model for multiple sclerosis course evolution","positive":"  Multiple Sclerosis is a degenerative condition of the central nervous system\nthat affects nearly 2.5 million of individuals in terms of their physical,\ncognitive, psychological and social capabilities. Researchers are currently\ninvestigating on the use of patient reported outcome measures for the\nassessment of impact and evolution of the disease on the life of the patients.\nTo date, a clear understanding on the use of such measures to predict the\nevolution of the disease is still lacking. In this work we resort to\nregularized machine learning methods for binary classification and multiple\noutput regression. We propose a pipeline that can be used to predict the\ndisease progression from patient reported measures. The obtained model is\ntested on a data set collected from an ongoing clinical research project.\n","negative":"  Energy disaggregation (a.k.a nonintrusive load monitoring, NILM), a\nsingle-channel blind source separation problem, aims to decompose the mains\nwhich records the whole house electricity consumption into appliance-wise\nreadings. This problem is difficult because it is inherently unidentifiable.\nRecent approaches have shown that the identifiability problem could be reduced\nby introducing domain knowledge into the model. Deep neural networks have been\nshown to be a promising approach for these problems, but sliding windows are\nnecessary to handle the long sequences which arise in signal processing\nproblems, which raises issues about how to combine predictions from different\nsliding windows. In this paper, we propose sequence-to-point learning, where\nthe input is a window of the mains and the output is a single point of the\ntarget appliance. We use convolutional neural networks to train the model.\nInterestingly, we systematically show that the convolutional neural networks\ncan inherently learn the signatures of the target appliances, which are\nautomatically added into the model to reduce the identifiability problem. We\napplied the proposed neural network approaches to real-world household energy\ndata, and show that the methods achieve state-of-the-art performance, improving\ntwo standard error measures by 84% and 92%.\n","id":450}
{"Unnamed: 0.1":11451,"Unnamed: 0":11451.0,"anchor":"A General Framework for Density Based Time Series Clustering Exploiting\n  a Novel Admissible Pruning Strategy","positive":"  Time Series Clustering is an important subroutine in many higher-level data\nmining analyses, including data editing for classifiers, summarization, and\noutlier detection. It is well known that for similarity search the superiority\nof Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as\nwe consider ever larger datasets. However, as we shall show, the same is not\ntrue for clustering. Clustering time series under DTW remains a computationally\nexpensive operation. In this work, we address this issue in two ways. We\npropose a novel pruning strategy that exploits both the upper and lower bounds\nto prune off a very large fraction of the expensive distance calculations. This\npruning strategy is admissible and gives us provably identical results to the\nbrute force algorithm, but is at least an order of magnitude faster. For\ndatasets where even this level of speedup is inadequate, we show that we can\nuse a simple heuristic to order the unavoidable calculations in a\nmost-useful-first ordering, thus casting the clustering into an anytime\nframework. We demonstrate the utility of our ideas with both single and\nmultidimensional case studies in the domains of astronomy, speech physiology,\nmedicine and entomology. In addition, we show the generality of our clustering\nframework to other domains by efficiently obtaining semantically significant\nclusters in protein sequences using the Edit Distance, the discrete data\nanalogue of DTW.\n","negative":"  In coming years residential consumers will face real-time electricity tariffs\nwith energy prices varying day to day, and effective energy saving will require\nautomation - a recommender system, which learns consumer's preferences from her\nactions. A consumer chooses a scenario of home appliance use to balance her\ncomfort level and the energy bill. We propose a Bayesian learning algorithm to\nestimate the comfort level function from the history of appliance use. In\nnumeric experiments with datasets generated from a simulation model of a\nconsumer interacting with small home appliances the algorithm outperforms\npopular regression analysis tools. Our approach can be extended to control an\nair heating and conditioning system, which is responsible for up to half of a\nhousehold's energy bill.\n","id":451}
{"Unnamed: 0.1":11452,"Unnamed: 0":11452.0,"anchor":"Inferring Cognitive Models from Data using Approximate Bayesian\n  Computation","positive":"  An important problem for HCI researchers is to estimate the parameter values\nof a cognitive model from behavioral data. This is a difficult problem, because\nof the substantial complexity and variety in human behavioral strategies. We\nreport an investigation into a new approach using approximate Bayesian\ncomputation (ABC) to condition model parameters to data and prior knowledge. As\nthe case study we examine menu interaction, where we have click time data only\nto infer a cognitive model that implements a search behaviour with parameters\nsuch as fixation duration and recall probability. Our results demonstrate that\nABC (i) improves estimates of model parameter values, (ii) enables meaningful\ncomparisons between model variants, and (iii) supports fitting models to\nindividual users. ABC provides ample opportunities for theoretical HCI research\nby allowing principled inference of model parameter values and their\nuncertainty.\n","negative":"  Time series (TS) occur in many scientific and commercial applications,\nranging from earth surveillance to industry automation to the smart grids. An\nimportant type of TS analysis is classification, which can, for instance,\nimprove energy load forecasting in smart grids by detecting the types of\nelectronic devices based on their energy consumption profiles recorded by\nautomatic sensors. Such sensor-driven applications are very often characterized\nby (a) very long TS and (b) very large TS datasets needing classification.\nHowever, current methods to time series classification (TSC) cannot cope with\nsuch data volumes at acceptable accuracy; they are either scalable but offer\nonly inferior classification quality, or they achieve state-of-the-art\nclassification quality but cannot scale to large data volumes.\n  In this paper, we present WEASEL (Word ExtrAction for time SEries\ncLassification), a novel TSC method which is both scalable and accurate. Like\nother state-of-the-art TSC methods, WEASEL transforms time series into feature\nvectors, using a sliding-window approach, which are then analyzed through a\nmachine learning classifier. The novelty of WEASEL lies in its specific method\nfor deriving features, resulting in a much smaller yet much more discriminative\nfeature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more\naccurate than the best current non-ensemble algorithms at orders-of-magnitude\nlower classification and training times, and it is almost as accurate as\nensemble classifiers, whose computational complexity makes them inapplicable\neven for mid-size datasets. The outstanding robustness of WEASEL is also\nconfirmed by experiments on two real smart grid datasets, where it\nout-of-the-box achieves almost the same accuracy as highly tuned,\ndomain-specific methods.\n","id":452}
{"Unnamed: 0.1":11453,"Unnamed: 0":11453.0,"anchor":"Predicting Patient State-of-Health using Sliding Window and Recurrent\n  Classifiers","positive":"  Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,\nslowing response times and desensitising nurses to alarms (Chambrin, 2001),\ncausing true alarms to be missed (Hug et al., 2011). We compare sliding window\npredictors with recurrent predictors to classify patient state-of-health from\nICU multivariate time series; we report slightly improved performance for the\nRNN for three out of four targets.\n","negative":"  Deep Neural Networks often require good regularizers to generalize well.\nDropout is one such regularizer that is widely used among Deep Learning\npractitioners. Recent work has shown that Dropout can also be viewed as\nperforming Approximate Bayesian Inference over the network parameters. In this\nwork, we generalize this notion and introduce a rich family of regularizers\nwhich we call Generalized Dropout. One set of methods in this family, called\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\nemerges as a special case of this method. Another member of this family selects\nthe width of neural network layers. Experiments show that these methods help in\nimproving generalization performance over Dropout.\n","id":453}
{"Unnamed: 0.1":11454,"Unnamed: 0":11454.0,"anchor":"Voxelwise nonlinear regression toolbox for neuroimage analysis:\n  Application to aging and neurodegenerative disease modeling","positive":"  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https:\/\/github.com\/imatge-upc\/VNeAT.\n","negative":"  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n","id":454}
{"Unnamed: 0.1":11455,"Unnamed: 0":11455.0,"anchor":"Reliable Evaluation of Neural Network for Multiclass Classification of\n  Real-world Data","positive":"  This paper presents a systematic evaluation of Neural Network (NN) for\nclassification of real-world data. In the field of machine learning, it is\noften seen that a single parameter that is 'predictive accuracy' is being used\nfor evaluating the performance of a classifier model. However, this parameter\nmight not be considered reliable given a dataset with very high level of\nskewness. To demonstrate such behavior, seven different types of datasets have\nbeen used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different\nparameters which include micro- and macro-level estimation. In the present\nstudy, the most common problem of prediction called 'multiclass' classification\nhas been considered. The results that are obtained for different parameters for\neach of the dataset could demonstrate interesting findings to support the\nusability of these set of performance evaluation parameters.\n","negative":"  Learning hash functions\/codes for similarity search over multi-view data is\nattracting increasing attention, where similar hash codes are assigned to the\ndata objects characterizing consistently neighborhood relationship across\nviews. Traditional methods in this category inherently suffer three\nlimitations: 1) they commonly adopt a two-stage scheme where similarity matrix\nis first constructed, followed by a subsequent hash function learning; 2) these\nmethods are commonly developed on the assumption that data samples with\nmultiple representations are noise-free,which is not practical in real-life\napplications; 3) they often incur cumbersome training model caused by the\nneighborhood graph construction using all $N$ points in the database ($O(N)$).\nIn this paper, we motivate the problem of jointly and efficiently training the\nrobust hash functions over data objects with multi-feature representations\nwhich may be noise corrupted. To achieve both the robustness and training\nefficiency, we propose an approach to effectively and efficiently learning\nlow-rank kernelized \\footnote{We use kernelized similarity rather than kernel,\nas it is not a squared symmetric matrix for data-landmark affinity matrix.}\nhash functions shared across views. Specifically, we utilize landmark graphs to\nconstruct tractable similarity matrices in multi-views to automatically\ndiscover neighborhood structure in the data. To learn robust hash functions, a\nlatent low-rank kernel function is used to construct hash functions in order to\naccommodate linearly inseparable data. In particular, a latent kernelized\nsimilarity matrix is recovered by rank minimization on multiple kernel-based\nsimilarity matrices. Extensive experiments on real-world multi-view datasets\nvalidate the efficacy of our method in the presence of error corruptions.\n","id":455}
{"Unnamed: 0.1":11456,"Unnamed: 0":11456.0,"anchor":"Identifying and Categorizing Anomalies in Retinal Imaging Data","positive":"  The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis and management of patients in clinical\npractice. Supervised- or weakly supervised training enables the detection of\nfindings that are known a priori. It does not scale well, and a priori\ndefinition limits the vocabulary of markers to known entities reducing the\naccuracy of diagnosis and prognosis. Here, we propose the identification of\nanomalies in large-scale medical imaging data using healthy examples as a\nreference. We detect and categorize candidates for anomaly findings untypical\nfor the observed data. A deep convolutional autoencoder is trained on healthy\nretinal images. The learned model generates a new feature representation, and\nthe distribution of healthy retinal patches is estimated by a one-class support\nvector machine. Results demonstrate that we can identify pathologic regions in\nimages without using expert annotations. A subsequent clustering categorizes\nfindings into clinically meaningful classes. In addition the learned features\noutperform standard embedding approaches in a classification task.\n","negative":"  Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop.\n","id":456}
{"Unnamed: 0.1":11457,"Unnamed: 0":11457.0,"anchor":"Probabilistic Neural Programs","positive":"  We present probabilistic neural programs, a framework for program induction\nthat permits flexible specification of both a computational model and inference\nalgorithm while simultaneously enabling the use of deep neural networks.\nProbabilistic neural programs combine a computation graph for specifying a\nneural network with an operator for weighted nondeterministic choice. Thus, a\nprogram describes both a collection of decisions as well as the neural network\narchitecture used to make each one. We evaluate our approach on a challenging\ndiagram question answering task where probabilistic neural programs correctly\nexecute nearly twice as many programs as a baseline model.\n","negative":"  We present a generalization of the Cauchy\/Lorentzian, Geman-McClure,\nWelsch\/Leclerc, generalized Charbonnier, Charbonnier\/pseudo-Huber\/L1-L2, and L2\nloss functions. By introducing robustness as a continuous parameter, our loss\nfunction allows algorithms built around robust loss minimization to be\ngeneralized, which improves performance on basic vision tasks such as\nregistration and clustering. Interpreting our loss as the negative log of a\nunivariate density yields a general probability distribution that includes\nnormal and Cauchy distributions as special cases. This probabilistic\ninterpretation enables the training of neural networks in which the robustness\nof the loss automatically adapts itself during training, which improves\nperformance on learning-based tasks such as generative image synthesis and\nunsupervised monocular depth estimation, without requiring any manual parameter\ntuning.\n","id":457}
{"Unnamed: 0.1":11458,"Unnamed: 0":11458.0,"anchor":"Cognitive Deep Machine Can Train Itself","positive":"  Machine learning is making substantial progress in diverse applications. The\nsuccess is mostly due to advances in deep learning. However, deep learning can\nmake mistakes and its generalization abilities to new tasks are questionable.\nWe ask when and how one can combine network outputs, when (i) details of the\nobservations are evaluated by learned deep components and (ii) facts and\nconfirmation rules are available in knowledge based systems. We show that in\nlimited contexts the required number of training samples can be low and\nself-improvement of pre-trained networks in more general context is possible.\nWe argue that the combination of sparse outlier detection with deep components\nthat can support each other diminish the fragility of deep methods, an\nimportant requirement for engineering applications. We argue that supervised\nlearning of labels may be fully eliminated under certain conditions: a\ncomponent based architecture together with a knowledge based system can train\nitself and provide high quality answers. We demonstrate these concepts on the\nState Farm Distracted Driver Detection benchmark. We argue that the view of the\nStudy Panel (2016) may overestimate the requirements on `years of focused\nresearch' and `careful, unique construction' for `AI systems'.\n","negative":"  Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available.\n","id":458}
{"Unnamed: 0.1":11459,"Unnamed: 0":11459.0,"anchor":"Asynchronous Stochastic Gradient MCMC with Elastic Coupling","positive":"  We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling\nfor problems where we can leverage (stochastic) gradients to define continuous\ndynamics which explore the target distribution. We outline a solution strategy\nfor this setting based on stochastic gradient Hamiltonian Monte Carlo sampling\n(SGHMC) which we alter to include an elastic coupling term that ties together\nmultiple MCMC instances. The proposed strategy turns inherently sequential HMC\nalgorithms into asynchronous parallel versions. First experiments empirically\nshow that the resulting parallel sampler significantly speeds up exploration of\nthe target distribution, when compared to standard SGHMC, and is less prone to\nthe harmful effects of stale gradients than a naive parallelization approach.\n","negative":"  In many real-world application, e.g., speech recognition or sleep stage\nclassification, data are captured over the course of time, constituting a\nTime-Series. Time-Series often contain temporal dependencies that cause two\notherwise identical points of time to belong to different classes or predict\ndifferent behavior. This characteristic generally increases the difficulty of\nanalysing them. Existing techniques often depended on hand-crafted features\nthat were expensive to create and required expert knowledge of the field. With\nthe advent of Deep Learning new models of unsupervised learning of features for\nTime-series analysis and forecast have been developed. Such new developments\nare the topic of this paper: a review of the main Deep Learning techniques is\npresented, and some applications on Time-Series analysis are summaried. The\nresults make it clear that Deep Learning has a lot to contribute to the field.\n","id":459}
{"Unnamed: 0.1":11460,"Unnamed: 0":11460.0,"anchor":"A simple squared-error reformulation for ordinal classification","positive":"  In this paper, we explore ordinal classification (in the context of deep\nneural networks) through a simple modification of the squared error loss which\nnot only allows it to not only be sensitive to class ordering, but also allows\nthe possibility of having a discrete probability distribution over the classes.\nOur formulation is based on the use of a softmax hidden layer, which has\nreceived relatively little attention in the literature. We empirically evaluate\nits performance on the Kaggle diabetic retinopathy dataset, an ordinal and\nhigh-resolution dataset and show that it outperforms all of the baselines\nemployed.\n","negative":"  In recent years, model-free methods that use deep learning have achieved\ngreat success in many different reinforcement learning environments. Most\nsuccessful approaches focus on solving a single task, while multi-task\nreinforcement learning remains an open problem. In this paper, we present a\nmodel based approach to deep reinforcement learning which we use to solve\ndifferent tasks simultaneously. We show that our approach not only does not\ndegrade but actually benefits from learning multiple tasks. For our model, we\nalso present a new kind of recurrent neural network inspired by residual\nnetworks that decouples memory from computation allowing to model complex\nenvironments that do not require lots of memory.\n","id":460}
{"Unnamed: 0.1":11461,"Unnamed: 0":11461.0,"anchor":"Overcoming catastrophic forgetting in neural networks","positive":"  The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.\n","negative":"  A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.\n","id":461}
{"Unnamed: 0.1":11462,"Unnamed: 0":11462.0,"anchor":"Restricted Strong Convexity Implies Weak Submodularity","positive":"  We connect high-dimensional subset selection and submodular maximization. Our\nresults extend the work of Das and Kempe (2011) from the setting of linear\nregression to arbitrary objective functions. For greedy feature selection, this\nconnection allows us to obtain strong multiplicative performance bounds on\nseveral methods without statistical modeling assumptions. We also derive\nrecovery guarantees of this form under standard assumptions. Our work shows\nthat greedy algorithms perform within a constant factor from the best possible\nsubset-selection solution for a broad class of general objective functions. Our\nmethods allow a direct control over the number of obtained features as opposed\nto regularization parameters that only implicitly control sparsity. Our proof\ntechnique uses the concept of weak submodularity initially defined by Das and\nKempe. We draw a connection between convex analysis and submodular set function\ntheory which may be of independent interest for other statistical learning\napplications that have combinatorial structure.\n","negative":"  A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure.\n","id":462}
{"Unnamed: 0.1":11463,"Unnamed: 0":11463.0,"anchor":"Perspective Transformer Nets: Learning Single-View 3D Object\n  Reconstruction without 3D Supervision","positive":"  Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved.\n","negative":"  Neural Style Transfer has recently demonstrated very exciting results which\ncatches eyes in both academia and industry. Despite the amazing results, the\nprinciple of neural style transfer, especially why the Gram matrices could\nrepresent style remains unclear. In this paper, we propose a novel\ninterpretation of neural style transfer by treating it as a domain adaptation\nproblem. Specifically, we theoretically show that matching the Gram matrices of\nfeature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with\nthe second order polynomial kernel. Thus, we argue that the essence of neural\nstyle transfer is to match the feature distributions between the style images\nand the generated images. To further support our standpoint, we experiment with\nseveral other distribution alignment methods, and achieve appealing results. We\nbelieve this novel interpretation connects these two important research fields,\nand could enlighten future researches.\n","id":463}
{"Unnamed: 0.1":11464,"Unnamed: 0":11464.0,"anchor":"Summary - TerpreT: A Probabilistic Programming Language for Program\n  Induction","positive":"  We study machine learning formulations of inductive program synthesis; that\nis, given input-output examples, synthesize source code that maps inputs to\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\nlanguage for expressing program synthesis problems. A TerpreT model is composed\nof a specification of a program representation and an interpreter that\ndescribes how programs map inputs to outputs. The inference task is to observe\na set of input-output examples and infer the underlying program. From a TerpreT\nmodel we automatically perform inference using four different back-ends:\ngradient descent (thus each TerpreT model can be seen as defining a\ndifferentiable interpreter), linear program (LP) relaxations for graphical\nmodels, discrete satisfiability solving, and the Sketch program synthesis\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\nrange of domains, program representations, and interpreter models. Second, it\nseparates the model specification from the inference algorithm, allowing proper\ncomparisons between different approaches to inference.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an extensive empirical comparison between alternative inference\nalgorithms on a variety of program models. To our knowledge, this is the first\nwork to compare gradient-based search over program space to traditional\nsearch-based alternatives. Our key empirical finding is that constraint solvers\ndominate the gradient descent and LP-based formulations.\n  This is a workshop summary of a longer report at arXiv:1608.04428\n","negative":"  Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer.\n","id":464}
{"Unnamed: 0.1":11465,"Unnamed: 0":11465.0,"anchor":"Learning with Hierarchical Gaussian Kernels","positive":"  We investigate iterated compositions of weighted sums of Gaussian kernels and\nprovide an interpretation of the construction that shows some similarities with\nthe architectures of deep neural networks. On the theoretical side, we show\nthat these kernels are universal and that SVMs using these kernels are\nuniversally consistent. We further describe a parameter optimization method for\nthe kernel parameters and empirically compare this method to SVMs, random\nforests, a multiple kernel learning approach, and to some deep neural networks.\n","negative":"  We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling\nfor problems where we can leverage (stochastic) gradients to define continuous\ndynamics which explore the target distribution. We outline a solution strategy\nfor this setting based on stochastic gradient Hamiltonian Monte Carlo sampling\n(SGHMC) which we alter to include an elastic coupling term that ties together\nmultiple MCMC instances. The proposed strategy turns inherently sequential HMC\nalgorithms into asynchronous parallel versions. First experiments empirically\nshow that the resulting parallel sampler significantly speeds up exploration of\nthe target distribution, when compared to standard SGHMC, and is less prone to\nthe harmful effects of stale gradients than a naive parallelization approach.\n","id":465}
{"Unnamed: 0.1":11466,"Unnamed: 0":11466.0,"anchor":"Learning Operations on a Stack with Neural Turing Machines","positive":"  Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed\nrecently to address the difficulty of storing information over long time\nperiods. In this paper, we experiment with the capacity of Neural Turing\nMachines (NTMs) to deal with these long-term dependencies on well-balanced\nstrings of parentheses. We show that not only does the NTM emulate a stack with\nits heads and learn an algorithm to recognize such words, but it is also\ncapable of strongly generalizing to much longer sequences.\n","negative":"  In this paper, we develop an agent-based version of the Diamond search\nequilibrium model - also called Coconut Model. In this model, agents are faced\nwith production decisions that have to be evaluated based on their expectations\nabout the future utility of the produced entity which in turn depends on the\nglobal production level via a trading mechanism. While the original dynamical\nsystems formulation assumes an infinite number of homogeneously adapting agents\nobeying strong rationality conditions, the agent-based setting allows to\ndiscuss the effects of heterogeneous and adaptive expectations and enables the\nanalysis of non-equilibrium trajectories. Starting from a baseline\nimplementation that matches the asymptotic behavior of the original model, we\nshow how agent heterogeneity can be accounted for in the aggregate dynamical\nequations. We then show that when agents adapt their strategies by a simple\ntemporal difference learning scheme, the system converges to one of the fixed\npoints of the original system. Systematic simulations reveal that this is the\nonly stable equilibrium solution.\n","id":466}
{"Unnamed: 0.1":11467,"Unnamed: 0":11467.0,"anchor":"Scribbler: Controlling Deep Image Synthesis with Sketch and Color","positive":"  Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images.\n","negative":"  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n","id":467}
{"Unnamed: 0.1":11468,"Unnamed: 0":11468.0,"anchor":"Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering","positive":"  Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.\n","negative":"  Existing methods for structure discovery in time series data construct\ninterpretable, compositional kernels for Gaussian process regression models.\nWhile the learned Gaussian process model provides posterior mean and variance\nestimates, typically the structure is learned via a greedy optimization\nprocedure. This restricts the space of possible solutions and leads to\nover-confident uncertainty estimates. We introduce a fully Bayesian approach,\ninferring a full posterior over structures, which more reliably captures the\nuncertainty of the model.\n","id":468}
{"Unnamed: 0.1":11469,"Unnamed: 0":11469.0,"anchor":"A novel multiclassSVM based framework to classify lithology from well\n  logs: a real-world application","positive":"  Support vector machines (SVMs) have been recognized as a potential tool for\nsupervised classification analyses in different domains of research. In\nessence, SVM is a binary classifier. Therefore, in case of a multiclass\nproblem, the problem is divided into a series of binary problems which are\nsolved by binary classifiers, and finally the classification results are\ncombined following either the one-against-one or one-against-all strategies. In\nthis paper, an attempt has been made to classify lithology using a multiclass\nSVM based framework using well logs as predictor variables. Here, the lithology\nis classified into four classes such as sand, shaly sand, sandy shale and shale\nbased on the relative values of sand and shale fractions as suggested by an\nexpert geologist. The available dataset consisting well logs (gamma ray,\nneutron porosity, density, and P-sonic) and class information from four closely\nspaced wells from an onshore hydrocarbon field is divided into training and\ntesting sets. We have used one-against-all strategy to combine the results of\nmultiple binary classifiers. The reported results established the superiority\nof multiclass SVM compared to other classifiers in terms of classification\naccuracy. The selection of kernel function and associated parameters has also\nbeen investigated here. It can be envisaged from the results achieved in this\nstudy that the proposed framework based on multiclass SVM can further be used\nto solve classification problems. In future research endeavor, seismic\nattributes can be introduced in the framework to classify the lithology\nthroughout a study area from seismic inputs.\n","negative":"  In this paper, we consider the problem of event classification with\nmulti-variate time series data consisting of heterogeneous (continuous and\ncategorical) variables. The complex temporal dependencies between the variables\ncombined with sparsity of the data makes the event classification problem\nparticularly challenging. Most state-of-art approaches address this either by\ndesigning hand-engineered features or breaking up the problem over homogeneous\nvariates. In this work, we propose and compare three representation learning\nalgorithms over symbolized sequences which enables classification of\nheterogeneous time-series data using a deep architecture. The proposed\nrepresentations are trained jointly along with the rest of the network\narchitecture in an end-to-end fashion that makes the learned features\ndiscriminative for the given task. Experiments on three real-world datasets\ndemonstrate the effectiveness of the proposed approaches.\n","id":469}
{"Unnamed: 0.1":11470,"Unnamed: 0":11470.0,"anchor":"A Novel Framework based on SVDD to Classify Water Saturation from\n  Seismic Attributes","positive":"  Water saturation is an important property in reservoir engineering domain.\nThus, satisfactory classification of water saturation from seismic attributes\nis beneficial for reservoir characterization. However, diverse and non-linear\nnature of subsurface attributes makes the classification task difficult. In\nthis context, this paper proposes a generalized Support Vector Data Description\n(SVDD) based novel classification framework to classify water saturation into\ntwo classes (Class high and Class low) from three seismic attributes seismic\nimpedance, amplitude envelop, and seismic sweetness. G-metric means and program\nexecution time are used to quantify the performance of the proposed framework\nalong with established supervised classifiers. The documented results imply\nthat the proposed framework is superior to existing classifiers. The present\nstudy is envisioned to contribute in further reservoir modeling.\n","negative":"  The accuracy of Optical Character Recognition (OCR) is crucial to the success\nof subsequent applications used in text analyzing pipeline. Recent models of\nOCR post-processing significantly improve the quality of OCR-generated text,\nbut are still prone to suggest correction candidates from limited observations\nwhile insufficiently accounting for the characteristics of OCR errors. In this\npaper, we show how to enlarge candidate suggestion space by using external\ncorpus and integrating OCR-specific features in a regression approach to\ncorrect OCR-generated errors. The evaluation results show that our model can\ncorrect 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of\nthe OCR-errors (considering the top 3 suggestions), for cases where the\ntheoretical correction upper-bound is 78%.\n","id":470}
{"Unnamed: 0.1":11471,"Unnamed: 0":11471.0,"anchor":"Success Probability of Exploration: a Concrete Analysis of Learning\n  Efficiency","positive":"  Exploration has been a crucial part of reinforcement learning, yet several\nimportant questions concerning exploration efficiency are still not answered\nsatisfactorily by existing analytical frameworks. These questions include\nexploration parameter setting, situation analysis, and hardness of MDPs, all of\nwhich are unavoidable for practitioners. To bridge the gap between the theory\nand practice, we propose a new analytical framework called the success\nprobability of exploration. We show that those important questions of\nexploration above can all be answered under our framework, and the answers\nprovided by our framework meet the needs of practitioners better than the\nexisting ones. More importantly, we introduce a concrete and practical approach\nto evaluating the success probabilities in certain MDPs without the need of\nactually running the learning algorithm. We then provide empirical results to\nverify our approach, and demonstrate how the success probability of exploration\ncan be used to analyse and predict the behaviours and possible outcomes of\nexploration, which are the keys to the answer of the important questions of\nexploration.\n","negative":"  We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.\n","id":471}
{"Unnamed: 0.1":11472,"Unnamed: 0":11472.0,"anchor":"Parameter Compression of Recurrent Neural Networks and Degradation of\n  Short-term Memory","positive":"  The significant computational costs of deploying neural networks in\nlarge-scale or resource constrained environments, such as data centers and\nmobile devices, has spurred interest in model compression, which can achieve a\nreduction in both arithmetic operations and storage memory. Several techniques\nhave been proposed for reducing or compressing the parameters for feed-forward\nand convolutional neural networks, but less is understood about the effect of\nparameter compression on recurrent neural networks (RNN). In particular, the\nextent to which the recurrent parameters can be compressed and the impact on\nshort-term memory performance, is not well understood. In this paper, we study\nthe effect of complexity reduction, through singular value decomposition rank\nreduction, on RNN and minimal gated recurrent unit (MGRU) networks for several\ntasks. We show that considerable rank reduction is possible when compressing\nrecurrent weights, even without fine tuning. Furthermore, we propose a\nperturbation model for the effect of general perturbations, such as a\ncompression, on the recurrent parameters of RNNs. The model is tested against a\nnoiseless memorization experiment that elucidates the short-term memory\nperformance. In this way, we demonstrate that the effect of compression of\nrecurrent parameters is dependent on the degree of temporal coherence present\nin the data and task. This work can guide on-the-fly RNN compression for novel\nenvironments or tasks, and provides insight for applying RNN compression in\nlow-power devices, such as hearing aids.\n","negative":"  We propose a novel training algorithm for reinforcement learning which\ncombines the strength of deep Q-learning with a constrained optimization\napproach to tighten optimality and encourage faster reward propagation. Our\nnovel technique makes deep reinforcement learning more practical by drastically\nreducing the training time. We evaluate the performance of our approach on the\n49 games of the challenging Arcade Learning Environment, and report significant\nimprovements in both training time and accuracy.\n","id":472}
{"Unnamed: 0.1":11473,"Unnamed: 0":11473.0,"anchor":"End-to-End Joint Learning of Natural Language Understanding and Dialogue\n  Manager","positive":"  Natural language understanding and dialogue policy learning are both\nessential in conversational systems that predict the next system actions in\nresponse to a current user utterance. Conventional approaches aggregate\nseparate models of natural language understanding (NLU) and system action\nprediction (SAP) as a pipeline that is sensitive to noisy outputs of\nerror-prone NLU. To address the issues, we propose an end-to-end deep recurrent\nneural network with limited contextual dialogue memory by jointly training NLU\nand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our\nproposed model significantly outperforms the state-of-the-art pipeline models\nfor both NLU and SAP, which indicates that our joint model is capable of\nmitigating the affects of noisy NLU outputs, and NLU model can be refined by\nerror flows backpropagating from the extra supervised signals of system\nactions.\n","negative":"  Generative adversarial networks (GANs) are a recently proposed class of\ngenerative models in which a generator is trained to optimize a cost function\nthat is being simultaneously learned by a discriminator. While the idea of\nlearning cost functions is relatively new to the field of generative modeling,\nlearning costs has long been studied in control and reinforcement learning (RL)\ndomains, typically for imitation learning from demonstrations. In these fields,\nlearning cost function underlying observed behavior is known as inverse\nreinforcement learning (IRL) or inverse optimal control. While at first the\nconnection between cost learning in RL and cost learning in generative modeling\nmay appear to be a superficial one, we show in this paper that certain IRL\nmethods are in fact mathematically equivalent to GANs. In particular, we\ndemonstrate an equivalence between a sample-based algorithm for maximum entropy\nIRL and a GAN in which the generator's density can be evaluated and is provided\nas an additional input to the discriminator. Interestingly, maximum entropy IRL\nis a special case of an energy-based model. We discuss the interpretation of\nGANs as an algorithm for training energy-based models, and relate this\ninterpretation to other recent work that seeks to connect GANs and EBMs. By\nformally highlighting the connection between GANs, IRL, and EBMs, we hope that\nresearchers in all three communities can better identify and apply transferable\nideas from one domain to another, particularly for developing more stable and\nscalable algorithms: a major challenge in all three domains.\n","id":473}
{"Unnamed: 0.1":11474,"Unnamed: 0":11474.0,"anchor":"Positive blood culture detection in time series data using a BiLSTM\n  network","positive":"  The presence of bacteria or fungi in the bloodstream of patients is abnormal\nand can lead to life-threatening conditions. A computational model based on a\nbidirectional long short-term memory artificial neural network, is explored to\nassist doctors in the intensive care unit to predict whether examination of\nblood cultures of patients will return positive. As input it uses nine\nmonitored clinical parameters, presented as time series data, collected from\n2177 ICU admissions at the Ghent University Hospital. Our main goal is to\ndetermine if general machine learning methods and more specific, temporal\nmodels, can be used to create an early detection system. This preliminary\nresearch obtains an area of 71.95% under the precision recall curve, proving\nthe potential of temporal neural networks in this context.\n","negative":"  The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set.\n","id":474}
{"Unnamed: 0.1":11475,"Unnamed: 0":11475.0,"anchor":"Estimating latent feature-feature interactions in large feature-rich\n  graphs","positive":"  Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions.\n","negative":"  We investigate the fundamental conditions on the sampling pattern, i.e.,\nlocations of the sampled entries, for finite completability of a low-rank\ntensor given some components of its Tucker rank. In order to find the\ndeterministic necessary and sufficient conditions, we propose an algebraic\ngeometric analysis on the Tucker manifold, which allows us to incorporate\nmultiple rank components in the proposed analysis in contrast with the\nconventional geometric approaches on the Grassmannian manifold. This analysis\ncharacterizes the algebraic independence of a set of polynomials defined based\non the sampling pattern, which is closely related to finite completion.\nProbabilistic conditions are then studied and a lower bound on the sampling\nprobability is given, which guarantees that the proposed deterministic\nconditions on the sampling patterns for finite completability hold with high\nprobability. Furthermore, using the proposed geometric approach for finite\ncompletability, we propose a sufficient condition on the sampling pattern that\nensures there exists exactly one completion for the sampled tensor.\n","id":475}
{"Unnamed: 0.1":11476,"Unnamed: 0":11476.0,"anchor":"Hypothesis Transfer Learning via Transformation Functions","positive":"  We consider the Hypothesis Transfer Learning (HTL) problem where one\nincorporates a hypothesis trained on the source domain into the learning\nprocedure of the target domain. Existing theoretical analysis either only\nstudies specific algorithms or only presents upper bounds on the generalization\nerror but not on the excess risk. In this paper, we propose a unified\nalgorithm-dependent framework for HTL through a novel notion of transformation\nfunction, which characterizes the relation between the source and the target\ndomains. We conduct a general risk analysis of this framework and in\nparticular, we show for the first time, if two domains are related, HTL enjoys\nfaster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge\nRegression than those of the classical non-transfer learning settings.\nExperiments on real world data demonstrate the effectiveness of our framework.\n","negative":"  Recent advances have enabled \"oracle\" classifiers that can classify across\nmany classes and input distributions with high accuracy without retraining.\nHowever, these classifiers are relatively heavyweight, so that applying them to\nclassify video is costly. We show that day-to-day video exhibits highly skewed\nclass distributions over the short term, and that these distributions can be\nclassified by much simpler models. We formulate the problem of detecting the\nshort-term skews online and exploiting models based on it as a new sequential\ndecision making problem dubbed the Online Bandit Problem, and present a new\nalgorithm to solve it. When applied to recognizing faces in TV shows and\nmovies, we realize end-to-end classification speedups of 2.4-7.8x\/2.6-11.2x (on\nGPU\/CPU) relative to a state-of-the-art convolutional neural network, at\ncompetitive accuracy.\n","id":476}
{"Unnamed: 0.1":11477,"Unnamed: 0":11477.0,"anchor":"Large scale modeling of antimicrobial resistance with interpretable\n  classifiers","positive":"  Antimicrobial resistance is an important public health concern that has\nimplications in the practice of medicine worldwide. Accurately predicting\nresistance phenotypes from genome sequences shows great promise in promoting\nbetter use of antimicrobial agents, by determining which antibiotics are likely\nto be effective in specific clinical cases. In healthcare, this would allow for\nthe design of treatment plans tailored for specific individuals, likely\nresulting in better clinical outcomes for patients with bacterial infections.\nIn this work, we present the recent work of Drouin et al. (2016) on using Set\nCovering Machines to learn highly interpretable models of antibiotic resistance\nand complement it by providing a large scale application of their method to the\nentire PATRIC database. We report prediction results for 36 new datasets and\npresent the Kover AMR platform, a new web-based tool allowing the visualization\nand interpretation of the generated models.\n","negative":"  As deep neural networks (DNNs) are applied to increasingly challenging\nproblems, they will need to be able to represent their own uncertainty.\nModeling uncertainty is one of the key features of Bayesian methods. Using\nBernoulli dropout with sampling at prediction time has recently been proposed\nas an efficient and well performing variational inference method for DNNs.\nHowever, sampling from other multiplicative noise based variational\ndistributions has not been investigated in depth. We evaluated Bayesian DNNs\ntrained with Bernoulli or Gaussian multiplicative masking of either the units\n(dropout) or the weights (dropconnect). We tested the calibration of the\nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on\nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of\nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or\nBernoulli, led to more robust representation of uncertainty compared to\nsampling of units. However, using either Gaussian or Bernoulli dropout led to\nincreased test set classification accuracy. Based on these findings we used\nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show\napproximates the use of a spike-and-slab variational distribution without\nincreasing the number of learned parameters. We found that spike-and-slab\nsampling had higher test set performance than Gaussian dropconnect and more\nrobustly represented its uncertainty compared to Bernoulli dropout.\n","id":477}
{"Unnamed: 0.1":11478,"Unnamed: 0":11478.0,"anchor":"Modeling trajectories of mental health: challenges and opportunities","positive":"  More than two thirds of mental health problems have their onset during\nchildhood or adolescence. Identifying children at risk for mental illness later\nin life and predicting the type of illness is not easy. We set out to develop a\nplatform to define subtypes of childhood social-emotional development using\nlongitudinal, multifactorial trait-based measures. Subtypes discovered through\nthis study could ultimately advance psychiatric knowledge of the early\nbehavioural signs of mental illness. To this extent we have examined two types\nof models: latent class mixture models and GP-based models. Our findings\nindicate that while GP models come close in accuracy of predicting future\ntrajectories, LCMMs predict the trajectories as well in a fraction of the time.\nUnfortunately, neither of the models are currently accurate enough to lead to\nimmediate clinical impact. The available data related to the development of\nchildhood mental health is often sparse with only a few time points measured\nand require novel methods with improved efficiency and accuracy.\n","negative":"  Following the very recent line of work on the ``generalized min-max'' (GMM)\nkernel, this study proposes the ``generalized intersection'' (GInt) kernel and\nthe related ``normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the ``asymmetrically transformed'' version of the GInt\nkernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search.\n","id":478}
{"Unnamed: 0.1":11479,"Unnamed: 0":11479.0,"anchor":"Algorithmic Songwriting with ALYSIA","positive":"  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems.\n","negative":"  Deep learning techniques have been widely applied, achieving state-of-the-art\nresults in various fields of study. This survey focuses on deep learning\nsolutions that target learning control policies for robotics applications. We\ncarry out our discussions on the two main paradigms for learning control with\ndeep networks: deep reinforcement learning and imitation learning. For deep\nreinforcement learning (DRL), we begin from traditional reinforcement learning\nalgorithms, showing how they are extended to the deep context and effective\nmechanisms that could be added on top of the DRL algorithms. We then introduce\nrepresentative works that utilize DRL to solve navigation and manipulation\ntasks in robotics. We continue our discussion on methods addressing the\nchallenge of the reality gap for transferring DRL policies trained in\nsimulation to real-world scenarios, and summarize robotics simulation platforms\nfor conducting DRL research. For imitation leaning, we go through its three\nmain categories, behavior cloning, inverse reinforcement learning and\ngenerative adversarial imitation learning, by introducing their formulations\nand their corresponding robotics applications. Finally, we discuss the open\nchallenges and research frontiers.\n","id":479}
{"Unnamed: 0.1":11480,"Unnamed: 0":11480.0,"anchor":"Trained Ternary Quantization","positive":"  Deep neural networks are widely used in machine learning applications.\nHowever, the deployment of large neural networks models can be difficult to\ndeploy on mobile devices with limited power budgets. To solve this problem, we\npropose Trained Ternary Quantization (TTQ), a method that can reduce the\nprecision of weights in neural networks to ternary values. This method has very\nlittle accuracy degradation and can even improve the accuracy of some models\n(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet\nmodel is trained from scratch, which means it's as easy as to train normal full\nprecision model. We highlight our trained quantization method that can learn\nboth ternary values and ternary assignment. During inference, only ternary\nvalues (2-bit weights) and scaling factors are needed, therefore our models are\nnearly 16x smaller than full-precision models. Our ternary models can also be\nviewed as sparse binary weight networks, which can potentially be accelerated\nwith custom circuit. Experiments on CIFAR-10 show that the ternary models\nobtained by trained quantization method outperform full-precision models of\nResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model\noutperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and\noutperforms previous ternary models by 3%.\n","negative":"  Classification, which involves finding rules that partition a given data set\ninto disjoint groups, is one class of data mining problems. Approaches proposed\nso far for mining classification rules for large databases are mainly decision\ntree based symbolic learning methods. The connectionist approach based on\nneural networks has been thought not well suited for data mining. One of the\nmajor reasons cited is that knowledge generated by neural networks is not\nexplicitly represented in the form of rules suitable for verification or\ninterpretation by humans. This paper examines this issue. With our newly\ndeveloped algorithms, rules which are similar to, or more concise than those\ngenerated by the symbolic methods can be extracted from the neural networks.\nThe data mining process using neural networks with the emphasis on rule\nextraction is described. Experimental results and comparison with previously\npublished works are presented.\n","id":480}
{"Unnamed: 0.1":11481,"Unnamed: 0":11481.0,"anchor":"Enhancing Use Case Points Estimation Method Using Soft Computing\n  Techniques","positive":"  Software estimation is a crucial task in software engineering. Software\nestimation encompasses cost, effort, schedule, and size. The importance of\nsoftware estimation becomes critical in the early stages of the software life\ncycle when the details of software have not been revealed yet. Several\ncommercial and non-commercial tools exist to estimate software in the early\nstages. Most software effort estimation methods require software size as one of\nthe important metric inputs and consequently, software size estimation in the\nearly stages becomes essential. One of the approaches that has been used for\nabout two decades in the early size and effort estimation is called use case\npoints. Use case points method relies on the use case diagram to estimate the\nsize and effort of software projects. Although the use case points method has\nbeen widely used, it has some limitations that might adversely affect the\naccuracy of estimation. This paper presents some techniques using fuzzy logic\nand neural networks to improve the accuracy of the use case points method.\nResults showed that an improvement up to 22% can be obtained using the proposed\napproach.\n","negative":"  Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.\n","id":481}
{"Unnamed: 0.1":11482,"Unnamed: 0":11482.0,"anchor":"Deep Learning of Robotic Tasks without a Simulator using Strong and Weak\n  Human Supervision","positive":"  We propose a scheme for training a computerized agent to perform complex\nhuman tasks such as highway steering. The scheme is designed to follow a\nnatural learning process whereby a human instructor teaches a computerized\ntrainee. The learning process consists of five elements: (i) unsupervised\nfeature learning; (ii) supervised imitation learning; (iii) supervised reward\ninduction; (iv) supervised safety module construction; and (v) reinforcement\nlearning. We implemented the last four elements of the scheme using deep\nconvolutional networks and applied it to successfully create a computerized\nagent capable of autonomous highway steering over the well-known racing game\nAssetto Corsa. We demonstrate that the use of the last four elements is\nessential to effectively carry out the steering task using vision alone,\nwithout access to a driving simulator internals, and operating in wall-clock\ntime. This is made possible also through the introduction of a safety network,\na novel way for preventing the agent from performing catastrophic mistakes\nduring the reinforcement learning stage.\n","negative":"  This paper presents privileged multi-label learning (PrML) to explore and\nexploit the relationship between labels in multi-label learning problems. We\nsuggest that for each individual label, it cannot only be implicitly connected\nwith other labels via the low-rank constraint over label predictors, but also\nits performance on examples can receive the explicit comments from other labels\ntogether acting as an \\emph{Oracle teacher}. We generate privileged label\nfeature for each example and its individual label, and then integrate it into\nthe framework of low-rank based multi-label learning. The proposed algorithm\ncan therefore comprehensively explore and exploit label relationships by\ninheriting all the merits of privileged information and low-rank constraints.\nWe show that PrML can be efficiently solved by dual coordinate descent\nalgorithm using iterative optimization strategy with cheap updates. Experiments\non benchmark datasets show that through privileged label features, the\nperformance can be significantly improved and PrML is superior to several\ncompeting methods in most cases.\n","id":482}
{"Unnamed: 0.1":11483,"Unnamed: 0":11483.0,"anchor":"Learning to superoptimize programs - Workshop Version","positive":"  Superoptimization requires the estimation of the best program for a given\ncomputational task. In order to deal with large programs, superoptimization\ntechniques perform a stochastic search. This involves proposing a modification\nof the current program, which is accepted or rejected based on the improvement\nachieved. The state of the art method uses uniform proposal distributions,\nwhich fails to exploit the problem structure to the fullest. To alleviate this\ndeficiency, we learn a proposal distribution over possible modifications using\nReinforcement Learning. We provide convincing results on the superoptimization\nof \"Hacker's Delight\" programs.\n","negative":"  In order to be useful, visualizations need to be interpretable. This paper\nuses a user-based approach to combine and assess quality measures in order to\nbetter model user preferences. Results show that cluster separability measures\nare outperformed by a neighborhood conservation measure, even though the former\nare usually considered as intuitively representative of user motives. Moreover,\ncombining measures, as opposed to using a single measure, further improves\nprediction performances.\n","id":483}
{"Unnamed: 0.1":11484,"Unnamed: 0":11484.0,"anchor":"Robust nonparametric nearest neighbor random process clustering","positive":"  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their generative models\nwithout prior knowledge of the model statistics and the number of generative\nmodels. Two algorithms, both using the $L^1$-distance between estimated power\nspectral densities (PSDs) as a measure of dissimilarity, are analyzed. The\nfirst one, termed nearest neighbor process clustering (NNPC), relies on\npartitioning the nearest neighbor graph of the observations via spectral\nclustering. The second algorithm, simply referred to as $k$-means (KM),\nconsists of a single $k$-means iteration with farthest point initialization and\nwas considered before in the literature, albeit with a different dissimilarity\nmeasure. We prove that both algorithms succeed with high probability in the\npresence of noise and missing entries, and even when the generative process\nPSDs overlap significantly, all provided that the observation length is\nsufficiently large. Our results quantify the tradeoff between the overlap of\nthe generative process PSDs, the observation length, the fraction of missing\nentries, and the noise variance. Finally, we provide extensive numerical\nresults for synthetic and real data and find that NNPC outperforms\nstate-of-the-art algorithms in human motion sequence clustering.\n","negative":"  We consider the problem of estimating the class prior in an unlabeled\ndataset. Under the assumption that an additional labeled dataset is available,\nthe class prior can be estimated by fitting a mixture of class-wise data\ndistributions to the unlabeled data distribution. However, in practice, such an\nadditional labeled dataset is often not available. In this paper, we show that,\nwith additional samples coming only from the positive class, the class prior of\nthe unlabeled dataset can be estimated correctly. Our key idea is to use\nproperly penalized divergences for model fitting to cancel the error caused by\nthe absence of negative samples. We further show that the use of the penalized\n$L_1$-distance gives a computationally efficient algorithm with an analytic\nsolution. The consistency, stability, and estimation error are theoretically\nanalyzed. Finally, we experimentally demonstrate the usefulness of the proposed\nmethod.\n","id":484}
{"Unnamed: 0.1":11485,"Unnamed: 0":11485.0,"anchor":"Properties and Bayesian fitting of restricted Boltzmann machines","positive":"  A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure.\n","negative":"  We develop a neural network model to classify liver cancer patients into\nhigh-risk and low-risk groups using genomic data. Our approach provides a novel\ntechnique to classify big data sets using neural network models. We preprocess\nthe data before training the neural network models. We first expand the data\nusing wavelet analysis. We then compress the wavelet coefficients by mapping\nthem onto a new scaled orthonormal coordinate system. Then the data is used to\ntrain a neural network model that enables us to classify cancer patients into\ntwo different classes of high-risk and low-risk patients. We use the\nleave-one-out approach to build a neural network model. This neural network\nmodel enables us to classify a patient using genomic data as a high-risk or\nlow-risk patient without any information about the survival time of the\npatient. The results from genomic data analysis are compared with survival time\nanalysis. It is shown that the expansion and compression of data using wavelet\nanalysis and singular value decomposition (SVD) is essential to train the\nneural network model.\n","id":485}
{"Unnamed: 0.1":11486,"Unnamed: 0":11486.0,"anchor":"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision (Short Version)","positive":"  Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge.\n","negative":"  Sparsity-based approaches have been popular in many applications in image\nprocessing and imaging. Compressed sensing exploits the sparsity of images in a\ntransform domain or dictionary to improve image recovery from undersampled\nmeasurements. In the context of inverse problems in dynamic imaging, recent\nresearch has demonstrated the promise of sparsity and low-rank techniques. For\nexample, the patches of the underlying data are modeled as sparse in an\nadaptive dictionary domain, and the resulting image and dictionary estimation\nfrom undersampled measurements is called dictionary-blind compressed sensing,\nor the dynamic image sequence is modeled as a sum of low-rank and sparse (in\nsome transform domain) components (L+S model) that are estimated from limited\nmeasurements. In this work, we investigate a data-adaptive extension of the L+S\nmodel, dubbed LASSI, where the temporal image sequence is decomposed into a\nlow-rank component and a component whose spatiotemporal (3D) patches are sparse\nin some adaptive dictionary domain. We investigate various formulations and\nefficient methods for jointly estimating the underlying dynamic signal\ncomponents and the spatiotemporal dictionary from limited measurements. We also\nobtain efficient sparsity penalized dictionary-blind compressed sensing methods\nas special cases of our LASSI approaches. Our numerical experiments demonstrate\nthe promising performance of LASSI schemes for dynamic magnetic resonance image\nreconstruction from limited k-t space data compared to recent methods such as\nk-t SLR and L+S, and compared to the proposed dictionary-blind compressed\nsensing method.\n","id":486}
{"Unnamed: 0.1":11487,"Unnamed: 0":11487.0,"anchor":"Intra-day Activity Better Predicts Chronic Conditions","positive":"  In this work we investigate intra-day patterns of activity on a population of\n7,261 users of mobile health wearable devices and apps. We show that: (1) using\nintra-day step and sleep data recorded from passive trackers significantly\nimproves classification performance on self-reported chronic conditions related\nto mental health and nervous system disorders, (2) Convolutional Neural\nNetworks achieve top classification performance vs. baseline models when\ntrained directly on multivariate time series of activity data, and (3) jointly\npredicting all condition classes via multi-task learning can be leveraged to\nextract features that generalize across data sets and achieve the highest\nclassification performance.\n","negative":"  Social dynamics is concerned primarily with interactions among individuals\nand the resulting group behaviors, modeling the temporal evolution of social\nsystems via the interactions of individuals within these systems. In\nparticular, the availability of large-scale data from social networks and\nsensor networks offers an unprecedented opportunity to predict state-changing\nevents at the individual level. Examples of such events include disease\ntransmission, opinion transition in elections, and rumor propagation. Unlike\nprevious research focusing on the collective effects of social systems, this\nstudy makes efficient inferences at the individual level. In order to cope with\ndynamic interactions among a large number of individuals, we introduce the\nstochastic kinetic model to capture adaptive transition probabilities and\npropose an efficient variational inference algorithm the complexity of which\ngrows linearly --- rather than exponentially --- with the number of\nindividuals. To validate this method, we have performed epidemic-dynamics\nexperiments on wireless sensor network data collected from more than ten\nthousand people over three years. The proposed algorithm was used to track\ndisease transmission and predict the probability of infection for each\nindividual. Our results demonstrate that this method is more efficient than\nsampling while nonetheless achieving high accuracy.\n","id":487}
{"Unnamed: 0.1":11488,"Unnamed: 0":11488.0,"anchor":"Optimal and Adaptive Off-policy Evaluation in Contextual Bandits","positive":"  We study the off-policy evaluation problem---estimating the value of a target\npolicy using data collected by another policy---under the contextual bandit\nmodel. We consider the general (agnostic) setting without access to a\nconsistent model of rewards and establish a minimax lower bound on the mean\nsquared error (MSE). The bound is matched up to constants by the inverse\npropensity scoring (IPS) and doubly robust (DR) estimators. This highlights the\ndifficulty of the agnostic contextual setting, in contrast with multi-armed\nbandits and contextual bandits with access to a consistent reward model, where\nIPS is suboptimal. We then propose the SWITCH estimator, which can use an\nexisting reward model (not necessarily consistent) to achieve a better\nbias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and\ndemonstrate its benefits empirically on a diverse collection of data sets,\noften outperforming prior work by orders of magnitude.\n","negative":"  The problem where a tropical cyclone intensifies dramatically within a short\nperiod of time is known as rapid intensification. This has been one of the\nmajor challenges for tropical weather forecasting. Recurrent neural networks\nhave been promising for time series problems which makes them appropriate for\nrapid intensification. In this paper, recurrent neural networks are used to\npredict rapid intensification cases of tropical cyclones from the South Pacific\nand South Indian Ocean regions. A class imbalanced problem is encountered which\nmakes it very challenging to achieve promising performance. A simple strategy\nwas proposed to include more positive cases for detection where the false\npositive rate was slightly improved. The limitations of building an efficient\nsystem remains due to the challenges of addressing the class imbalance problem\nencountered for rapid intensification prediction. This motivates further\nresearch in using innovative machine learning methods.\n","id":488}
{"Unnamed: 0.1":11489,"Unnamed: 0":11489.0,"anchor":"Deep Metric Learning via Facility Location","positive":"  Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.\n","negative":"  Recurrent neural networks have been very successful at predicting sequences\nof words in tasks such as language modeling. However, all such models are based\non the conventional classification framework, where the model is trained\nagainst one-hot targets, and each word is represented both as an input and as\nan output in isolation. This causes inefficiencies in learning both in terms of\nutilizing all of the information and in terms of the number of parameters\nneeded to train. We introduce a novel theoretical framework that facilitates\nbetter learning in language modeling, and show that our framework leads to\ntying together the input embedding and the output projection matrices, greatly\nreducing the number of trainable variables. Our framework leads to state of the\nart performance on the Penn Treebank with a variety of network models.\n","id":489}
{"Unnamed: 0.1":11490,"Unnamed: 0":11490.0,"anchor":"Known Unknowns: Uncertainty Quality in Bayesian Neural Networks","positive":"  We evaluate the uncertainty quality in neural networks using anomaly\ndetection. We extract uncertainty measures (e.g. entropy) from the predictions\nof candidate models, use those measures as features for an anomaly detector,\nand gauge how well the detector differentiates known from unknown classes. We\nassign higher uncertainty quality to candidate models that lead to better\ndetectors. We also propose a novel method for sampling a variational\napproximation of a Bayesian neural network, called One-Sample Bayesian\nApproximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We\ncompare the following candidate neural network models: Maximum Likelihood,\nBayesian Dropout, OSBA, and --- for MNIST --- the standard variational\napproximation. We show that Bayesian Dropout and OSBA provide better\nuncertainty information than Maximum Likelihood, and are essentially equivalent\nto the standard variational approximation, but much faster.\n","negative":"  Despite the overwhelming success of deep learning in various speech\nprocessing tasks, the problem of separating simultaneous speakers in a mixture\nremains challenging. Two major difficulties in such systems are the arbitrary\nsource permutation and unknown number of sources in the mixture. We propose a\nnovel deep learning framework for single channel speech separation by creating\nattractor points in high dimensional embedding space of the acoustic signals\nwhich pull together the time-frequency bins corresponding to each source.\nAttractor points in this study are created by finding the centroids of the\nsources in the embedding space, which are subsequently used to determine the\nsimilarity of each bin in the mixture to each source. The network is then\ntrained to minimize the reconstruction error of each source by optimizing the\nembeddings. The proposed model is different from prior works in that it\nimplements an end-to-end training, and it does not depend on the number of\nsources in the mixture. Two strategies are explored in the test time, K-means\nand fixed attractor points, where the latter requires no post-processing and\ncan be implemented in real-time. We evaluated our system on Wall Street Journal\ndataset and show 5.49\\% improvement over the previous state-of-the-art methods.\n","id":490}
{"Unnamed: 0.1":11491,"Unnamed: 0":11491.0,"anchor":"Deep Image Category Discovery using a Transferred Similarity Function","positive":"  Automatically discovering image categories in unlabeled natural images is one\nof the important goals of unsupervised learning. However, the task is\nchallenging and even human beings define visual categories based on a large\namount of prior knowledge. In this paper, we similarly utilize prior knowledge\nto facilitate the discovery of image categories. We present a novel end-to-end\nnetwork to map unlabeled images to categories as a clustering network. We\npropose that this network can be learned with contrastive loss which is only\nbased on weak binary pair-wise constraints. Such binary constraints can be\nlearned from datasets in other domains as transferred similarity functions,\nwhich mimic a simple knowledge transfer. We first evaluate our experiments on\nthe MNIST dataset as a proof of concept, based on predicted similarities\ntrained on Omniglot, showing a 99\\% accuracy which significantly outperforms\nclustering based approaches. Then we evaluate the discovery performance on\nCifar-10, STL-10, and ImageNet, which achieves both state-of-the-art accuracy\nand shows it can be scalable to various large natural images.\n","negative":"  Clustering is a widely used unsupervised learning method for finding\nstructure in the data. However, the resulting clusters are typically presented\nwithout any guarantees on their robustness; slightly changing the used data\nsample or re-running a clustering algorithm involving some stochastic component\nmay lead to completely different clusters. There is, hence, a need for\ntechniques that can quantify the instability of the generated clusters. In this\nstudy, we propose a technique for quantifying the instability of a clustering\nsolution and for finding robust clusters, termed core clusters, which\ncorrespond to clusters where the co-occurrence probability of each data item\nwithin a cluster is at least $1 - \\alpha$. We demonstrate how solving the core\nclustering problem is linked to finding the largest maximal cliques in a graph.\nWe show that the method can be used with both clustering and classification\nalgorithms. The proposed method is tested on both simulated and real datasets.\nThe results show that the obtained clusters indeed meet the guarantees on\nrobustness.\n","id":491}
{"Unnamed: 0.1":11492,"Unnamed: 0":11492.0,"anchor":"Deep Symbolic Representation Learning for Heterogeneous Time-series\n  Classification","positive":"  In this paper, we consider the problem of event classification with\nmulti-variate time series data consisting of heterogeneous (continuous and\ncategorical) variables. The complex temporal dependencies between the variables\ncombined with sparsity of the data makes the event classification problem\nparticularly challenging. Most state-of-art approaches address this either by\ndesigning hand-engineered features or breaking up the problem over homogeneous\nvariates. In this work, we propose and compare three representation learning\nalgorithms over symbolized sequences which enables classification of\nheterogeneous time-series data using a deep architecture. The proposed\nrepresentations are trained jointly along with the rest of the network\narchitecture in an end-to-end fashion that makes the learned features\ndiscriminative for the given task. Experiments on three real-world datasets\ndemonstrate the effectiveness of the proposed approaches.\n","negative":"  The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.\n","id":492}
{"Unnamed: 0.1":11493,"Unnamed: 0":11493.0,"anchor":"Cryptocurrency Portfolio Management with Deep Reinforcement Learning","positive":"  Portfolio management is the decision-making process of allocating an amount\nof fund into different financial investment products. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. This paper presents a\nmodel-less convolutional neural network with historic prices of a set of\nfinancial assets as its input, outputting portfolio weights of the set. The\nnetwork is trained with 0.7 years' price data from a cryptocurrency exchange.\nThe training is done in a reinforcement manner, maximizing the accumulative\nreturn, which is regarded as the reward function of the network. Backtest\ntrading experiments with trading period of 30 minutes is conducted in the same\nmarket, achieving 10-fold returns in 1.8 months' periods. Some recently\npublished portfolio selection strategies are also used to perform the same\nback-tests, whose results are compared with the neural network. The network is\nnot limited to cryptocurrency, but can be applied to any other financial\nmarkets.\n","negative":"  In recent years, interest in recommender research has shifted from explicit\nfeedback towards implicit feedback data. A diversity of complex models has been\nproposed for a wide variety of applications. Despite this, learning from\nimplicit feedback is still computationally challenging. So far, most work\nrelies on stochastic gradient descent (SGD) solvers which are easy to derive,\nbut in practice challenging to apply, especially for tasks with many items. For\nthe simple matrix factorization model, an efficient coordinate descent (CD)\nsolver has been previously proposed. However, efficient CD approaches have not\nbeen derived for more complex models.\n  In this paper, we provide a new framework for deriving efficient CD\nalgorithms for complex recommender models. We identify and introduce the\nproperty of k-separable models. We show that k-separability is a sufficient\nproperty to allow efficient optimization of implicit recommender problems with\nCD. We illustrate this framework on a variety of state-of-the-art models\nincluding factorization machines and Tucker decomposition. To summarize, our\nwork provides the theory and building blocks to derive efficient implicit CD\nalgorithms for complex recommender models.\n","id":493}
{"Unnamed: 0.1":11494,"Unnamed: 0":11494.0,"anchor":"Message Passing Multi-Agent GANs","positive":"  Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution.\n","negative":"  Reinforcement learning optimizes policies for expected cumulative reward.\nNeed the supervision be so narrow? Reward is delayed and sparse for many tasks,\nmaking it a difficult and impoverished signal for end-to-end optimization. To\naugment reward, we consider a range of self-supervised tasks that incorporate\nstates, actions, and successors to provide auxiliary losses. These losses offer\nubiquitous and instantaneous supervision for representation learning even in\nthe absence of reward. While current results show that learning from reward\nalone is feasible, pure reinforcement learning methods are constrained by\ncomputational and data efficiency issues that can be remedied by auxiliary\nlosses. Self-supervised pre-training and joint optimization improve the data\nefficiency and policy returns of end-to-end reinforcement learning.\n","id":494}
{"Unnamed: 0.1":11495,"Unnamed: 0":11495.0,"anchor":"Ranking Biomarkers Through Mutual Information","positive":"  We study information theoretic methods for ranking biomarkers. In clinical\ntrials there are two, closely related, types of biomarkers: predictive and\nprognostic, and disentangling them is a key challenge. Our first step is to\nphrase biomarker ranking in terms of optimizing an information theoretic\nquantity. This formalization of the problem will enable us to derive rankings\nof predictive\/prognostic biomarkers, by estimating different, high dimensional,\nconditional mutual information terms. To estimate these terms, we suggest\nefficient low dimensional approximations, and we derive an empirical Bayes\nestimator, which is suitable for small or sparse datasets. Finally, we\nintroduce a new visualisation tool that captures the prognostic and the\npredictive strength of a set of biomarkers. We believe this representation will\nprove to be a powerful tool in biomarker discovery.\n","negative":"  We propose a method to classify the causal relationship between two discrete\nvariables given only the joint distribution of the variables, acknowledging\nthat the method is subject to an inherent baseline error. We assume that the\ncausal system is acyclicity, but we do allow for hidden common causes. Our\nalgorithm presupposes that the probability distributions $P(C)$ of a cause $C$\nis independent from the probability distribution $P(E\\mid C)$ of the\ncause-effect mechanism. While our classifier is trained with a Bayesian\nassumption of flat hyperpriors, we do not make this assumption about our test\ndata. This work connects to recent developments on the identifiability of\ncausal models over continuous variables under the assumption of \"independent\nmechanisms\". Carefully-commented Python notebooks that reproduce all our\nexperiments are available online at\nhttp:\/\/vision.caltech.edu\/~kchalupk\/code.html.\n","id":495}
{"Unnamed: 0.1":11496,"Unnamed: 0":11496.0,"anchor":"A One class Classifier based Framework using SVDD : Application to an\n  Imbalanced Geological Dataset","positive":"  Evaluation of hydrocarbon reservoir requires classification of petrophysical\nproperties from available dataset. However, characterization of reservoir\nattributes is difficult due to the nonlinear and heterogeneous nature of the\nsubsurface physical properties. In this context, present study proposes a\ngeneralized one class classification framework based on Support Vector Data\nDescription (SVDD) to classify a reservoir characteristic water saturation into\ntwo classes (Class high and Class low) from four logs namely gamma ray, neutron\nporosity, bulk density, and P sonic using an imbalanced dataset. A comparison\nis carried out among proposed framework and different supervised classification\nalgorithms in terms of g metric means and execution time. Experimental results\nshow that proposed framework has outperformed other classifiers in terms of\nthese performance evaluators. It is envisaged that the classification analysis\nperformed in this study will be useful in further reservoir modeling.\n","negative":"  Introduction to deep neural networks and their history.\n","id":496}
{"Unnamed: 0.1":11497,"Unnamed: 0":11497.0,"anchor":"Diagnostic Prediction Using Discomfort Drawings","positive":"  In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we extend a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. Experimental results show reasonable predictions of\ndiagnostic labels given an unseen discomfort drawing. The positive result\nindicates a significant potential of machine learning to be used for parts of\nthe pain diagnostic process and to be a decision support system for physicians\nand other health care personnel.\n","negative":"  Many time series are generated by a set of entities that interact with one\nanother over time. This paper introduces a broad, flexible framework to learn\nfrom multiple inter-dependent time series generated by such entities. Our\nframework explicitly models the entities and their interactions through time.\nIt achieves this by building on the capabilities of Recurrent Neural Networks,\nwhile also offering several ways to incorporate domain knowledge\/constraints\ninto the model architecture. The capabilities of our approach are showcased\nthrough an application to weather prediction, which shows gains over strong\nbaselines.\n","id":497}
{"Unnamed: 0.1":11498,"Unnamed: 0":11498.0,"anchor":"An Asymptotically Optimal Contextual Bandit Algorithm Using Hierarchical\n  Structures","positive":"  We propose online algorithms for sequential learning in the contextual\nmulti-armed bandit setting. Our approach is to partition the context space and\nthen optimally combine all of the possible mappings between the partition\nregions and the set of bandit arms in a data driven manner. We show that in our\napproach, the best mapping is able to approximate the best arm selection policy\nto any desired degree under mild Lipschitz conditions. Therefore, we design our\nalgorithms based on the optimal adaptive combination and asymptotically achieve\nthe performance of the best mapping as well as the best arm selection policy.\nThis optimality is also guaranteed to hold even in adversarial environments\nsince we do not rely on any statistical assumptions regarding the contexts or\nthe loss of the bandit arms. Moreover, we design efficient implementations for\nour algorithms in various hierarchical partitioning structures such as\nlexicographical or arbitrary position splitting and binary trees (and several\nother partitioning examples). For instance, in the case of binary tree\npartitioning, the computational complexity is only log-linear in the number of\nregions in the finest partition. In conclusion, we provide significant\nperformance improvements by introducing upper bounds (w.r.t. the best arm\nselection policy) that are mathematically proven to vanish in the average loss\nper round sense at a faster rate compared to the state-of-the-art. Our\nexperimental work extensively covers various scenarios ranging from bandit\nsettings to multi-class classification with real and synthetic data. In these\nexperiments, we show that our algorithms are highly superior over the\nstate-of-the-art techniques while maintaining the introduced mathematical\nguarantees and a computationally decent scalability.\n","negative":"  The combinatorial stochastic semi-bandit problem is an extension of the\nclassical multi-armed bandit problem in which an algorithm pulls more than one\narm at each stage and the rewards of all pulled arms are revealed. One\ndifference with the single arm variant is that the dependency structure of the\narms is crucial. Previous works on this setting either used a worst-case\napproach or imposed independence of the arms. We introduce a way to quantify\nthe dependency structure of the problem and design an algorithm that adapts to\nit. The algorithm is based on linear regression and the analysis develops\ntechniques from the linear bandit literature. By comparing its performance to a\nnew lower bound, we prove that it is optimal, up to a poly-logarithmic factor\nin the number of pulled arms.\n","id":498}
{"Unnamed: 0.1":11499,"Unnamed: 0":11499.0,"anchor":"Implicit Modeling -- A Generalization of Discriminative and Generative\n  Approaches","positive":"  We propose a new modeling approach that is a generalization of generative and\ndiscriminative models. The core idea is to use an implicit parameterization of\na joint probability distribution by specifying only the conditional\ndistributions. The proposed scheme combines the advantages of both worlds -- it\ncan use powerful complex discriminative models as its parts, having at the same\ntime better generalization capabilities. We thoroughly evaluate the proposed\nmethod for a simple classification task with artificial data and illustrate its\nadvantages for real-word scenarios on a semantic image segmentation problem.\n","negative":"  We present a new notion of probabilistic duality for random variables\ninvolving mixture distributions. Using this notion, we show how to implement a\nhighly-parallelizable Gibbs sampler for weakly coupled discrete pairwise\ngraphical models with strictly positive factors that requires almost no\npreprocessing and is easy to implement. Moreover, we show how our method can be\ncombined with blocking to improve mixing. Even though our method leads to\ninferior mixing times compared to a sequential Gibbs sampler, we argue that our\nmethod is still very useful for large dynamic networks, where factors are added\nand removed on a continuous basis, as it is hard to maintain a graph coloring\nin this setup. Similarly, our method is useful for parallelizing Gibbs sampling\nin graphical models that do not allow for graph colorings with a small number\nof colors such as densely connected graphs.\n","id":499}
{"Unnamed: 0.1":11500,"Unnamed: 0":11500.0,"anchor":"Learning Adversary-Resistant Deep Neural Networks","positive":"  Deep neural networks (DNNs) have proven to be quite effective in a vast array\nof machine learning tasks, with recent examples in cyber security and\nautonomous vehicles. Despite the superior performance of DNNs in these\napplications, it has been recently shown that these models are susceptible to a\nparticular type of attack that exploits a fundamental flaw in their design.\nThis attack consists of generating particular synthetic examples referred to as\nadversarial samples. These samples are constructed by slightly manipulating\nreal data-points in order to \"fool\" the original DNN model, forcing it to\nmis-classify previously correctly classified samples with high confidence.\nAddressing this flaw in the model is essential if DNNs are to be used in\ncritical applications such as those in cyber security.\n  Previous work has provided various learning algorithms to enhance the\nrobustness of DNN models, and they all fall into the tactic of \"security\nthrough obscurity\". This means security can be guaranteed only if one can\nobscure the learning algorithms from adversaries. Once the learning technique\nis disclosed, DNNs protected by these defense mechanisms are still susceptible\nto adversarial samples. In this work, we investigate this issue shared across\nprevious research work and propose a generic approach to escalate a DNN's\nresistance to adversarial samples. More specifically, our approach integrates a\ndata transformation module with a DNN, making it robust even if we reveal the\nunderlying learning algorithm. To demonstrate the generality of our proposed\napproach and its potential for handling cyber security applications, we\nevaluate our method and several other existing solutions on datasets publicly\navailable. Our results indicate that our approach typically provides superior\nclassification performance and resistance in comparison with state-of-art\nsolutions.\n","negative":"  Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics.\n","id":500}
{"Unnamed: 0.1":11501,"Unnamed: 0":11501.0,"anchor":"Semi-Supervised Learning via Sparse Label Propagation","positive":"  This work proposes a novel method for semi-supervised learning from partially\nlabeled massive network-structured datasets, i.e., big data over networks. We\nmodel the underlying hypothesis, which relates data points to labels, as a\ngraph signal, defined over some graph (network) structure intrinsic to the\ndataset. Following the key principle of supervised learning, i.e., similar\ninputs yield similar outputs, we require the graph signals induced by labels to\nhave small total variation. Accordingly, we formulate the problem of learning\nthe labels of data points as a non-smooth convex optimization problem which\namounts to balancing between the empirical loss, i.e., the discrepancy with\nsome partially available label information, and the smoothness quantified by\nthe total variation of the learned graph signal. We solve this optimization\nproblem by appealing to a recently proposed preconditioned variant of the\npopular primal-dual method by Pock and Chambolle, which results in a sparse\nlabel propagation algorithm. This learning algorithm allows for a highly\nscalable implementation as message passing over the underlying data graph. By\napplying concepts of compressed sensing to the learning problem, we are also\nable to provide a transparent sufficient condition on the underlying network\nstructure such that accurate learning of the labels is possible. We also\npresent an implementation of the message passing formulation allows for a\nhighly scalable implementation in big data frameworks.\n","negative":"  We propose a new modeling approach that is a generalization of generative and\ndiscriminative models. The core idea is to use an implicit parameterization of\na joint probability distribution by specifying only the conditional\ndistributions. The proposed scheme combines the advantages of both worlds -- it\ncan use powerful complex discriminative models as its parts, having at the same\ntime better generalization capabilities. We thoroughly evaluate the proposed\nmethod for a simple classification task with artificial data and illustrate its\nadvantages for real-word scenarios on a semantic image segmentation problem.\n","id":501}
{"Unnamed: 0.1":11502,"Unnamed: 0":11502.0,"anchor":"Zeroth-order Asynchronous Doubly Stochastic Algorithm with Variance\n  Reduction","positive":"  Zeroth-order (derivative-free) optimization attracts a lot of attention in\nmachine learning, because explicit gradient calculations may be computationally\nexpensive or infeasible. To handle large scale problems both in volume and\ndimension, recently asynchronous doubly stochastic zeroth-order algorithms were\nproposed. The convergence rate of existing asynchronous doubly stochastic\nzeroth order algorithms is $O(\\frac{1}{\\sqrt{T}})$ (also for the sequential\nstochastic zeroth-order optimization algorithms). In this paper, we focus on\nthe finite sums of smooth but not necessarily convex functions, and propose an\nasynchronous doubly stochastic zeroth-order optimization algorithm using the\naccelerated technology of variance reduction (AsyDSZOVR). Rigorous theoretical\nanalysis show that the convergence rate can be improved from\n$O(\\frac{1}{\\sqrt{T}})$ the best result of existing algorithms to\n$O(\\frac{1}{T})$. Also our theoretical results is an improvement to the ones of\nthe sequential stochastic zeroth-order optimization algorithms.\n","negative":"  The fuzzy $K$-means problem is a popular generalization of the well-known\n$K$-means problem to soft clusterings. We present the first coresets for fuzzy\n$K$-means with size linear in the dimension, polynomial in the number of\nclusters, and poly-logarithmic in the number of points. We show that these\ncoresets can be employed in the computation of a $(1+\\epsilon)$-approximation\nfor fuzzy $K$-means, improving previously presented results. We further show\nthat our coresets can be maintained in an insertion-only streaming setting,\nwhere data points arrive one-by-one.\n","id":502}
{"Unnamed: 0.1":11503,"Unnamed: 0":11503.0,"anchor":"Extracting Implicit Social Relation for Social Recommendation Techniques\n  in User Rating Prediction","positive":"  Recommendation plays an increasingly important role in our daily lives.\nRecommender systems automatically suggest items to users that might be\ninteresting for them. Recent studies illustrate that incorporating social trust\nin Matrix Factorization methods demonstrably improves accuracy of rating\nprediction. Such approaches mainly use the trust scores explicitly expressed by\nusers. However, it is often challenging to have users provide explicit trust\nscores of each other. There exist quite a few works, which propose Trust\nMetrics to compute and predict trust scores between users based on their\ninteractions. In this paper, first we present how social relation can be\nextracted from users' ratings to items by describing Hellinger distance between\nusers in recommender systems. Then, we propose to incorporate the predicted\ntrust scores into social matrix factorization models. By analyzing social\nrelation extraction from three well-known real-world datasets, which both:\ntrust and recommendation data available, we conclude that using the implicit\nsocial relation in social recommendation techniques has almost the same\nperformance compared to the actual trust scores explicitly expressed by users.\nHence, we build our method, called Hell-TrustSVD, on top of the\nstate-of-the-art social recommendation technique to incorporate both the\nextracted implicit social relations and ratings given by users on the\nprediction of items for an active user. To the best of our knowledge, this is\nthe first work to extend TrustSVD with extracted social trust information. The\nexperimental results support the idea of employing implicit trust into matrix\nfactorization whenever explicit trust is not available, can perform much better\nthan the state-of-the-art approaches in user rating prediction.\n","negative":"  Sequence labeling architectures use word embeddings for capturing similarity,\nbut suffer when handling previously unseen or rare words. We investigate\ncharacter-level extensions to such models and propose a novel architecture for\ncombining alternative word representations. By using an attention mechanism,\nthe model is able to dynamically decide how much information to use from a\nword- or character-level component. We evaluated different architectures on a\nrange of sequence labeling datasets, and character-level extensions were found\nto improve performance on every benchmark. In addition, the proposed\nattention-based architecture delivered the best results even with a smaller\nnumber of trainable parameters.\n","id":503}
{"Unnamed: 0.1":11504,"Unnamed: 0":11504.0,"anchor":"Understanding and Optimizing the Performance of Distributed Machine\n  Learning Applications on Apache Spark","positive":"  In this paper we explore the performance limits of Apache Spark for machine\nlearning applications. We begin by analyzing the characteristics of a\nstate-of-the-art distributed machine learning algorithm implemented in Spark\nand compare it to an equivalent reference implementation using the high\nperformance computing framework MPI. We identify critical bottlenecks of the\nSpark framework and carefully study their implications on the performance of\nthe algorithm. In order to improve Spark performance we then propose a number\nof practical techniques to alleviate some of its overheads. However, optimizing\ncomputational efficiency and framework related overheads is not the only key to\nperformance -- we demonstrate that in order to get the best performance out of\nany implementation it is necessary to carefully tune the algorithm to the\nrespective trade-off between computation time and communication latency. The\noptimal trade-off depends on both the properties of the distributed algorithm\nas well as infrastructure and framework-related characteristics. Finally, we\napply these technical and algorithmic optimizations to three different\ndistributed linear machine learning algorithms that have been implemented in\nSpark. We present results using five large datasets and demonstrate that by\nusing the proposed optimizations, we can achieve a reduction in the performance\ndifference between Spark and MPI from 20x to 2x.\n","negative":"  Both scientists and children make important structural discoveries, yet their\ncomputational underpinnings are not well understood. Structure discovery has\npreviously been formalized as probabilistic inference about the right\nstructural form --- where form could be a tree, ring, chain, grid, etc. [Kemp &\nTenenbaum (2008). The discovery of structural form. PNAS, 105(3), 10687-10692].\nWhile this approach can learn intuitive organizations, including a tree for\nanimals and a ring for the color circle, it assumes a strong inductive bias\nthat considers only these particular forms, and each form is explicitly\nprovided as initial knowledge. Here we introduce a new computational model of\nhow organizing structure can be discovered, utilizing a broad hypothesis space\nwith a preference for sparse connectivity. Given that the inductive bias is\nmore general, the model's initial knowledge shows little qualitative\nresemblance to some of the discoveries it supports. As a consequence, the model\ncan also learn complex structures for domains that lack intuitive description,\nas well as predict human property induction judgments without explicit\nstructural forms. By allowing form to emerge from sparsity, our approach\nclarifies how both the richness and flexibility of human conceptual\norganization can coexist.\n","id":504}
{"Unnamed: 0.1":11505,"Unnamed: 0":11505.0,"anchor":"Support vector regression model for BigData systems","positive":"  Nowadays Big Data are becoming more and more important. Many sectors of our\neconomy are now guided by data-driven decision processes. Big Data and business\nintelligence applications are facilitated by the MapReduce programming model\nwhile, at infrastructural layer, cloud computing provides flexible and cost\neffective solutions for allocating on demand large clusters. In such systems,\ncapacity allocation, which is the ability to optimally size minimal resources\nfor achieve a certain level of performance, is a key challenge to enhance\nperformance for MapReduce jobs and minimize cloud resource costs. In order to\ndo so, one of the biggest challenge is to build an accurate performance model\nto estimate job execution time of MapReduce systems. Previous works applied\nsimulation based models for modeling such systems. Although this approach can\naccurately describe the behavior of Big Data clusters, it is too\ncomputationally expensive and does not scale to large system. We try to\novercome these issues by applying machine learning techniques. More precisely\nwe focus on Support Vector Regression (SVR) which is intrinsically more robust\nw.r.t other techniques, like, e.g., neural networks, and less sensitive to\noutliers in the training set. To better investigate these benefits, we compare\nSVR to linear regression.\n","negative":"  The inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations.\n","id":505}
{"Unnamed: 0.1":11506,"Unnamed: 0":11506.0,"anchor":"Simple and Scalable Predictive Uncertainty Estimation using Deep\n  Ensembles","positive":"  Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.\n","negative":"  Neural networks with rectified linear unit activations are essentially\nmultivariate linear splines. As such, one of many ways to measure the\n\"complexity\" or \"expressivity\" of a neural network is to count the number of\nknots in the spline model. We study the number of knots in fully-connected\nfeedforward neural networks with rectified linear unit activation functions. We\nintentionally keep the neural networks very simple, so as to make theoretical\nanalyses more approachable. An induction on the number of layers $l$ reveals a\ntight upper bound on the number of knots in $\\mathbb{R} \\to \\mathbb{R}^p$ deep\nneural networks. With $n_i \\gg 1$ neurons in layer $i = 1, \\dots, l$, the upper\nbound is approximately $n_1 \\dots n_l$. We then show that the exact upper bound\nis tight, and we demonstrate the upper bound with an example. The purpose of\nthese analyses is to pave a path for understanding the behavior of general\n$\\mathbb{R}^q \\to \\mathbb{R}^p$ neural networks.\n","id":506}
{"Unnamed: 0.1":11507,"Unnamed: 0":11507.0,"anchor":"Generalized RBF kernel for incomplete data","positive":"  We construct $\\bf genRBF$ kernel, which generalizes the classical Gaussian\nRBF kernel to the case of incomplete data. We model the uncertainty contained\nin missing attributes making use of data distribution and associate every point\nwith a conditional probability density function. This allows to embed\nincomplete data into the function space and to define a kernel between two\nmissing data points based on scalar product in $L_2$. Experiments show that\nintroduced kernel applied to SVM classifier gives better results than other\nstate-of-the-art methods, especially in the case when large number of features\nis missing. Moreover, it is easy to implement and can be used together with any\nkernel approaches with no additional modifications.\n","negative":"  Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.\n","id":507}
{"Unnamed: 0.1":11508,"Unnamed: 0":11508.0,"anchor":"A Nonparametric Latent Factor Model For Location-Aware Video\n  Recommendations","positive":"  We are interested in learning customers' video preferences from their\nhistoric viewing patterns and geographical location. We consider a Bayesian\nlatent factor modeling approach for this task. In order to tune the complexity\nof the model to best represent the data, we make use of Bayesian nonparameteric\ntechniques. We describe an inference technique that can scale to large\nreal-world data sets. Finally we show results obtained by applying the model to\na large internal Netflix data set, that illustrates that the model was able to\ncapture interesting relationships between viewing patterns and geographical\nlocation.\n","negative":"  Music summarization allows for higher efficiency in processing, storage, and\nsharing of datasets. Machine-oriented approaches, being agnostic to human\nconsumption, optimize these aspects even further. Such summaries have already\nbeen successfully validated in some MIR tasks. We now generalize previous\nconclusions by evaluating the impact of generic summarization of music from a\nprobabilistic perspective. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy, in order to measure\ninformation loss incurred by summarization. Our results suggest that relative\nentropy is a good predictor of summarization performance in the context of\ntasks relying on a bag-of-features model. Based on this observation, we further\npropose a straightforward yet expressive summarizer, which minimizes relative\nentropy with respect to the original song, that objectively outperforms\nprevious methods and is better suited to avoid potential copyright issues.\n","id":508}
{"Unnamed: 0.1":11509,"Unnamed: 0":11509.0,"anchor":"Towards the Limit of Network Quantization","positive":"  Network quantization is one of network compression techniques to reduce the\nredundancy of deep neural networks. It reduces the number of distinct network\nparameter values by quantization in order to save the storage for them. In this\npaper, we design network quantization schemes that minimize the performance\nloss due to quantization given a compression ratio constraint. We analyze the\nquantitative relation of quantization errors to the neural network loss\nfunction and identify that the Hessian-weighted distortion measure is locally\nthe right objective function for the optimization of network quantization. As a\nresult, Hessian-weighted k-means clustering is proposed for clustering network\nparameters to quantize. When optimal variable-length binary codes, e.g.,\nHuffman codes, are employed for further compression, we derive that the network\nquantization problem can be related to the entropy-constrained scalar\nquantization (ECSQ) problem in information theory and consequently propose two\nsolutions of ECSQ for network quantization, i.e., uniform quantization and an\niterative solution similar to Lloyd's algorithm. Finally, using the simple\nuniform quantization followed by Huffman coding, we show from our experiments\nthat the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,\n32-layer ResNet and AlexNet, respectively.\n","negative":"  Random backpropagation (RBP) is a variant of the backpropagation algorithm\nfor training neural networks, where the transpose of the forward matrices are\nreplaced by fixed random matrices in the calculation of the weight updates. It\nis remarkable both because of its effectiveness, in spite of using random\nmatrices to communicate error information, and because it completely removes\nthe taxing requirement of maintaining symmetric weights in a physical neural\nsystem. To better understand random backpropagation, we first connect it to the\nnotions of local learning and learning channels. Through this connection, we\nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP\n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their\ncomputational complexity. We then study their behavior through simulations\nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that\nmost of these variants work robustly, almost as well as backpropagation, and\nthat multiplication by the derivatives of the activation functions is\nimportant. As a follow-up, we study also the low-end of the number of bits\nrequired to communicate error information over the learning channel. We then\nprovide partial intuitive explanations for some of the remarkable properties of\nRBP and its variations. Finally, we prove several mathematical results,\nincluding the convergence to fixed points of linear chains of arbitrary length,\nthe convergence to fixed points of linear autoencoders with decorrelated data,\nthe long-term existence of solutions for linear systems with a single hidden\nlayer and convergence in special cases, and the convergence to fixed points of\nnon-linear chains, when the derivative of the activation functions is included.\n","id":509}
{"Unnamed: 0.1":11510,"Unnamed: 0":11510.0,"anchor":"Improving the Performance of Neural Networks in Regression Tasks Using\n  Drawering","positive":"  The method presented extends a given regression neural network to make its\nperformance improve. The modification affects the learning procedure only,\nhence the extension may be easily omitted during evaluation without any change\nin prediction. It means that the modified model may be evaluated as quickly as\nthe original one but tends to perform better.\n  This improvement is possible because the modification gives better expressive\npower, provides better behaved gradients and works as a regularization. The\nknowledge gained by the temporarily extended neural network is contained in the\nparameters shared with the original neural network.\n  The only cost is an increase in learning time.\n","negative":"  With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\n","id":510}
{"Unnamed: 0.1":11511,"Unnamed: 0":11511.0,"anchor":"Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-Tucker-Rank Tensor","positive":"  We investigate the fundamental conditions on the sampling pattern, i.e.,\nlocations of the sampled entries, for finite completability of a low-rank\ntensor given some components of its Tucker rank. In order to find the\ndeterministic necessary and sufficient conditions, we propose an algebraic\ngeometric analysis on the Tucker manifold, which allows us to incorporate\nmultiple rank components in the proposed analysis in contrast with the\nconventional geometric approaches on the Grassmannian manifold. This analysis\ncharacterizes the algebraic independence of a set of polynomials defined based\non the sampling pattern, which is closely related to finite completion.\nProbabilistic conditions are then studied and a lower bound on the sampling\nprobability is given, which guarantees that the proposed deterministic\nconditions on the sampling patterns for finite completability hold with high\nprobability. Furthermore, using the proposed geometric approach for finite\ncompletability, we propose a sufficient condition on the sampling pattern that\nensures there exists exactly one completion for the sampled tensor.\n","negative":"  Deep learning classifiers are known to be inherently vulnerable to\nmanipulation by intentionally perturbed inputs, named adversarial examples. In\nthis work, we establish that reinforcement learning techniques based on Deep\nQ-Networks (DQNs) are also vulnerable to adversarial input perturbations, and\nverify the transferability of adversarial examples across different DQN models.\nFurthermore, we present a novel class of attacks based on this vulnerability\nthat enable policy manipulation and induction in the learning process of DQNs.\nWe propose an attack mechanism that exploits the transferability of adversarial\nexamples to implement policy induction attacks on DQNs, and demonstrate its\nefficacy and impact through experimental study of a game-learning scenario.\n","id":511}
{"Unnamed: 0.1":11512,"Unnamed: 0":11512.0,"anchor":"Distributed Gaussian Learning over Time-varying Directed Graphs","positive":"  We present a distributed (non-Bayesian) learning algorithm for the problem of\nparameter estimation with Gaussian noise. The algorithm is expressed as\nexplicit updates on the parameters of the Gaussian beliefs (i.e. means and\nprecision). We show a convergence rate of $O(1\/k)$ with the constant term\ndepending on the number of agents and the topology of the network. Moreover, we\nshow almost sure convergence to the optimal solution of the estimation problem\nfor the general case of time-varying directed graphs.\n","negative":"  A commonly used heuristic in non-convex optimization is Normalized Gradient\nDescent (NGD) - a variant of gradient descent in which only the direction of\nthe gradient is taken into account and its magnitude ignored. We analyze this\nheuristic and show that with carefully chosen parameters and noise injection,\nthis method can provably evade saddle points. We establish the convergence of\nNGD to a local minimum, and demonstrate rates which improve upon the fastest\nknown first order algorithm due to Ge e al. (2015).\n  The effectiveness of our method is demonstrated via an application to the\nproblem of online tensor decomposition; a task for which saddle point evasion\nis known to result in convergence to global minima.\n","id":512}
{"Unnamed: 0.1":11513,"Unnamed: 0":11513.0,"anchor":"Efficient Non-oblivious Randomized Reduction for Risk Minimization with\n  Improved Excess Risk Guarantee","positive":"  In this paper, we address learning problems for high dimensional data.\nPreviously, oblivious random projection based approaches that project high\ndimensional features onto a random subspace have been used in practice for\ntackling high-dimensionality challenge in machine learning. Recently, various\nnon-oblivious randomized reduction methods have been developed and deployed for\nsolving many numerical problems such as matrix product approximation, low-rank\nmatrix approximation, etc. However, they are less explored for the machine\nlearning tasks, e.g., classification. More seriously, the theoretical analysis\nof excess risk bounds for risk minimization, an important measure of\ngeneralization performance, has not been established for non-oblivious\nrandomized reduction methods. It therefore remains an open problem what is the\nbenefit of using them over previous oblivious random projection based\napproaches. To tackle these challenges, we propose an algorithmic framework for\nemploying non-oblivious randomized reduction method for general empirical risk\nminimizing in machine learning tasks, where the original high-dimensional\nfeatures are projected onto a random subspace that is derived from the data\nwith a small matrix approximation error. We then derive the first excess risk\nbound for the proposed non-oblivious randomized reduction approach without\nrequiring strong assumptions on the training data. The established excess risk\nbound exhibits that the proposed approach provides much better generalization\nperformance and it also sheds more insights about different randomized\nreduction approaches. Finally, we conduct extensive experiments on both\nsynthetic and real-world benchmark datasets, whose dimension scales to\n$O(10^7)$, to demonstrate the efficacy of our proposed approach.\n","negative":"  A multi-way factor analysis model is introduced for tensor-variate data of\nany order. Each data item is represented as a (sparse) sum of Kruskal\ndecompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can\ninfer both the tensor-rank of each dictionary atom and the number of dictionary\natoms. The model is adapted for online learning, which allows dictionary\nlearning on large data sets. After KFA is introduced, the model is extended to\na deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The\nexperiments section demonstrates the improvement of KFA over vectorized\napproaches (e.g., BPFA), tensor decompositions, and convolutional neural\nnetworks (CNN) in multi-way denoising, blind inpainting, and image\nclassification. The improvement in PSNR for the inpainting results over other\nmethods exceeds 1dB in several cases and we achieve state of the art results on\nCaltech101 image classification.\n","id":513}
{"Unnamed: 0.1":11514,"Unnamed: 0":11514.0,"anchor":"Statistical mechanics of unsupervised feature learning in a restricted\n  Boltzmann machine with binary synapses","positive":"  Revealing hidden features in unlabeled data is called unsupervised feature\nlearning, which plays an important role in pretraining a deep neural network.\nHere we provide a statistical mechanics analysis of the unsupervised learning\nin a restricted Boltzmann machine with binary synapses. A message passing\nequation to infer the hidden feature is derived, and furthermore, variants of\nthis equation are analyzed. A statistical analysis by replica theory describes\nthe thermodynamic properties of the model. Our analysis confirms an entropy\ncrisis preceding the non-convergence of the message passing equation,\nsuggesting a discontinuous phase transition as a key characteristic of the\nrestricted Boltzmann machine. Continuous phase transition is also confirmed\ndepending on the embedded feature strength in the data. The mean-field result\nunder the replica symmetric assumption agrees with that obtained by running\nmessage passing algorithms on single instances of finite sizes. Interestingly,\nin an approximate Hopfield model, the entropy crisis is absent, and a\ncontinuous phase transition is observed instead. We also develop an iterative\nequation to infer the hyper-parameter (temperature) hidden in the data, which\nin physics corresponds to iteratively imposing Nishimori condition. Our study\nprovides insights towards understanding the thermodynamic properties of the\nrestricted Boltzmann machine learning, and moreover important theoretical basis\nto build simplified deep networks.\n","negative":"  One-Class Classification (OCC) has been prime concern for researchers and\neffectively employed in various disciplines. But, traditional methods based\none-class classifiers are very time consuming due to its iterative process and\nvarious parameters tuning. In this paper, we present six OCC methods based on\nextreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed\nclassifiers mainly lie in two categories: reconstruction based and boundary\nbased, which supports both types of learning viz., online and offline learning.\nOut of various proposed methods, four are offline and remaining two are online\nmethods. Out of four offline methods, two methods perform random feature\nmapping and two methods perform kernel feature mapping. Kernel feature mapping\nbased approaches have been tested with RBF kernel and online version of\none-class classifiers are tested with both types of nodes viz., additive and\nRBF. It is well known fact that threshold decision is a crucial factor in case\nof OCC, so, three different threshold deciding criteria have been employed so\nfar and analyses the effectiveness of one threshold deciding criteria over\nanother. Further, these methods are tested on two artificial datasets to check\nthere boundary construction capability and on eight benchmark datasets from\ndifferent discipline to evaluate the performance of the classifiers. Our\nproposed classifiers exhibit better performance compared to ten traditional\none-class classifiers and ELM based two one-class classifiers. Through proposed\none-class classifiers, we intend to expand the functionality of the most used\ntoolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with\nall the present features of the toolbox.\n","id":514}
{"Unnamed: 0.1":11515,"Unnamed: 0":11515.0,"anchor":"Factored Contextual Policy Search with Bayesian Optimization","positive":"  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different\n\"contexts\". Bayesian optimization approaches to contextual policy search (CPS)\noffer data-efficient policy learning that generalize over a context space. We\npropose to improve data-efficiency by factoring typically considered contexts\ninto two components: target-type contexts that correspond to a desired outcome\nof the learned behavior, e.g. target position for throwing a ball; and\nenvironment type contexts that correspond to some state of the environment,\ne.g. initial ball position or wind speed. Our key observation is that\nexperience can be directly generalized over target-type contexts. Based on that\nwe introduce Factored Contextual Policy Search with Bayesian Optimization for\nboth passive and active learning settings. Preliminary results show faster\npolicy generalization on a simulated toy problem. A full paper extension is\navailable at arXiv:1904.11761\n","negative":"  The identification of sources of advection-diffusion transport is based\nusually on solving complex ill-posed inverse models against the available\nstate- variable data records. However, if there are several sources with\ndifferent locations and strengths, the data records represent mixtures rather\nthan the separate influences of the original sources. Importantly, the number\nof these original release sources is typically unknown, which hinders\nreliability of the classical inverse-model analyses. To address this challenge,\nwe present here a novel hybrid method for identification of the unknown number\nof release sources. Our hybrid method, called HNMF, couples unsupervised\nlearning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis\nGreen functions method. HNMF synergistically performs decomposition of the\nrecorded mixtures, finds the number of the unknown sources and uses the Green\nfunction of advection-diffusion equation to identify their characteristics. In\nthe paper, we introduce the method and demonstrate that it is capable of\nidentifying the advection velocity and dispersivity of the medium as well as\nthe unknown number, locations, and properties of various sets of synthetic\nrelease sources with different space and time dependencies, based only on the\nrecorded data. HNMF can be applied directly to any problem controlled by a\npartial-differential parabolic equation where mixtures of an unknown number of\nsources are measured at multiple locations.\n","id":515}
{"Unnamed: 0.1":11516,"Unnamed: 0":11516.0,"anchor":"Video Ladder Networks","positive":"  We present the Video Ladder Network (VLN) for efficiently generating future\nvideo frames. VLN is a neural encoder-decoder model augmented at all layers by\nboth recurrent and feedforward lateral connections. At each layer, these\nconnections form a lateral recurrent residual block, where the feedforward\nconnection represents a skip connection and the recurrent connection represents\nthe residual. Thanks to the recurrent connections, the decoder can exploit\ntemporal summaries generated from all layers of the encoder. This way, the top\nlayer is relieved from the pressure of modeling lower-level spatial and\ntemporal details. Furthermore, we extend the basic version of VLN to\nincorporate ResNet-style residual blocks in the encoder and decoder, which help\nimproving the prediction results. VLN is trained in self-supervised regime on\nthe Moving MNIST dataset, achieving competitive results while having very\nsimple structure and providing fast inference.\n","negative":"  Recent advances in machine learning have made significant contributions to\ndrug discovery. Deep neural networks in particular have been demonstrated to\nprovide significant boosts in predictive power when inferring the properties\nand activities of small-molecule compounds. However, the applicability of these\ntechniques has been limited by the requirement for large amounts of training\ndata. In this work, we demonstrate how one-shot learning can be used to\nsignificantly lower the amounts of data required to make meaningful predictions\nin drug discovery applications. We introduce a new architecture, the residual\nLSTM embedding, that, when combined with graph convolutional neural networks,\nsignificantly improves the ability to learn meaningful distance metrics over\nsmall-molecules. We open source all models introduced in this work as part of\nDeepChem, an open-source framework for deep-learning in drug discovery.\n","id":516}
{"Unnamed: 0.1":11517,"Unnamed: 0":11517.0,"anchor":"Control Matching via Discharge Code Sequences","positive":"  In this paper, we consider the patient similarity matching problem over a\ncancer cohort of more than 220,000 patients. Our approach first leverages on\nWord2Vec framework to embed ICD codes into vector-valued representation. We\nthen propose a sequential algorithm for case-control matching on this\nrepresentation space of diagnosis codes. The novel practice of applying the\nsequential matching on the vector representation lifted the matching accuracy\nmeasured through multiple clinical outcomes. We reported the results on a\nlarge-scale dataset to demonstrate the effectiveness of our method. For such a\nlarge dataset where most clinical information has been codified, the new method\nis particularly relevant.\n","negative":"  This paper introduces a new large-scale music dataset, MusicNet, to serve as\na source of supervision and evaluation of machine learning methods for music\nresearch. MusicNet consists of hundreds of freely-licensed classical music\nrecordings by 10 composers, written for 11 instruments, together with\ninstrument\/note annotations resulting in over 1 million temporal labels on 34\nhours of chamber music performances under various studio and microphone\nconditions.\n  The paper defines a multi-label classification task to predict notes in\nmusical recordings, along with an evaluation protocol, and benchmarks several\nmachine learning architectures for this task: i) learning from spectrogram\nfeatures; ii) end-to-end learning with a neural net; iii) end-to-end learning\nwith a convolutional neural net. These experiments show that end-to-end models\ntrained for note prediction learn frequency selective filters as a low-level\nrepresentation of audio.\n","id":517}
{"Unnamed: 0.1":11518,"Unnamed: 0":11518.0,"anchor":"Combinatorial semi-bandit with known covariance","positive":"  The combinatorial stochastic semi-bandit problem is an extension of the\nclassical multi-armed bandit problem in which an algorithm pulls more than one\narm at each stage and the rewards of all pulled arms are revealed. One\ndifference with the single arm variant is that the dependency structure of the\narms is crucial. Previous works on this setting either used a worst-case\napproach or imposed independence of the arms. We introduce a way to quantify\nthe dependency structure of the problem and design an algorithm that adapts to\nit. The algorithm is based on linear regression and the analysis develops\ntechniques from the linear bandit literature. By comparing its performance to a\nnew lower bound, we prove that it is optimal, up to a poly-logarithmic factor\nin the number of pulled arms.\n","negative":"  The paper studies distributed Dictionary Learning (DL) problems where the\nlearning task is distributed over a multi-agent network with time-varying\n(nonsymmetric) connectivity. This formulation is relevant, for instance, in\nbig-data scenarios where massive amounts of data are collected\/stored in\ndifferent spatial locations and it is unfeasible to aggregate and\/or process\nall the data in a fusion center, due to resource limitations, communication\noverhead or privacy considerations. We develop a general distributed\nalgorithmic framework for the (nonconvex) DL problem and establish its\nasymptotic convergence. The new method hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a gradient tracking mechanism\ninstrumental to locally estimate the missing global information; and ii) a\nconsensus step, as a mechanism to distribute the computations among the agents.\nTo the best of our knowledge, this is the first distributed algorithm with\nprovable convergence for the DL problem and, more in general, bi-convex\noptimization problems over (time-varying) directed graphs.\n","id":518}
{"Unnamed: 0.1":11519,"Unnamed: 0":11519.0,"anchor":"Microseismic events enhancement and detection in sensor arrays using\n  autocorrelation based filtering","positive":"  Passive microseismic data are commonly buried in noise, which presents a\nsignificant challenge for signal detection and recovery. For recordings from a\nsurface sensor array where each trace contains a time-delayed arrival from the\nevent, we propose an autocorrelation-based stacking method that designs a\ndenoising filter from all the traces, as well as a multi-channel detection\nscheme. This approach circumvents the issue of time aligning the traces prior\nto stacking because every trace's autocorrelation is centered at zero in the\nlag domain. The effect of white noise is concentrated near zero lag, so the\nfilter design requires a predictable adjustment of the zero-lag value.\nTruncation of the autocorrelation is employed to smooth the impulse response of\nthe denoising filter. In order to extend the applicability of the algorithm, we\nalso propose a noise prewhitening scheme that addresses cases with colored\nnoise. The simplicity and robustness of this method are validated with\nsynthetic and real seismic traces.\n","negative":"  Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs\/episodes required by about a third, vs.\nstandard policy gradient methods.\n","id":519}
{"Unnamed: 0.1":11520,"Unnamed: 0":11520.0,"anchor":"Invariant Representations for Noisy Speech Recognition","positive":"  Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.\n","negative":"  We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs.\n","id":520}
{"Unnamed: 0.1":11521,"Unnamed: 0":11521.0,"anchor":"A Probabilistic Framework for Deep Learning","positive":"  We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark.\n","negative":"  We present Manifold Alignment Determination (MAD), an algorithm for learning\nalignments between data points from multiple views or modalities. The approach\nis capable of learning correspondences between views as well as correspondences\nbetween individual data-points. The proposed method requires only a few aligned\nexamples from which it is capable to recover a global alignment through a\nprobabilistic model. The strong, yet flexible regularization provided by the\ngenerative model is sufficient to align the views. We provide experiments on\nboth synthetic and real data to highlight the benefit of the proposed approach.\n","id":521}
{"Unnamed: 0.1":11522,"Unnamed: 0":11522.0,"anchor":"Semi-Supervised Learning with the Deep Rendering Mixture Model","positive":"  Semi-supervised learning algorithms reduce the high cost of acquiring labeled\ntraining data by using both labeled and unlabeled data during learning. Deep\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\nand as such have been widely employed in the semi-supervised learning. In this\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\nprobabilistic generative model that models latent nuisance variation, and whose\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\nwe introduce a novel non-negativity constraint and a variational inference\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\nsemi-supervised setting represents latent nuisance variation using\nsynthetically rendered images. Taken together, our work provides a unified\nframework for supervised, unsupervised, and semi-supervised learning.\n","negative":"  Neural networks are a powerful class of functions that can be trained with\nsimple gradient descent to achieve state-of-the-art performance on a variety of\napplications. Despite their practical success, there is a paucity of results\nthat provide theoretical guarantees on why they are so effective. Lying in the\ncenter of the problem is the difficulty of analyzing the non-convex loss\nfunction with potentially numerous local minima and saddle points. Can neural\nnetworks corresponding to the stationary points of the loss function learn the\ntrue target function? If yes, what are the key factors contributing to such\nnice optimization properties?\n  In this paper, we answer these questions by analyzing one-hidden-layer neural\nnetworks with ReLU activation, and show that despite the non-convexity, neural\nnetworks with diverse units have no spurious local minima. We bypass the\nnon-convexity issue by directly analyzing the first order optimality condition,\nand show that the loss can be made arbitrarily small if the minimum singular\nvalue of the \"extended feature matrix\" is large enough. We make novel use of\ntechniques from kernel methods and geometric discrepancy, and identify a new\nrelation linking the smallest singular value to the spectrum of a kernel\nfunction associated with the activation function and to the diversity of the\nunits. Our results also suggest a novel regularization function to promote unit\ndiversity for potentially better generalization.\n","id":522}
{"Unnamed: 0.1":11523,"Unnamed: 0":11523.0,"anchor":"Segmental Convolutional Neural Networks for Detection of Cardiac\n  Abnormality With Noisy Heart Sound Recordings","positive":"  Heart diseases constitute a global health burden, and the problem is\nexacerbated by the error-prone nature of listening to and interpreting heart\nsounds. This motivates the development of automated classification to screen\nfor abnormal heart sounds. Existing machine learning-based systems achieve\naccurate classification of heart sound recordings but rely on expert features\nthat have not been thoroughly evaluated on noisy recordings. Here we propose a\nsegmental convolutional neural network architecture that achieves automatic\nfeature learning from noisy heart sound recordings. Our experiments show that\nour best model, trained on noisy recording segments acquired with an existing\nhidden semi-markov model-based approach, attains a classification accuracy of\n87.5% on the 2016 PhysioNet\/CinC Challenge dataset, compared to the 84.6%\naccuracy of the state-of-the-art statistical classifier trained and evaluated\non the same dataset. Our results indicate the potential of using neural\nnetwork-based methods to increase the accuracy of automated classification of\nheart sound recordings for improved screening of heart diseases.\n","negative":"  The gold standard for discovering causal relations is by means of\nexperimentation. Over the last decades, alternative methods have been proposed\nthat can infer causal relations between variables from certain statistical\npatterns in purely observational data. We introduce Joint Causal Inference\n(JCI), a novel approach to causal discovery from multiple data sets from\ndifferent contexts that elegantly unifies both approaches. JCI is a causal\nmodeling framework rather than a specific algorithm, and it can be implemented\nusing any causal discovery algorithm that can take into account certain\nbackground knowledge. JCI can deal with different types of interventions (e.g.,\nperfect, imperfect, stochastic, etc.) in a unified fashion, and does not\nrequire knowledge of intervention targets or types in case of interventional\ndata. We explain how several well-known causal discovery algorithms can be seen\nas addressing special cases of the JCI framework, and we also propose novel\nimplementations that extend existing causal discovery methods for purely\nobservational data to the JCI setting. We evaluate different JCI\nimplementations on synthetic data and on flow cytometry protein expression data\nand conclude that JCI implementations can considerably outperform\nstate-of-the-art causal discovery algorithms.\n","id":523}
{"Unnamed: 0.1":11524,"Unnamed: 0":11524.0,"anchor":"Core Sampling Framework for Pixel Classification","positive":"  The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset.\n","negative":"  We study Generalised Restricted Boltzmann Machines with generic priors for\nunits and weights, interpolating between Boolean and Gaussian variables. We\npresent a complete analysis of the replica symmetric phase diagram of these\nsystems, which can be regarded as Generalised Hopfield models. We underline the\nrole of the retrieval phase for both inference and learning processes and we\nshow that retrieval is robust for a large class of weight and unit priors,\nbeyond the standard Hopfield scenario. Furthermore we show how the paramagnetic\nphase boundary is directly related to the optimal size of the training set\nnecessary for good generalisation in a teacher-student scenario of unsupervised\nlearning.\n","id":524}
{"Unnamed: 0.1":11525,"Unnamed: 0":11525.0,"anchor":"Local Group Invariant Representations via Orbit Embeddings","positive":"  Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN.\n","negative":"  Recently, several algorithms for symbolic regression (SR) emerged which\nemploy a form of multiple linear regression (LR) to produce generalized linear\nmodels. The use of LR allows the algorithms to create models with relatively\nsmall error right from the beginning of the search; such algorithms are thus\nclaimed to be (sometimes by orders of magnitude) faster than SR algorithms\nbased on vanilla genetic programming. However, a systematic comparison of these\nalgorithms on a common set of problems is still missing. In this paper we\nconceptually and experimentally compare several representatives of such\nalgorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf,\nready-to-use techniques, mostly using their default settings. The methods are\ncompared on several synthetic and real-world SR benchmark problems. Their\nperformance is also related to the performance of three conventional machine\nlearning algorithms --- multiple regression, random forests and support vector\nregression.\n","id":525}
{"Unnamed: 0.1":11526,"Unnamed: 0":11526.0,"anchor":"Statistical and Computational Guarantees of Lloyd's Algorithm and its\n  Variants","positive":"  Clustering is a fundamental problem in statistics and machine learning.\nLloyd's algorithm, proposed in 1957, is still possibly the most widely used\nclustering algorithm in practice due to its simplicity and empirical\nperformance. However, there has been little theoretical investigation on the\nstatistical and computational guarantees of Lloyd's algorithm. This paper is an\nattempt to bridge this gap between practice and theory. We investigate the\nperformance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an\nappropriate initialization for labels or centers, we show that Lloyd's\nalgorithm converges to an exponentially small clustering error after an order\nof $\\log n$ iterations, where $n$ is the sample size. The error rate is shown\nto be minimax optimal. For the two-mixture case, we only require the\ninitializer to be slightly better than random guess.\n  In addition, we extend the Lloyd's algorithm and its analysis to community\ndetection and crowdsourcing, two problems that have received a lot of attention\nrecently in statistics and machine learning. Two variants of Lloyd's algorithm\nare proposed respectively for community detection and crowdsourcing. On the\ntheoretical side, we provide statistical and computational guarantees of the\ntwo algorithms, and the results improve upon some previous signal-to-noise\nratio conditions in literature for both problems. Experimental results on\nsimulated and real data sets demonstrate competitive performance of our\nalgorithms to the state-of-the-art methods.\n","negative":"  TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.\n","id":526}
{"Unnamed: 0.1":11527,"Unnamed: 0":11527.0,"anchor":"Predictive Business Process Monitoring with LSTM Neural Networks","positive":"  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n","negative":"  We propose online algorithms for sequential learning in the contextual\nmulti-armed bandit setting. Our approach is to partition the context space and\nthen optimally combine all of the possible mappings between the partition\nregions and the set of bandit arms in a data driven manner. We show that in our\napproach, the best mapping is able to approximate the best arm selection policy\nto any desired degree under mild Lipschitz conditions. Therefore, we design our\nalgorithms based on the optimal adaptive combination and asymptotically achieve\nthe performance of the best mapping as well as the best arm selection policy.\nThis optimality is also guaranteed to hold even in adversarial environments\nsince we do not rely on any statistical assumptions regarding the contexts or\nthe loss of the bandit arms. Moreover, we design efficient implementations for\nour algorithms in various hierarchical partitioning structures such as\nlexicographical or arbitrary position splitting and binary trees (and several\nother partitioning examples). For instance, in the case of binary tree\npartitioning, the computational complexity is only log-linear in the number of\nregions in the finest partition. In conclusion, we provide significant\nperformance improvements by introducing upper bounds (w.r.t. the best arm\nselection policy) that are mathematically proven to vanish in the average loss\nper round sense at a faster rate compared to the state-of-the-art. Our\nexperimental work extensively covers various scenarios ranging from bandit\nsettings to multi-class classification with real and synthetic data. In these\nexperiments, we show that our algorithms are highly superior over the\nstate-of-the-art techniques while maintaining the introduced mathematical\nguarantees and a computationally decent scalability.\n","id":527}
{"Unnamed: 0.1":11528,"Unnamed: 0":11528.0,"anchor":"Mode Regularized Generative Adversarial Networks","positive":"  Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.\n","negative":"  In an attempt at exploring the limitations of simple approaches to the task\nof piano transcription (as usually defined in MIR), we conduct an in-depth\nanalysis of neural network-based framewise transcription. We systematically\ncompare different popular input representations for transcription systems to\ndetermine the ones most suitable for use with neural networks. Exploiting\nrecent advances in training techniques and new regularizers, and taking into\naccount hyper-parameter tuning, we show that it is possible, by simple\nbottom-up frame-wise processing, to obtain a piano transcriber that outperforms\nthe current published state of the art on the publicly available MAPS dataset\n-- without any complex post-processing steps. Thus, we propose this simple\napproach as a new baseline for this dataset, for future transcription research\nto build on and improve.\n","id":528}
{"Unnamed: 0.1":11529,"Unnamed: 0":11529.0,"anchor":"Measuring the non-asymptotic convergence of sequential Monte Carlo\n  samplers using probabilistic programming","positive":"  A key limitation of sampling algorithms for approximate inference is that it\nis difficult to quantify their approximation error. Widely used sampling\nschemes, such as sequential importance sampling with resampling and\nMetropolis-Hastings, produce output samples drawn from a distribution that may\nbe far from the target posterior distribution. This paper shows how to\nupper-bound the symmetric KL divergence between the output distribution of a\nbroad class of sequential Monte Carlo (SMC) samplers and their target posterior\ndistributions, subject to assumptions about the accuracy of a separate\ngold-standard sampler. The proposed method applies to samplers that combine\nmultiple particles, multinomial resampling, and rejuvenation kernels. The\nexperiments show the technique being used to estimate bounds on the divergence\nof SMC samplers for posterior inference in a Bayesian linear regression model\nand a Dirichlet process mixture model.\n","negative":"  Gene expression data is widely used in disease analysis and cancer diagnosis.\nHowever, since gene expression data could contain thousands of genes\nsimultaneously, successful microarray classification is rather difficult.\nFeature selection is an important pre-treatment for any classification process.\nSelecting a useful gene subset as a classifier not only decreases the\ncomputational time and cost, but also increases classification accuracy. In\nthis study, we applied the information gain method as a filter approach, and an\nimproved binary particle swarm optimization as a wrapper approach to implement\nfeature selection; selected gene subsets were used to evaluate the performance\nof classification. Experimental results show that by employing the proposed\nmethod fewer gene subsets needed to be selected and better classification\naccuracy could be obtained.\n","id":529}
{"Unnamed: 0.1":11530,"Unnamed: 0":11530.0,"anchor":"Model-based Adversarial Imitation Learning","positive":"  Generative adversarial learning is a popular new approach to training\ngenerative models which has been proven successful for other related problems\nas well. The general idea is to maintain an oracle $D$ that discriminates\nbetween the expert's data distribution and that of the generative model $G$.\nThe generative model is trained to capture the expert's distribution by\nmaximizing the probability of $D$ misclassifying the data it generates.\nOverall, the system is \\emph{differentiable} end-to-end and is trained using\nbasic backpropagation. This type of learning was successfully applied to the\nproblem of policy imitation in a model-free setup. However, a model-free\napproach does not allow the system to be differentiable, which requires the use\nof high-variance gradient estimations. In this paper we introduce the Model\nbased Adversarial Imitation Learning (MAIL) algorithm. A model-based approach\nfor the problem of adversarial imitation learning. We show how to use a forward\nmodel to make the system fully differentiable, which enables us to train\npolicies using the (stochastic) gradient of $D$. Moreover, our approach\nrequires relatively few environment interactions, and fewer hyper-parameters to\ntune. We test our method on the MuJoCo physics simulator and report initial\nresults that surpass the current state-of-the-art.\n","negative":"  We introduce dropout compaction, a novel method for training feed-forward\nneural networks which realizes the performance gains of training a large model\nwith dropout regularization, yet extracts a compact neural network for run-time\nefficiency. In the proposed method, we introduce a sparsity-inducing prior on\nthe per unit dropout retention probability so that the optimizer can\neffectively prune hidden units during training. By changing the prior\nhyperparameters, we can control the size of the resulting network. We performed\na systematic comparison of dropout compaction and competing methods on several\nreal-world speech recognition tasks and found that dropout compaction achieved\ncomparable accuracy with fewer than 50% of the hidden units, translating to a\n2.5x speedup in run-time.\n","id":530}
{"Unnamed: 0.1":11531,"Unnamed: 0":11531.0,"anchor":"Fast Adaptation in Generative Models with Generative Matching Networks","positive":"  Despite recent advances, the remaining bottlenecks in deep generative models\nare necessity of extensive training and difficulties with generalization from\nsmall number of training examples. We develop a new generative model called\nGenerative Matching Network which is inspired by the recently proposed matching\nnetworks for one-shot learning in discriminative tasks. By conditioning on the\nadditional input dataset, our model can instantly learn new concepts that were\nnot available in the training data but conform to a similar generative process.\nThe proposed framework does not explicitly restrict diversity of the\nconditioning data and also does not require an extensive inference procedure\nfor training or adaptation. Our experiments on the Omniglot dataset demonstrate\nthat Generative Matching Networks significantly improve predictive performance\non the fly as more additional data is available and outperform existing state\nof the art conditional generative models.\n","negative":"  Deep reinforcement learning (RL) can acquire complex behaviors from low-level\ninputs, such as images. However, real-world applications of such methods\nrequire generalizing to the vast variability of the real world. Deep networks\nare known to achieve remarkable generalization when provided with massive\namounts of labeled data, but can we provide this breadth of experience to an RL\nagent, such as a robot? The robot might continuously learn as it explores the\nworld around it, even while deployed. However, this learning requires access to\na reward function, which is often hard to measure in real-world domains, where\nthe reward could depend on, for example, unknown positions of objects or the\nemotional state of the user. Conversely, it is often quite practical to provide\nthe agent with reward functions in a limited set of situations, such as when a\nhuman supervisor is present or in a controlled setting. Can we make use of this\nlimited supervision, and still benefit from the breadth of experience an agent\nmight collect on its own? In this paper, we formalize this problem as\nsemisupervised reinforcement learning, where the reward function can only be\nevaluated in a set of \"labeled\" MDPs, and the agent must generalize its\nbehavior to the wide range of states it might encounter in a set of \"unlabeled\"\nMDPs, by using experience from both settings. Our proposed method infers the\ntask objective in the unlabeled MDPs through an algorithm that resembles\ninverse RL, using the agent's own prior experience in the labeled MDPs as a\nkind of demonstration of optimal behavior. We evaluate our method on\nchallenging tasks that require control directly from images, and show that our\napproach can improve the generalization of a learned deep neural network policy\nby using experience for which no reward function is available. We also show\nthat our method outperforms direct supervised learning of the reward.\n","id":531}
{"Unnamed: 0.1":11532,"Unnamed: 0":11532.0,"anchor":"A Communication-Efficient Parallel Method for Group-Lasso","positive":"  Group-Lasso (gLasso) identifies important explanatory factors in predicting\nthe response variable by considering the grouping structure over input\nvariables. However, most existing algorithms for gLasso are not scalable to\ndeal with large-scale datasets, which are becoming a norm in many applications.\nIn this paper, we present a divide-and-conquer based parallel algorithm\n(DC-gLasso) to scale up gLasso in the tasks of regression with grouping\nstructures. DC-gLasso only needs two iterations to collect and aggregate the\nlocal estimates on subsets of the data, and is provably correct to recover the\ntrue model under certain conditions. We further extend it to deal with\noverlappings between groups. Empirical results on a wide range of synthetic and\nreal-world datasets show that DC-gLasso can significantly improve the time\nefficiency without sacrificing regression accuracy.\n","negative":"  Nowadays, the number of layers and of neurons in each layer of a deep network\nare typically set manually. While very deep and wide networks have proven\neffective in general, they come at a high memory and computation cost, thus\nmaking them impractical for constrained platforms. These networks, however, are\nknown to have many redundant parameters, and could thus, in principle, be\nreplaced by more compact architectures. In this paper, we introduce an approach\nto automatically determining the number of neurons in each layer of a deep\nnetwork during learning. To this end, we propose to make use of structured\nsparsity during learning. More precisely, we use a group sparsity regularizer\non the parameters of the network, where each group is defined to act on a\nsingle neuron. Starting from an overcomplete network, we show that our approach\ncan reduce the number of parameters by up to 80\\% while retaining or even\nimproving the network accuracy.\n","id":532}
{"Unnamed: 0.1":11533,"Unnamed: 0":11533.0,"anchor":"Large-Margin Softmax Loss for Convolutional Neural Networks","positive":"  Cross-entropy loss together with softmax is arguably one of the most common\nused supervision components in convolutional neural networks (CNNs). Despite\nits simplicity, popularity and excellent performance, the component does not\nexplicitly encourage discriminative learning of features. In this paper, we\npropose a generalized large-margin softmax (L-Softmax) loss which explicitly\nencourages intra-class compactness and inter-class separability between learned\nfeatures. Moreover, L-Softmax not only can adjust the desired margin but also\ncan avoid overfitting. We also show that the L-Softmax loss can be optimized by\ntypical stochastic gradient descent. Extensive experiments on four benchmark\ndatasets demonstrate that the deeply-learned features with L-softmax loss\nbecome more discriminative, hence significantly boosting the performance on a\nvariety of visual classification and verification tasks.\n","negative":"  We present LTLS, a technique for multiclass and multilabel prediction that\ncan perform training and inference in logarithmic time and space. LTLS embeds\nlarge classification problems into simple structured prediction problems and\nrelies on efficient dynamic programming algorithms for inference. We train LTLS\nwith stochastic gradient descent on a number of multiclass and multilabel\ndatasets and show that despite its small memory footprint it is often\ncompetitive with existing approaches.\n","id":533}
{"Unnamed: 0.1":11534,"Unnamed: 0":11534.0,"anchor":"Spatially Adaptive Computation Time for Residual Networks","positive":"  This paper proposes a deep learning architecture based on Residual Network\nthat dynamically adjusts the number of executed layers for the regions of the\nimage. This architecture is end-to-end trainable, deterministic and\nproblem-agnostic. It is therefore applicable without any modifications to a\nwide range of computer vision problems such as image classification, object\ndetection and image segmentation. We present experimental results showing that\nthis model improves the computational efficiency of Residual Networks on the\nchallenging ImageNet classification and COCO object detection datasets.\nAdditionally, we evaluate the computation time maps on the visual saliency\ndataset cat2000 and find that they correlate surprisingly well with human eye\nfixation positions.\n","negative":"  Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian\nnon-parametrics. While these models free from choosing the number of components\na-priori, computationally attractive variational inference often reintroduces\nthe need to do so, via a truncation on the variational distribution. In this\npaper we present a truncation-free hybrid inference for DPMM, combining the\nadvantages of sampling-based MCMC and variational methods. The proposed\nhybridization enables more efficient variational updates, while increasing\nmodel complexity only if needed. We evaluate the properties of the hybrid\nupdates and their empirical performance in single- as well as mixed-membership\nmodels. Our method is easy to implement and performs favorably compared to\nexisting schemas.\n","id":534}
{"Unnamed: 0.1":11535,"Unnamed: 0":11535.0,"anchor":"Extend natural neighbor: a novel classification method with\n  self-adaptive neighborhood parameters in different stages","positive":"  Various kinds of k-nearest neighbor (KNN) based classification methods are\nthe bases of many well-established and high-performance pattern-recognition\ntechniques, but both of them are vulnerable to their parameter choice.\nEssentially, the challenge is to detect the neighborhood of various data sets,\nwhile utterly ignorant of the data characteristic. This article introduces a\nnew supervised classification method: the extend natural neighbor (ENaN)\nmethod, and shows that it provides a better classification result without\nchoosing the neighborhood parameter artificially. Unlike the original KNN based\nmethod which needs a prior k, the ENaNE method predicts different k in\ndifferent stages. Therefore, the ENaNE method is able to learn more from\nflexible neighbor information both in training stage and testing stage, and\nprovide a better classification result.\n","negative":"  We study the problem of troubleshooting machine learning systems that rely on\nanalytical pipelines of distinct components. Understanding and fixing errors\nthat arise in such integrative systems is difficult as failures can occur at\nmultiple points in the execution workflow. Moreover, errors can propagate,\nbecome amplified or be suppressed, making blame assignment difficult. We\npropose a human-in-the-loop methodology which leverages human intellect for\ntroubleshooting system failures. The approach simulates potential component\nfixes through human computation tasks and measures the expected improvements in\nthe holistic behavior of the system. The method provides guidance to designers\nabout how they can best improve the system. We demonstrate the effectiveness of\nthe approach on an automated image captioning system that has been pressed into\nreal-world use.\n","id":535}
{"Unnamed: 0.1":11536,"Unnamed: 0":11536.0,"anchor":"Robust Low-Complexity Randomized Methods for Locating Outliers in Large\n  Matrices","positive":"  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank matrix, in settings where {}{the data} are noisy, or where\nthe overall matrix has missing elements. We propose a randomized two-step\ninference framework, and establish sufficient conditions on the required sample\ncomplexities under which these methods succeed (with high probability) in\naccurately locating the outliers for each task. Comprehensive numerical\nexperimental results are provided to verify the theoretical bounds and\ndemonstrate the computational efficiency of the proposed algorithm.\n","negative":"  Mobile robots with complex morphology are essential for traversing rough\nterrains in Urban Search & Rescue missions (USAR). Since teleoperation of the\ncomplex morphology causes high cognitive load of the operator, the morphology\nis controlled autonomously. The autonomous control measures the robot state and\nsurrounding terrain which is usually only partially observable, and thus the\ndata are often incomplete. We marginalize the control over the missing\nmeasurements and evaluate an explicit safety condition. If the safety condition\nis violated, tactile terrain exploration by the body-mounted robotic arm\ngathers the missing data.\n","id":536}
{"Unnamed: 0.1":11537,"Unnamed: 0":11537.0,"anchor":"An Information-theoretic Approach to Machine-oriented Music\n  Summarization","positive":"  Music summarization allows for higher efficiency in processing, storage, and\nsharing of datasets. Machine-oriented approaches, being agnostic to human\nconsumption, optimize these aspects even further. Such summaries have already\nbeen successfully validated in some MIR tasks. We now generalize previous\nconclusions by evaluating the impact of generic summarization of music from a\nprobabilistic perspective. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy, in order to measure\ninformation loss incurred by summarization. Our results suggest that relative\nentropy is a good predictor of summarization performance in the context of\ntasks relying on a bag-of-features model. Based on this observation, we further\npropose a straightforward yet expressive summarizer, which minimizes relative\nentropy with respect to the original song, that objectively outperforms\nprevious methods and is better suited to avoid potential copyright issues.\n","negative":"  This paper presents a framework to tackle combinatorial optimization problems\nusing neural networks and reinforcement learning. We focus on the traveling\nsalesman problem (TSP) and train a recurrent network that, given a set of city\ncoordinates, predicts a distribution over different city permutations. Using\nnegative tour length as the reward signal, we optimize the parameters of the\nrecurrent network using a policy gradient method. We compare learning the\nnetwork parameters on a set of training graphs against learning them on\nindividual test graphs. Despite the computational expense, without much\nengineering and heuristic designing, Neural Combinatorial Optimization achieves\nclose to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied\nto the KnapSack, another NP-hard problem, the same method obtains optimal\nsolutions for instances with up to 200 items.\n","id":537}
{"Unnamed: 0.1":11538,"Unnamed: 0":11538.0,"anchor":"Improving the Performance of Neural Machine Translation Involving\n  Morphologically Rich Languages","positive":"  The advent of the attention mechanism in neural machine translation models\nhas improved the performance of machine translation systems by enabling\nselective lookup into the source sentence. In this paper, the efficiencies of\ntranslation using bidirectional encoder attention decoder models were studied\nwith respect to translation involving morphologically rich languages. The\nEnglish - Tamil language pair was selected for this analysis. First, the use of\nWord2Vec embedding for both the English and Tamil words improved the\ntranslation results by 0.73 BLEU points over the baseline RNNSearch model with\n4.84 BLEU score. The use of morphological segmentation before word\nvectorization to split the morphologically rich Tamil words into their\nrespective morphemes before the translation, caused a reduction in the target\nvocabulary size by a factor of 8. Also, this model (RNNMorph) improved the\nperformance of neural machine translation by 7.05 BLEU points over the\nRNNSearch model used over the same corpus. Since the BLEU evaluation of the\nRNNMorph model might be unreliable due to an increase in the number of matching\ntokens per sentence, the performances of the translations were also compared by\nmeans of human evaluation metrics of adequacy, fluency and relative ranking.\nFurther, the use of morphological segmentation also improved the efficacy of\nthe attention mechanism.\n","negative":"  Temporal point processes have been widely applied to model event sequence\ndata generated by online users. In this paper, we consider the problem of how\nto design the optimal control policy for point processes, such that the\nstochastic system driven by the point process is steered to a target state. In\nparticular, we exploit the key insight to view the stochastic optimal control\nproblem from the perspective of optimal measure and variational inference. We\nfurther propose a convex optimization framework and an efficient algorithm to\nupdate the policy adaptively to the current system state. Experiments on\nsynthetic and real-world data show that our algorithm can steer the user\nactivities much more accurately and efficiently than other stochastic control\nmethods.\n","id":538}
{"Unnamed: 0.1":11539,"Unnamed: 0":11539.0,"anchor":"Interactive Elicitation of Knowledge on Feature Relevance Improves\n  Predictions in Small Data Sets","positive":"  Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain.\n","negative":"  In this study, an Artificial Neural Network (ANN) approach is utilized to\nperform a parametric study on the process of extraction of lubricants from\nheavy petroleum cuts. To train the model, we used field data collected from an\nindustrial plant. Operational conditions of feed and solvent flow rate,\nTemperature of streams and mixing rate were considered as the input to the\nmodel, whereas the flow rate of the main product was considered as the output\nof the ANN model. A feed-forward Multi-Layer Perceptron Neural Network was\nsuccessfully applied to capture the relationship between inputs and output\nparameters.\n","id":539}
{"Unnamed: 0.1":11540,"Unnamed: 0":11540.0,"anchor":"Bridging Medical Data Inference to Achilles Tendon Rupture\n  Rehabilitation","positive":"  Imputing incomplete medical tests and predicting patient outcomes are crucial\nfor guiding the decision making for therapy, such as after an Achilles Tendon\nRupture (ATR). We formulate the problem of data imputation and prediction for\nATR relevant medical measurements into a recommender system framework. By\napplying MatchBox, which is a collaborative filtering approach, on a real\ndataset collected from 374 ATR patients, we aim at offering personalized\nmedical data imputation and prediction. In this work, we show the feasibility\nof this approach and discuss potential research directions by conducting\ninitial qualitative evaluations.\n","negative":"  We evaluate the uncertainty quality in neural networks using anomaly\ndetection. We extract uncertainty measures (e.g. entropy) from the predictions\nof candidate models, use those measures as features for an anomaly detector,\nand gauge how well the detector differentiates known from unknown classes. We\nassign higher uncertainty quality to candidate models that lead to better\ndetectors. We also propose a novel method for sampling a variational\napproximation of a Bayesian neural network, called One-Sample Bayesian\nApproximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We\ncompare the following candidate neural network models: Maximum Likelihood,\nBayesian Dropout, OSBA, and --- for MNIST --- the standard variational\napproximation. We show that Bayesian Dropout and OSBA provide better\nuncertainty information than Maximum Likelihood, and are essentially equivalent\nto the standard variational approximation, but much faster.\n","id":540}
{"Unnamed: 0.1":11541,"Unnamed: 0":11541.0,"anchor":"Prediction with a Short Memory","positive":"  We consider the problem of predicting the next observation given a sequence\nof past observations, and consider the extent to which accurate prediction\nrequires complex algorithms that explicitly leverage long-range dependencies.\nPerhaps surprisingly, our positive results show that for a broad class of\nsequences, there is an algorithm that predicts well on average, and bases its\npredictions only on the most recent few observation together with a set of\nsimple summary statistics of the past observations. Specifically, we show that\nfor any distribution over observations, if the mutual information between past\nobservations and future observations is upper bounded by $I$, then a simple\nMarkov model over the most recent $I\/\\epsilon$ observations obtains expected KL\nerror $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to\nthe optimal predictor that has access to the entire past and knows the data\ngenerating distribution. For a Hidden Markov Model with $n$ hidden states, $I$\nis bounded by $\\log n$, a quantity that does not depend on the mixing time, and\nwe show that the trivial prediction algorithm based on the empirical\nfrequencies of length $O(\\log n\/\\epsilon)$ windows of observations achieves\nthis error, provided the length of the sequence is $d^{\\Omega(\\log\nn\/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n  We also establish that this result cannot be improved upon, even for the\nclass of HMMs, in the following two senses: First, for HMMs with $n$ hidden\nstates, a window length of $\\log n\/\\epsilon$ is information-theoretically\nnecessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the\n$d^{\\Theta(\\log n\/\\epsilon)}$ samples required to estimate the Markov model for\nan observation alphabet of size $d$ is necessary for any computationally\ntractable learning algorithm, assuming the hardness of strongly refuting a\ncertain class of CSPs.\n","negative":"  One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors.\n","id":541}
{"Unnamed: 0.1":11542,"Unnamed: 0":11542.0,"anchor":"Predicting brain age with deep learning from raw imaging data results in\n  a reliable and heritable biomarker","positive":"  Machine learning analysis of neuroimaging data can accurately predict\nchronological age in healthy people and deviations from healthy brain ageing\nhave been associated with cognitive impairment and disease. Here we sought to\nfurther establish the credentials of \"brain-predicted age\" as a biomarker of\nindividual differences in the brain ageing process, using a predictive\nmodelling approach based on deep learning, and specifically convolutional\nneural networks (CNN), and applied to both pre-processed and raw T1-weighted\nMRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted\nage using a large dataset of healthy adults (N = 2001). Next, we sought to\nestablish the heritability of brain-predicted age using a sample of monozygotic\nand dizygotic female twins (N = 62). Thirdly, we examined the test-retest and\nmulti-centre reliability of brain-predicted age using two samples\n(within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were\ngenerated and compared to a Gaussian Process Regression (GPR) approach, on all\ndatasets. Input data were grey matter (GM) or white matter (WM) volumetric maps\ngenerated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted\nage represents an accurate, highly reliable and genetically-valid phenotype,\nthat has potential to be used as a biomarker of brain ageing. Moreover, age\npredictions can be accurately generated on raw T1-MRI data, substantially\nreducing computation time for novel data, bringing the process closer to giving\nreal-time information on brain health in clinical settings.\n","negative":"  Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM\/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM\/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB\n","id":542}
{"Unnamed: 0.1":11543,"Unnamed: 0":11543.0,"anchor":"Towards Information-Seeking Agents","positive":"  We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired.\n","negative":"  The omnipresence of deep learning architectures such as deep convolutional\nneural networks (CNN)s is fueled by the synergistic combination of\never-increasing labeled datasets and specialized hardware. Despite the\nindisputable success, the reliance on huge amounts of labeled data and\nspecialized hardware can be a limiting factor when approaching new\napplications. To help alleviating these limitations, we propose an efficient\nlearning strategy for layer-wise unsupervised training of deep CNNs on\nconventional hardware in acceptable time. Our proposed strategy consists of\nrandomly convexifying the reconstruction contractive auto-encoding (RCAE)\nlearning objective and solving the resulting large-scale convex minimization\nproblem in the frequency domain via coordinate descent (CD). The main\nadvantages of our proposed learning strategy are: (1) single tunable\noptimization parameter; (2) fast and guaranteed convergence; (3) possibilities\nfor full parallelization. Numerical experiments show that our proposed learning\nstrategy scales (in the worst case) linearly with image size, number of filters\nand filter size.\n","id":543}
{"Unnamed: 0.1":11544,"Unnamed: 0":11544.0,"anchor":"Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016","positive":"  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n","negative":"  A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics.\n","id":544}
{"Unnamed: 0.1":11545,"Unnamed: 0":11545.0,"anchor":"Towards better decoding and language model integration in sequence to\n  sequence models","positive":"  The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.\n","negative":"  We introduce the hierarchical compositional network (HCN), a directed\ngenerative model able to discover and disentangle, without supervision, the\nbuilding blocks of a set of binary images. The building blocks are binary\nfeatures defined hierarchically as a composition of some of the features in the\nlayer immediately below, arranged in a particular manner. At a high level, HCN\nis similar to a sigmoid belief network with pooling. Inference and learning in\nHCN are very challenging and existing variational approximations do not work\nsatisfactorily. A main contribution of this work is to show that both can be\naddressed using max-product message passing (MPMP) with a particular schedule\n(no EM required). Also, using MPMP as an inference engine for HCN makes new\ntasks simple: adding supervision information, classifying images, or performing\ninpainting all correspond to clamping some variables of the model to their\nknown values and running MPMP on the rest. When used for classification, fast\ninference with HCN has exactly the same functional form as a convolutional\nneural network (CNN) with linear activations and binary weights. However, HCN's\nfeatures are qualitatively very different.\n","id":545}
{"Unnamed: 0.1":11546,"Unnamed: 0":11546.0,"anchor":"A note on the triangle inequality for the Jaccard distance","positive":"  Two simple proofs of the triangle inequality for the Jaccard distance in\nterms of nonnegative, monotone, submodular functions are given and discussed.\n","negative":"  For any business, planning is a continuous process, and typically\nbusiness-owners focus on making both long-term planning aligned with a\nparticular strategy as well as short-term planning that accommodates the\ndynamic market situations. An ability to perform an accurate financial forecast\nis crucial for effective planning. In this paper, we focus on providing an\nintelligent and efficient solution that will help in forecasting revenue using\nmachine learning algorithms. We experiment with three different revenue\nforecasting models, and here we provide detailed insights into the methodology\nand their relative performance measured on real finance data. As a real-world\napplication of our models, we partner with Microsoft's Finance organization\n(department that reports Microsoft's finances) to provide them a guidance on\nthe projected revenue for upcoming quarters.\n","id":546}
{"Unnamed: 0.1":11547,"Unnamed: 0":11547.0,"anchor":"CrowdMI: Multiple Imputation via Crowdsourcing","positive":"  Can humans impute missing data with similar proficiency as machines? This is\nthe question we aim to answer in this paper. We present a novel idea of\nconverting observations with missing data in to a survey questionnaire, which\nis presented to crowdworkers for completion. We replicate a multiple imputation\nframework by having multiple unique crowdworkers complete our questionnaire.\nExperimental results demonstrate that using our method, it is possible to\ngenerate valid imputations for qualitative and quantitative missing data, with\nresults comparable to imputations generated by complex statistical models.\n","negative":"  Cancer survival prediction is an active area of research that can help\nprevent unnecessary therapies and improve patient's quality of life. Gene\nexpression profiling is being widely used in cancer studies to discover\ninformative biomarkers that aid predict different clinical endpoint prediction.\nWe use multiple modalities of data derived from RNA deep-sequencing (RNA-seq)\nto predict survival of cancer patients. Despite the wealth of information\navailable in expression profiles of cancer tumors, fulfilling the\naforementioned objective remains a big challenge, for the most part, due to the\npaucity of data samples compared to the high dimension of the expression\nprofiles. As such, analysis of transcriptomic data modalities calls for\nstate-of-the-art big-data analytics techniques that can maximally use all the\navailable data to discover the relevant information hidden within a significant\namount of noise. In this paper, we propose a pipeline that predicts cancer\npatients' survival by exploiting the structure of the input (manifold learning)\nand by leveraging the unlabeled samples using Laplacian support vector\nmachines, a graph-based semi supervised learning (GSSL) paradigm. We show that\nunder certain circumstances, no single modality per se will result in the best\naccuracy and by fusing different models together via a stacked generalization\nstrategy, we may boost the accuracy synergistically. We apply our approach to\ntwo cancer datasets and present promising results. We maintain that a similar\npipeline can be used for predictive tasks where labeled samples are expensive\nto acquire.\n","id":547}
{"Unnamed: 0.1":11548,"Unnamed: 0":11548.0,"anchor":"Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks","positive":"  A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1\/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a\/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability.\n","negative":"  Today, smartphone devices are owned by a large portion of the population and\nhave become a very popular platform for accessing the Internet. Smartphones\nprovide the user with immediate access to information and services. However,\nthey can easily expose the user to many privacy risks. Applications that are\ninstalled on the device and entities with access to the device's Internet\ntraffic can reveal private information about the smartphone user and steal\nsensitive content stored on the device or transmitted by the device over the\nInternet. In this paper, we present a method to reveal various demographics and\ntechnical computer skills of smartphone users by their Internet traffic\nrecords, using machine learning classification models. We implement and\nevaluate the method on real life data of smartphone users and show that\nsmartphone users can be classified by their gender, smoking habits, software\nprogramming experience, and other characteristics.\n","id":548}
{"Unnamed: 0.1":11549,"Unnamed: 0":11549.0,"anchor":"Learning in the Machine: Random Backpropagation and the Deep Learning\n  Channel","positive":"  Random backpropagation (RBP) is a variant of the backpropagation algorithm\nfor training neural networks, where the transpose of the forward matrices are\nreplaced by fixed random matrices in the calculation of the weight updates. It\nis remarkable both because of its effectiveness, in spite of using random\nmatrices to communicate error information, and because it completely removes\nthe taxing requirement of maintaining symmetric weights in a physical neural\nsystem. To better understand random backpropagation, we first connect it to the\nnotions of local learning and learning channels. Through this connection, we\nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP\n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their\ncomputational complexity. We then study their behavior through simulations\nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that\nmost of these variants work robustly, almost as well as backpropagation, and\nthat multiplication by the derivatives of the activation functions is\nimportant. As a follow-up, we study also the low-end of the number of bits\nrequired to communicate error information over the learning channel. We then\nprovide partial intuitive explanations for some of the remarkable properties of\nRBP and its variations. Finally, we prove several mathematical results,\nincluding the convergence to fixed points of linear chains of arbitrary length,\nthe convergence to fixed points of linear autoencoders with decorrelated data,\nthe long-term existence of solutions for linear systems with a single hidden\nlayer and convergence in special cases, and the convergence to fixed points of\nnon-linear chains, when the derivative of the activation functions is included.\n","negative":"  Speech is one of the most effective ways of communication among humans. Even\nthough audio is the most common way of transmitting speech, very important\ninformation can be found in other modalities, such as vision. Vision is\nparticularly useful when the acoustic signal is corrupted. Multi-modal speech\nrecognition however has not yet found wide-spread use, mostly because the\ntemporal alignment and fusion of the different information sources is\nchallenging.\n  This paper presents an end-to-end audiovisual speech recognizer (AVSR), based\non recurrent neural networks (RNN) with a connectionist temporal classification\n(CTC) loss function. CTC creates sparse \"peaky\" output activations, and we\nanalyze the differences in the alignments of output targets (phonemes or\nvisemes) between audio-only, video-only, and audio-visual feature\nrepresentations. We present the first such experiments on the large vocabulary\nIBM ViaVoice database, which outperform previously published approaches on\nphone accuracy in clean and noisy conditions.\n","id":549}
{"Unnamed: 0.1":11550,"Unnamed: 0":11550.0,"anchor":"Controlling Robot Morphology from Incomplete Measurements","positive":"  Mobile robots with complex morphology are essential for traversing rough\nterrains in Urban Search & Rescue missions (USAR). Since teleoperation of the\ncomplex morphology causes high cognitive load of the operator, the morphology\nis controlled autonomously. The autonomous control measures the robot state and\nsurrounding terrain which is usually only partially observable, and thus the\ndata are often incomplete. We marginalize the control over the missing\nmeasurements and evaluate an explicit safety condition. If the safety condition\nis violated, tactile terrain exploration by the body-mounted robotic arm\ngathers the missing data.\n","negative":"  We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively.\n","id":550}
{"Unnamed: 0.1":11551,"Unnamed: 0":11551.0,"anchor":"Coupling Distributed and Symbolic Execution for Natural Language Queries","positive":"  Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.\n","negative":"  Despite recent advances, the remaining bottlenecks in deep generative models\nare necessity of extensive training and difficulties with generalization from\nsmall number of training examples. We develop a new generative model called\nGenerative Matching Network which is inspired by the recently proposed matching\nnetworks for one-shot learning in discriminative tasks. By conditioning on the\nadditional input dataset, our model can instantly learn new concepts that were\nnot available in the training data but conform to a similar generative process.\nThe proposed framework does not explicitly restrict diversity of the\nconditioning data and also does not require an extensive inference procedure\nfor training or adaptation. Our experiments on the Omniglot dataset demonstrate\nthat Generative Matching Networks significantly improve predictive performance\non the fly as more additional data is available and outperform existing state\nof the art conditional generative models.\n","id":551}
{"Unnamed: 0.1":11552,"Unnamed: 0":11552.0,"anchor":"Protein-Ligand Scoring with Convolutional Neural Networks","positive":"  Computational approaches to drug discovery can reduce the time and cost\nassociated with experimental assays and enable the screening of novel\nchemotypes. Structure-based drug design methods rely on scoring functions to\nrank and predict binding affinities and poses. The ever-expanding amount of\nprotein-ligand binding and structural data enables the use of deep machine\nlearning techniques for protein-ligand scoring.\n  We describe convolutional neural network (CNN) scoring functions that take as\ninput a comprehensive 3D representation of a protein-ligand interaction. A CNN\nscoring function automatically learns the key features of protein-ligand\ninteractions that correlate with binding. We train and optimize our CNN scoring\nfunctions to discriminate between correct and incorrect binding poses and known\nbinders and non-binders. We find that our CNN scoring function outperforms the\nAutoDock Vina scoring function when ranking poses both for pose prediction and\nvirtual screening.\n","negative":"  Despite of the pain and limited accuracy of blood tests for early recognition\nof cardiovascular disease, they dominate risk screening and triage. On the\nother hand, heart rate variability is non-invasive and cheap, but not\nconsidered accurate enough for clinical practice. Here, we tackle heart beat\ninterval based classification with deep learning. We introduce an end to end\ndifferentiable hybrid architecture, consisting of a layer of biological neuron\nmodels of cardiac dynamics (modified FitzHugh Nagumo neurons) and several\nlayers of a standard feed-forward neural network. The proposed model is\nevaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,\nand 1172 chest pain patients of an emergency department. We show that it can\nsignificantly outperform models based on traditional heart rate variability\npredictors, as well as approaching or in some cases outperforming clinical\nblood tests, based only on 60 seconds of inter-beat intervals.\n","id":552}
{"Unnamed: 0.1":11553,"Unnamed: 0":11553.0,"anchor":"Improved generator objectives for GANs","positive":"  We present a framework to understand GAN training as alternating density\nratio estimation and approximate divergence minimization. This provides an\ninterpretation for the mismatched GAN generator and discriminator objectives\noften used in practice, and explains the problem of poor sample diversity. We\nalso derive a family of generator objectives that target arbitrary\n$f$-divergences without minimizing a lower bound, and use them to train\ngenerative image models that target either improved sample quality or greater\nsample diversity.\n","negative":"  Generative adversarial learning is a popular new approach to training\ngenerative models which has been proven successful for other related problems\nas well. The general idea is to maintain an oracle $D$ that discriminates\nbetween the expert's data distribution and that of the generative model $G$.\nThe generative model is trained to capture the expert's distribution by\nmaximizing the probability of $D$ misclassifying the data it generates.\nOverall, the system is \\emph{differentiable} end-to-end and is trained using\nbasic backpropagation. This type of learning was successfully applied to the\nproblem of policy imitation in a model-free setup. However, a model-free\napproach does not allow the system to be differentiable, which requires the use\nof high-variance gradient estimations. In this paper we introduce the Model\nbased Adversarial Imitation Learning (MAIL) algorithm. A model-based approach\nfor the problem of adversarial imitation learning. We show how to use a forward\nmodel to make the system fully differentiable, which enables us to train\npolicies using the (stochastic) gradient of $D$. Moreover, our approach\nrequires relatively few environment interactions, and fewer hyper-parameters to\ntune. We test our method on the MuJoCo physics simulator and report initial\nresults that surpass the current state-of-the-art.\n","id":553}
{"Unnamed: 0.1":11554,"Unnamed: 0":11554.0,"anchor":"Interactive Prior Elicitation of Feature Similarities for Small Sample\n  Size Prediction","positive":"  Regression under the \"small $n$, large $p$\" conditions, of small sample size\n$n$ and large number of features $p$ in the learning data set, is a recurring\nsetting in which learning from data is difficult. With prior knowledge about\nrelationships of the features, $p$ can effectively be reduced, but explicating\nsuch prior knowledge is difficult for experts. In this paper we introduce a new\nmethod for eliciting expert prior knowledge about the similarity of the roles\nof features in the prediction task. The key idea is to use an interactive\nmultidimensional-scaling (MDS) type scatterplot display of the features to\nelicit the similarity relationships, and then use the elicited relationships in\nthe prior distribution of prediction parameters. Specifically, for learning to\npredict a target variable with Bayesian linear regression, the feature\nrelationships are used to construct a Gaussian prior with a full covariance\nmatrix for the regression coefficients. Evaluation of our method in experiments\nwith simulated and real users on text data confirm that prior elicitation of\nfeature similarities improves prediction accuracy. Furthermore, elicitation\nwith an interactive scatterplot display outperforms straightforward elicitation\nwhere the users choose feature pairs from a feature list.\n","negative":"  Recently developed deep learning techniques have significantly improved the\naccuracy of various speech and image recognition systems. In this paper we\nadapt some of these techniques for protein secondary structure prediction. We\nfirst train a series of deep neural networks to predict eight-class secondary\nstructure labels given a protein's amino acid sequence information and find\nthat using recent methods for regularization, such as dropout and weight-norm\nconstraining, leads to measurable gains in accuracy. We then adapt recent\nconvolutional neural network architectures--Inception, ReSNet, and DenseNet\nwith Batch Normalization--to the problem of protein structure prediction. These\nconvolutional architectures make heavy use of multi-scale filter layers that\nsimultaneously compute features on several scales, and use residual connections\nto prevent underfitting. Using a carefully modified version of these\narchitectures, we achieve state-of-the-art performance of 70.0% per amino acid\naccuracy on the public CB513 benchmark dataset. Finally, we explore additions\nfrom sequence-to-sequence learning, altering the model to make its predictions\nconditioned on both the protein's amino acid sequence and its past secondary\nstructure labels. We introduce a new method of ensembling such a conditional\nmodel with our convolutional model, an approach which reaches 70.6% Q8 accuracy\non CB513. We argue that these results can be further refined for larger boosts\nin prediction accuracy through more sophisticated attempts to control\noverfitting of conditional models. We aim to release the code for these\nexperiments as part of the TensorFlow repository.\n","id":554}
{"Unnamed: 0.1":11555,"Unnamed: 0":11555.0,"anchor":"The Physical Systems Behind Optimization Algorithms","positive":"  We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex).\n","negative":"  Stochastic gradient descent (SGD) is a standard optimization method to\nminimize a training error with respect to network parameters in modern neural\nnetwork learning. However, it typically suffers from proliferation of saddle\npoints in the high-dimensional parameter space. Therefore, it is highly\ndesirable to design an efficient algorithm to escape from these saddle points\nand reach a parameter region of better generalization capabilities. Here, we\npropose a simple extension of SGD, namely reinforced SGD, which simply adds\nprevious first-order gradients in a stochastic manner with a probability that\nincreases with learning time. As verified in a simple synthetic dataset, this\nmethod significantly accelerates learning compared with the original SGD.\nSurprisingly, it dramatically reduces over-fitting effects, even compared with\nstate-of-the-art adaptive learning algorithm---Adam. For a benchmark\nhandwritten digits dataset, the learning performance is comparable to Adam, yet\nwith an extra advantage of requiring one-fold less computer memory. The\nreinforced SGD is also compared with SGD with fixed or adaptive momentum\nparameter and Nesterov's momentum, which shows that the proposed framework is\nable to reach a similar generalization accuracy with less computational costs.\nOverall, our method introduces stochastic memory into gradients, which plays an\nimportant role in understanding how gradient-based training algorithms can work\nand its relationship with generalization abilities of deep networks.\n","id":555}
{"Unnamed: 0.1":11556,"Unnamed: 0":11556.0,"anchor":"Task-Guided and Path-Augmented Heterogeneous Network Embedding for\n  Author Identification","positive":"  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n","negative":"  Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.\n","id":556}
{"Unnamed: 0.1":11557,"Unnamed: 0":11557.0,"anchor":"Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis","positive":"  A multi-way factor analysis model is introduced for tensor-variate data of\nany order. Each data item is represented as a (sparse) sum of Kruskal\ndecompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can\ninfer both the tensor-rank of each dictionary atom and the number of dictionary\natoms. The model is adapted for online learning, which allows dictionary\nlearning on large data sets. After KFA is introduced, the model is extended to\na deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The\nexperiments section demonstrates the improvement of KFA over vectorized\napproaches (e.g., BPFA), tensor decompositions, and convolutional neural\nnetworks (CNN) in multi-way denoising, blind inpainting, and image\nclassification. The improvement in PSNR for the inpainting results over other\nmethods exceeds 1dB in several cases and we achieve state of the art results on\nCaltech101 image classification.\n","negative":"  Generative Adversarial Networks have become one of the most studied\nframeworks for unsupervised learning due to their intuitive formulation. They\nhave also been shown to be capable of generating convincing examples in limited\ndomains, such as low-resolution images. However, they still prove difficult to\ntrain in practice and tend to ignore modes of the data generating distribution.\nQuantitatively capturing effects such as mode coverage and more generally the\nquality of the generative model still remain elusive. We propose Generative\nAdversarial Parallelization, a framework in which many GANs or their variants\nare trained simultaneously, exchanging their discriminators. This eliminates\nthe tight coupling between a generator and discriminator, leading to improved\nconvergence and improved coverage of modes. We also propose an improved variant\nof the recently proposed Generative Adversarial Metric and show how it can\nscore individual GANs or their collections under the GAP model.\n","id":557}
{"Unnamed: 0.1":11558,"Unnamed: 0":11558.0,"anchor":"Learning Representations by Stochastic Meta-Gradient Descent in Neural\n  Networks","positive":"  Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.\n","negative":"  In this paper we present a technique to train neural network models on small\namounts of data. Current methods for training neural networks on small amounts\nof rich data typically rely on strategies such as fine-tuning a pre-trained\nneural network or the use of domain-specific hand-engineered features. Here we\ntake the approach of treating network layers, or entire networks, as modules\nand combine pre-trained modules with untrained modules, to learn the shift in\ndistributions between data sets. The central impact of using a modular approach\ncomes from adding new representations to a network, as opposed to replacing\nrepresentations via fine-tuning. Using this technique, we are able surpass\nresults using standard fine-tuning transfer learning approaches, and we are\nalso able to significantly increase performance over such approaches when using\nsmaller amounts of data.\n","id":558}
{"Unnamed: 0.1":11559,"Unnamed: 0":11559.0,"anchor":"A Review of Intelligent Practices for Irrigation Prediction","positive":"  Population growth and increasing droughts are creating unprecedented strain\non the continued availability of water resources. Since irrigation is a major\nconsumer of fresh water, wastage of resources in this sector could have strong\nconsequences. To address this issue, irrigation water management and prediction\ntechniques need to be employed effectively and should be able to account for\nthe variabilities present in the environment. The different techniques surveyed\nin this paper can be classified into two categories: computational and\nstatistical. Computational methods deal with scientific correlations between\nphysical parameters whereas statistical methods involve specific prediction\nalgorithms that can be used to automate the process of irrigation water\nprediction. These algorithms interpret semantic relationships between the\nvarious parameters of temperature, pressure, evapotranspiration etc. and store\nthem as numerical precomputed entities specific to the conditions and the area\nused as the data for the training corpus used to train it. We focus on\nreviewing the computational methods used to determine Evapotranspiration and\nits implications. We compare the efficiencies of different data mining and\nmachine learning methods implemented in this area, such as Logistic Regression,\nDecision Tress Classifier, SysFor, Support Vector Machine(SVM), Fuzzy Logic\ntechniques, Artifical Neural Networks(ANNs) and various hybrids of Genetic\nAlgorithms (GA) applied to irrigation prediction. We also recommend a possible\ntechnique for the same based on its superior results in other such time series\nanalysis tasks.\n","negative":"  Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution.\n","id":559}
{"Unnamed: 0.1":11560,"Unnamed: 0":11560.0,"anchor":"Environmental Modeling Framework using Stacked Gaussian Processes","positive":"  A network of independently trained Gaussian processes (StackedGP) is\nintroduced to obtain predictions of quantities of interest with quantified\nuncertainties. The main applications of the StackedGP framework are to\nintegrate different datasets through model composition, enhance predictions of\nquantities of interest through a cascade of intermediate predictions, and to\npropagate uncertainties through emulated dynamical systems driven by uncertain\nforcing variables. By using analytical first and second-order moments of a\nGaussian process with uncertain inputs using squared exponential and polynomial\nkernels, approximated expectations of quantities of interests that require an\narbitrary composition of functions can be obtained. The StackedGP model is\nextended to any number of layers and nodes per layer, and it provides\nflexibility in kernel selection for the input nodes. The proposed nonparametric\nstacked model is validated using synthetic datasets, and its performance in\nmodel composition and cascading predictions is measured in two applications\nusing real data.\n","negative":"  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n","id":560}
{"Unnamed: 0.1":11561,"Unnamed: 0":11561.0,"anchor":"A series of maximum entropy upper bounds of the differential entropy","positive":"  We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds.\n","negative":"  In this work we propose a simple unsupervised approach for next frame\nprediction in video. Instead of directly predicting the pixels in a frame given\npast frames, we predict the transformations needed for generating the next\nframe in a sequence, given the transformations of the past frames. This leads\nto sharper results, while using a smaller prediction model.\n  In order to enable a fair comparison between different video frame prediction\nmodels, we also propose a new evaluation protocol. We use generated frames as\ninput to a classifier trained with ground truth sequences. This criterion\nguarantees that models scoring high are those producing sequences which\npreserve discrim- inative features, as opposed to merely penalizing any\ndeviation, plausible or not, from the ground truth. Our proposed approach\ncompares favourably against more sophisticated ones on the UCF-101 data set,\nwhile also being more efficient in terms of the number of parameters and\ncomputational cost.\n","id":561}
{"Unnamed: 0.1":11562,"Unnamed: 0":11562.0,"anchor":"BaTFLED: Bayesian Tensor Factorization Linked to External Data","positive":"  The vast majority of current machine learning algorithms are designed to\npredict single responses or a vector of responses, yet many types of response\nare more naturally organized as matrices or higher-order tensor objects where\ncharacteristics are shared across modes. We present a new machine learning\nalgorithm BaTFLED (Bayesian Tensor Factorization Linked to External Data) that\npredicts values in a three-dimensional response tensor using input features for\neach of the dimensions. BaTFLED uses a probabilistic Bayesian framework to\nlearn projection matrices mapping input features for each mode into latent\nrepresentations that multiply to form the response tensor. By utilizing a\nTucker decomposition, the model can capture weights for interactions between\nlatent factors for each mode in a small core tensor. Priors that encourage\nsparsity in the projection matrices and core tensor allow for feature selection\nand model regularization. This method is shown to far outperform elastic net\nand neural net models on 'cold start' tasks from data simulated in a three-mode\nstructure. Additionally, we apply the model to predict dose-response curves in\na panel of breast cancer cell lines treated with drug compounds that was used\nas a Dialogue for Reverse Engineering Assessments and Methods (DREAM)\nchallenge.\n","negative":"  Integrated Computational Materials Engineering (ICME) aims to accelerate\noptimal design of complex material systems by integrating material science and\ndesign automation. For tractable ICME, it is required that (1) a structural\nfeature space be identified to allow reconstruction of new designs, and (2) the\nreconstruction process be property-preserving. The majority of existing\nstructural presentation schemes rely on the designer's understanding of\nspecific material systems to identify geometric and statistical features, which\ncould be biased and insufficient for reconstructing physically meaningful\nmicrostructures of complex material systems. In this paper, we develop a\nfeature learning mechanism based on convolutional deep belief network to\nautomate a two-way conversion between microstructures and their\nlower-dimensional feature representations, and to achieves a 1000-fold\ndimension reduction from the microstructure space. The proposed model is\napplied to a wide spectrum of heterogeneous material systems with distinct\nmicrostructural features including Ti-6Al-4V alloy, Pb63-Sn37 alloy,\nFontainebleau sandstone, and Spherical colloids, to produce material\nreconstructions that are close to the original samples with respect to 2-point\ncorrelation functions and mean critical fracture strength. This capability is\nnot achieved by existing synthesis methods that rely on the Markovian\nassumption of material microstructures.\n","id":562}
{"Unnamed: 0.1":11563,"Unnamed: 0":11563.0,"anchor":"Clipper: A Low-Latency Online Prediction Serving System","positive":"  Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions.\n","negative":"  The $k$-means clustering algorithm is popular but has the following main\ndrawbacks: 1) the number of clusters, $k$, needs to be provided by the user in\nadvance, 2) it can easily reach local minima with randomly selected initial\ncenters, 3) it is sensitive to outliers, and 4) it can only deal with well\nseparated hyperspherical clusters. In this paper, we propose a Local Density\nPeaks Searching (LDPS) initialization framework to address these issues. The\nLDPS framework includes two basic components: one of them is the local density\nthat characterizes the density distribution of a data set, and the other is the\nlocal distinctiveness index (LDI) which we introduce to characterize how\ndistinctive a data point is compared with its neighbors. Based on these two\ncomponents, we search for the local density peaks which are characterized with\nhigh local densities and high LDIs to deal with 1) and 2). Moreover, we detect\noutliers characterized with low local densities but high LDIs, and exclude them\nout before clustering begins. Finally, we apply the LDPS initialization\nframework to $k$-medoids, which is a variant of $k$-means and chooses data\nsamples as centers, with diverse similarity measures other than the Euclidean\ndistance to fix the last drawback of $k$-means. Combining the LDPS\ninitialization framework with $k$-means and $k$-medoids, we obtain two novel\nclustering methods called LDPS-means and LDPS-medoids, respectively.\nExperiments on synthetic data sets verify the effectiveness of the proposed\nmethods, especially when the ground truth of the cluster number $k$ is large.\nFurther, experiments on several real world data sets, Handwritten Pendigits,\nCoil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give\na superior performance than the analogous approaches on both estimating $k$ and\nunsupervised object categorization.\n","id":563}
{"Unnamed: 0.1":11564,"Unnamed: 0":11564.0,"anchor":"Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and\n  Length-Scale Cool Down","positive":"  Bayesian Optimization (BO) has become a core method for solving expensive\nblack-box optimization problems. While much research focussed on the choice of\nthe acquisition function, we focus on online length-scale adaption and the\nchoice of kernel function. Instead of choosing hyperparameters in view of\nmaximum likelihood on past data, we propose to use the acquisition function to\ndecide on hyperparameter adaptation more robustly and in view of the future\noptimization progress. Further, we propose a particular kernel function that\nincludes non-stationarity and local anisotropy and thereby implicitly\nintegrates the efficiency of local convex optimization with global Bayesian\noptimization. Comparisons to state-of-the art BO methods underline the\nefficiency of these mechanisms on global optimization benchmarks.\n","negative":"  The problem of information fusion from multiple data-sets acquired by\nmultimodal sensors has drawn significant research attention over the years. In\nthis paper, we focus on a particular problem setting consisting of a physical\nphenomenon or a system of interest observed by multiple sensors. We assume that\nall sensors measure some aspects of the system of interest with additional\nsensor-specific and irrelevant components. Our goal is to recover the variables\nrelevant to the observed system and to filter out the nuisance effects of the\nsensor-specific variables. We propose an approach based on manifold learning,\nwhich is particularly suitable for problems with multiple modalities, since it\naims to capture the intrinsic structure of the data and relies on minimal prior\nmodel knowledge. Specifically, we propose a nonlinear filtering scheme, which\nextracts the hidden sources of variability captured by two or more sensors,\nthat are independent of the sensor-specific components. In addition to\npresenting a theoretical analysis, we demonstrate our technique on real\nmeasured data for the purpose of sleep stage assessment based on multiple,\nmultimodal sensor measurements. We show that without prior knowledge on the\ndifferent modalities and on the measured system, our method gives rise to a\ndata-driven representation that is well correlated with the underlying sleep\nprocess and is robust to noise and sensor-specific effects.\n","id":564}
{"Unnamed: 0.1":11565,"Unnamed: 0":11565.0,"anchor":"Phase transitions in Restricted Boltzmann Machines with generic priors","positive":"  We study Generalised Restricted Boltzmann Machines with generic priors for\nunits and weights, interpolating between Boolean and Gaussian variables. We\npresent a complete analysis of the replica symmetric phase diagram of these\nsystems, which can be regarded as Generalised Hopfield models. We underline the\nrole of the retrieval phase for both inference and learning processes and we\nshow that retrieval is robust for a large class of weight and unit priors,\nbeyond the standard Hopfield scenario. Furthermore we show how the paramagnetic\nphase boundary is directly related to the optimal size of the training set\nnecessary for good generalisation in a teacher-student scenario of unsupervised\nlearning.\n","negative":"  There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks.\n","id":565}
{"Unnamed: 0.1":11566,"Unnamed: 0":11566.0,"anchor":"Testing Ising Models","positive":"  Given samples from an unknown multivariate distribution $p$, is it possible\nto distinguish whether $p$ is the product of its marginals versus $p$ being far\nfrom every product distribution? Similarly, is it possible to distinguish\nwhether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from\neach other? These problems of testing independence and goodness-of-fit have\nreceived enormous attention in statistics, information theory, and theoretical\ncomputer science, with sample-optimal algorithms known in several interesting\nregimes of parameters. Unfortunately, it has also been understood that these\nproblems become intractable in large dimensions, necessitating exponential\nsample complexity.\n  Motivated by the exponential lower bounds for general distributions as well\nas the ubiquity of Markov Random Fields (MRFs) in the modeling of\nhigh-dimensional distributions, we initiate the study of distribution testing\non structured multivariate distributions, and in particular the prototypical\nexample of MRFs: the Ising Model. We demonstrate that, in this structured\nsetting, we can avoid the curse of dimensionality, obtaining sample and time\nefficient testers for independence and goodness-of-fit. One of the key\ntechnical challenges we face along the way is bounding the variance of\nfunctions of the Ising model.\n","negative":"  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, and demonstrate\nparameter-free RICE-OCELAD metric learning on both synthetic data and a highly\nnonstationary Twitter dataset. We show significant performance improvements and\nincreased robustness to nonstationary effects relative to previously proposed\nbatch and online distance metric learning algorithms.\n","id":566}
{"Unnamed: 0.1":11567,"Unnamed: 0":11567.0,"anchor":"Optimal mean-based algorithms for trace reconstruction","positive":"  In the (deletion-channel) trace reconstruction problem, there is an unknown\n$n$-bit source string $x$. An algorithm is given access to independent traces\nof $x$, where a trace is formed by deleting each bit of~$x$ independently with\nprobability~$\\delta$. The goal of the algorithm is to recover~$x$ exactly (with\nhigh probability), while minimizing samples (number of traces) and running\ntime.\n  Previously, the best known algorithm for the trace reconstruction problem was\ndue to Holenstein~et~al.; it uses $\\exp(\\tilde{O}(n^{1\/2}))$ samples and\nrunning time for any fixed $0 < \\delta < 1$. It is also what we call a\n\"mean-based algorithm\", meaning that it only uses the empirical means of the\nindividual bits of the traces. Holenstein~et~al.~also gave a lower bound,\nshowing that any mean-based algorithm must use at least $n^{\\tilde{\\Omega}(\\log\nn)}$ samples.\n  In this paper we improve both of these results, obtaining matching upper and\nlower bounds for mean-based trace reconstruction. For any constant deletion\nrate $0 < \\delta < 1$, we give a mean-based algorithm that uses\n$\\exp(O(n^{1\/3}))$ time and traces; we also prove that any mean-based algorithm\nmust use at least $\\exp(\\Omega(n^{1\/3}))$ traces. In fact, we obtain matching\nupper and lower bounds even for $\\delta$ subconstant and $\\rho := 1-\\delta$\nsubconstant: when $(\\log^3 n)\/n \\ll \\delta \\leq 1\/2$ the bound is\n$\\exp(-\\Theta(\\delta n)^{1\/3})$, and when $1\/\\sqrt{n} \\ll \\rho \\leq 1\/2$ the\nbound is $\\exp(-\\Theta(n\/\\rho)^{1\/3})$.\n  Our proofs involve estimates for the maxima of Littlewood polynomials on\ncomplex disks. We show that these techniques can also be used to perform trace\nreconstruction with random insertions and bit-flips in addition to deletions.\nWe also find a surprising result: for deletion probabilities $\\delta > 1\/2$,\nthe presence of insertions can actually help with trace reconstruction.\n","negative":"  This paper proposes a general method for improving the structure and quality\nof sequences generated by a recurrent neural network (RNN), while maintaining\ninformation originally learned from data, as well as sample diversity. An RNN\nis first pre-trained on data using maximum likelihood estimation (MLE), and the\nprobability distribution over the next token in the sequence learned by this\nmodel is treated as a prior policy. Another RNN is then trained using\nreinforcement learning (RL) to generate higher-quality outputs that account for\ndomain-specific incentives while retaining proximity to the prior policy of the\nMLE RNN. To formalize this objective, we derive novel off-policy RL methods for\nRNNs from KL-control. The effectiveness of the approach is demonstrated on two\napplications; 1) generating novel musical melodies, and 2) computational\nmolecular generation. For both problems, we show that the proposed method\nimproves the desired properties and structure of the generated sequences, while\nmaintaining information learned from data.\n","id":567}
{"Unnamed: 0.1":11568,"Unnamed: 0":11568.0,"anchor":"Testing Bayesian Networks","positive":"  This work initiates a systematic investigation of testing high-dimensional\nstructured distributions by focusing on testing Bayesian networks -- the\nprototypical family of directed graphical models. A Bayesian network is defined\nby a directed acyclic graph, where we associate a random variable with each\nnode. The value at any particular node is conditionally independent of all the\nother non-descendant nodes once its parents are fixed. Specifically, we study\nthe properties of identity testing and closeness testing of Bayesian networks.\nOur main contribution is the first non-trivial efficient testing algorithms for\nthese problems and corresponding information-theoretic lower bounds. For a wide\nrange of parameter settings, our testing algorithms have sample complexity\nsublinear in the dimension and are sample-optimal, up to constant factors.\n","negative":"  This paper advocates Riemannian multi-manifold modeling in the context of\nnetwork-wide non-stationary time-series analysis. Time-series data, collected\nsequentially over time and across a network, yield features which are viewed as\npoints in or close to a union of multiple submanifolds of a Riemannian\nmanifold, and distinguishing disparate time series amounts to clustering\nmultiple Riemannian submanifolds. To support the claim that exploiting the\nlatent Riemannian geometry behind many statistical features of time series is\nbeneficial to learning from network data, this paper focuses on brain networks\nand puts forth two feature-generation schemes for network-wide dynamic time\nseries. The first is motivated by Granger-causality arguments and uses an\nauto-regressive moving average model to map low-rank linear vector subspaces,\nspanned by column vectors of appropriately defined observability matrices, to\npoints into the Grassmann manifold. The second utilizes (non-linear)\ndependencies among network nodes by introducing kernel-based partial\ncorrelations to generate points in the manifold of positive-definite matrices.\nCapitilizing on recently developed research on clustering Riemannian\nsubmanifolds, an algorithm is provided for distinguishing time series based on\ntheir geometrical properties, revealed within Riemannian feature spaces.\nExtensive numerical tests demonstrate that the proposed framework outperforms\nclassical and state-of-the-art techniques in clustering brain-network\nstates\/structures hidden beneath synthetic fMRI time series and brain-activity\nsignals generated from real brain-network structural connectivity matrices.\n","id":568}
{"Unnamed: 0.1":11569,"Unnamed: 0":11569.0,"anchor":"Square Hellinger Subadditivity for Bayesian Networks and its\n  Applications to Identity Testing","positive":"  We show that the square Hellinger distance between two Bayesian networks on\nthe same directed graph, $G$, is subadditive with respect to the neighborhoods\nof $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two\nBayesian networks on the same DAG, our inequality states that the square\nHellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the\nsum, $\\sum_v H^2(P_{\\{v\\} \\cup \\Pi_v}, Q_{\\{v\\} \\cup \\Pi_v})$, of the square\nHellinger distances between the marginals of $P$ and $Q$ on every node $v$ and\nits parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the\nconditionals but the marginals of $P$ and $Q$. We derive a similar inequality\nfor more general Markov Random Fields.\n  As an application of our inequality, we show that distinguishing whether two\nBayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy\n$P=Q$ vs $d_{\\rm TV}(P,Q)>\\epsilon$ can be performed from\n$\\tilde{O}(|\\Sigma|^{3\/4(d+1)} \\cdot n\/\\epsilon^2)$ samples, where $d$ is the\nmaximum in-degree of the DAG and $\\Sigma$ the domain of each variable of the\nBayesian networks. If $P$ and $Q$ are defined on potentially different and\npotentially unknown trees, the sample complexity becomes\n$\\tilde{O}(|\\Sigma|^{4.5} n\/\\epsilon^2)$, whose dependence on $n, \\epsilon$ is\noptimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product\ndistributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes\n$O(\\sqrt{n}\/\\epsilon^2)$, which is optimal up to constant factors.\n","negative":"  Nearest neighbor search is a very active field in machine learning for it\nappears in many application cases, including classification and object\nretrieval. In its canonical version, the complexity of the search is linear\nwith both the dimension and the cardinal of the collection of vectors the\nsearch is performed in. Recently many works have focused on reducing the\ndimension of vectors using quantization techniques or hashing, while providing\nan approximate result. In this paper we focus instead on tackling the cardinal\nof the collection of vectors. Namely, we introduce a technique that partitions\nthe collection of vectors and stores each part in its own associative memory.\nWhen a query vector is given to the system, associative memories are polled to\nidentify which one contain the closest match. Then an exhaustive search is\nconducted only on the part of vectors stored in the selected associative\nmemory. We study the effectiveness of the system when messages to store are\ngenerated from i.i.d. uniform $\\pm$1 random variables or 0-1 sparse i.i.d.\nrandom variables. We also conduct experiment on both synthetic data and real\ndata and show it is possible to achieve interesting trade-offs between\ncomplexity and accuracy.\n","id":569}
{"Unnamed: 0.1":11570,"Unnamed: 0":11570.0,"anchor":"Low-Rank Inducing Norms with Optimality Interpretations","positive":"  Optimization problems with rank constraints appear in many diverse fields\nsuch as control, machine learning and image analysis. Since the rank constraint\nis non-convex, these problems are often approximately solved via convex\nrelaxations. Nuclear norm regularization is the prevailing convexifying\ntechnique for dealing with these types of problem. This paper introduces a\nfamily of low-rank inducing norms and regularizers which includes the nuclear\nnorm as a special case. A posteriori guarantees on solving an underlying rank\nconstrained optimization problem with these convex relaxations are provided. We\nevaluate the performance of the low-rank inducing norms on three matrix\ncompletion problems. In all examples, the nuclear norm heuristic is\noutperformed by convex relaxations based on other low-rank inducing norms. For\ntwo of the problems there exist low-rank inducing norms that succeed in\nrecovering the partially unknown matrix, while the nuclear norm fails. These\nlow-rank inducing norms are shown to be representable as semi-definite\nprograms. Moreover, these norms have cheaply computable proximal mappings,\nwhich makes it possible to also solve problems of large size using first-order\nmethods.\n","negative":"  To be practically useful, modern static analyzers must precisely model the\neffect of both, statements in the programming language as well as frameworks\nused by the program under analysis. While important, manually addressing these\nchallenges is difficult for at least two reasons: (i) the effects on the\noverall analysis can be non-trivial, and (ii) as the size and complexity of\nmodern libraries increase, so is the number of cases the analysis must handle.\n  In this paper we present a new, automated approach for creating static\nanalyzers: instead of manually providing the various inference rules of the\nanalyzer, the key idea is to learn these rules from a dataset of programs. Our\nmethod consists of two ingredients: (i) a synthesis algorithm capable of\nlearning a candidate analyzer from a given dataset, and (ii) a counter-example\nguided learning procedure which generates new programs beyond those in the\ninitial dataset, critical for discovering corner cases and ensuring the learned\nanalysis generalizes to unseen programs.\n  We implemented and instantiated our approach to the task of learning\nJavaScript static analysis rules for a subset of points-to analysis and for\nallocation sites analysis. These are challenging yet important problems that\nhave received significant research attention. We show that our approach is\neffective: our system automatically discovered practical and useful inference\nrules for many cases that are tricky to manually identify and are missed by\nstate-of-the-art, manually tuned analyzers.\n","id":570}
{"Unnamed: 0.1":11571,"Unnamed: 0":11571.0,"anchor":"DeepCancer: Detecting Cancer through Gene Expressions via Deep\n  Generative Learning","positive":"  Transcriptional profiling on microarrays to obtain gene expressions has been\nused to facilitate cancer diagnosis. We propose a deep generative machine\nlearning architecture (called DeepCancer) that learn features from unlabeled\nmicroarray data. These models have been used in conjunction with conventional\nclassifiers that perform classification of the tissue samples as either being\ncancerous or non-cancerous. The proposed model has been tested on two different\nclinical datasets. The evaluation demonstrates that DeepCancer model achieves a\nvery high precision score, while significantly controlling the false positive\nand false negative scores.\n","negative":"  This paper presents the development of a hybrid learning system based on\nSupport Vector Machines (SVM), Adaptive Neuro-Fuzzy Inference System (ANFIS)\nand domain knowledge to solve prediction problem. The proposed two-stage Domain\nKnowledge based Fuzzy Information System (DKFIS) improves the prediction\naccuracy attained by ANFIS alone. The proposed framework has been implemented\non a noisy and incomplete dataset acquired from a hydrocarbon field located at\nwestern part of India. Here, oil saturation has been predicted from four\ndifferent well logs i.e. gamma ray, resistivity, density, and clay volume. In\nthe first stage, depending on zero or near zero and non-zero oil saturation\nlevels the input vector is classified into two classes (Class 0 and Class 1)\nusing SVM. The classification results have been further fine-tuned applying\nexpert knowledge based on the relationship among predictor variables i.e. well\nlogs and target variable - oil saturation. Second, an ANFIS is designed to\npredict non-zero (Class 1) oil saturation values from predictor logs. The\npredicted output has been further refined based on expert knowledge. It is\napparent from the experimental results that the expert intervention with\nqualitative judgment at each stage has rendered the prediction into the\nfeasible and realistic ranges. The performance analysis of the prediction in\nterms of four performance metrics such as correlation coefficient (CC), root\nmean square error (RMSE), and absolute error mean (AEM), scatter index (SI) has\nestablished DKFIS as a useful tool for reservoir characterization.\n","id":571}
{"Unnamed: 0.1":11572,"Unnamed: 0":11572.0,"anchor":"Towards deep learning with spiking neurons in energy based models with\n  contrastive Hebbian plasticity","positive":"  In machine learning, error back-propagation in multi-layer neural networks\n(deep learning) has been impressively successful in supervised and\nreinforcement learning tasks. As a model for learning in the brain, however,\ndeep learning has long been regarded as implausible, since it relies in its\nbasic form on a non-local plasticity rule. To overcome this problem,\nenergy-based models with local contrastive Hebbian learning were proposed and\ntested on a classification task with networks of rate neurons. We extended this\nwork by implementing and testing such a model with networks of leaky\nintegrate-and-fire neurons. Preliminary results indicate that it is possible to\nlearn a non-linear regression task with hidden layers, spiking neurons and a\nlocal synaptic plasticity rule.\n","negative":"  The rapid growth in stored time-oriented data necessitates the development of\nnew methods for handling, processing, and interpreting large amounts of\ntemporal data. One important example of such processing is detecting anomalies\nin time-oriented data. The Knowledge-Based Temporal Abstraction method was\npreviously proposed for intelligent interpretation of temporal data based on\npredefined domain knowledge. In this study we propose a framework that\nintegrates the KBTA method with a temporal pattern mining process for anomaly\ndetection. According to the proposed method a temporal pattern mining process\nis applied on a dataset of basic temporal abstraction database in order to\nextract patterns representing normal behavior. These patterns are then analyzed\nin order to identify abnormal time periods characterized by a significantly\nsmall number of normal patterns. The proposed approach was demonstrated using a\ndataset collected from a real server.\n","id":572}
{"Unnamed: 0.1":11573,"Unnamed: 0":11573.0,"anchor":"Optimal Generalized Decision Trees via Integer Programming","positive":"  Decision trees have been a very popular class of predictive models for\ndecades due to their interpretability and good performance on categorical\nfeatures. However, they are not always robust and tend to overfit the data.\nAdditionally, if allowed to grow large, they lose interpretability. In this\npaper, we present a mixed integer programming formulation to construct optimal\ndecision trees of a prespecified size. We take the special structure of\ncategorical features into account and allow combinatorial decisions (based on\nsubsets of values of features) at each node. Our approach can also handle\nnumerical features via thresholding. We show that very good accuracy can be\nachieved with small trees using moderately-sized training sets. The\noptimization problems we solve are tractable with modern solvers.\n","negative":"  Count-based exploration algorithms are known to perform near-optimally when\nused in conjunction with tabular reinforcement learning (RL) methods for\nsolving small discrete Markov decision processes (MDPs). It is generally\nthought that count-based methods cannot be applied in high-dimensional state\nspaces, since most states will only occur once. Recent deep RL exploration\nstrategies are able to deal with high-dimensional continuous state spaces\nthrough complex heuristics, often relying on optimism in the face of\nuncertainty or intrinsic motivation. In this work, we describe a surprising\nfinding: a simple generalization of the classic count-based approach can reach\nnear state-of-the-art performance on various high-dimensional and\/or continuous\ndeep RL benchmarks. States are mapped to hash codes, which allows to count\ntheir occurrences with a hash table. These counts are then used to compute a\nreward bonus according to the classic count-based exploration theory. We find\nthat simple hash functions can achieve surprisingly good results on many\nchallenging tasks. Furthermore, we show that a domain-dependent learned hash\ncode may further improve these results. Detailed analysis reveals important\naspects of a good hash function: 1) having appropriate granularity and 2)\nencoding information relevant to solving the MDP. This exploration strategy\nachieves near state-of-the-art performance on both continuous control tasks and\nAtari 2600 games, hence providing a simple yet powerful baseline for solving\nMDPs that require considerable exploration.\n","id":573}
{"Unnamed: 0.1":11574,"Unnamed: 0":11574.0,"anchor":"Active Learning for Speech Recognition: the Power of Gradients","positive":"  In training speech recognition systems, labeling audio clips can be\nexpensive, and not all data is equally valuable. Active learning aims to label\nonly the most informative samples to reduce cost. For speech recognition,\nconfidence scores and other likelihood-based active learning methods have been\nshown to be effective. Gradient-based active learning methods, however, are\nstill not well-understood. This work investigates the Expected Gradient Length\n(EGL) approach in active learning for end-to-end speech recognition. We justify\nEGL from a variance reduction perspective, and observe that EGL's measure of\ninformativeness picks novel samples uncorrelated with confidence scores.\nExperimentally, we show that EGL can reduce word errors by 11\\%, or\nalternatively, reduce the number of samples to label by 50\\%, when compared to\nrandom sampling.\n","negative":"  Laplacian eigenmap algorithm is a typical nonlinear model for dimensionality\nreduction in classical machine learning. We propose an efficient quantum\nLaplacian eigenmap algorithm to exponentially speed up the original\ncounterparts. In our work, we demonstrate that the Hermitian chain product\nproposed in quantum linear discriminant analysis (arXiv:1510.00113,2015) can be\napplied to implement quantum Laplacian eigenmap algorithm. While classical\nLaplacian eigenmap algorithm requires polynomial time to solve the eigenvector\nproblem, our algorithm is able to exponentially speed up nonlinear\ndimensionality reduction.\n","id":574}
{"Unnamed: 0.1":11575,"Unnamed: 0":11575.0,"anchor":"Generalized Deep Image to Image Regression","positive":"  We present a Deep Convolutional Neural Network architecture which serves as a\ngeneric image-to-image regressor that can be trained end-to-end without any\nfurther machinery. Our proposed architecture: the Recursively Branched\nDeconvolutional Network (RBDN) develops a cheap multi-context image\nrepresentation very early on using an efficient recursive branching scheme with\nextensive parameter sharing and learnable upsampling. This multi-context\nrepresentation is subjected to a highly non-linear locality preserving\ntransformation by the remainder of our network comprising of a series of\nconvolutions\/deconvolutions without any spatial downsampling. The RBDN\narchitecture is fully convolutional and can handle variable sized images during\ninference. We provide qualitative\/quantitative results on $3$ diverse tasks:\nrelighting, denoising and colorization and show that our proposed RBDN\narchitecture obtains comparable results to the state-of-the-art on each of\nthese tasks when used off-the-shelf without any post processing or\ntask-specific architectural modifications.\n","negative":"  Collective classification of vertices is a task of assigning categories to\neach vertex in a graph based on both vertex attributes and link structure.\nNevertheless, some existing approaches do not use the features of neighbouring\nvertices properly, due to the noise introduced by these features. In this\npaper, we propose a graph-based recursive neural network framework for\ncollective vertex classification. In this framework, we generate hidden\nrepresentations from both attributes of vertices and representations of\nneighbouring vertices via recursive neural networks. Under this framework, we\nexplore two types of recursive neural units, naive recursive neural unit and\nlong short-term memory unit. We have conducted experiments on four real-world\nnetwork datasets. The experimental results show that our frame- work with long\nshort-term memory model achieves better results and outperforms several\ncompetitive baseline methods.\n","id":575}
{"Unnamed: 0.1":11576,"Unnamed: 0":11576.0,"anchor":"Gradient Coding","positive":"  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n","negative":"  We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.\n","id":576}
{"Unnamed: 0.1":11577,"Unnamed: 0":11577.0,"anchor":"Knowledge Elicitation via Sequential Probabilistic Inference for\n  High-Dimensional Prediction","positive":"  Prediction in a small-sized sample with a large number of covariates, the\n\"small n, large p\" problem, is challenging. This setting is encountered in\nmultiple applications, such as precision medicine, where obtaining additional\nsamples can be extremely costly or even impossible, and extensive research\neffort has recently been dedicated to finding principled solutions for accurate\nprediction. However, a valuable source of additional information, domain\nexperts, has not yet been efficiently exploited. We formulate knowledge\nelicitation generally as a probabilistic inference process, where expert\nknowledge is sequentially queried to improve predictions. In the specific case\nof sparse linear regression, where we assume the expert has knowledge about the\nvalues of the regression coefficients or about the relevance of the features,\nwe propose an algorithm and computational approximation for fast and efficient\ninteraction, which sequentially identifies the most informative features on\nwhich to query expert knowledge. Evaluations of our method in experiments with\nsimulated and real users show improved prediction accuracy already with a small\neffort from the expert.\n","negative":"  We investigate adversarial attacks for autoencoders. We propose a procedure\nthat distorts the input image to mislead the autoencoder in reconstructing a\ncompletely different target image. We attack the internal latent\nrepresentations, attempting to make the adversarial input produce an internal\nrepresentation as similar as possible as the target's. We find that\nautoencoders are much more robust to the attack than classifiers: while some\nexamples have tolerably small input distortion, and reasonable similarity to\nthe target image, there is a quasi-linear trade-off between those aims. We\nreport results on MNIST and SVHN datasets, and also test regular deterministic\nautoencoders, reaching similar conclusions in all cases. Finally, we show that\nthe usual adversarial attack for classifiers, while being much easier, also\npresents a direct proportion between distortion on the input, and misdirection\non the output. That proportionality however is hidden by the normalization of\nthe output, which maps a linear layer into non-linear probabilities.\n","id":577}
{"Unnamed: 0.1":11578,"Unnamed: 0":11578.0,"anchor":"An Empirical Study of ADMM for Nonconvex Problems","positive":"  The alternating direction method of multipliers (ADMM) is a common\noptimization tool for solving constrained and non-differentiable problems. We\nprovide an empirical study of the practical performance of ADMM on several\nnonconvex applications, including l0 regularized linear regression, l0\nregularized image denoising, phase retrieval, and eigenvector computation. Our\nexperiments suggest that ADMM performs well on a broad class of non-convex\nproblems. Moreover, recently proposed adaptive ADMM methods, which\nautomatically tune penalty parameters as the method runs, can improve algorithm\nefficiency and solution quality compared to ADMM with a non-tuned penalty.\n","negative":"  We address the issue of speeding up the training of convolutional networks.\nHere we study a distributed method adapted to stochastic gradient descent\n(SGD). The parallel optimization setup uses several threads, each applying\nindividual gradient descents on a local variable. We propose a new way to share\ninformation between different threads inspired by gossip algorithms and showing\ngood consensus convergence properties. Our method called GoSGD has the\nadvantage to be fully asynchronous and decentralized. We compared our method to\nthe recent EASGD in \\cite{elastic} on CIFAR-10 show encouraging results.\n","id":578}
{"Unnamed: 0.1":11579,"Unnamed: 0":11579.0,"anchor":"Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts","positive":"  We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS.\n","negative":"  We tackle the issue of finding a good policy when the number of policy\nupdates is limited. This is done by approximating the expected policy reward as\na sequence of concave lower bounds which can be efficiently maximized,\ndrastically reducing the number of policy updates required to achieve good\nperformance. We also extend existing methods to negative rewards, enabling the\nuse of control variates.\n","id":579}
{"Unnamed: 0.1":11580,"Unnamed: 0":11580.0,"anchor":"Technical Report: A Generalized Matching Pursuit Approach for\n  Graph-Structured Sparsity","positive":"  Sparsity-constrained optimization is an important and challenging problem\nthat has wide applicability in data mining, machine learning, and statistics.\nIn this paper, we focus on sparsity-constrained optimization in cases where the\ncost function is a general nonlinear function and, in particular, the sparsity\nconstraint is defined by a graph-structured sparsity model. Existing methods\nexplore this problem in the context of sparse estimation in linear models. To\nthe best of our knowledge, this is the first work to present an efficient\napproximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),\nto optimize a general nonlinear function subject to graph-structured\nconstraints. We prove that our algorithm enjoys the strong guarantees analogous\nto those designed for linear models in terms of convergence rate and\napproximation accuracy. As a case study, we specialize Graph-Mp to optimize a\nnumber of well-known graph scan statistic models for the connected subgraph\ndetection task, and empirical evidence demonstrates that our general algorithm\nperforms superior over state-of-the-art methods that are designed specifically\nfor the task of connected subgraph detection.\n","negative":"  We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference.\n","id":580}
{"Unnamed: 0.1":11581,"Unnamed: 0":11581.0,"anchor":"Non-Redundant Spectral Dimensionality Reduction","positive":"  Spectral dimensionality reduction algorithms are widely used in numerous\ndomains, including for recognition, segmentation, tracking and visualization.\nHowever, despite their popularity, these algorithms suffer from a major\nlimitation known as the \"repeated Eigen-directions\" phenomenon. That is, many\nof the embedding coordinates they produce typically capture the same direction\nalong the data manifold. This leads to redundant and inefficient\nrepresentations that do not reveal the true intrinsic dimensionality of the\ndata. In this paper, we propose a general method for avoiding redundancy in\nspectral algorithms. Our approach relies on replacing the orthogonality\nconstraints underlying those methods by unpredictability constraints.\nSpecifically, we require that each embedding coordinate be unpredictable (in\nthe statistical sense) from all previous ones. We prove that these constraints\nnecessarily prevent redundancy, and provide a simple technique to incorporate\nthem into existing methods. As we illustrate on challenging high-dimensional\nscenarios, our approach produces significantly more informative and compact\nrepresentations, which improve visualization and classification tasks.\n","negative":"  Curriculum Learning emphasizes the order of training instances in a\ncomputational learning setup. The core hypothesis is that simpler instances\nshould be learned early as building blocks to learn more complex ones. Despite\nits usefulness, it is still unknown how exactly the internal representation of\nmodels are affected by curriculum learning. In this paper, we study the effect\nof curriculum learning on Long Short-Term Memory (LSTM) networks, which have\nshown strong competency in many Natural Language Processing (NLP) problems. Our\nexperiments on sentiment analysis task and a synthetic task similar to sequence\nprediction tasks in NLP show that curriculum learning has a positive effect on\nthe LSTM's internal states by biasing the model towards building constructive\nrepresentations i.e. the internal representation at the previous timesteps are\nused as building blocks for the final prediction. We also find that smaller\nmodels significantly improves when they are trained with curriculum learning.\nLastly, we show that curriculum learning helps more when the amount of training\ndata is limited.\n","id":581}
{"Unnamed: 0.1":11582,"Unnamed: 0":11582.0,"anchor":"Lock-Free Optimization for Non-Convex Problems","positive":"  Stochastic gradient descent~(SGD) and its variants have attracted much\nattention in machine learning due to their efficiency and effectiveness for\noptimization. To handle large-scale problems, researchers have recently\nproposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for\nmulti-core systems. However, existing works have only proved the convergence of\nthese LF-PSGD methods for convex problems. To the best of our knowledge, no\nwork has proved the convergence of the LF-PSGD methods for non-convex problems.\nIn this paper, we provide the theoretical proof about the convergence of two\nrepresentative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems.\nEmpirical results also show that both Hogwild! and AsySVRG are convergent on\nnon-convex problems, which successfully verifies our theoretical results.\n","negative":"  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1\/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n","id":582}
{"Unnamed: 0.1":11583,"Unnamed: 0":11583.0,"anchor":"Noisy subspace clustering via matching pursuits","positive":"  Sparsity-based subspace clustering algorithms have attracted significant\nattention thanks to their excellent performance in practical applications. A\nprominent example is the sparse subspace clustering (SSC) algorithm by\nElhamifar and Vidal, which performs spectral clustering based on an adjacency\nmatrix obtained by sparsely representing each data point in terms of all the\nother data points via the Lasso. When the number of data points is large or the\ndimension of the ambient space is high, the computational complexity of SSC\nquickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by\nreplacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm\nresults in significantly lower computational complexity, while often yielding\ncomparable performance. The central goal of this paper is an analytical\nperformance characterization of SSC-OMP for noisy data. Moreover, we introduce\nand analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu\nof OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces\nintersect and when the data points are contaminated by severe noise. The\nclustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for\nSSC and for the thresholding-based subspace clustering (TSC) algorithm due to\nHeckel and B\\\"olcskei. Analytical results in combination with numerical results\nindicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion\nautomatically detect the dimensions of the subspaces underlying the data.\nMoreover, experiments on synthetic and on real data show that SSC-MP compares\nvery favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor\nalgorithm, both in terms of clustering performance and running time. In\naddition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is\nvery robust with respect to the choice of parameters in the stopping criteria.\n","negative":"  We construct $\\bf genRBF$ kernel, which generalizes the classical Gaussian\nRBF kernel to the case of incomplete data. We model the uncertainty contained\nin missing attributes making use of data distribution and associate every point\nwith a conditional probability density function. This allows to embed\nincomplete data into the function space and to define a kernel between two\nmissing data points based on scalar product in $L_2$. Experiments show that\nintroduced kernel applied to SVM classifier gives better results than other\nstate-of-the-art methods, especially in the case when large number of features\nis missing. Moreover, it is easy to implement and can be used together with any\nkernel approaches with no additional modifications.\n","id":583}
{"Unnamed: 0.1":11584,"Unnamed: 0":11584.0,"anchor":"Self-calibrating Neural Networks for Dimensionality Reduction","positive":"  Recently, a novel family of biologically plausible online algorithms for\nreducing the dimensionality of streaming data has been derived from the\nsimilarity matching principle. In these algorithms, the number of output\ndimensions can be determined adaptively by thresholding the singular values of\nthe input data matrix. However, setting such threshold requires knowing the\nmagnitude of the desired singular values in advance. Here we propose online\nalgorithms where the threshold is self-calibrating based on the singular values\ncomputed from the existing observations. To derive these algorithms from the\nsimilarity matching cost function we propose novel regularizers. As before,\nthese online algorithms can be implemented by Hebbian\/anti-Hebbian neural\nnetworks in which the learning rule depends on the chosen regularizer. We\ndemonstrate both mathematically and via simulation the effectiveness of these\nonline algorithms in various settings.\n","negative":"  At present, designing convolutional neural network (CNN) architectures\nrequires both human expertise and labor. New architectures are handcrafted by\ncareful experimentation or modified from a handful of existing networks. We\nintroduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to\nautomatically generate high-performing CNN architectures for a given learning\ntask. The learning agent is trained to sequentially choose CNN layers using\n$Q$-learning with an $\\epsilon$-greedy exploration strategy and experience\nreplay. The agent explores a large but finite space of possible architectures\nand iteratively discovers designs with improved performance on the learning\ntask. On image classification benchmarks, the agent-designed networks\n(consisting of only standard convolution, pooling, and fully-connected layers)\nbeat existing networks designed with the same layer types and are competitive\nagainst the state-of-the-art methods that use more complex layer types. We also\noutperform existing meta-modeling approaches for network design on image\nclassification tasks.\n","id":584}
{"Unnamed: 0.1":11585,"Unnamed: 0":11585.0,"anchor":"Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs","positive":"  Graph-based methods pervade the inference toolkits of numerous disciplines\nincluding sociology, biology, neuroscience, physics, chemistry, and\nengineering. A challenging problem encountered in this context pertains to\ndetermining the attributes of a set of vertices given those of another subset\nat possibly different time instants. Leveraging spatiotemporal dynamics can\ndrastically reduce the number of observed vertices, and hence the cost of\nsampling. Alleviating the limited flexibility of existing approaches, the\npresent paper broadens the existing kernel-based graph function reconstruction\nframework to accommodate time-evolving functions over possibly time-evolving\ntopologies. This approach inherits the versatility and generality of\nkernel-based methods, for which no knowledge on distributions or second-order\nstatistics is required. Systematic guidelines are provided to construct two\nfamilies of space-time kernels with complementary strengths. The first\nfacilitates judicious control of regularization on a space-time frequency\nplane, whereas the second can afford time-varying topologies. Batch and online\nestimators are also put forth, and a novel kernel Kalman filter is developed to\nobtain these estimates at affordable computational cost. Numerical tests with\nreal data sets corroborate the merits of the proposed methods relative to\ncompeting alternatives.\n","negative":"  We consider the learning of algorithmic tasks by mere observation of\ninput-output pairs. Rather than studying this as a black-box discrete\nregression problem with no assumption whatsoever on the input-output mapping,\nwe concentrate on tasks that are amenable to the principle of divide and\nconquer, and study what are its implications in terms of learning. This\nprinciple creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two\nscale-invariant atomic operations: how to split a given input into smaller\nsets, and how to merge two partially solved tasks into a larger partial\nsolution. Our model can be trained in weakly supervised environments, namely by\njust observing input-output pairs, and in even weaker environments, using a\nnon-differentiable reward signal. Moreover, thanks to the dynamic aspect of our\narchitecture, we can incorporate the computational complexity as a\nregularization term that can be optimized by backpropagation. We demonstrate\nthe flexibility and efficiency of the Divide-and-Conquer Network on several\ncombinatorial and geometric tasks: convex hull, clustering, knapsack and\neuclidean TSP. Thanks to the dynamic programming nature of our model, we show\nsignificant improvements in terms of generalization error and computational\ncomplexity.\n","id":585}
{"Unnamed: 0.1":11586,"Unnamed: 0":11586.0,"anchor":"FastText.zip: Compressing text classification models","positive":"  We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.\n","negative":"  Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.\n","id":586}
{"Unnamed: 0.1":11587,"Unnamed: 0":11587.0,"anchor":"Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification","positive":"  Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.\n","negative":"  Many real-world applications are characterized by a number of conflicting\nperformance measures. As optimizing in a multi-objective setting leads to a set\nof non-dominated solutions, a preference function is required for selecting the\nsolution with the appropriate trade-off between the objectives. The question\nis: how good do estimations of these objectives have to be in order for the\nsolution maximizing the preference function to remain unchanged? In this paper,\nwe introduce the concept of preference radius to characterize the robustness of\nthe preference function and provide guidelines for controlling the quality of\nestimations in the multi-objective setting. More specifically, we provide a\ngeneral formulation of multi-objective optimization under the bandits setting.\nWe show how the preference radius relates to the optimal gap and we use this\nconcept to provide a theoretical analysis of the Thompson sampling algorithm\nfrom multivariate normal priors. We finally present experiments to support the\ntheoretical results and highlight the fact that one cannot simply scalarize\nmulti-objective problems into single-objective problems.\n","id":587}
{"Unnamed: 0.1":11588,"Unnamed: 0":11588.0,"anchor":"Neurogenesis Deep Learning","positive":"  Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.\n","negative":"  This paper introduces a potential learning scheme that can dynamically\npredict the stability of the reconnection of sub-networks to a main grid. As\nthe future electrical power systems tend towards smarter and greener\ntechnology, the deployment of self sufficient networks, or microgrids, becomes\nmore likely. Microgrids may operate on their own or synchronized with the main\ngrid, thus control methods need to take into account islanding and reconnecting\nof said networks. The ability to optimally and safely reconnect a portion of\nthe grid is not well understood and, as of now, limited to raw synchronization\nbetween interconnection points. A support vector machine (SVM) leveraging\nreal-time data from phasor measurement units (PMUs) is proposed to predict in\nreal time whether the reconnection of a sub-network to the main grid would lead\nto stability or instability. A dynamics simulator fed with pre-acquired system\nparameters is used to create training data for the SVM in various operating\nstates. The classifier was tested on a variety of cases and operating points to\nensure diversity. Accuracies of approximately 85% were observed throughout most\nconditions when making dynamic predictions of a given network.\n","id":588}
{"Unnamed: 0.1":11589,"Unnamed: 0":11589.0,"anchor":"Online Reinforcement Learning for Real-Time Exploration in Continuous\n  State and Action Markov Decision Processes","positive":"  This paper presents a new method to learn online policies in continuous\nstate, continuous action, model-free Markov decision processes, with two\nproperties that are crucial for practical applications. First, the policies are\nimplementable with a very low computational cost: once the policy is computed,\nthe action corresponding to a given state is obtained in logarithmic time with\nrespect to the number of samples used. Second, our method is versatile: it does\nnot rely on any a priori knowledge of the structure of optimal policies. We\nbuild upon the Fitted Q-iteration algorithm which represents the $Q$-value as\nthe average of several regression trees. Our algorithm, the Fitted Policy\nForest algorithm (FPF), computes a regression forest representing the Q-value\nand transforms it into a single tree representing the policy, while keeping\ncontrol on the size of the policy using resampling and leaf merging. We\nintroduce an adaptation of Multi-Resolution Exploration (MRE) which is\nparticularly suited to FPF. We assess the performance of FPF on three classical\nbenchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double\nIntegrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other\nalgorithms, although these algorithms rely on the use of particular\nrepresentations of the policies, especially chosen in order to fit each of the\nthree problems. Finally, we exhibit that the combination of FPF and MRE allows\nto find nearly optimal solutions in problems where $\\epsilon$-greedy approaches\nwould fail.\n","negative":"  Mirror neurons have been observed in the primary motor cortex of primate\nspecies, in particular in humans and monkeys. A mirror neuron fires when a\nperson performs a certain action, and also when he observes the same action\nbeing performed by another person. A crucial step towards building fully\nautonomous intelligent systems with human-like learning abilities is the\ncapability in modeling the mirror neuron. On one hand, the abundance of\negocentric cameras in the past few years has offered the opportunity to study a\nlot of vision problems from the first-person perspective. A great deal of\ninteresting research has been done during the past few years, trying to explore\nvarious computer vision tasks from the perspective of the self. On the other\nhand, videos recorded by traditional static cameras, capture humans performing\ndifferent actions from an exocentric third-person perspective. In this work, we\ntake the first step towards relating motion information across these two\nperspectives. We train models that predict motion in an egocentric view, by\nobserving it from an exocentric view, and vice versa. This allows models to\npredict how an egocentric motion would look like from outside. To do so, we\ntrain linear and nonlinear models and evaluate their performance in terms of\nretrieving the egocentric (exocentric) motion features, while having access to\nan exocentric (egocentric) motion feature. Our experimental results demonstrate\nthat motion information can be successfully transferred across the two views.\n","id":589}
{"Unnamed: 0.1":11590,"Unnamed: 0":11590.0,"anchor":"A Unit Selection Methodology for Music Generation Using Deep Neural\n  Networks","positive":"  Several methods exist for a computer to generate music based on data\nincluding Markov chains, recurrent neural networks, recombinancy, and grammars.\nWe explore the use of unit selection and concatenation as a means of generating\nmusic using a procedure based on ranking, where, we consider a unit to be a\nvariable length number of measures of music. We first examine whether a unit\nselection method, that is restricted to a finite size unit library, can be\nsufficient for encompassing a wide spectrum of music. We do this by developing\na deep autoencoder that encodes a musical input and reconstructs the input by\nselecting from the library. We then describe a generative model that combines a\ndeep structured semantic model (DSSM) with an LSTM to predict the next unit,\nwhere units consist of four, two, and one measures of music. We evaluate the\ngenerative model using objective metrics including mean rank and accuracy and\nwith a subjective listening test in which expert musicians are asked to\ncomplete a forced-choiced ranking task. We compare our model to a note-level\ngenerative baseline that consists of a stacked LSTM trained to predict forward\nby one note.\n","negative":"  Many aspects of people's lives are proven to be deeply connected to their\njobs. In this paper, we first investigate the distinct characteristics of major\noccupation categories based on tweets. From multiple social media platforms, we\ngather several types of user information. From users' LinkedIn webpages, we\nlearn their proficiencies. To overcome the ambiguity of self-reported\ninformation, a soft clustering approach is applied to extract occupations from\ncrowd-sourced data. Eight job categories are extracted, including Marketing,\nAdministrator, Start-up, Editor, Software Engineer, Public Relation, Office\nClerk, and Designer. Meanwhile, users' posts on Twitter provide cues for\nunderstanding their linguistic styles, interests, and personalities. Our\nresults suggest that people of different jobs have unique tendencies in certain\nlanguage styles and interests. Our results also clearly reveal distinctive\nlevels in terms of Big Five Traits for different jobs. Finally, a classifier is\nbuilt to predict job types based on the features extracted from tweets. A high\naccuracy indicates a strong discrimination power of language features for job\nprediction task.\n","id":590}
{"Unnamed: 0.1":11591,"Unnamed: 0":11591.0,"anchor":"Generalizable Features From Unsupervised Learning","positive":"  Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution\n","negative":"  We propose a language-agnostic way of automatically generating sets of\nsemantically similar clusters of entities along with sets of \"outlier\"\nelements, which may then be used to perform an intrinsic evaluation of word\nembeddings in the outlier detection task. We used our methodology to create a\ngold-standard dataset, which we call WikiSem500, and evaluated multiple\nstate-of-the-art embeddings. The results show a correlation between performance\non this dataset and performance on sentiment analysis.\n","id":591}
{"Unnamed: 0.1":11592,"Unnamed: 0":11592.0,"anchor":"Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD)","positive":"  Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases.\n","negative":"  The loss surface of deep neural networks has recently attracted interest in\nthe optimization and machine learning communities as a prime example of\nhigh-dimensional non-convex problem. Some insights were recently gained using\nspin glass models and mean-field approximations, but at the expense of strongly\nsimplifying the nonlinear nature of the model.\n  In this work, we do not make any such assumption and study conditions on the\ndata distribution and model architecture that prevent the existence of bad\nlocal minima. Our theoretical work quantifies and formalizes two important\n\\emph{folklore} facts: (i) the landscape of deep linear networks has a\nradically different topology from that of deep half-rectified ones, and (ii)\nthat the energy landscape in the non-linear case is fundamentally controlled by\nthe interplay between the smoothness of the data distribution and model\nover-parametrization. Our main theoretical contribution is to prove that\nhalf-rectified single layer networks are asymptotically connected, and we\nprovide explicit bounds that reveal the aforementioned interplay.\n  The conditioning of gradient descent is the next challenge we address. We\nstudy this question through the geometry of the level sets, and we introduce an\nalgorithm to efficiently estimate the regularity of such sets on large-scale\nnetworks. Our empirical results show that these level sets remain connected\nthroughout all the learning phase, suggesting a near convex behavior, but they\nbecome exponentially more curvy as the energy level decays, in accordance to\nwhat is observed in practice with very low curvature attractors.\n","id":592}
{"Unnamed: 0.1":11593,"Unnamed: 0":11593.0,"anchor":"Knowledge Completion for Generics using Guided Tensor Factorization","positive":"  Given a knowledge base or KB containing (noisy) facts about common nouns or\ngenerics, such as \"all trees produce oxygen\" or \"some animals live in forests\",\nwe consider the problem of inferring additional such facts at a precision\nsimilar to that of the starting KB. Such KBs capture general knowledge about\nthe world, and are crucial for various applications such as question answering.\nDifferent from commonly studied named entity KBs such as Freebase, generics KBs\ninvolve quantification, have more complex underlying regularities, tend to be\nmore incomplete, and violate the commonly used locally closed world assumption\n(LCWA). We show that existing KB completion methods struggle with this new\ntask, and present the first approach that is successful. Our results\ndemonstrate that external information, such as relation schemas and entity\ntaxonomies, if used appropriately, can be a surprisingly powerful tool in this\nsetting. First, our simple yet effective knowledge guided tensor factorization\napproach achieves state-of-the-art results on two generics KBs (80% precise)\nfor science, doubling their size at 74%-86% precision. Second, our novel\ntaxonomy guided, submodular, active learning method for collecting annotations\nabout rare entities (e.g., oriole, a bird) is 6x more effective at inferring\nfurther new facts about them than multiple active learning baselines.\n","negative":"  Behavioral annotation using signal processing and machine learning is highly\ndependent on training data and manual annotations of behavioral labels.\nPrevious studies have shown that speech information encodes significant\nbehavioral information and be used in a variety of automated behavior\nrecognition tasks. However, extracting behavior information from speech is\nstill a difficult task due to the sparseness of training data coupled with the\ncomplex, high-dimensionality of speech, and the complex and multiple\ninformation streams it encodes. In this work we exploit the slow varying\nproperties of human behavior. We hypothesize that nearby segments of speech\nshare the same behavioral context and hence share a similar underlying\nrepresentation in a latent space. Specifically, we propose a Deep Neural\nNetwork (DNN) model to connect behavioral context and derive the behavioral\nmanifold in an unsupervised manner. We evaluate the proposed manifold in the\ncouples therapy domain and also provide examples from publicly available data\n(e.g. stand-up comedy). We further investigate training within the couples'\ntherapy domain and from movie data. The results are extremely encouraging and\npromise improved behavioral quantification in an unsupervised manner and\nwarrants further investigation in a range of applications.\n","id":593}
{"Unnamed: 0.1":11594,"Unnamed: 0":11594.0,"anchor":"Inverse Compositional Spatial Transformer Networks","positive":"  In this paper, we establish a theoretical connection between the classical\nLucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer\nNetworks (STNs). STNs are of interest to the vision and learning communities\ndue to their natural ability to combine alignment and classification within the\nsame theoretical framework. Inspired by the Inverse Compositional (IC) variant\nof the LK algorithm, we present Inverse Compositional Spatial Transformer\nNetworks (IC-STNs). We demonstrate that IC-STNs can achieve better performance\nthan conventional STNs with less model capacity; in particular, we show\nsuperior performance in pure image alignment tasks as well as joint\nalignment\/classification problems on real-world problems.\n","negative":"  This paper describes a method for using Generative Adversarial Networks to\nlearn distributed representations of natural language documents. We propose a\nmodel that is based on the recently proposed Energy-Based GAN, but instead uses\na Denoising Autoencoder as the discriminator network. Document representations\nare extracted from the hidden layer of the discriminator and evaluated both\nquantitatively and qualitatively.\n","id":594}
{"Unnamed: 0.1":11595,"Unnamed: 0":11595.0,"anchor":"Design of Data-Driven Mathematical Laws for Optimal Statistical\n  Classification Systems","positive":"  This article will devise data-driven, mathematical laws that generate\noptimal, statistical classification systems which achieve minimum error rates\nfor data distributions with unchanging statistics. Thereby, I will design\nlearning machines that minimize the expected risk or probability of\nmisclassification. I will devise a system of fundamental equations of binary\nclassification for a classification system in statistical equilibrium. I will\nuse this system of equations to formulate the problem of learning unknown,\nlinear and quadratic discriminant functions from data as a locus problem,\nthereby formulating geometric locus methods within a statistical framework.\nSolving locus problems involves finding equations of curves or surfaces defined\nby given properties and finding graphs or loci of given equations. I will\ndevise three systems of data-driven, locus equations that generate optimal,\nstatistical classification systems. Each class of learning machines satisfies\nfundamental statistical laws for a classification system in statistical\nequilibrium. Thereby, I will formulate three classes of learning machines that\nare scalable modules for optimal, statistical pattern recognition systems, all\nof which are capable of performing a wide variety of statistical pattern\nrecognition tasks, where any given M-class statistical pattern recognition\nsystem exhibits optimal generalization performance for an M-class feature\nspace.\n","negative":"  We examine the effect of the Group Lasso (gLasso) regularizer in selecting\nthe salient nodes of Deep Neural Network (DNN) hidden layers by applying a\nDNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of\ngLasso regularization, one for outgoing weight vectors and another for incoming\nweight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096\nnodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment\nresults demonstrate that our DNN training, in which the gLasso regularizer was\nembedded, successfully selected the hidden layer nodes that are necessary and\nsufficient for achieving high classification power.\n","id":595}
{"Unnamed: 0.1":11596,"Unnamed: 0":11596.0,"anchor":"Identification of release sources in advection-diffusion system by\n  machine learning combined with Green function inverse method","positive":"  The identification of sources of advection-diffusion transport is based\nusually on solving complex ill-posed inverse models against the available\nstate- variable data records. However, if there are several sources with\ndifferent locations and strengths, the data records represent mixtures rather\nthan the separate influences of the original sources. Importantly, the number\nof these original release sources is typically unknown, which hinders\nreliability of the classical inverse-model analyses. To address this challenge,\nwe present here a novel hybrid method for identification of the unknown number\nof release sources. Our hybrid method, called HNMF, couples unsupervised\nlearning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis\nGreen functions method. HNMF synergistically performs decomposition of the\nrecorded mixtures, finds the number of the unknown sources and uses the Green\nfunction of advection-diffusion equation to identify their characteristics. In\nthe paper, we introduce the method and demonstrate that it is capable of\nidentifying the advection velocity and dispersivity of the medium as well as\nthe unknown number, locations, and properties of various sets of synthetic\nrelease sources with different space and time dependencies, based only on the\nrecorded data. HNMF can be applied directly to any problem controlled by a\npartial-differential parabolic equation where mixtures of an unknown number of\nsources are measured at multiple locations.\n","negative":"  A novel data representation method of convolutional neural net- work (CNN) is\nproposed in this paper to represent data of different modalities. We learn a\nCNN model for the data of each modality to map the data of differ- ent\nmodalities to a common space, and regularize the new representations in the\ncommon space by a cross-model relevance matrix. We further impose that the\nclass label of data points can also be predicted from the CNN representa- tions\nin the common space. The learning problem is modeled as a minimiza- tion\nproblem, which is solved by an augmented Lagrange method (ALM) with updating\nrules of Alternating direction method of multipliers (ADMM). The experiments\nover benchmark of sequence data of multiple modalities show its advantage.\n","id":596}
{"Unnamed: 0.1":11597,"Unnamed: 0":11597.0,"anchor":"Nonnegative Matrix Factorization for identification of unknown number of\n  sources emitting delayed signals","positive":"  Factor analysis is broadly used as a powerful unsupervised machine learning\ntool for reconstruction of hidden features in recorded mixtures of signals. In\nthe case of a linear approximation, the mixtures can be decomposed by a variety\nof model-free Blind Source Separation (BSS) algorithms. Most of the available\nBSS algorithms consider an instantaneous mixing of signals, while the case when\nthe mixtures are linear combinations of signals with delays is less explored.\nEspecially difficult is the case when the number of sources of the signals with\ndelays is unknown and has to be determined from the data as well. To address\nthis problem, in this paper, we present a new method based on Nonnegative\nMatrix Factorization (NMF) that is capable of identifying: (a) the unknown\nnumber of the sources, (b) the delays and speed of propagation of the signals,\nand (c) the locations of the sources. Our method can be used to decompose\nrecords of mixtures of signals with delays emitted by an unknown number of\nsources in a nondispersive medium, based only on recorded data. This is the\ncase, for example, when electromagnetic signals from multiple antennas are\nreceived asynchronously; or mixtures of acoustic or seismic signals recorded by\nsensors located at different positions; or when a shift in frequency is induced\nby the Doppler effect. By applying our method to synthetic datasets, we\ndemonstrate its ability to identify the unknown number of sources as well as\nthe waveforms, the delays, and the strengths of the signals. Using Bayesian\nanalysis, we also evaluate estimation uncertainties and identify the region of\nlikelihood where the positions of the sources can be found.\n","negative":"  User acceptance of artificial intelligence agents might depend on their\nability to explain their reasoning, which requires adding an interpretability\nlayer that fa- cilitates users to understand their behavior. This paper focuses\non adding an in- terpretable layer on top of Semantic Textual Similarity (STS),\nwhich measures the degree of semantic equivalence between two sentences. The\ninterpretability layer is formalized as the alignment between pairs of segments\nacross the two sentences, where the relation between the segments is labeled\nwith a relation type and a similarity score. We present a publicly available\ndataset of sentence pairs annotated following the formalization. We then\ndevelop a system trained on this dataset which, given a sentence pair, explains\nwhat is similar and different, in the form of graded and typed segment\nalignments. When evaluated on the dataset, the system performs better than an\ninformed baseline, showing that the dataset and task are well-defined and\nfeasible. Most importantly, two user studies show how the system output can be\nused to automatically produce explanations in natural language. Users performed\nbetter when having access to the explanations, pro- viding preliminary evidence\nthat our dataset and method to automatically produce explanations is useful in\nreal applications.\n","id":597}
{"Unnamed: 0.1":11598,"Unnamed: 0":11598.0,"anchor":"Hybrid Repeat\/Multi-point Sampling for Highly Volatile Objective\n  Functions","positive":"  A key drawback of the current generation of artificial decision-makers is\nthat they do not adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, will optimize behavior with respect to an\nobjective function that must be evaluated and learned through simulations. Once\nthis objective function has been modeled, the agent can then choose its desired\nbehavior in different situations. Bayesian optimization with a Gaussian Process\nsurrogate is used as the method for investigating the objective function. One\nkey benefit is that during optimization the Gaussian Process learns a global\nestimate of the true objective function, with predicted outcomes and a\nstatistical measure of confidence in areas that haven't been investigated yet.\nHowever, standard Bayesian optimization does not perform consistently or\nprovide an accurate Gaussian Process surrogate function for highly volatile\nobjective functions. We treat these problems by introducing a novel sampling\ntechnique called Hybrid Repeat\/Multi-point Sampling. This technique gives the\nAI ability to learn optimum behaviors in a highly uncertain environment. More\nimportantly, it not only improves the reliability of the optimization, but also\ncreates a better model of the entire objective surface. With this improved\nmodel the agent is equipped to better adapt behaviors.\n","negative":"  Pixel wise image labeling is an interesting and challenging problem with\ngreat significance in the computer vision community. In order for a dense\nlabeling algorithm to be able to achieve accurate and precise results, it has\nto consider the dependencies that exist in the joint space of both the input\nand the output variables. An implicit approach for modeling those dependencies\nis by training a deep neural network that, given as input an initial estimate\nof the output labels and the input image, it will be able to predict a new\nrefined estimate for the labels. In this context, our work is concerned with\nwhat is the optimal architecture for performing the label improvement task. We\nargue that the prior approaches of either directly predicting new label\nestimates or predicting residual corrections w.r.t. the initial labels with\nfeed-forward deep network architectures are sub-optimal. Instead, we propose a\ngeneric architecture that decomposes the label improvement task to three steps:\n1) detecting the initial label estimates that are incorrect, 2) replacing the\nincorrect labels with new ones, and finally 3) refining the renewed labels by\npredicting residual corrections w.r.t. them. Furthermore, we explore and\ncompare various other alternative architectures that consist of the\naforementioned Detection, Replace, and Refine components. We extensively\nevaluate the examined architectures in the challenging task of dense disparity\nestimation (stereo matching) and we report both quantitative and qualitative\nresults on three different datasets. Finally, our dense disparity estimation\nnetwork that implements the proposed generic architecture, achieves\nstate-of-the-art results in the KITTI 2015 test surpassing prior approaches by\na significant margin.\n","id":598}
{"Unnamed: 0.1":11599,"Unnamed: 0":11599.0,"anchor":"An empirical analysis of the optimization of deep network loss surfaces","positive":"  The success of deep neural networks hinges on our ability to accurately and\nefficiently optimize high-dimensional, non-convex functions. In this paper, we\nempirically investigate the loss functions of state-of-the-art networks, and\nhow commonly-used stochastic gradient descent variants optimize these loss\nfunctions. To do this, we visualize the loss function by projecting them down\nto low-dimensional spaces chosen based on the convergence points of different\noptimization algorithms. Our observations suggest that optimization algorithms\nencounter and choose different descent directions at many saddle points to find\ndifferent final weights. Based on consistency we observe across re-runs of the\nsame stochastic optimization algorithm, we hypothesize that each optimization\nalgorithm makes characteristic choices at these saddle points.\n","negative":"  We are now witnessing the increasing availability of event stream data, i.e.,\na sequence of events with each event typically being denoted by the time it\noccurs and its mark information (e.g., event type). A fundamental problem is to\nmodel and predict such kind of marked temporal dynamics, i.e., when the next\nevent will take place and what its mark will be. Existing methods either\npredict only the mark or the time of the next event, or predict both of them,\nyet separately. Indeed, in marked temporal dynamics, the time and the mark of\nthe next event are highly dependent on each other, requiring a method that\ncould simultaneously predict both of them. To tackle this problem, in this\npaper, we propose to model marked temporal dynamics by using a mark-specific\nintensity function to explicitly capture the dependency between the mark and\nthe time of the next event. Extensive experiments on two datasets demonstrate\nthat the proposed method outperforms state-of-the-art methods at predicting\nmarked temporal dynamics.\n","id":599}
{"Unnamed: 0.1":11600,"Unnamed: 0":11600.0,"anchor":"Generative Adversarial Parallelization","positive":"  Generative Adversarial Networks have become one of the most studied\nframeworks for unsupervised learning due to their intuitive formulation. They\nhave also been shown to be capable of generating convincing examples in limited\ndomains, such as low-resolution images. However, they still prove difficult to\ntrain in practice and tend to ignore modes of the data generating distribution.\nQuantitatively capturing effects such as mode coverage and more generally the\nquality of the generative model still remain elusive. We propose Generative\nAdversarial Parallelization, a framework in which many GANs or their variants\nare trained simultaneously, exchanging their discriminators. This eliminates\nthe tight coupling between a generator and discriminator, leading to improved\nconvergence and improved coverage of modes. We also propose an improved variant\nof the recently proposed Generative Adversarial Metric and show how it can\nscore individual GANs or their collections under the GAP model.\n","negative":"  Adaptive gradient methods for stochastic optimization adjust the learning\nrate for each parameter locally. However, there is also a global learning rate\nwhich must be tuned in order to get the best performance. In this paper, we\npresent a new algorithm that adapts the learning rate locally for each\nparameter separately, and also globally for all parameters together.\nSpecifically, we modify Adam, a popular method for training deep learning\nmodels, with a coefficient that captures properties of the objective function.\nEmpirically, we show that our method, which we call Eve, outperforms Adam and\nother popular methods in training deep neural networks, like convolutional\nneural networks for image classification, and recurrent neural networks for\nlanguage tasks.\n","id":600}
{"Unnamed: 0.1":11601,"Unnamed: 0":11601.0,"anchor":"Distributed Multi-Task Relationship Learning","positive":"  Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence.\n","negative":"  Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved.\n","id":601}
{"Unnamed: 0.1":11602,"Unnamed: 0":11602.0,"anchor":"DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving\n  Backpropagation","positive":"  The vanishing and exploding gradient problems are well-studied obstacles that\nmake it difficult for recurrent neural networks to learn long-term time\ndependencies. We propose a reparameterization of standard recurrent neural\nnetworks to update linear transformations in a provably norm-preserving way\nthrough Givens rotations. Additionally, we use the absolute value function as\nan element-wise non-linearity to preserve the norm of backpropagated signals\nover the entire network. We show that this reparameterization reduces the\nnumber of parameters and maintains the same algorithmic complexity as a\nstandard recurrent neural network, while outperforming standard recurrent\nneural networks with orthogonal initializations and Long Short-Term Memory\nnetworks on the copy problem.\n","negative":"  We initiate the study of fairness in reinforcement learning, where the\nactions of a learning algorithm may affect its environment and future rewards.\nOur fairness constraint requires that an algorithm never prefers one action\nover another if the long-term (discounted) reward of choosing the latter action\nis higher. Our first result is negative: despite the fact that fairness is\nconsistent with the optimal policy, any learning algorithm satisfying fairness\nmust take time exponential in the number of states to achieve non-trivial\napproximation to the optimal policy. We then provide a provably fair polynomial\ntime algorithm under an approximate notion of fairness, thus establishing an\nexponential gap between exact and approximate fairness\n","id":602}
{"Unnamed: 0.1":11603,"Unnamed: 0":11603.0,"anchor":"Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks","positive":"  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n","negative":"  The recently proposed \"generalized min-max\" (GMM) kernel can be efficiently\nlinearized, with direct applications in large-scale statistical learning and\nfast near neighbor search. The linearized GMM kernel was extensively compared\nin with linearized radial basis function (RBF) kernel. On a large number of\nclassification tasks, the tuning-free GMM kernel performs (surprisingly) well\ncompared to the best-tuned RBF kernel. Nevertheless, one would naturally expect\nthat the GMM kernel ought to be further improved if we introduce tuning\nparameters.\n  In this paper, we study three simple constructions of tunable GMM kernels:\n(i) the exponentiated-GMM (or eGMM) kernel, (ii) the powered-GMM (or pGMM)\nkernel, and (iii) the exponentiated-powered-GMM (epGMM) kernel. The pGMM kernel\ncan still be efficiently linearized by modifying the original hashing procedure\nfor the GMM kernel. On about 60 publicly available classification datasets, we\nverify that the proposed tunable GMM kernels typically improve over the\noriginal GMM kernel. On some datasets, the improvements can be astonishingly\nsignificant.\n  For example, on 11 popular datasets which were used for testing deep learning\nalgorithms and tree methods, our experiments show that the proposed tunable GMM\nkernels are strong competitors to trees and deep nets. The previous studies\ndeveloped tree methods including \"abc-robust-logitboost\" and demonstrated the\nexcellent performance on those 11 datasets (and other datasets), by\nestablishing the second-order tree-split formula and new derivatives for\nmulti-class logistic loss. Compared to tree methods like\n\"abc-robust-logitboost\" (which are slow and need substantial model sizes), the\ntunable GMM kernels produce largely comparable results.\n","id":603}
{"Unnamed: 0.1":11604,"Unnamed: 0":11604.0,"anchor":"Joint Bayesian Gaussian discriminant analysis for speaker verification","positive":"  State-of-the-art i-vector based speaker verification relies on variants of\nProbabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We\nare mainly motivated by the recent work of the joint Bayesian (JB) method,\nwhich is originally proposed for discriminant analysis in face verification. We\napply JB to speaker verification and make three contributions beyond the\noriginal JB. 1) In contrast to the EM iterations with approximated statistics\nin the original JB, the EM iterations with exact statistics are employed and\ngive better performance. 2) We propose to do simultaneous diagonalization (SD)\nof the within-class and between-class covariance matrices to achieve efficient\ntesting, which has broader application scope than the SVD-based efficient\ntesting method in the original JB. 3) We scrutinize similarities and\ndifferences between various Gaussian PLDAs and JB, complementing the previous\nanalysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are\nconducted on NIST SRE10 core condition 5, empirically validating the\nsuperiority of JB with faster convergence rate and 9-13% EER reduction compared\nwith state-of-the-art PLDA.\n","negative":"  Automatically discovering image categories in unlabeled natural images is one\nof the important goals of unsupervised learning. However, the task is\nchallenging and even human beings define visual categories based on a large\namount of prior knowledge. In this paper, we similarly utilize prior knowledge\nto facilitate the discovery of image categories. We present a novel end-to-end\nnetwork to map unlabeled images to categories as a clustering network. We\npropose that this network can be learned with contrastive loss which is only\nbased on weak binary pair-wise constraints. Such binary constraints can be\nlearned from datasets in other domains as transferred similarity functions,\nwhich mimic a simple knowledge transfer. We first evaluate our experiments on\nthe MNIST dataset as a proof of concept, based on predicted similarities\ntrained on Omniglot, showing a 99\\% accuracy which significantly outperforms\nclustering based approaches. Then we evaluate the discovery performance on\nCifar-10, STL-10, and ImageNet, which achieves both state-of-the-art accuracy\nand shows it can be scalable to various large natural images.\n","id":604}
{"Unnamed: 0.1":11605,"Unnamed: 0":11605.0,"anchor":"Corporate Disruption in the Science of Machine Learning","positive":"  This MSc dissertation considers the effects of the current corporate interest\non researchers in the field of machine learning. Situated within the field's\ncyclical history of academic, public and corporate interest, this dissertation\ninvestigates how current researchers view recent developments and negotiate\ntheir own research practices within an environment of increased commercial\ninterest and funding. The original research consists of in-depth interviews\nwith 12 machine learning researchers working in both academia and industry.\nBuilding on theory from science, technology and society studies, this\ndissertation problematizes the traditional narratives of the neoliberalization\nof academic research by allowing the researchers themselves to discuss how\ntheir career choices, working environments and interactions with others in the\nfield have been affected by the reinvigorated corporate interest of recent\nyears.\n","negative":"  Inverse classification, the process of making meaningful perturbations to a\ntest point such that it is more likely to have a desired classification, has\npreviously been addressed using data from a single static point in time. Such\nan approach yields inflated probability estimates, stemming from an implicitly\nmade assumption that recommendations are implemented instantaneously. We\npropose using longitudinal data to alleviate such issues in two ways. First, we\nuse past outcome probabilities as features in the present. Use of such past\nprobabilities ties historical behavior to the present, allowing for more\ninformation to be taken into account when making initial probability estimates\nand subsequently performing inverse classification. Secondly, following inverse\nclassification application, optimized instances' unchangeable features\n(e.g.,~age) are updated using values from the next longitudinal time period.\nOptimized test instance probabilities are then reassessed. Updating the\nunchangeable features in this manner reflects the notion that improvements in\noutcome likelihood, which result from following the inverse classification\nrecommendations, do not materialize instantaneously. As our experiments\ndemonstrate, more realistic estimates of probability can be obtained by\nfactoring in such considerations.\n","id":605}
{"Unnamed: 0.1":11606,"Unnamed: 0":11606.0,"anchor":"Parsimonious Online Learning with Kernels via Sparse Projections in\n  Function Space","positive":"  Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method.\n","negative":"  Signed networks allow to model positive and negative relationships. We\nanalyze existing extensions of spectral clustering to signed networks. It turns\nout that existing approaches do not recover the ground truth clustering in\nseveral situations where either the positive or the negative network structures\ncontain no noise. Our analysis shows that these problems arise as existing\napproaches take some form of arithmetic mean of the Laplacians of the positive\nand negative part. As a solution we propose to use the geometric mean of the\nLaplacians of positive and negative part and show that it outperforms the\nexisting approaches. While the geometric mean of matrices is computationally\nexpensive, we show that eigenvectors of the geometric mean can be computed\nefficiently, leading to a numerical scheme for sparse matrices which is of\nindependent interest.\n","id":606}
{"Unnamed: 0.1":11607,"Unnamed: 0":11607.0,"anchor":"Upper Bound of Bayesian Generalization Error in Non-negative Matrix\n  Factorization","positive":"  Non-negative matrix factorization (NMF) is a new knowledge discovery method\nthat is used for text mining, signal processing, bioinformatics, and consumer\nanalysis. However, its basic property as a learning machine is not yet\nclarified, as it is not a regular statistical model, resulting that theoretical\noptimization method of NMF has not yet established. In this paper, we study the\nreal log canonical threshold of NMF and give an upper bound of the\ngeneralization error in Bayesian learning. The results show that the\ngeneralization error of the matrix factorization can be made smaller than\nregular statistical models if Bayesian learning is applied.\n","negative":"  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an\neffective method for training generative models of complex data such as natural\nimages. However, they are notoriously hard to train and can suffer from the\nproblem of missing modes where the model is not able to produce examples in\ncertain regions of the space. We propose an iterative procedure, called AdaGAN,\nwhere at every step we add a new component into a mixture model by running a\nGAN algorithm on a reweighted sample. This is inspired by boosting algorithms,\nwhere many potentially weak individual predictors are greedily aggregated to\nform a strong composite predictor. We prove that such an incremental procedure\nleads to convergence to the true distribution in a finite number of steps if\neach step is optimal, and convergence at an exponential rate otherwise. We also\nillustrate experimentally that this procedure addresses the problem of missing\nmodes.\n","id":607}
{"Unnamed: 0.1":11608,"Unnamed: 0":11608.0,"anchor":"Information Extraction with Character-level Neural Networks and Free\n  Noisy Supervision","positive":"  We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text.\n","negative":"  Complex activity recognition is challenging due to the inherent uncertainty\nand diversity of performing a complex activity. Normally, each instance of a\ncomplex activity has its own configuration of atomic actions and their temporal\ndependencies. We propose in this paper an atomic action-based Bayesian model\nthat constructs Allen's interval relation networks to characterize complex\nactivities with structural varieties in a probabilistic generative way: By\nintroducing latent variables from the Chinese restaurant process, our approach\nis able to capture all possible styles of a particular complex activity as a\nunique set of distributions over atomic actions and relations. We also show\nthat local temporal dependencies can be retained and are globally consistent in\nthe resulting interval network. Moreover, network structure can be learned from\nempirical data. A new dataset of complex hand activities has been constructed\nand made publicly available, which is much larger in size than any existing\ndatasets. Empirical evaluations on benchmark datasets as well as our in-house\ndataset demonstrate the competitiveness of our approach.\n","id":608}
{"Unnamed: 0.1":11609,"Unnamed: 0":11609.0,"anchor":"TF.Learn: TensorFlow's High-level Module for Distributed Machine\n  Learning","positive":"  TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.\n","negative":"  Latent representation learned from multi-layered neural networks via\nhierarchical feature abstraction enables recent success of deep learning. Under\nthe deep learning framework, generalization performance highly depends on the\nlearned latent representation which is obtained from an appropriate training\nscenario with a task-specific objective on a designed network model. In this\nwork, we propose a novel latent space modeling method to learn better latent\nrepresentation. We designed a neural network model based on the assumption that\ngood base representation can be attained by maximizing the total correlation\nbetween the input, latent, and output variables. From the base model, we\nintroduce a semantic noise modeling method which enables class-conditional\nperturbation on latent space to enhance the representational power of learned\nlatent feature. During training, latent vector representation can be\nstochastically perturbed by a modeled class-conditional additive noise while\nmaintaining its original semantic feature. It implicitly brings the effect of\nsemantic augmentation on the latent space. The proposed model can be easily\nlearned by back-propagation with common gradient-based optimization algorithms.\nExperimental results show that the proposed method helps to achieve performance\nbenefits against various previous approaches. We also provide the empirical\nanalyses for the proposed class-conditional perturbation process including\nt-SNE visualization.\n","id":609}
{"Unnamed: 0.1":11610,"Unnamed: 0":11610.0,"anchor":"Neuro-symbolic representation learning on biological knowledge graphs","positive":"  Motivation: Biological data and knowledge bases increasingly rely on Semantic\nWeb technologies and the use of knowledge graphs for data integration,\nretrieval and federated queries. In the past years, feature learning methods\nthat are applicable to graph-structured data are becoming available, but have\nnot yet widely been applied and evaluated on structured biological knowledge.\nResults: We develop a novel method for feature learning on biological knowledge\ngraphs. Our method combines symbolic methods, in particular knowledge\nrepresentation using symbolic logic and automated reasoning, with neural\nnetworks to generate embeddings of nodes that encode for related information\nwithin knowledge graphs. Through the use of symbolic logic, these embeddings\ncontain both explicit and implicit information. We apply these embeddings to\nthe prediction of edges in the knowledge graph representing problems of\nfunction prediction, finding candidate genes of diseases, protein-protein\ninteractions, or drug target relations, and demonstrate performance that\nmatches and sometimes outperforms traditional approaches based on manually\ncrafted features. Our method can be applied to any biological knowledge graph,\nand will thereby open up the increasing amount of Semantic Web based knowledge\nbases in biology to use in machine learning and data analytics. Availability\nand Implementation:\nhttps:\/\/github.com\/bio-ontology-research-group\/walking-rdf-and-owl Contact:\nrobert.hoehndorf@kaust.edu.sa\n","negative":"  We introduce a hybrid CPU\/GPU version of the Asynchronous Advantage\nActor-Critic (A3C) algorithm, currently the state-of-the-art method in\nreinforcement learning for various gaming tasks. We analyze its computational\ntraits and concentrate on aspects critical to leveraging the GPU's\ncomputational power. We introduce a system of queues and a dynamic scheduling\nstrategy, potentially helpful for other asynchronous algorithms as well. Our\nhybrid CPU\/GPU version of A3C, based on TensorFlow, achieves a significant\nspeed up compared to a CPU implementation; we make it publicly available to\nother researchers at https:\/\/github.com\/NVlabs\/GA3C .\n","id":610}
{"Unnamed: 0.1":11611,"Unnamed: 0":11611.0,"anchor":"An equation-of-state-meter of QCD transition from deep learning","positive":"  Supervised learning with a deep convolutional neural network is used to\nidentify the QCD equation of state (EoS) employed in relativistic hydrodynamic\nsimulations of heavy-ion collisions from the simulated final-state particle\nspectra $\\rho(p_T,\\Phi)$. High-level correlations of $\\rho(p_T,\\Phi)$ learned\nby the neural network act as an effective \"EoS-meter\" in detecting the nature\nof the QCD transition. The EoS-meter is model independent and insensitive to\nother simulation inputs, especially the initial conditions. Thus it provides a\npowerful direct-connection of heavy-ion collision observables with the bulk\nproperties of QCD.\n","negative":"  We consider the multi-view data completion problem, i.e., to complete a\nmatrix $\\mathbf{U}=[\\mathbf{U}_1|\\mathbf{U}_2]$ where the ranks of\n$\\mathbf{U},\\mathbf{U}_1$, and $\\mathbf{U}_2$ are given. In particular, we\ninvestigate the fundamental conditions on the sampling pattern, i.e., locations\nof the sampled entries for finite completability of such a multi-view data\ngiven the corresponding rank constraints. In contrast with the existing\nanalysis on Grassmannian manifold for a single-view matrix, i.e., conventional\nmatrix completion, we propose a geometric analysis on the manifold structure\nfor multi-view data to incorporate more than one rank constraint. We provide a\ndeterministic necessary and sufficient condition on the sampling pattern for\nfinite completability. We also give a probabilistic condition in terms of the\nnumber of samples per column that guarantees finite completability with high\nprobability. Finally, using the developed tools, we derive the deterministic\nand probabilistic guarantees for unique completability.\n","id":611}
{"Unnamed: 0.1":11612,"Unnamed: 0":11612.0,"anchor":"Towards Adaptive Training of Agent-based Sparring Partners for Fighter\n  Pilots","positive":"  A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat\/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately\/efficiently\npredict performance in unexplored scenarios.\n","negative":"  This paper proposed a class of novel Deep Recurrent Neural Networks which can\nincorporate language-level information into acoustic models. For simplicity, we\nnamed these networks Recurrent Deep Language Networks (RDLNs). Multiple\nvariants of RDLNs were considered, including two kinds of context information,\ntwo methods to process the context, and two methods to incorporate the\nlanguage-level information. RDLNs provided possible methods to fine-tune the\nwhole Automatic Speech Recognition (ASR) system in the acoustic modeling\nprocess.\n","id":612}
{"Unnamed: 0.1":11613,"Unnamed: 0":11613.0,"anchor":"Incorporating Human Domain Knowledge into Large Scale Cost Function\n  Learning","positive":"  Recent advances have shown the capability of Fully Convolutional Neural\nNetworks (FCN) to model cost functions for motion planning in the context of\nlearning driving preferences purely based on demonstration data from human\ndrivers. While pure learning from demonstrations in the framework of Inverse\nReinforcement Learning (IRL) is a promising approach, we can benefit from well\ninformed human priors and incorporate them into the learning process. Our work\nachieves this by pretraining a model to regress to a manual cost function and\nrefining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When\ninjecting prior knowledge as pretraining for the network, we achieve higher\nrobustness, more visually distinct obstacle boundaries, and the ability to\ncapture instances of obstacles that elude models that purely learn from\ndemonstration data. Furthermore, by exploiting these human priors, the\nresulting model can more accurately handle corner cases that are scarcely seen\nin the demonstration data, such as stairs, slopes, and underpasses.\n","negative":"  Importance sampling is often used in machine learning when training and\ntesting data come from different distributions. In this paper we propose a new\nvariant of importance sampling that can reduce the variance of importance\nsampling-based estimates by orders of magnitude when the supports of the\ntraining and testing distributions differ. After motivating and presenting our\nnew importance sampling estimator, we provide a detailed theoretical analysis\nthat characterizes both its bias and variance relative to the ordinary\nimportance sampling estimator (in various settings, which include cases where\nordinary importance sampling is biased, while our new estimator is not, and\nvice versa). We conclude with an example of how our new importance sampling\nestimator can be used to improve estimates of how well a new treatment policy\nfor diabetes will work for an individual, using only data from when the\nindividual used a previous treatment policy.\n","id":613}
{"Unnamed: 0.1":11614,"Unnamed: 0":11614.0,"anchor":"Fast Patch-based Style Transfer of Arbitrary Style","positive":"  Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.\n","negative":"  We present AdversariaLib, an open-source python library for the security\nevaluation of machine learning (ML) against carefully-targeted attacks. It\nsupports the implementation of several attacks proposed thus far in the\nliterature of adversarial learning, allows for the evaluation of a wide range\nof ML algorithms, runs on multiple platforms, and has multi-processing enabled.\nThe library has a modular architecture that makes it easy to use and to extend\nby implementing novel attacks and countermeasures. It relies on other\nwidely-used open-source ML libraries, including scikit-learn and FANN.\nClassification algorithms are implemented and optimized in C\/C++, allowing for\na fast evaluation of the simulated attacks. The package is distributed under\nthe GNU General Public License v3, and it is available for download at\nhttp:\/\/sourceforge.net\/projects\/adversarialib.\n","id":614}
{"Unnamed: 0.1":11615,"Unnamed: 0":11615.0,"anchor":"End-to-End Deep Reinforcement Learning for Lane Keeping Assist","positive":"  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes, but it has not yet been successfully used for automotive\napplications. There has recently been a revival of interest in the topic,\nhowever, driven by the ability of deep learning algorithms to learn good\nrepresentations of the environment. Motivated by Google DeepMind's successful\ndemonstrations of learning for games from Breakout to Go, we will propose\ndifferent methods for autonomous driving using deep reinforcement learning.\nThis is of particular interest as it is difficult to pose autonomous driving as\na supervised learning problem as it has a strong interaction with the\nenvironment including other vehicles, pedestrians and roadworks. As this is a\nrelatively new area of research for autonomous driving, we will formulate two\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\nactions category. For the discrete actions category, we will deal with Deep\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\nthat, We will also discover the performance of these two categories on an open\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\ncar Simulator. Our simulation results demonstrate learning of autonomous\nmaneuvering in a scenario of complex road curvatures and simple interaction\nwith other vehicles. Finally, we explain the effect of some restricted\nconditions, put on the car during the learning phase, on the convergence time\nfor finishing its learning phase.\n","negative":"  In this paper, we try to predict the winning team of a match in the\nmultiplayer eSports game Dota 2. To address the weaknesses of previous work, we\nconsider more aspects of prior (pre-match) features from individual players'\nmatch history, as well as real-time (during-match) features at each minute as\nthe match progresses. We use logistic regression, the proposed Attribute\nSequence Model, and their combinations as the prediction models. In a dataset\nof 78362 matches where 20631 matches contain replay data, our experiments show\nthat adding more aspects of prior features improves accuracy from 58.69% to\n71.49%, and introducing real-time features achieves up to 93.73% accuracy when\npredicting at the 40th minute.\n","id":615}
{"Unnamed: 0.1":11616,"Unnamed: 0":11616.0,"anchor":"Stacked Generative Adversarial Networks","positive":"  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n","negative":"  We adopted an approach based on an LSTM neural network to monitor and detect\nfaults in industrial multivariate time series data. To validate the approach we\ncreated a Modelica model of part of a real gasoil plant. By introducing hacks\ninto the logic of the Modelica model, we were able to generate both the roots\nand causes of fault behavior in the plant. Having a self-consistent data set\nwith labeled faults, we used an LSTM architecture with a forecasting error\nthreshold to obtain precision and recall quality metrics. The dependency of the\nquality metric on the threshold level is considered. An appropriate mechanism\nsuch as \"one handle\" was introduced for filtering faults that are outside of\nthe plant operator field of interest.\n","id":616}
{"Unnamed: 0.1":11617,"Unnamed: 0":11617.0,"anchor":"Inferring object rankings based on noisy pairwise comparisons from\n  multiple annotators","positive":"  Ranking a set of objects involves establishing an order allowing for\ncomparisons between any pair of objects in the set. Oftentimes, due to the\nunavailability of a ground truth of ranked orders, researchers resort to\nobtaining judgments from multiple annotators followed by inferring the ground\ntruth based on the collective knowledge of the crowd. However, the aggregation\nis often ad-hoc and involves imposing stringent assumptions in inferring the\nground truth (e.g. majority vote). In this work, we propose\nExpectation-Maximization (EM) based algorithms that rely on the judgments from\nmultiple annotators and the object attributes for inferring the latent ground\ntruth. The algorithm learns the relation between the latent ground truth and\nobject attributes as well as annotator specific probabilities of flipping, a\nmetric to assess annotator quality. We further extend the EM algorithm to allow\nfor a variable probability of flipping based on the pair of objects at hand. We\ntest our algorithms on two data sets with synthetic annotations and investigate\nthe impact of annotator quality and quantity on the inferred ground truth. We\nalso obtain the results on two other data sets with annotations from\nmachine\/human annotators and interpret the output trends based on the data\ncharacteristics.\n","negative":"  Domain adaptation leverages the knowledge in one domain - the source domain -\nto improve learning efficiency in another domain - the target domain. Existing\nheterogeneous domain adaptation research is relatively well-progressed, but\nonly in situations where the target domain contains at least a few labeled\ninstances. In contrast, heterogeneous domain adaptation with an unlabeled\ntarget domain has not been well-studied. To contribute to the research in this\nemerging field, this paper presents: (1) an unsupervised knowledge transfer\ntheorem that guarantees the correctness of transferring knowledge; and (2) a\nprincipal angle-based metric to measure the distance between two pairs of\ndomains: one pair comprises the original source and target domains and the\nother pair comprises two homogeneous representations of two domains. The\ntheorem and the metric have been implemented in an innovative transfer model,\ncalled a Grassmann-Linear monotonic maps-geodesic flow kernel (GLG), that is\nspecifically designed for heterogeneous unsupervised domain adaptation (HeUDA).\nThe linear monotonic maps meet the conditions of the theorem and are used to\nconstruct homogeneous representations of the heterogeneous domains. The metric\nshows the extent to which the homogeneous representations have preserved the\ninformation in the original source and target domains. By minimizing the\nproposed metric, the GLG model learns the homogeneous representations of\nheterogeneous domains and transfers knowledge through these learned\nrepresentations via a geodesic flow kernel. To evaluate the model, five public\ndatasets were reorganized into ten HeUDA tasks across three applications:\ncancer detection, credit assessment, and text classification. The experiments\ndemonstrate that the proposed model delivers superior performance over the\nexisting baselines.\n","id":617}
{"Unnamed: 0.1":11618,"Unnamed: 0":11618.0,"anchor":"User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation","positive":"  Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.\n","negative":"  Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method.\n","id":618}
{"Unnamed: 0.1":11619,"Unnamed: 0":11619.0,"anchor":"Improving Neural Language Models with a Continuous Cache","positive":"  We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n","negative":"  The advent of data science has spurred interest in estimating properties of\ndistributions over large alphabets. Fundamental symmetric properties such as\nsupport size, support coverage, entropy, and proximity to uniformity, received\nmost attention, with each property estimated using a different technique and\noften intricate analysis tools.\n  We prove that for all these properties, a single, simple, plug-in\nestimator---profile maximum likelihood (PML)---performs as well as the best\nspecialized techniques. This raises the possibility that PML may optimally\nestimate many other symmetric properties.\n","id":619}
{"Unnamed: 0.1":11620,"Unnamed: 0":11620.0,"anchor":"Identification of Cancer Patient Subgroups via Smoothed Shortest Path\n  Graph Kernel","positive":"  Characterizing patient somatic mutations through next-generation sequencing\ntechnologies opens up possibilities for refining cancer subtypes. However,\ncatalogues of mutations reveal that only a small fraction of the genes are\naltered frequently in patients. On the other hand different genomic alterations\nmay perturb the same pathways. We propose a novel clustering procedure that\nquantifies the similarities of patients from their mutational profile on\npathways via a novel graph kernel. We represent each KEGG pathway as an\nundirected graph. For each patient the vertex labels are assigned based on her\naltered genes. Smoothed shortest path graph kernel (smSPK) evaluates each pair\nof patients by comparing their vertex labeled pathway graphs. Our clustering\nprocedure involves two steps: the smSPK kernel matrix derived for each pathway\nare input to kernel k-means algorithm and each pathway is evaluated\nindividually. In the next step, only those pathways that are successful are\ncombined in to a single kernel input to kernel k-means to stratify patients.\nEvaluating the procedure on simulated data showed that smSPK clusters patients\nup to 88\\% accuracy. Finally to identify ovarian cancer patient subgroups, we\napply our methodology to the cancer genome atlas ovarian data that involves 481\npatients. The identified subgroups are evaluated through survival analysis.\nGrouping patients into four clusters results with patients groups that are\nsignificantly different in their survival times ($p$-value $\\le 0.005$).\n","negative":"  Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.\n","id":620}
{"Unnamed: 0.1":11621,"Unnamed: 0":11621.0,"anchor":"Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders","positive":"  There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks.\n","negative":"  Many theories have emerged which investigate how in- variance is generated in\nhierarchical networks through sim- ple schemes such as max and mean pooling.\nThe restriction to max\/mean pooling in theoretical and empirical studies has\ndiverted attention away from a more general way of generating invariance to\nnuisance transformations. We con- jecture that hierarchically building\nselective invariance (i.e. carefully choosing the range of the transformation\nto be in- variant to at each layer of a hierarchical network) is im- portant\nfor pattern recognition. We utilize a novel pooling layer called adaptive\npooling to find linear pooling weights within networks. These networks with the\nlearnt pooling weights have performances on object categorization tasks that\nare comparable to max\/mean pooling networks. In- terestingly, adaptive pooling\ncan converge to mean pooling (when initialized with random pooling weights),\nfind more general linear pooling schemes or even decide not to pool at all. We\nillustrate the general notion of selective invari- ance through object\ncategorization experiments on large- scale datasets such as SVHN and ILSVRC\n2012.\n","id":621}
{"Unnamed: 0.1":11622,"Unnamed: 0":11622.0,"anchor":"Retrieving sinusoids from nonuniformly sampled data using recursive\n  formulation","positive":"  A heuristic procedure based on novel recursive formulation of sinusoid (RFS)\nand on regression with predictive least-squares (LS) enables to decompose both\nuniformly and nonuniformly sampled 1-d signals into a sparse set of sinusoids\n(SSS). An optimal SSS is found by Levenberg-Marquardt (LM) optimization of RFS\nparameters of near-optimal sinusoids combined with common criteria for the\nestimation of the number of sinusoids embedded in noise. The procedure\nestimates both the cardinality and the parameters of SSS. The proposed\nalgorithm enables to identify the RFS parameters of a sinusoid from a data\nsequence containing only a fraction of its cycle. In extreme cases when the\nfrequency of a sinusoid approaches zero the algorithm is able to detect a\nlinear trend in data. Also, an irregular sampling pattern enables the algorithm\nto correctly reconstruct the under-sampled sinusoid. Parsimonious nature of the\nobtaining models opens the possibilities of using the proposed method in\nmachine learning and in expert and intelligent systems needing analysis and\nsimple representation of 1-d signals. The properties of the proposed algorithm\nare evaluated on examples of irregularly sampled artificial signals in noise\nand are compared with high accuracy frequency estimation algorithms based on\nlinear prediction (LP) approach, particularly with respect to Cramer-Rao Bound\n(CRB).\n","negative":"  We describe a framework for multitask deep reinforcement learning guided by\npolicy sketches. Sketches annotate tasks with sequences of named subtasks,\nproviding information about high-level structural relationships among tasks but\nnot how to implement them---specifically not providing the detailed guidance\nused by much previous work on learning policy abstractions for RL (e.g.\nintermediate rewards, subtask completion signals, or intrinsic motivations). To\nlearn from sketches, we present a model that associates every subtask with a\nmodular subpolicy, and jointly maximizes reward over full task-specific\npolicies by tying parameters across shared subpolicies. Optimization is\naccomplished via a decoupled actor--critic training objective that facilitates\nlearning common behaviors from multiple dissimilar reward functions. We\nevaluate the effectiveness of our approach in three environments featuring both\ndiscrete and continuous control, and with sparse rewards that can be obtained\nonly after completing a number of high-level subgoals. Experiments show that\nusing our approach to learn policies guided by sketches gives better\nperformance than existing techniques for learning task-specific or shared\npolicies, while naturally inducing a library of interpretable primitive\nbehaviors that can be recombined to rapidly adapt to new tasks.\n","id":622}
{"Unnamed: 0.1":11623,"Unnamed: 0":11623.0,"anchor":"Predicting Process Behaviour using Deep Learning","positive":"  Predicting business process behaviour is an important aspect of business\nprocess management. Motivated by research in natural language processing, this\npaper describes an application of deep learning with recurrent neural networks\nto the problem of predicting the next event in a business process. This is both\na novel method in process prediction, which has largely relied on explicit\nprocess models, and also a novel application of deep learning methods. The\napproach is evaluated on two real datasets and our results surpass the\nstate-of-the-art in prediction precision.\n","negative":"  In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1\/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets.\n","id":623}
{"Unnamed: 0.1":11624,"Unnamed: 0":11624.0,"anchor":"Harmonic Networks: Deep Translation and Rotation Equivariance","positive":"  Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges.\n","negative":"  This paper presents an efficient implementation of the Wavenet generation\nprocess called Fast Wavenet. Compared to a naive implementation that has\ncomplexity O(2^L) (L denotes the number of layers in the network), our proposed\napproach removes redundant convolution operations by caching previous\ncalculations, thereby reducing the complexity to O(L) time. Timing experiments\nshow significant advantages of our fast implementation over a naive one. While\nthis method is presented for Wavenet, the same scheme can be applied anytime\none wants to perform autoregressive generation or online prediction using a\nmodel with dilated convolution layers. The code for our method is publicly\navailable.\n","id":624}
{"Unnamed: 0.1":11625,"Unnamed: 0":11625.0,"anchor":"Stable Memory Allocation in the Hippocampus: Fundamental Limits and\n  Neural Realization","positive":"  It is believed that hippocampus functions as a memory allocator in brain, the\nmechanism of which remains unrevealed. In Valiant's neuroidal model, the\nhippocampus was described as a randomly connected graph, the computation on\nwhich maps input to a set of activated neuroids with stable size. Valiant\nproposed three requirements for the hippocampal circuit to become a stable\nmemory allocator (SMA): stability, continuity and orthogonality. The\nfunctionality of SMA in hippocampus is essential in further computation within\ncortex, according to Valiant's model.\n  In this paper, we put these requirements for memorization functions into\nrigorous mathematical formulation and introduce the concept of capacity, based\non the probability of erroneous allocation. We prove fundamental limits for the\ncapacity and error probability of SMA, in both data-independent and\ndata-dependent settings. We also establish an example of stable memory\nallocator that can be implemented via neuroidal circuits. Both theoretical\nbounds and simulation results show that the neural SMA functions well.\n","negative":"  We study, to the best of our knowledge, the first Bayesian algorithm for\nunimodal Multi-Armed Bandit (MAB) problems with graph structure. In this\nsetting, each arm corresponds to a node of a graph and each edge provides a\nrelationship, unknown to the learner, between two nodes in terms of expected\nreward. Furthermore, for any node of the graph there is a path leading to the\nunique node providing the maximum expected reward, along which the expected\nreward is monotonically increasing. Previous results on this setting describe\nthe behavior of frequentist MAB algorithms. In our paper, we design a Thompson\nSampling-based algorithm whose asymptotic pseudo-regret matches the lower bound\nfor the considered setting. We show that -as it happens in a wide number of\nscenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In\nparticular, we provide a thorough experimental evaluation of the performance of\nour and state-of-the-art algorithms as the properties of the graph vary.\n","id":625}
{"Unnamed: 0.1":11626,"Unnamed: 0":11626.0,"anchor":"An Architecture for Deep, Hierarchical Generative Models","positive":"  We present an architecture which lets us train deep, directed generative\nmodels with many layers of latent variables. We include deterministic paths\nbetween all latent variables and the generated output, and provide a richer set\nof connections between computations for inference and generation, which enables\nmore effective communication of information throughout the model during\ntraining. To improve performance on natural images, we incorporate a\nlightweight autoregressive model in the reconstruction distribution. These\ntechniques permit end-to-end training of models with 10+ layers of latent\nvariables. Experiments show that our approach achieves state-of-the-art\nperformance on standard image modelling benchmarks, can expose latent class\nstructure in the absence of label information, and can provide convincing\nimputations of occluded regions in natural images.\n","negative":"  The past several years have seen remarkable progress in generative models\nwhich produce convincing samples of images and other modalities. A shared\ncomponent of many powerful generative models is a decoder network, a parametric\ndeep neural net that defines a generative distribution. Examples include\nvariational autoencoders, generative adversarial networks, and generative\nmoment matching networks. Unfortunately, it can be difficult to quantify the\nperformance of these models because of the intractability of log-likelihood\nestimation, and inspecting samples can be misleading. We propose to use\nAnnealed Importance Sampling for evaluating log-likelihoods for decoder-based\nmodels and validate its accuracy using bidirectional Monte Carlo. The\nevaluation code is provided at https:\/\/github.com\/tonywu95\/eval_gen. Using this\ntechnique, we analyze the performance of decoder-based models, the\neffectiveness of existing log-likelihood estimators, the degree of overfitting,\nand the degree to which these models miss important modes of the data\ndistribution.\n","id":626}
{"Unnamed: 0.1":11627,"Unnamed: 0":11627.0,"anchor":"Incorporating Language Level Information into Acoustic Models","positive":"  This paper proposed a class of novel Deep Recurrent Neural Networks which can\nincorporate language-level information into acoustic models. For simplicity, we\nnamed these networks Recurrent Deep Language Networks (RDLNs). Multiple\nvariants of RDLNs were considered, including two kinds of context information,\ntwo methods to process the context, and two methods to incorporate the\nlanguage-level information. RDLNs provided possible methods to fine-tune the\nwhole Automatic Speech Recognition (ASR) system in the acoustic modeling\nprocess.\n","negative":"  Deep neural networks (DNNs) have proven to be quite effective in a vast array\nof machine learning tasks, with recent examples in cyber security and\nautonomous vehicles. Despite the superior performance of DNNs in these\napplications, it has been recently shown that these models are susceptible to a\nparticular type of attack that exploits a fundamental flaw in their design.\nThis attack consists of generating particular synthetic examples referred to as\nadversarial samples. These samples are constructed by slightly manipulating\nreal data-points in order to \"fool\" the original DNN model, forcing it to\nmis-classify previously correctly classified samples with high confidence.\nAddressing this flaw in the model is essential if DNNs are to be used in\ncritical applications such as those in cyber security.\n  Previous work has provided various learning algorithms to enhance the\nrobustness of DNN models, and they all fall into the tactic of \"security\nthrough obscurity\". This means security can be guaranteed only if one can\nobscure the learning algorithms from adversaries. Once the learning technique\nis disclosed, DNNs protected by these defense mechanisms are still susceptible\nto adversarial samples. In this work, we investigate this issue shared across\nprevious research work and propose a generic approach to escalate a DNN's\nresistance to adversarial samples. More specifically, our approach integrates a\ndata transformation module with a DNN, making it robust even if we reveal the\nunderlying learning algorithm. To demonstrate the generality of our proposed\napproach and its potential for handling cyber security applications, we\nevaluate our method and several other existing solutions on datasets publicly\navailable. Our results indicate that our approach typically provides superior\nclassification performance and resistance in comparison with state-of-art\nsolutions.\n","id":627}
{"Unnamed: 0.1":11628,"Unnamed: 0":11628.0,"anchor":"Encapsulating models and approximate inference programs in probabilistic\n  modules","positive":"  This paper introduces the probabilistic module interface, which allows\nencapsulation of complex probabilistic models with latent variables alongside\ncustom stochastic approximate inference machinery, and provides a\nplatform-agnostic abstraction barrier separating the model internals from the\nhost probabilistic inference system. The interface can be seen as a stochastic\ngeneralization of a standard simulation and density interface for probabilistic\nprimitives. We show that sound approximate inference algorithms can be\nconstructed for networks of probabilistic modules, and we demonstrate that the\ninterface can be implemented using learned stochastic inference networks and\nMCMC and SMC approximate inference programs.\n","negative":"  We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired.\n","id":628}
{"Unnamed: 0.1":11629,"Unnamed: 0":11629.0,"anchor":"Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise\n  Labeling","positive":"  Pixel wise image labeling is an interesting and challenging problem with\ngreat significance in the computer vision community. In order for a dense\nlabeling algorithm to be able to achieve accurate and precise results, it has\nto consider the dependencies that exist in the joint space of both the input\nand the output variables. An implicit approach for modeling those dependencies\nis by training a deep neural network that, given as input an initial estimate\nof the output labels and the input image, it will be able to predict a new\nrefined estimate for the labels. In this context, our work is concerned with\nwhat is the optimal architecture for performing the label improvement task. We\nargue that the prior approaches of either directly predicting new label\nestimates or predicting residual corrections w.r.t. the initial labels with\nfeed-forward deep network architectures are sub-optimal. Instead, we propose a\ngeneric architecture that decomposes the label improvement task to three steps:\n1) detecting the initial label estimates that are incorrect, 2) replacing the\nincorrect labels with new ones, and finally 3) refining the renewed labels by\npredicting residual corrections w.r.t. them. Furthermore, we explore and\ncompare various other alternative architectures that consist of the\naforementioned Detection, Replace, and Refine components. We extensively\nevaluate the examined architectures in the challenging task of dense disparity\nestimation (stereo matching) and we report both quantitative and qualitative\nresults on three different datasets. Finally, our dense disparity estimation\nnetwork that implements the proposed generic architecture, achieves\nstate-of-the-art results in the KITTI 2015 test surpassing prior approaches by\na significant margin.\n","negative":"  The method presented extends a given regression neural network to make its\nperformance improve. The modification affects the learning procedure only,\nhence the extension may be easily omitted during evaluation without any change\nin prediction. It means that the modified model may be evaluated as quickly as\nthe original one but tends to perform better.\n  This improvement is possible because the modification gives better expressive\npower, provides better behaved gradients and works as a regularization. The\nknowledge gained by the temporarily extended neural network is contained in the\nparameters shared with the original neural network.\n  The only cost is an increase in learning time.\n","id":629}
{"Unnamed: 0.1":11630,"Unnamed: 0":11630.0,"anchor":"Deep Function Machines: Generalized Neural Networks for Topological\n  Layer Expression","positive":"  In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance.\n","negative":"  Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality.\n","id":630}
{"Unnamed: 0.1":11631,"Unnamed: 0":11631.0,"anchor":"Anomaly Detection Using the Knowledge-based Temporal Abstraction Method","positive":"  The rapid growth in stored time-oriented data necessitates the development of\nnew methods for handling, processing, and interpreting large amounts of\ntemporal data. One important example of such processing is detecting anomalies\nin time-oriented data. The Knowledge-Based Temporal Abstraction method was\npreviously proposed for intelligent interpretation of temporal data based on\npredefined domain knowledge. In this study we propose a framework that\nintegrates the KBTA method with a temporal pattern mining process for anomaly\ndetection. According to the proposed method a temporal pattern mining process\nis applied on a dataset of basic temporal abstraction database in order to\nextract patterns representing normal behavior. These patterns are then analyzed\nin order to identify abnormal time periods characterized by a significantly\nsmall number of normal patterns. The proposed approach was demonstrated using a\ndataset collected from a real server.\n","negative":"  In this work we study the problem of network morphism, an effective learning\nscheme to morph a well-trained neural network to a new one with the network\nfunction completely preserved. Different from existing work where basic\nmorphing types on the layer level were addressed, we target at the central\nproblem of network morphism at a higher level, i.e., how a convolutional layer\ncan be morphed into an arbitrary module of a neural network. To simplify the\nrepresentation of a network, we abstract a module as a graph with blobs as\nvertices and convolutional layers as edges, based on which the morphing process\nis able to be formulated as a graph transformation problem. Two atomic morphing\noperations are introduced to compose the graphs, based on which modules are\nclassified into two families, i.e., simple morphable modules and complex\nmodules. We present practical morphing solutions for both of these two\nfamilies, and prove that any reasonable module can be morphed from a single\nconvolutional layer. Extensive experiments have been conducted based on the\nstate-of-the-art ResNet on benchmark datasets, and the effectiveness of the\nproposed solution has been verified.\n","id":631}
{"Unnamed: 0.1":11632,"Unnamed: 0":11632.0,"anchor":"Uncovering the Dynamics of Crowdlearning and the Value of Knowledge","positive":"  Learning from the crowd has become increasingly popular in the Web and social\nmedia. There is a wide variety of crowdlearning sites in which, on the one\nhand, users learn from the knowledge that other users contribute to the site,\nand, on the other hand, knowledge is reviewed and curated by the same users\nusing assessment measures such as upvotes or likes.\n  In this paper, we present a probabilistic modeling framework of\ncrowdlearning, which uncovers the evolution of a user's expertise over time by\nleveraging other users' assessments of her contributions. The model allows for\nboth off-site and on-site learning and captures forgetting of knowledge. We\nthen develop a scalable estimation method to fit the model parameters from\nmillions of recorded learning and contributing events. We show the\neffectiveness of our model by tracing activity of ~25 thousand users in Stack\nOverflow over a 4.5 year period. We find that answers with high knowledge value\nare rare. Newbies and experts tend to acquire less knowledge than users in the\nmiddle range. Prolific learners tend to be also proficient contributors that\npost answers with high knowledge value.\n","negative":"  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks, state of the art models in\nspeech recognition and translation. Our approach to increasing interpretability\nis by combining a long short-term memory (LSTM) model with a hidden Markov\nmodel (HMM), a simpler and more transparent model. We add the HMM state\nprobabilities to the output layer of the LSTM, and then train the HMM and LSTM\neither sequentially or jointly. The LSTM can make use of the information from\nthe HMM, and fill in the gaps when the HMM is not performing well. A small\nhybrid model usually performs better than a standalone LSTM of the same size,\nespecially on smaller data sets. We test the algorithms on text data and\nmedical time series data, and find that the LSTM and HMM learn complementary\ninformation about the features in the text.\n","id":632}
{"Unnamed: 0.1":11633,"Unnamed: 0":11633.0,"anchor":"Constraint Selection in Metric Learning","positive":"  A number of machine learning algorithms are using a metric, or a distance, in\norder to compare individuals. The Euclidean distance is usually employed, but\nit may be more efficient to learn a parametric distance such as Mahalanobis\nmetric. Learning such a metric is a hot topic since more than ten years now,\nand a number of methods have been proposed to efficiently learn it. However,\nthe nature of the problem makes it quite difficult for large scale data, as\nwell as data for which classes overlap. This paper presents a simple way of\nimproving accuracy and scalability of any iterative metric learning algorithm,\nwhere constraints are obtained prior to the algorithm. The proposed approach\nrelies on a loss-dependent weighted selection of constraints that are used for\nlearning the metric. Using the corresponding dedicated loss function, the\nmethod clearly allows to obtain better results than state-of-the-art methods,\nboth in terms of accuracy and time complexity. Some experimental results on\nreal world, and potentially large, datasets are demonstrating the effectiveness\nof our proposition.\n","negative":"  X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies.\n","id":633}
{"Unnamed: 0.1":11634,"Unnamed: 0":11634.0,"anchor":"Bayesian Optimization for Machine Learning : A Practical Guidebook","positive":"  The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.\n","negative":"  The complexity of deep neural network algorithms for hardware implementation\ncan be lowered either by scaling the number of units or reducing the\nword-length of weights. Both approaches, however, can accompany the performance\ndegradation although many types of research are conducted to relieve this\nproblem. Thus, it is an important question which one, between the network size\nscaling and the weight quantization, is more effective for hardware\noptimization. For this study, the performances of fully-connected deep neural\nnetworks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while\nchanging the network complexity and the word-length of weights. Based on these\nexperiments, we present the effective compression ratio (ECR) to guide the\ntrade-off between the network size and the precision of weights when the\nhardware resource is limited.\n","id":634}
{"Unnamed: 0.1":11635,"Unnamed: 0":11635.0,"anchor":"Interpretable Semantic Textual Similarity: Finding and explaining\n  differences between sentences","positive":"  User acceptance of artificial intelligence agents might depend on their\nability to explain their reasoning, which requires adding an interpretability\nlayer that fa- cilitates users to understand their behavior. This paper focuses\non adding an in- terpretable layer on top of Semantic Textual Similarity (STS),\nwhich measures the degree of semantic equivalence between two sentences. The\ninterpretability layer is formalized as the alignment between pairs of segments\nacross the two sentences, where the relation between the segments is labeled\nwith a relation type and a similarity score. We present a publicly available\ndataset of sentence pairs annotated following the formalization. We then\ndevelop a system trained on this dataset which, given a sentence pair, explains\nwhat is similar and different, in the form of graded and typed segment\nalignments. When evaluated on the dataset, the system performs better than an\ninformed baseline, showing that the dataset and task are well-defined and\nfeasible. Most importantly, two user studies show how the system output can be\nused to automatically produce explanations in natural language. Users performed\nbetter when having access to the explanations, pro- viding preliminary evidence\nthat our dataset and method to automatically produce explanations is useful in\nreal applications.\n","negative":"  Deep neural networks have shown promise in collaborative filtering (CF).\nHowever, existing neural approaches are either user-based or item-based, which\ncannot leverage all the underlying information explicitly. We propose CF-UIcA,\na neural co-autoregressive model for CF tasks, which exploits the structural\ncorrelation in the domains of both users and items. The co-autoregression\nallows extra desired properties to be incorporated for different tasks.\nFurthermore, we develop an efficient stochastic learning algorithm to handle\nlarge scale datasets. We evaluate CF-UIcA on two popular benchmarks: MovieLens\n1M and Netflix, and achieve state-of-the-art performance in both rating\nprediction and top-N recommendation tasks, which demonstrates the effectiveness\nof CF-UIcA.\n","id":635}
{"Unnamed: 0.1":11636,"Unnamed: 0":11636.0,"anchor":"A Data-Driven Compressive Sensing Framework Tailored For\n  Energy-Efficient Wearable Sensing","positive":"  Compressive sensing (CS) is a promising technology for realizing\nenergy-efficient wireless sensors for long-term health monitoring. However,\nconventional model-driven CS frameworks suffer from limited compression ratio\nand reconstruction quality when dealing with physiological signals due to\ninaccurate models and the overlook of individual variability. In this paper, we\npropose a data-driven CS framework that can learn signal characteristics and\npersonalized features from any individual recording of physiologic signals to\nenhance CS performance with a minimized number of measurements. Such\nimprovements are accomplished by a co-training approach that optimizes the\nsensing matrix and the dictionary towards improved restricted isometry property\nand signal sparsity, respectively. Experimental results upon ECG signals show\nthat the proposed method, at a compression ratio of 10x, successfully reduces\nthe isometry constant of the trained sensing matrices by 86% against random\nmatrices and improves the overall reconstructed signal-to-noise ratio by 15dB\nover conventional model-driven approaches.\n","negative":"  Multi-domain learning aims to benefit from simultaneously learning across\nseveral different but related domains. In this chapter, we propose a single\nframework that unifies multi-domain learning (MDL) and the related but better\nstudied area of multi-task learning (MTL). By exploiting the concept of a\n\\emph{semantic descriptor} we show how our framework encompasses various\nclassic and recent MDL\/MTL algorithms as special cases with different semantic\ndescriptor encodings. As a second contribution, we present a higher order\ngeneralisation of this framework, capable of simultaneous\nmulti-task-multi-domain learning. This generalisation has two mathematically\nequivalent views in multi-linear algebra and gated neural networks\nrespectively. Moreover, by exploiting the semantic descriptor, it provides\nneural networks the capability of zero-shot learning (ZSL), where a classifier\nis generated for an unseen class without any training data; as well as\nzero-shot domain adaptation (ZSDA), where a model is generated for an unseen\ndomain without any training data. In practice, this framework provides a\npowerful yet easy to implement method that can be flexibly applied to MTL, MDL,\nZSL and ZSDA.\n","id":636}
{"Unnamed: 0.1":11637,"Unnamed: 0":11637.0,"anchor":"Deep learning is effective for the classification of OCT images of\n  normal versus Age-related Macular Degeneration","positive":"  Objective: The advent of Electronic Medical Records (EMR) with large\nelectronic imaging databases along with advances in deep neural networks with\nmachine learning has provided a unique opportunity to achieve milestones in\nautomated image analysis. Optical coherence tomography (OCT) is the most\ncommonly obtained imaging modality in ophthalmology and represents a dense and\nrich dataset when combined with labels derived from the EMR. We sought to\ndetermine if deep learning could be utilized to distinguish normal OCT images\nfrom images from patients with Age-related Macular Degeneration (AMD). Methods:\nAutomated extraction of an OCT imaging database was performed and linked to\nclinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg\nSpectralis, and each OCT scan was linked to EMR clinical endpoints extracted\nfrom EPIC. The central 11 images were selected from each OCT scan of two\ncohorts of patients: normal and AMD. Cross-validation was performed using a\nrandom subset of patients. Area under receiver operator curves (auROC) were\nconstructed at an independent image level, macular OCT level, and patient\nlevel. Results: Of an extraction of 2.6 million OCT images linked to clinical\ndatapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were\nselected. A deep neural network was trained to categorize images as either\nnormal or AMD. At the image level, we achieved an auROC of 92.78% with an\naccuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an\naccuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an\naccuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were\n92.64% and 93.69% respectively. Conclusions: Deep learning techniques are\neffective for classifying OCT images. These findings have important\nimplications in utilizing OCT in automated screening and computer aided\ndiagnosis tools.\n","negative":"  We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata.\n","id":637}
{"Unnamed: 0.1":11638,"Unnamed: 0":11638.0,"anchor":"Efficient Distributed Semi-Supervised Learning using Stochastic\n  Regularization over Affinity Graphs","positive":"  We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.\n","negative":"  Mixture of Experts (MoE) is a popular framework in the fields of statistics\nand machine learning for modeling heterogeneity in data for regression,\nclassification and clustering. MoE for continuous data are usually based on the\nnormal distribution. However, it is known that for data with asymmetric\nbehavior, heavy tails and atypical observations, the use of the normal\ndistribution is unsuitable. We introduce a new robust non-normal mixture of\nexperts modeling using the skew $t$ distribution. The proposed skew $t$ mixture\nof experts, named STMoE, handles these issues of the normal mixtures experts\nregarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated\nexpectation conditional maximization (ECM) algorithm to estimate the model\nparameters by monotonically maximizing the observed data log-likelihood. We\ndescribe how the presented model can be used in prediction and in model-based\nclustering of regression data. Numerical experiments carried out on simulated\ndata show the effectiveness and the robustness of the proposed model in fitting\nnon-linear regression functions as well as in model-based clustering. Then, the\nproposed model is applied to the real-world data of tone perception for musical\ndata analysis, and the one of temperature anomalies for the analysis of climate\nchange data. The obtained results confirm the usefulness of the model for\npractical data analysis applications.\n","id":638}
{"Unnamed: 0.1":11639,"Unnamed: 0":11639.0,"anchor":"Semi-Supervised Phone Classification using Deep Neural Networks and\n  Stochastic Graph-Based Entropic Regularization","positive":"  We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case.\n","negative":"  Functional MRI (fMRI) and diffusion MRI (dMRI) are non-invasive imaging\nmodalities that allow in-vivo analysis of a patient's brain network (known as a\nconnectome). Use of these technologies has enabled faster and better diagnoses\nand treatments of neurological disorders and a deeper understanding of the\nhuman brain. Recently, researchers have been exploring the application of\nmachine learning models to connectome data in order to predict clinical\noutcomes and analyze the importance of subnetworks in the brain. Connectome\ndata has unique properties, which present both special challenges and\nopportunities when used for machine learning. The purpose of this work is to\nreview the literature on the topic of applying machine learning models to\nMRI-based connectome data. This field is growing rapidly and now encompasses a\nlarge body of research. To summarize the research done to date, we provide a\ncomparative, structured summary of 77 relevant works, tabulated according to\ndifferent criteria, that represent the majority of the literature on this\ntopic. (We also published a living version of this table online at\nhttp:\/\/connectomelearning.cs.sfu.ca that the community can continue to\ncontribute to.) After giving an overview of how connectomes are constructed\nfrom dMRI and fMRI data, we discuss the variety of machine learning tasks that\nhave been explored with connectome data. We then compare the advantages and\ndrawbacks of different machine learning approaches that have been employed,\ndiscussing different feature selection and feature extraction schemes, as well\nas the learning models and regularization penalties themselves. Throughout this\ndiscussion, we focus particularly on how the methods are adapted to the unique\nnature of graphical connectome data. Finally, we conclude by summarizing the\ncurrent state of the art and by outlining what we believe are strategic\ndirections for future research.\n","id":639}
{"Unnamed: 0.1":11640,"Unnamed: 0":11640.0,"anchor":"Dynamical Kinds and their Discovery","positive":"  We demonstrate the possibility of classifying causal systems into kinds that\nshare a common structure without first constructing an explicit dynamical model\nor using prior knowledge of the system dynamics. The algorithmic ability to\ndetermine whether arbitrary systems are governed by causal relations of the\nsame form offers significant practical applications in the development and\nvalidation of dynamical models. It is also of theoretical interest as an\nessential stage in the scientific inference of laws from empirical data. The\nalgorithm presented is based on the dynamical symmetry approach to dynamical\nkinds. A dynamical symmetry with respect to time is an intervention on one or\nmore variables of a system that commutes with the time evolution of the system.\nA dynamical kind is a class of systems sharing a set of dynamical symmetries.\nThe algorithm presented classifies deterministic, time-dependent causal systems\nby directly comparing their exhibited symmetries. Using simulated, noisy data\nfrom a variety of nonlinear systems, we show that this algorithm correctly\nsorts systems into dynamical kinds. It is robust under significant sampling\nerror, is immune to violations of normality in sampling error, and fails\ngracefully with increasing dynamical similarity. The algorithm we demonstrate\nis the first to address this aspect of automated scientific discovery.\n","negative":"  Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed\nrecently to address the difficulty of storing information over long time\nperiods. In this paper, we experiment with the capacity of Neural Turing\nMachines (NTMs) to deal with these long-term dependencies on well-balanced\nstrings of parentheses. We show that not only does the NTM emulate a stack with\nits heads and learn an algorithm to recognize such words, but it is also\ncapable of strongly generalizing to much longer sequences.\n","id":640}
{"Unnamed: 0.1":11641,"Unnamed: 0":11641.0,"anchor":"Improving Neural Network Generalization by Combining Parallel Circuits\n  with Dropout","positive":"  In an attempt to solve the lengthy training times of neural networks, we\nproposed Parallel Circuits (PCs), a biologically inspired architecture.\nPrevious work has shown that this approach fails to maintain generalization\nperformance in spite of achieving sharp speed gains. To address this issue, and\nmotivated by the way Dropout prevents node co-adaption, in this paper, we\nsuggest an improvement by extending Dropout to the PC architecture. The paper\nprovides multiple insights into this combination, including a variety of fusion\napproaches. Experiments show promising results in which improved error rates\nare achieved in most cases, whilst maintaining the speed advantage of the PC\napproach.\n","negative":"  We introduce a simple permutation equivariant layer for deep learning with\nset structure.This type of layer, obtained by parameter-sharing, has a simple\nimplementation and linear-time complexity in the size of each set. We use deep\npermutation-invariant networks to perform point-could classification and\nMNIST-digit summation, where in both cases the output is invariant to\npermutations of the input. In a semi-supervised setting, where the goal is make\npredictions for each instance within a set, we demonstrate the usefulness of\nthis type of layer in set-outlier detection as well as semi-supervised learning\nwith clustering side-information.\n","id":641}
{"Unnamed: 0.1":11642,"Unnamed: 0":11642.0,"anchor":"Graph-based semi-supervised learning for relational networks","positive":"  We address the problem of semi-supervised learning in relational networks,\nnetworks in which nodes are entities and links are the relationships or\ninteractions between them. Typically this problem is confounded with the\nproblem of graph-based semi-supervised learning (GSSL), because both problems\nrepresent the data as a graph and predict the missing class labels of nodes.\nHowever, not all graphs are created equally. In GSSL a graph is constructed,\noften from independent data, based on similarity. As such, edges tend to\nconnect instances with the same class label. Relational networks, however, can\nbe more heterogeneous and edges do not always indicate similarity. For\ninstance, instead of links being more likely to connect nodes with the same\nclass label, they may occur more frequently between nodes with different class\nlabels (link-heterogeneity). Or nodes with the same class label do not\nnecessarily have the same type of connectivity across the whole network\n(class-heterogeneity), e.g. in a network of sexual interactions we may observe\nlinks between opposite genders in some parts of the graph and links between the\nsame genders in others. Performing classification in networks with different\ntypes of heterogeneity is a hard problem that is made harder still when we do\nnot know a-priori the type or level of heterogeneity. Here we present two\nscalable approaches for graph-based semi-supervised learning for the more\ngeneral case of relational networks. We demonstrate these approaches on\nsynthetic and real-world networks that display different link patterns within\nand between classes. Compared to state-of-the-art approaches, ours give better\nclassification performance without prior knowledge of how classes interact. In\nparticular, our two-step label propagation algorithm gives consistently good\naccuracy and runs on networks of over 1.6 million nodes and 30 million edges in\naround 12 seconds.\n","negative":"  Optimal control of thermostatically controlled loads connected to a district\nheating network is considered a sequential decision- making problem under\nuncertainty. The practicality of a direct model-based approach is compromised\nby two challenges, namely scalability due to the large dimensionality of the\nproblem and the system identification required to identify an accurate model.\nTo help in mitigating these problems, this paper leverages on recent\ndevelopments in reinforcement learning in combination with a market-based\nmulti-agent system to obtain a scalable solution that obtains a significant\nperformance improvement in a practical learning time. The control approach is\napplied on a scenario comprising 100 thermostatically controlled loads\nconnected to a radial district heating network supplied by a central combined\nheat and power plant. Both for an energy arbitrage and a peak shaving\nobjective, the control approach requires 60 days to obtain a performance within\n65% of a theoretical lower bound on the cost.\n","id":642}
{"Unnamed: 0.1":11643,"Unnamed: 0":11643.0,"anchor":"Optimal structure and parameter learning of Ising models","positive":"  Reconstruction of structure and parameters of an Ising model from binary\nsamples is a problem of practical importance in a variety of disciplines,\nranging from statistical physics and computational biology to image processing\nand machine learning. The focus of the research community shifted towards\ndeveloping universal reconstruction algorithms which are both computationally\nefficient and require the minimal amount of expensive data. We introduce a new\nmethod, Interaction Screening, which accurately estimates the model parameters\nusing local optimization problems. The algorithm provably achieves perfect\ngraph structure recovery with an information-theoretically optimal number of\nsamples, notably in the low-temperature regime which is known to be the hardest\nfor learning. The efficacy of Interaction Screening is assessed through\nextensive numerical tests on synthetic Ising models of various topologies with\ndifferent types of interactions, as well as on a real data produced by a D-Wave\nquantum computer. This study shows that the Interaction Screening method is an\nexact, tractable and optimal technique universally solving the inverse Ising\nproblem.\n","negative":"  In computer vision pixelwise dense prediction is the task of predicting a\nlabel for each pixel in the image. Convolutional neural networks achieve good\nperformance on this task, while being computationally efficient. In this paper\nwe carry these ideas over to the problem of assigning a sequence of labels to a\nset of speech frames, a task commonly known as framewise classification. We\nshow that dense prediction view of framewise classification offers several\nadvantages and insights, including computational efficiency and the ability to\napply batch normalization. When doing dense prediction we pay specific\nattention to strided pooling in time and introduce an asymmetric dilated\nconvolution, called time-dilated convolution, that allows for efficient and\nelegant implementation of pooling in time. We show results using time-dilated\nconvolutions in a very deep VGG-style CNN with batch normalization on the Hub5\nSwitchboard-2000 benchmark task. With a big n-gram language model, we achieve\n7.7% WER which is the best single model single-pass performance reported so\nfar.\n","id":643}
{"Unnamed: 0.1":11644,"Unnamed: 0":11644.0,"anchor":"Towards Score Following in Sheet Music Images","positive":"  This paper addresses the matching of short music audio snippets to the\ncorresponding pixel location in images of sheet music. A system is presented\nthat simultaneously learns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the sheet. It consists of\nan end-to-end multi-modal convolutional neural network that takes as input\nimages of sheet music and spectrograms of the respective audio snippets. It\nlearns to predict, for a given unseen audio snippet (covering approximately one\nbar of music), the corresponding position in the respective score line. Our\nresults suggest that with the use of (deep) neural networks -- which have\nproven to be powerful image processing models -- working with sheet music\nbecomes feasible and a promising future research direction.\n","negative":"  Graph-based methods pervade the inference toolkits of numerous disciplines\nincluding sociology, biology, neuroscience, physics, chemistry, and\nengineering. A challenging problem encountered in this context pertains to\ndetermining the attributes of a set of vertices given those of another subset\nat possibly different time instants. Leveraging spatiotemporal dynamics can\ndrastically reduce the number of observed vertices, and hence the cost of\nsampling. Alleviating the limited flexibility of existing approaches, the\npresent paper broadens the existing kernel-based graph function reconstruction\nframework to accommodate time-evolving functions over possibly time-evolving\ntopologies. This approach inherits the versatility and generality of\nkernel-based methods, for which no knowledge on distributions or second-order\nstatistics is required. Systematic guidelines are provided to construct two\nfamilies of space-time kernels with complementary strengths. The first\nfacilitates judicious control of regularization on a space-time frequency\nplane, whereas the second can afford time-varying topologies. Batch and online\nestimators are also put forth, and a novel kernel Kalman filter is developed to\nobtain these estimates at affordable computational cost. Numerical tests with\nreal data sets corroborate the merits of the proposed methods relative to\ncompeting alternatives.\n","id":644}
{"Unnamed: 0.1":11645,"Unnamed: 0":11645.0,"anchor":"Graphical RNN Models","positive":"  Many time series are generated by a set of entities that interact with one\nanother over time. This paper introduces a broad, flexible framework to learn\nfrom multiple inter-dependent time series generated by such entities. Our\nframework explicitly models the entities and their interactions through time.\nIt achieves this by building on the capabilities of Recurrent Neural Networks,\nwhile also offering several ways to incorporate domain knowledge\/constraints\ninto the model architecture. The capabilities of our approach are showcased\nthrough an application to weather prediction, which shows gains over strong\nbaselines.\n","negative":"  This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.\n","id":645}
{"Unnamed: 0.1":11646,"Unnamed: 0":11646.0,"anchor":"Feature Learning for Chord Recognition: The Deep Chroma Extractor","positive":"  We explore frame-level audio feature learning for chord recognition using\nartificial neural networks. We present the argument that chroma vectors\npotentially hold enough information to model harmonic content of audio for\nchord recognition, but that standard chroma extractors compute too noisy\nfeatures. This leads us to propose a learned chroma feature extractor based on\nartificial neural networks. It is trained to compute chroma features that\nencode harmonic information important for chord recognition, while being robust\nto irrelevant interferences. We achieve this by feeding the network an audio\nspectrum with context instead of a single frame as input. This way, the network\ncan learn to selectively compensate noise and resolve harmonic ambiguities.\n  We compare the resulting features to hand-crafted ones by using a simple\nlinear frame-wise classifier for chord recognition on various data sets. The\nresults show that the learned feature extractor produces superior chroma\nvectors for chord recognition.\n","negative":"  Drug repositioning offers an effective solution to drug discovery, saving\nboth time and resources by finding new indications for existing drugs.\nTypically, a drug takes effect via its protein targets in the cell. As a\nresult, it is necessary for drug development studies to conduct an\ninvestigation into the interrelationships of drugs, protein targets, and\ndiseases. Although previous studies have made a strong case for the\neffectiveness of integrative network-based methods for predicting these\ninterrelationships, little progress has been achieved in this regard within\ndrug repositioning research. Moreover, the interactions of new drugs and\ntargets (lacking any known targets and drugs, respectively) cannot be\naccurately predicted by most established methods. In this paper, we propose a\nnovel semi-supervised heterogeneous label propagation algorithm named Heter-LP,\nwhich applies both local as well as global network features for data\nintegration. To predict drug-target, disease-target, and drug-disease\nassociations, we use information about drugs, diseases, and targets as\ncollected from multiple sources at different levels. Our algorithm integrates\nthese various types of data into a heterogeneous network and implements a label\npropagation algorithm to find new interactions. Statistical analyses of 10-fold\ncross-validation results and experimental analysis support the effectiveness of\nthe proposed algorithm.\n","id":646}
{"Unnamed: 0.1":11647,"Unnamed: 0":11647.0,"anchor":"Towards End-to-End Audio-Sheet-Music Retrieval","positive":"  This paper demonstrates the feasibility of learning to retrieve short\nsnippets of sheet music (images) when given a short query excerpt of music\n(audio) -- and vice versa --, without any symbolic representation of music or\nscores. This would be highly useful in many content-based musical retrieval\nscenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)\nand learns correlated latent spaces allowing for cross-modality retrieval in\nboth directions. Initial experiments with relatively simple monophonic music\nshow promising results.\n","negative":"  One of the classical problems in machine learning and data mining is feature\nselection. A feature selection algorithm is expected to be quick, and at the\nsame time it should show high performance. MeLiF algorithm effectively solves\nthis problem using ensembles of ranking filters. This article describes two\ndifferent ways to improve MeLiF algorithm performance with parallelization.\nExperiments show that proposed schemes significantly improves algorithm\nperformance and increase feature selection quality.\n","id":647}
{"Unnamed: 0.1":11648,"Unnamed: 0":11648.0,"anchor":"A Fully Convolutional Deep Auditory Model for Musical Chord Recognition","positive":"  Chord recognition systems depend on robust feature extraction pipelines.\nWhile these pipelines are traditionally hand-crafted, recent advances in\nend-to-end machine learning have begun to inspire researchers to explore\ndata-driven methods for such tasks. In this paper, we present a chord\nrecognition system that uses a fully convolutional deep auditory model for\nfeature extraction. The extracted features are processed by a Conditional\nRandom Field that decodes the final chord sequence. Both processing stages are\ntrained automatically and do not require expert knowledge for optimising\nparameters. We show that the learned auditory system extracts musically\ninterpretable features, and that the proposed chord recognition system achieves\nresults on par or better than state-of-the-art algorithms.\n","negative":"  Machine learning models, such as neural networks, decision trees, random\nforests, and gradient boosting machines, accept a feature vector, and provide a\nprediction. These models learn in a supervised fashion where we provide feature\nvectors mapped to the expected output. It is common practice to engineer new\nfeatures from the provided feature set. Such engineered features will either\naugment or replace portions of the existing feature vector. These engineered\nfeatures are essentially calculated fields based on the values of the other\nfeatures.\n  Engineering such features is primarily a manual, time-consuming task.\nAdditionally, each type of model will respond differently to different kinds of\nengineered features. This paper reports empirical research to demonstrate what\nkinds of engineered features are best suited to various machine learning model\ntypes. We provide this recommendation by generating several datasets that we\ndesigned to benefit from a particular type of engineered feature. The\nexperiment demonstrates to what degree the machine learning model can\nsynthesize the needed feature on its own. If a model can synthesize a planned\nfeature, it is not necessary to provide that feature. The research demonstrated\nthat the studied models do indeed perform differently with various types of\nengineered features.\n","id":648}
{"Unnamed: 0.1":11649,"Unnamed: 0":11649.0,"anchor":"Coupling Adaptive Batch Sizes with Learning Rates","positive":"  Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available.\n","negative":"  This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\n","id":649}
{"Unnamed: 0.1":11650,"Unnamed: 0":11650.0,"anchor":"On the Potential of Simple Framewise Approaches to Piano Transcription","positive":"  In an attempt at exploring the limitations of simple approaches to the task\nof piano transcription (as usually defined in MIR), we conduct an in-depth\nanalysis of neural network-based framewise transcription. We systematically\ncompare different popular input representations for transcription systems to\ndetermine the ones most suitable for use with neural networks. Exploiting\nrecent advances in training techniques and new regularizers, and taking into\naccount hyper-parameter tuning, we show that it is possible, by simple\nbottom-up frame-wise processing, to obtain a piano transcriber that outperforms\nthe current published state of the art on the publicly available MAPS dataset\n-- without any complex post-processing steps. Thus, we propose this simple\napproach as a new baseline for this dataset, for future transcription research\nto build on and improve.\n","negative":"  We present the Video Ladder Network (VLN) for efficiently generating future\nvideo frames. VLN is a neural encoder-decoder model augmented at all layers by\nboth recurrent and feedforward lateral connections. At each layer, these\nconnections form a lateral recurrent residual block, where the feedforward\nconnection represents a skip connection and the recurrent connection represents\nthe residual. Thanks to the recurrent connections, the decoder can exploit\ntemporal summaries generated from all layers of the encoder. This way, the top\nlayer is relieved from the pressure of modeling lower-level spatial and\ntemporal details. Furthermore, we extend the basic version of VLN to\nincorporate ResNet-style residual blocks in the encoder and decoder, which help\nimproving the prediction results. VLN is trained in self-supervised regime on\nthe Moving MNIST dataset, achieving competitive results while having very\nsimple structure and providing fast inference.\n","id":650}
{"Unnamed: 0.1":11651,"Unnamed: 0":11651.0,"anchor":"Separation of Concerns in Reinforcement Learning","positive":"  In this paper, we propose a framework for solving a single-agent task by\nusing multiple agents, each focusing on different aspects of the task. This\napproach has two main advantages: 1) it allows for training specialized agents\non different parts of the task, and 2) it provides a new way to transfer\nknowledge, by transferring trained agents. Our framework generalizes the\ntraditional hierarchical decomposition, in which, at any moment in time, a\nsingle agent has control until it has solved its particular subtask. We\nillustrate our framework with empirical experiments on two domains.\n","negative":"  In view of solving convex optimization problems with noisy gradient input, we\nanalyze the asymptotic behavior of gradient-like flows under stochastic\ndisturbances. Specifically, we focus on the widely studied class of mirror\ndescent schemes for convex programs with compact feasible regions, and we\nexamine the dynamics' convergence and concentration properties in the presence\nof noise. In the vanishing noise limit, we show that the dynamics converge to\nthe solution set of the underlying problem (a.s.). Otherwise, when the noise is\npersistent, we show that the dynamics are concentrated around interior\nsolutions in the long run, and they converge to boundary solutions that are\nsufficiently \"sharp\". Finally, we show that a suitably rectified variant of the\nmethod converges irrespective of the magnitude of the noise (or the structure\nof the underlying convex program), and we derive an explicit estimate for its\nrate of convergence.\n","id":651}
{"Unnamed: 0.1":11652,"Unnamed: 0":11652.0,"anchor":"CSVideoNet: A Real-time End-to-end Learning Framework for\n  High-frame-rate Video Compressive Sensing","positive":"  This paper addresses the real-time encoding-decoding problem for\nhigh-frame-rate video compressive sensing (CS). Unlike prior works that perform\nreconstruction using iterative optimization-based approaches, we propose a\nnon-iterative model, named \"CSVideoNet\". CSVideoNet directly learns the inverse\nmapping of CS and reconstructs the original input in a single forward\npropagation. To overcome the limitations of existing CS cameras, we propose a\nmulti-rate CNN and a synthesizing RNN to improve the trade-off between\ncompression ratio (CR) and spatial-temporal resolution of the reconstructed\nvideos. The experiment results demonstrate that CSVideoNet significantly\noutperforms the state-of-the-art approaches. With no pre\/post-processing, we\nachieve 25dB PSNR recovery quality at 100x CR, with a frame rate of 125 fps on\na Titan X GPU. Due to the feedforward and high-data-concurrency natures of\nCSVideoNet, it can take advantage of GPU acceleration to achieve three orders\nof magnitude speed-up over conventional iterative-based approaches. We share\nthe source code at https:\/\/github.com\/PSCLab-ASU\/CSVideoNet.\n","negative":"  The k-fold cross-validation is commonly used to evaluate the effectiveness of\nSVMs with the selected hyper-parameters. It is known that the SVM k-fold\ncross-validation is expensive, since it requires training k SVMs. However,\nlittle work has explored reusing the h-th SVM for training the (h+1)-th SVM for\nimproving the efficiency of k-fold cross-validation. In this paper, we propose\nthree algorithms that reuse the h-th SVM for improving the efficiency of\ntraining the (h+1)-th SVM. Our key idea is to efficiently identify the support\nvectors and to accurately estimate their associated weights (also called alpha\nvalues) of the next SVM by using the previous SVM. Our experimental results\nshow that our algorithms are several times faster than the k-fold\ncross-validation which does not make use of the previously trained SVM.\nMoreover, our algorithms produce the same results (hence same accuracy) as the\nk-fold cross-validation which does not make use of the previously trained SVM.\n","id":652}
{"Unnamed: 0.1":11653,"Unnamed: 0":11653.0,"anchor":"Tunable Efficient Unitary Neural Networks (EUNN) and their application\n  to RNNs","positive":"  Using unitary (instead of general) matrices in artificial neural networks\n(ANNs) is a promising way to solve the gradient explosion\/vanishing problem, as\nwell as to enable ANNs to learn long-term correlations in the data. This\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\nIn this work, we present a new architecture for implementing an Efficient\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\nSecondly, the computational complexity for training an EUNN is merely\n$\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\narchitecture, in terms of the final performance and\/or the wall-clock training\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\nvariety of applications.\n","negative":"  This paper investigates the problem of recovering missing samples using\nmethods based on sparse representation adapted especially for image signals.\nInstead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality\nmeasure is used as the similarity criterion between the original and the\nreconstructed images. The proposed criterion called Convex SIMilarity (CSIM)\nindex is a modified version of the Structural SIMilarity (SSIM) index, which\ndespite its predecessor, is convex and uni-modal. We derive mathematical\nproperties for the proposed index and show how to optimally choose the\nparameters of the proposed criterion, investigating the Restricted Isometry\n(RIP) and error-sensitivity properties. We also propose an iterative sparse\nrecovery method based on a constrained $l_1$-norm minimization problem,\nincorporating CSIM as the fidelity criterion. The resulting convex optimization\nproblem is solved via an algorithm based on Alternating Direction Method of\nMultipliers (ADMM). Taking advantage of the convexity of the CSIM index, we\nalso prove the convergence of the algorithm to the globally optimal solution of\nthe proposed optimization problem, starting from any arbitrary point.\nSimulation results confirm the performance of the new similarity index as well\nas the proposed algorithm for missing sample recovery of image patch signals.\n","id":653}
{"Unnamed: 0.1":11654,"Unnamed: 0":11654.0,"anchor":"Private Learning on Networks","positive":"  Continual data collection and widespread deployment of machine learning\nalgorithms, particularly the distributed variants, have raised new privacy\nchallenges. In a distributed machine learning scenario, the dataset is stored\namong several machines and they solve a distributed optimization problem to\ncollectively learn the underlying model. We present a secure multi-party\ncomputation inspired privacy preserving distributed algorithm for optimizing a\nconvex function consisting of several possibly non-convex functions. Each\nindividual objective function is privately stored with an agent while the\nagents communicate model parameters with neighbor machines connected in a\nnetwork. We show that our algorithm can correctly optimize the overall\nobjective function and learn the underlying model accurately. We further prove\nthat under a vertex connectivity condition on the topology, our algorithm\npreserves privacy of individual objective functions. We establish limits on the\nwhat a coalition of adversaries can learn by observing the messages and states\nshared over a network.\n","negative":"  Evaluating surgeon skill has predominantly been a subjective task.\nDevelopment of objective methods for surgical skill assessment are of increased\ninterest. Recently, with technological advances such as robotic-assisted\nminimally invasive surgery (RMIS), new opportunities for objective and\nautomated assessment frameworks have arisen. In this paper, we applied machine\nlearning methods to automatically evaluate performance of the surgeon in RMIS.\nSix important movement features were used in the evaluation including\ncompletion time, path length, depth perception, speed, smoothness and\ncurvature. Different classification methods applied to discriminate expert and\nnovice surgeons. We test our method on real surgical data for suturing task and\ncompare the classification result with the ground truth data (obtained by\nmanual labeling). The experimental results show that the proposed framework can\nclassify surgical skill level with relatively high accuracy of 85.7%. This\nstudy demonstrates the ability of machine learning methods to automatically\nclassify expert and novice surgeons using movement features for different RMIS\ntasks. Due to the simplicity and generalizability of the introduced\nclassification method, it is easy to implement in existing trainers.\n","id":654}
{"Unnamed: 0.1":11655,"Unnamed: 0":11655.0,"anchor":"A Simple Approach to Multilingual Polarity Classification in Twitter","positive":"  Recently, sentiment analysis has received a lot of attention due to the\ninterest in mining opinions of social media users. Sentiment analysis consists\nin determining the polarity of a given text, i.e., its degree of positiveness\nor negativeness. Traditionally, Sentiment Analysis algorithms have been\ntailored to a specific language given the complexity of having a number of\nlexical variations and errors introduced by the people generating content. In\nthis contribution, our aim is to provide a simple to implement and easy to use\nmultilingual framework, that can serve as a baseline for sentiment analysis\ncontests, and as starting point to build new sentiment analysis systems. We\ncompare our approach in eight different languages, three of them have important\ninternational contests, namely, SemEval (English), TASS (Spanish), and\nSENTIPOLC (Italian). Within the competitions our approach reaches from medium\nto high positions in the rankings; whereas in the remaining languages our\napproach outperforms the reported results.\n","negative":"  This paper addresses the matching of short music audio snippets to the\ncorresponding pixel location in images of sheet music. A system is presented\nthat simultaneously learns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the sheet. It consists of\nan end-to-end multi-modal convolutional neural network that takes as input\nimages of sheet music and spectrograms of the respective audio snippets. It\nlearns to predict, for a given unseen audio snippet (covering approximately one\nbar of music), the corresponding position in the respective score line. Our\nresults suggest that with the use of (deep) neural networks -- which have\nproven to be powerful image processing models -- working with sheet music\nbecomes feasible and a promising future research direction.\n","id":655}
{"Unnamed: 0.1":11656,"Unnamed: 0":11656.0,"anchor":"Automatic time-series phenotyping using massive feature extraction","positive":"  Across a far-reaching diversity of scientific and industrial applications, a\ngeneral key problem involves relating the structure of time-series data to a\nmeaningful outcome, such as detecting anomalous events from sensor recordings,\nor diagnosing patients from physiological time-series measurements like heart\nrate or brain activity. Currently, researchers must devote considerable effort\nmanually devising, or searching for, properties of their time series that are\nsuitable for the particular analysis problem at hand. Addressing this\nnon-systematic and time-consuming procedure, here we introduce a new tool,\nhctsa, that selects interpretable and useful properties of time series\nautomatically, by comparing implementations over 7700 time-series features\ndrawn from diverse scientific literatures. Using two exemplar biological\napplications, we show how hctsa allows researchers to leverage decades of\ntime-series research to quantify and understand informative structure in their\ntime-series data.\n","negative":"  Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.\n","id":656}
{"Unnamed: 0.1":11657,"Unnamed: 0":11657.0,"anchor":"A Survey of Inductive Biases for Factorial Representation-Learning","positive":"  With the resurgence of interest in neural networks, representation learning\nhas re-emerged as a central focus in artificial intelligence. Representation\nlearning refers to the discovery of useful encodings of data that make\ndomain-relevant information explicit. Factorial representations identify\nunderlying independent causal factors of variation in data. A factorial\nrepresentation is compact and faithful, makes the causal factors explicit, and\nfacilitates human interpretation of data. Factorial representations support a\nvariety of applications, including the generation of novel examples, indexing\nand search, novelty detection, and transfer learning.\n  This article surveys various constraints that encourage a learning algorithm\nto discover factorial representations. I dichotomize the constraints in terms\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\nexploit assumptions about the environment, such as the statistical distribution\nof factor coefficients, assumptions about the perturbations a factor should be\ninvariant to (e.g. a representation of an object can be invariant to rotation,\ntranslation or scaling), and assumptions about how factors are combined to\nsynthesize an observation. Supervised inductive biases are constraints on the\nrepresentations based on additional information connected to observations.\nSupervisory labels come in variety of types, which vary in how strongly they\nconstrain the representation, how many factors are labeled, how many\nobservations are labeled, and whether or not we know the associations between\nthe constraints and the factors they are related to.\n  This survey brings together a wide variety of models that all touch on the\nproblem of learning factorial representations and lays out a framework for\ncomparing these models based on the strengths of the underlying supervised and\nunsupervised inductive biases.\n","negative":"  Recent work in the literature has shown experimentally that one can use the\nlower layers of a trained convolutional neural network (CNN) to model natural\ntextures. More interestingly, it has also been experimentally shown that only\none layer with random filters can also model textures although with less\nvariability. In this paper we ask the question as to why one layer CNNs with\nrandom filters are so effective in generating textures? We theoretically show\nthat one layer convolutional architectures (without a non-linearity) paired\nwith the an energy function used in previous literature, can in fact preserve\nand modulate frequency coefficients in a manner so that random weights and\npretrained weights will generate the same type of images. Based on the results\nof this analysis we question whether similar properties hold in the case where\none uses one convolution layer with a non-linearity. We show that in the case\nof ReLu non-linearity there are situations where only one input will give the\nminimum possible energy whereas in the case of no nonlinearity, there are\nalways infinite solutions that will give the minimum possible energy. Thus we\ncan show that in certain situations adding a ReLu non-linearity generates less\nvariable images.\n","id":657}
{"Unnamed: 0.1":11658,"Unnamed: 0":11658.0,"anchor":"Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme\n  under Weak Strong Convexity Assumption","positive":"  We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss.\n","negative":"  The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset.\n","id":658}
{"Unnamed: 0.1":11659,"Unnamed: 0":11659.0,"anchor":"Neural networks based EEG-Speech Models","positive":"  In this paper, we propose an end-to-end neural network (NN) based EEG-speech\n(NES) modeling framework, in which three network structures are developed to\nmap imagined EEG signals to phonemes. The proposed NES models incorporate a\nlanguage model based EEG feature extraction layer, an acoustic feature mapping\nlayer, and a restricted Boltzmann machine (RBM) based the feature learning\nlayer. The NES models can jointly realize the representation of multichannel\nEEG signals and the projection of acoustic speech signals. Among three proposed\nNES models, two augmented networks utilize spoken EEG signals as either bias or\ngate information to strengthen the feature learning and translation of imagined\nEEG signals. Experimental results show that all three proposed NES models\noutperform the baseline support vector machine (SVM) method on EEG-speech\nclassification. With respect to binary classification, our approach achieves\ncomparable results relative to deep believe network approach.\n","negative":"  External neural memory structures have recently become a popular tool for\nalgorithmic deep learning (Graves et al. 2014, Weston et al. 2014). These\nmodels generally utilize differentiable versions of traditional discrete\nmemory-access structures (random access, stacks, tapes) to provide the storage\nnecessary for computational tasks. In this work, we argue that these neural\nmemory systems lack specific structure important for relative indexing, and\npropose an alternative model, Lie-access memory, that is explicitly designed\nfor the neural setting. In this paradigm, memory is accessed using a continuous\nhead in a key-space manifold. The head is moved via Lie group actions, such as\nshifts or rotations, generated by a controller, and memory access is performed\nby linear smoothing in key space. We argue that Lie groups provide a natural\ngeneralization of discrete memory structures, such as Turing machines, as they\nprovide inverse and identity operators while maintaining differentiability. To\nexperiment with this approach, we implement a simplified Lie-access neural\nTuring machine (LANTM) with different Lie groups. We find that this approach is\nable to perform well on a range of algorithmic tasks.\n","id":659}
{"Unnamed: 0.1":11660,"Unnamed: 0":11660.0,"anchor":"Defensive Player Classification in the National Basketball Association","positive":"  The National Basketball Association(NBA) has expanded their data gathering\nand have heavily invested in new technologies to gather advanced performance\nmetrics on players. This expanded data set allows analysts to use unique\nperformance metrics in models to estimate and classify player performance.\nInstead of grouping players together based on physical attributes and positions\nplayed, analysts can group together players that play similar to each other\nbased on these tracked metrics. Existing methods for player classification have\ntypically used offensive metrics for clustering [1]. There have been attempts\nto classify players using past defensive metrics, but the lack of quality\nmetrics has not produced promising results. The classifications presented in\nthe paper use newly introduced defensive metrics to find different defensive\npositions for each player. Without knowing the number of categories that\nplayers can be cast into, Gaussian Mixture Models (GMM) can be applied to find\nthe optimal number of clusters. In the model presented, five different\ndefensive player types can be identified.\n","negative":"  We study the problem of combining multiple bandit algorithms (that is, online\nlearning algorithms with partial feedback) with the goal of creating a master\nalgorithm that performs almost as well as the best base algorithm if it were to\nbe run on its own. The main challenge is that when run with a master, base\nalgorithms unavoidably receive much less feedback and it is thus critical that\nthe master not starve a base algorithm that might perform uncompetitively\ninitially but would eventually outperform others if given enough feedback. We\naddress this difficulty by devising a version of Online Mirror Descent with a\nspecial mirror map together with a sophisticated learning rate scheme. We show\nthat this approach manages to achieve a more delicate balance between\nexploiting and exploring base algorithms than previous works yielding superior\nregret bounds.\n  Our results are applicable to many settings, such as multi-armed bandits,\ncontextual bandits, and convex bandits. As examples, we present two main\napplications. The first is to create an algorithm that enjoys worst-case\nrobustness while at the same time performing much better when the environment\nis relatively easy. The second is to create an algorithm that works\nsimultaneously under different assumptions of the environment, such as\ndifferent priors or different loss structures.\n","id":660}
{"Unnamed: 0.1":11661,"Unnamed: 0":11661.0,"anchor":"Deep Reinforcement Learning with Successor Features for Navigation\n  across Similar Environments","positive":"  In this paper we consider the problem of robot navigation in simple maze-like\nenvironments where the robot has to rely on its onboard sensors to perform the\nnavigation task. In particular, we are interested in solutions to this problem\nthat do not require localization, mapping or planning. Additionally, we require\nthat our solution can quickly adapt to new situations (e.g., changing\nnavigation goals and environments). To meet these criteria we frame this\nproblem as a sequence of related reinforcement learning tasks. We propose a\nsuccessor feature based deep reinforcement learning algorithm that can learn to\ntransfer knowledge from previously mastered navigation tasks to new problem\ninstances. Our algorithm substantially decreases the required learning time\nafter the first task instance has been solved, which makes it easily adaptable\nto changing environments. We validate our method in both simulated and real\nrobot experiments with a Robotino and compare it to a set of baseline methods\nincluding classical planning-based navigation.\n","negative":"  Approximations of Laplace-Beltrami operators on manifolds through graph\nLapla-cians have become popular tools in data analysis and machine learning.\nThese discretized operators usually depend on bandwidth parameters whose tuning\nremains a theoretical and practical problem. In this paper, we address this\nproblem for the unnormalized graph Laplacian by establishing an oracle\ninequality that opens the door to a well-founded data-driven procedure for the\nbandwidth selection. Our approach relies on recent results by Lacour and\nMassart [LM15] on the so-called Lepski's method.\n","id":661}
{"Unnamed: 0.1":11662,"Unnamed: 0":11662.0,"anchor":"Models, networks and algorithmic complexity","positive":"  I aim to show that models, classification or generating functions,\ninvariances and datasets are algorithmically equivalent concepts once properly\ndefined, and provide some concrete examples of them. I then show that a) neural\nnetworks (NNs) of different kinds can be seen to implement models, b) that\nperturbations of inputs and nodes in NNs trained to optimally implement simple\nmodels propagate strongly, c) that there is a framework in which recurrent,\ndeep and shallow networks can be seen to fall into a descriptive power\nhierarchy in agreement with notions from the theory of recursive functions. The\nmotivation for these definitions and following analysis lies in the context of\ncognitive neuroscience, and in particular in Ruffini (2016), where the concept\nof model is used extensively, as is the concept of algorithmic complexity.\n","negative":"  Ensembles of decision trees perform well on many problems, but are not\ninterpretable. In contrast to existing approaches in interpretability that\nfocus on explaining relationships between features and predictions, we propose\nan alternative approach to interpret tree ensemble classifiers by surfacing\nrepresentative points for each class -- prototypes. We introduce a new distance\nfor Gradient Boosted Tree models, and propose new, adaptive prototype selection\nmethods with theoretical guarantees, with the flexibility to choose a different\nnumber of prototypes in each class. We demonstrate our methods on random\nforests and gradient boosted trees, showing that the prototypes can perform as\nwell as or even better than the original tree ensemble when used as a\nnearest-prototype classifier. In a user study, humans were better at predicting\nthe output of a tree ensemble classifier when using prototypes than when using\nShapley values, a popular feature attribution method. Hence, prototypes present\na viable alternative to feature-based explanations for tree ensembles.\n","id":662}
{"Unnamed: 0.1":11663,"Unnamed: 0":11663.0,"anchor":"An Alternative Softmax Operator for Reinforcement Learning","positive":"  A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice.\n","negative":"  Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN.\n","id":663}
{"Unnamed: 0.1":11664,"Unnamed: 0":11664.0,"anchor":"A User Simulator for Task-Completion Dialogues","positive":"  Despite widespread interests in reinforcement-learning for task-oriented\ndialogue systems, several obstacles can frustrate research and development\nprogress. First, reinforcement learners typically require interaction with the\nenvironment, so conventional dialogue corpora cannot be used directly. Second,\neach task presents specific challenges, requiring separate corpus of\ntask-specific annotated data. Third, collecting and annotating human-machine or\nhuman-human conversations for task-oriented dialogues requires extensive domain\nknowledge. Because building an appropriate dataset can be both financially\ncostly and time-consuming, one popular approach is to build a user simulator\nbased upon a corpus of example dialogues. Then, one can train reinforcement\nlearning agents in an online fashion as they interact with the simulator.\nDialogue agents trained on these simulators can serve as an effective starting\npoint. Once agents master the simulator, they may be deployed in a real\nenvironment to interact with humans, and continue to be trained online. To ease\nempirical algorithmic comparisons in dialogues, this paper introduces a new,\npublicly available simulation framework, where our simulator, designed for the\nmovie-booking domain, leverages both rules and collected data. The simulator\nsupports two tasks: movie ticket booking and movie seeking. Finally, we\ndemonstrate several agents and detail the procedure to add and test your own\nagent in the proposed framework.\n","negative":"  In this paper, we extend the geometric descent method recently proposed by\nBubeck, Lee and Singh to tackle nonsmooth and strongly convex composite\nproblems. We prove that our proposed algorithm, dubbed geometric proximal\ngradient method (GeoPG), converges with a linear rate $(1-1\/\\sqrt{\\kappa})$ and\nthus achieves the optimal rate among first-order methods, where $\\kappa$ is the\ncondition number of the problem. Numerical results on linear regression and\nlogistic regression with elastic net regularization show that GeoPG compares\nfavorably with Nesterov's accelerated proximal gradient method, especially when\nthe problem is ill-conditioned.\n","id":664}
{"Unnamed: 0.1":11665,"Unnamed: 0":11665.0,"anchor":"Reinforcement Learning Using Quantum Boltzmann Machines","positive":"  We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs.\n","negative":"  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n","id":665}
{"Unnamed: 0.1":11666,"Unnamed: 0":11666.0,"anchor":"Mutual information for fitting deep nonlinear models","positive":"  Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer.\n","negative":"  We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity.\n","id":666}
{"Unnamed: 0.1":11667,"Unnamed: 0":11667.0,"anchor":"Exploiting sparsity to build efficient kernel based collaborative\n  filtering for top-N item recommendation","positive":"  The increasing availability of implicit feedback datasets has raised the\ninterest in developing effective collaborative filtering techniques able to\ndeal asymmetrically with unambiguous positive feedback and ambiguous negative\nfeedback. In this paper, we propose a principled kernel-based collaborative\nfiltering method for top-N item recommendation with implicit feedback. We\npresent an efficient implementation using the linear kernel, and we show how to\ngeneralize it to kernels of the dot product family preserving the efficiency.\nWe also investigate on the elements which influence the sparsity of a standard\ncosine kernel. This analysis shows that the sparsity of the kernel strongly\ndepends on the properties of the dataset, in particular on the long tail\ndistribution. We compare our method with state-of-the-art algorithms achieving\ngood results both in terms of efficiency and effectiveness.\n","negative":"  Chord recognition systems depend on robust feature extraction pipelines.\nWhile these pipelines are traditionally hand-crafted, recent advances in\nend-to-end machine learning have begun to inspire researchers to explore\ndata-driven methods for such tasks. In this paper, we present a chord\nrecognition system that uses a fully convolutional deep auditory model for\nfeature extraction. The extracted features are processed by a Conditional\nRandom Field that decodes the final chord sequence. Both processing stages are\ntrained automatically and do not require expert knowledge for optimising\nparameters. We show that the learned auditory system extracts musically\ninterpretable features, and that the proposed chord recognition system achieves\nresults on par or better than state-of-the-art algorithms.\n","id":667}
{"Unnamed: 0.1":11668,"Unnamed: 0":11668.0,"anchor":"Towards Wide Learning: Experiments in Healthcare","positive":"  In this paper, a Wide Learning architecture is proposed that attempts to\nautomate the feature engineering portion of the machine learning (ML) pipeline.\nFeature engineering is widely considered as the most time consuming and expert\nknowledge demanding portion of any ML task. The proposed feature recommendation\napproach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016\ndataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure\nclassification dataset of photoplethysmogram (PPG) signals and c) an emotion\nclassification dataset of PPG signals. While the proposed method beats the\nstate of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the\naccuracy level of the winner of PhysioNet Challenge 2016. In all cases, the\neffort to reach a satisfactory performance was drastically less (a few days)\nthan manual feature engineering.\n","negative":"  Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning.\n","id":668}
{"Unnamed: 0.1":11669,"Unnamed: 0":11669.0,"anchor":"Machine Learning, Linear and Bayesian Models for Logistic Regression in\n  Failure Detection Problems","positive":"  In this work, we study the use of logistic regression in manufacturing\nfailures detection. As a data set for the analysis, we used the data from\nKaggle competition Bosch Production Line Performance. We considered the use of\nmachine learning, linear and Bayesian models. For machine learning approach, we\nanalyzed XGBoost tree based classifier to obtain high scored classification.\nUsing the generalized linear model for logistic regression makes it possible to\nanalyze the influence of the factors under study. The Bayesian approach for\nlogistic regression gives the statistical distribution for the parameters of\nthe model. It can be useful in the probabilistic analysis, e.g. risk\nassessment.\n","negative":"  Correntropy is a second order statistical measure in kernel space, which has\nbeen successfully applied in robust learning and signal processing. In this\npaper, we define a nonsecond order statistical measure in kernel space, called\nthe kernel mean-p power error (KMPE), including the correntropic loss (CLoss)\nas a special case. Some basic properties of KMPE are presented. In particular,\nwe apply the KMPE to extreme learning machine (ELM) and principal component\nanalysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and\nPCA-KMPE. Experimental results on synthetic and benchmark data show that the\ndeveloped algorithms can achieve consistently better performance when compared\nwith some existing methods.\n","id":669}
{"Unnamed: 0.1":11670,"Unnamed: 0":11670.0,"anchor":"Learning to predict where to look in interactive environments using deep\n  recurrent q-learning","positive":"  Bottom-Up (BU) saliency models do not perform well in complex interactive\nenvironments where humans are actively engaged in tasks (e.g., sandwich making\nand playing the video games). In this paper, we leverage Reinforcement Learning\n(RL) to highlight task-relevant locations of input frames. We propose a soft\nattention mechanism combined with the Deep Q-Network (DQN) model to teach an RL\nagent how to play a game and where to look by focusing on the most pertinent\nparts of its visual input. Our evaluations on several Atari 2600 games show\nthat the soft attention based model could predict fixation locations\nsignificantly better than bottom-up models such as Itti-Kochs saliency and\nGraph-Based Visual Saliency (GBVS) models.\n","negative":"  We present online prediction methods for time series that let us explicitly\nhandle nonstationary artifacts (e.g. trend and seasonality) present in most\nreal time series. Specifically, we show that applying appropriate\ntransformations to such time series before prediction can lead to improved\ntheoretical and empirical prediction performance. Moreover, since these\ntransformations are usually unknown, we employ the learning with experts\nsetting to develop a fully online method (NonSTOP-NonSTationary Online\nPrediction) for predicting nonstationary time series. This framework allows for\nseasonality and\/or other trends in univariate time series and cointegration in\nmultivariate time series. Our algorithms and regret analysis subsume recent\nrelated work while significantly expanding the applicability of such methods.\nFor all the methods, we provide sub-linear regret bounds using relaxed\nassumptions. The theoretical guarantees do not fully capture the benefits of\nthe transformations, thus we provide a data-dependent analysis of the\nfollow-the-leader algorithm that provides insight into the success of using\nsuch transformations. We support all of our results with experiments on\nsimulated and real data.\n","id":670}
{"Unnamed: 0.1":11671,"Unnamed: 0":11671.0,"anchor":"A new recurrent neural network based predictive model for Faecal\n  Calprotectin analysis: A retrospective study","positive":"  Faecal Calprotectin (FC) is a surrogate marker for intestinal inflammation,\ntermed Inflammatory Bowel Disease (IBD), but not for cancer. In this\nretrospective study of 804 patients, an enhanced benchmark predictive model for\nanalyzing FC is developed, based on a novel state-of-the-art Echo State Network\n(ESN), an advanced dynamic recurrent neural network which implements a\nbiologically plausible architecture, and a supervised learning mechanism. The\nproposed machine learning driven predictive model is benchmarked against a\nconventional logistic regression model, demonstrating statistically significant\nperformance improvements.\n","negative":"  We study the off-policy evaluation problem---estimating the value of a target\npolicy using data collected by another policy---under the contextual bandit\nmodel. We consider the general (agnostic) setting without access to a\nconsistent model of rewards and establish a minimax lower bound on the mean\nsquared error (MSE). The bound is matched up to constants by the inverse\npropensity scoring (IPS) and doubly robust (DR) estimators. This highlights the\ndifficulty of the agnostic contextual setting, in contrast with multi-armed\nbandits and contextual bandits with access to a consistent reward model, where\nIPS is suboptimal. We then propose the SWITCH estimator, which can use an\nexisting reward model (not necessarily consistent) to achieve a better\nbias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and\ndemonstrate its benefits empirically on a diverse collection of data sets,\noften outperforming prior work by orders of magnitude.\n","id":671}
{"Unnamed: 0.1":11672,"Unnamed: 0":11672.0,"anchor":"EgoTransfer: Transferring Motion Across Egocentric and Exocentric\n  Domains using Deep Neural Networks","positive":"  Mirror neurons have been observed in the primary motor cortex of primate\nspecies, in particular in humans and monkeys. A mirror neuron fires when a\nperson performs a certain action, and also when he observes the same action\nbeing performed by another person. A crucial step towards building fully\nautonomous intelligent systems with human-like learning abilities is the\ncapability in modeling the mirror neuron. On one hand, the abundance of\negocentric cameras in the past few years has offered the opportunity to study a\nlot of vision problems from the first-person perspective. A great deal of\ninteresting research has been done during the past few years, trying to explore\nvarious computer vision tasks from the perspective of the self. On the other\nhand, videos recorded by traditional static cameras, capture humans performing\ndifferent actions from an exocentric third-person perspective. In this work, we\ntake the first step towards relating motion information across these two\nperspectives. We train models that predict motion in an egocentric view, by\nobserving it from an exocentric view, and vice versa. This allows models to\npredict how an egocentric motion would look like from outside. To do so, we\ntrain linear and nonlinear models and evaluate their performance in terms of\nretrieving the egocentric (exocentric) motion features, while having access to\nan exocentric (egocentric) motion feature. Our experimental results demonstrate\nthat motion information can be successfully transferred across the two views.\n","negative":"  The goal of predictive maintenance is to forecast the occurrence of faults of\nan appliance, in order to proactively take the necessary actions to ensure its\navailability. In many application scenarios, predictive maintenance is applied\nto a set of homogeneous appliances. In this paper, we firstly review taxonomies\nand main methodologies currently used for condition-based maintenance;\nsecondly, we argue that the mutual dissimilarities of the behaviours of all\nappliances of this set (the \"cohort\") can be exploited to detect upcoming\nfaults. Specifically, inspired by dissimilarity-based representations, we\npropose a novel machine learning approach based on the analysis of concurrent\nmutual differences of the measurements coming from the cohort. We evaluate our\nmethod over one year of historical data from a cohort of 17 HVAC (Heating,\nVentilation and Air Conditioning) systems installed in an Italian hospital. We\nshow that certain kinds of faults can be foreseen with an accuracy, measured in\nterms of area under the ROC curve, as high as 0.96.\n","id":672}
{"Unnamed: 0.1":11673,"Unnamed: 0":11673.0,"anchor":"Building Diversified Multiple Trees for Classification in High\n  Dimensional Noisy Biomedical Data","positive":"  It is common that a trained classification model is applied to the operating\ndata that is deviated from the training data because of noise. This paper\ndemonstrates that an ensemble classifier, Diversified Multiple Tree (DMT), is\nmore robust in classifying noisy data than other widely used ensemble methods.\nDMT is tested on three real world biomedical data sets from different\nlaboratories in comparison with four benchmark ensemble classifiers.\nExperimental results show that DMT is significantly more accurate than other\nbenchmark ensemble classifiers on noisy test data. We also discuss a limitation\nof DMT and its possible variations.\n","negative":"  We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n","id":673}
{"Unnamed: 0.1":11674,"Unnamed: 0":11674.0,"anchor":"Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification","positive":"  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods requires great effort to annotate the\ntraining data by costly manual labeling and specialized computational models to\ndetect these annotations during test. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\nfor labeling a set of instances\/patches, we propose end-to-end trained deep\nmulti-instance networks for mass classification based on whole mammogram\nwithout the aforementioned costly need to annotate the training data. We\nexplore three different schemes to construct deep multi-instance networks for\nwhole mammogram classification. Experimental results on the INbreast dataset\ndemonstrate the robustness of proposed deep networks compared to previous work\nusing segmentation and detection annotations in the training.\n","negative":"  We consider a declarative framework for machine learning where concepts and\nhypotheses are defined by formulas of a logic over some background structure.\nWe show that within this framework, concepts defined by first-order formulas\nover a background structure of at most polylogarithmic degree can be learned in\npolylogarithmic time in the \"probably approximately correct\" learning sense.\n","id":674}
{"Unnamed: 0.1":11675,"Unnamed: 0":11675.0,"anchor":"Adversarial Deep Structural Networks for Mammographic Mass Segmentation","positive":"  Mass segmentation is an important task in mammogram analysis, providing\neffective morphological features and regions of interest (ROI) for mass\ndetection and classification. Inspired by the success of using deep\nconvolutional features for natural image analysis and conditional random fields\n(CRF) for structural learning, we propose an end-to-end network for\nmammographic mass segmentation. The network employs a fully convolutional\nnetwork (FCN) to model potential function, followed by a CRF to perform\nstructural learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with position priori for the task. Due to the\nsmall size of mammogram datasets, we use adversarial training to control\nover-fitting. Four models with different convolutional kernels are further\nfused to improve the segmentation results. Experimental results on two public\ndatasets, INbreast and DDSM-BCRP, show that our end-to-end network combined\nwith adversarial training achieves the-state-of-the-art results.\n","negative":"  We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.\n","id":675}
{"Unnamed: 0.1":11676,"Unnamed: 0":11676.0,"anchor":"An IoT Endpoint System-on-Chip for Secure and Energy-Efficient\n  Near-Sensor Analytics","positive":"  Near-sensor data analytics is a promising direction for IoT endpoints, as it\nminimizes energy spent on communication and reduces network load - but it also\nposes security concerns, as valuable data is stored or sent over the network at\nvarious stages of the analytics pipeline. Using encryption to protect sensitive\ndata at the boundary of the on-chip analytics engine is a way to address data\nsecurity issues. To cope with the combined workload of analytics and encryption\nin a tight power envelope, we propose Fulmine, a System-on-Chip based on a\ntightly-coupled multi-core cluster augmented with specialized blocks for\ncompute-intensive data processing and encryption functions, supporting software\nprogrammability for regular computing tasks. The Fulmine SoC, fabricated in\n65nm technology, consumes less than 20mW on average at 0.8V achieving an\nefficiency of up to 70pJ\/B in encryption, 50pJ\/px in convolution, or up to\n25MIPS\/mW in software. As a strong argument for real-life flexible application\nof our platform, we show experimental results for three secure analytics use\ncases: secure autonomous aerial surveillance with a state-of-the-art deep CNN\nconsuming 3.16pJ per equivalent RISC op; local CNN-based face detection with\nsecured remote recognition in 5.74pJ\/op; and seizure detection with encrypted\ndata collection from EEG within 12.7pJ\/op.\n","negative":"  Ensemble learning has been widely employed by mobile applications, ranging\nfrom environmental sensing to activity recognitions. One of the fundamental\nissue in ensemble learning is the trade-off between classification accuracy and\ncomputational costs, which is the goal of ensemble pruning. During\ncrowdsourcing, the centralized aggregator releases ensemble learning models to\na large number of mobile participants for task evaluation or as the\ncrowdsourcing learning results, while different participants may seek for\ndifferent levels of the accuracy-cost trade-off. However, most of existing\nensemble pruning approaches consider only one identical level of such\ntrade-off. In this study, we present an efficient ensemble pruning framework\nfor personalized accuracy-cost trade-offs via multi-objective optimization.\nSpecifically, for the commonly used linear-combination style of the trade-off,\nwe provide an objective-mixture optimization to further reduce the number of\nensemble candidates. Experimental results show that our framework is highly\nefficient for personalized ensemble pruning, and achieves much better pruning\nperformance with objective-mixture optimization when compared to state-of-art\napproaches.\n","id":676}
{"Unnamed: 0.1":11677,"Unnamed: 0":11677.0,"anchor":"Sample-efficient Deep Reinforcement Learning for Dialog Control","positive":"  Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs\/episodes required by about a third, vs.\nstandard policy gradient methods.\n","negative":"  There has been considerable work on improving popular clustering algorithm\n`K-means' in terms of mean squared error (MSE) and speed, both. However, most\nof the k-means variants tend to compute distance of each data point to each\ncluster centroid for every iteration. We propose a fast heuristic to overcome\nthis bottleneck with only marginal increase in MSE. We observe that across all\niterations of K-means, a data point changes its membership only among a small\nsubset of clusters. Our heuristic predicts such clusters for each data point by\nlooking at nearby clusters after the first iteration of k-means. We augment\nwell known variants of k-means with our heuristic to demonstrate effectiveness\nof our heuristic. For various synthetic and real-world datasets, our heuristic\nachieves speed-up of up-to 3 times when compared to efficient variants of\nk-means.\n","id":677}
{"Unnamed: 0.1":11678,"Unnamed: 0":11678.0,"anchor":"Inexact Proximal Gradient Methods for Non-convex and Non-smooth\n  Optimization","positive":"  In machine learning research, the proximal gradient methods are popular for\nsolving various optimization problems with non-smooth regularization. Inexact\nproximal gradient methods are extremely important when exactly solving the\nproximal operator is time-consuming, or the proximal operator does not have an\nanalytic solution. However, existing inexact proximal gradient methods only\nconsider convex problems. The knowledge of inexact proximal gradient methods in\nthe non-convex setting is very limited. % Moreover, for some machine learning\nmodels, there is still no proposed solver for exactly solving the proximal\noperator. To address this challenge, in this paper, we first propose three\ninexact proximal gradient algorithms, including the basic version and\nNesterov's accelerated version. After that, we provide the theoretical analysis\nto the basic and Nesterov's accelerated versions. The theoretical results show\nthat our inexact proximal gradient algorithms can have the same convergence\nrates as the ones of exact proximal gradient algorithms in the non-convex\nsetting.\n  Finally, we show the applications of our inexact proximal gradient algorithms\non three representative non-convex learning problems. All experimental results\nconfirm the superiority of our new inexact proximal gradient algorithms.\n","negative":"  Event-driven programming frameworks, such as Android, are based on components\nwith asynchronous interfaces. The protocols for interacting with these\ncomponents can often be described by finite-state machines we dub *callback\ntypestates*. Callback typestates are akin to classical typestates, with the\ndifference that their outputs (callbacks) are produced asynchronously. While\nuseful, these specifications are not commonly available, because writing them\nis difficult and error-prone.\n  Our goal is to make the task of producing callback typestates significantly\neasier. We present a callback typestate assistant tool, DroidStar, that\nrequires only limited user interaction to produce a callback typestate. Our\napproach is based on an active learning algorithm, L*. We improved the\nscalability of equivalence queries (a key component of L*), thus making active\nlearning tractable on the Android system.\n  We use DroidStar to learn callback typestates for Android classes both for\ncases where one is already provided by the documentation, and for cases where\nthe documentation is unclear. The results show that DroidStar learns callback\ntypestates accurately and efficiently. Moreover, in several cases, the\nsynthesized callback typestates uncovered surprising and undocumented\nbehaviors.\n","id":678}
{"Unnamed: 0.1":11679,"Unnamed: 0":11679.0,"anchor":"Self-Correcting Models for Model-Based Reinforcement Learning","positive":"  When an agent cannot represent a perfectly accurate model of its\nenvironment's dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model's ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.\n","negative":"  There has been a growing interest for Wireless Distributed Computing (WDC),\nwhich leverages collaborative computing over multiple wireless devices. WDC\nenables complex applications that a single device cannot support individually.\nHowever, the problem of assigning tasks over multiple devices becomes\nchallenging in the dynamic environments encountered in real-world settings,\nconsidering that the resource availability and channel conditions change over\ntime in unpredictable ways due to mobility and other factors. In this paper, we\nformulate a task assignment problem as an online learning problem using an\nadversarial multi-armed bandit framework. We propose MABSTA, a novel online\nlearning algorithm that learns the performance of unknown devices and channel\nqualities continually through exploratory probing and makes task assignment\ndecisions by exploiting the gained knowledge. For maximal adaptability, MABSTA\nis designed to make no stochastic assumption about the environment. We analyze\nit mathematically and provide a worst-case performance guarantee for any\ndynamic environment. We also compare it with the optimal offline policy as well\nas other baselines via emulations on trace-data obtained from a wireless IoT\ntestbed, and show that it offers competitive and robust performance in all\ncases. To the best of our knowledge, MABSTA is the first online algorithm in\nthis domain of task assignment problems and provides provable performance\nguarantee.\n","id":679}
{"Unnamed: 0.1":11680,"Unnamed: 0":11680.0,"anchor":"Quantization and Training of Low Bit-Width Convolutional Neural Networks\n  for Object Detection","positive":"  We present LBW-Net, an efficient optimization based method for quantization\nand training of the low bit-width convolutional neural networks (CNNs).\nSpecifically, we quantize the weights to zero or powers of two by minimizing\nthe Euclidean distance between full-precision weights and quantized weights\nduring backpropagation. We characterize the combinatorial nature of the low\nbit-width quantization problem. For 2-bit (ternary) CNNs, the quantization of\n$N$ weights can be done by an exact formula in $O(N\\log N)$ complexity. When\nthe bit-width is three and above, we further propose a semi-analytical\nthresholding scheme with a single free parameter for quantization that is\ncomputationally inexpensive. The free parameter is further determined by\nnetwork retraining and object detection tests. LBW-Net has several desirable\nadvantages over full-precision CNNs, including considerable memory savings,\nenergy efficiency, and faster deployment. Our experiments on PASCAL VOC dataset\nshow that compared with its 32-bit floating-point counterpart, the performance\nof the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can\neven do better in some real world visual scenes, while empirically enjoying\nmore than 4$\\times$ faster deployment.\n","negative":"  We start with an overview of a class of submodular functions called SCMMs\n(sums of concave composed with non-negative modular functions plus a final\narbitrary modular). We then define a new class of submodular functions we call\n{\\em deep submodular functions} or DSFs. We show that DSFs are a flexible\nparametric family of submodular functions that share many of the properties and\nadvantages of deep neural networks (DNNs). DSFs can be motivated by considering\na hierarchy of descriptive concepts over ground elements and where one wishes\nto allow submodular interaction throughout this hierarchy. Results in this\npaper show that DSFs constitute a strictly larger class of submodular functions\nthan SCMMs. We show that, for any integer $k>0$, there are $k$-layer DSFs that\ncannot be represented by a $k'$-layer DSF for any $k'<k$. This implies that,\nlike DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs\nstrictly increase with depth. Despite this, we show (using a \"backpropagation\"\nlike method) that DSFs, even with arbitrarily large $k$, do not comprise all\nsubmodular functions. In offering the above results, we also define the notion\nof an antitone superdifferential of a concave function and show how this\nrelates to submodular functions (in general), DSFs (in particular), negative\nsecond-order partial derivatives, continuous submodularity, and concave\nextensions. To further motivate our analysis, we provide various special case\nresults from matroid theory, comparing DSFs with forms of matroid rank, in\nparticular the laminar matroid. Lastly, we discuss strategies to learn DSFs,\nand define the classes of deep supermodular functions, deep difference of\nsubmodular functions, and deep multivariate submodular functions, and discuss\nwhere these can be useful in applications.\n","id":680}
{"Unnamed: 0.1":11681,"Unnamed: 0":11681.0,"anchor":"On Random Weights for Texture Generation in One Layer Neural Networks","positive":"  Recent work in the literature has shown experimentally that one can use the\nlower layers of a trained convolutional neural network (CNN) to model natural\ntextures. More interestingly, it has also been experimentally shown that only\none layer with random filters can also model textures although with less\nvariability. In this paper we ask the question as to why one layer CNNs with\nrandom filters are so effective in generating textures? We theoretically show\nthat one layer convolutional architectures (without a non-linearity) paired\nwith the an energy function used in previous literature, can in fact preserve\nand modulate frequency coefficients in a manner so that random weights and\npretrained weights will generate the same type of images. Based on the results\nof this analysis we question whether similar properties hold in the case where\none uses one convolution layer with a non-linearity. We show that in the case\nof ReLu non-linearity there are situations where only one input will give the\nminimum possible energy whereas in the case of no nonlinearity, there are\nalways infinite solutions that will give the minimum possible energy. Thus we\ncan show that in certain situations adding a ReLu non-linearity generates less\nvariable images.\n","negative":"  Multivariate Pattern (MVP) classification holds enormous potential for\ndecoding visual stimuli in the human brain by employing task-based fMRI data\nsets. There is a wide range of challenges in the MVP techniques, i.e.\ndecreasing noise and sparsity, defining effective regions of interest (ROIs),\nvisualizing results, and the cost of brain studies. In overcoming these\nchallenges, this paper proposes a novel model of neural representation, which\ncan automatically detect the active regions for each visual stimulus and then\nutilize these anatomical regions for visualizing and analyzing the functional\nactivities. Therefore, this model provides an opportunity for neuroscientists\nto ask this question: what is the effect of a stimulus on each of the detected\nregions instead of just study the fluctuation of voxels in the manually\nselected ROIs. Moreover, our method introduces analyzing snapshots of brain\nimage for decreasing sparsity rather than using the whole of fMRI time series.\nFurther, a new Gaussian smoothing method is proposed for removing noise of\nvoxels in the level of ROIs. The proposed method enables us to combine\ndifferent fMRI data sets for reducing the cost of brain studies. Experimental\nstudies on 4 visual categories (words, consonants, objects and nonsense photos)\nconfirm that the proposed method achieves superior performance to\nstate-of-the-art methods.\n","id":681}
{"Unnamed: 0.1":11682,"Unnamed: 0":11682.0,"anchor":"Hierarchical Partitioning of the Output Space in Multi-label Data","positive":"  Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning\nalgorithm that breaks the initial learning task to several, easier sub-tasks by\nfirst constructing a hierarchy of labels from a given label set and secondly\nemploying a given base multi-label classifier (MLC) to the resulting\nsub-problems. The primary goal is to effectively address class imbalance and\nscalability issues that often arise in real-world multi-label classification\nproblems. In this work, we present the general setup for a HOMER model and a\nsimple extension of the algorithm that is suited for MLCs that output rankings.\nFurthermore, we provide a detailed analysis of the properties of the algorithm,\nboth from an aspect of effectiveness and computational complexity. A secondary\ncontribution involves the presentation of a balanced variant of the k means\nalgorithm, which serves in the first step of the label hierarchy construction.\nWe conduct extensive experiments on six real-world datasets, studying\nempirically HOMER's parameters and providing examples of instantiations of the\nalgorithm with different clustering approaches and MLCs, The empirical results\ndemonstrate a significant improvement over the given base MLC.\n","negative":"  We prove several results giving new and stronger connections between\nlearning, circuit lower bounds and pseudorandomness. Among other results, we\nshow a generic learning speedup lemma, equivalences between various learning\nmodels in the exponential time and subexponential time regimes, a dichotomy\nbetween learning and pseudorandomness, consequences of non-trivial learning for\ncircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,\nand NC$^1$-hardness for the Minimum Circuit Size Problem.\n","id":682}
{"Unnamed: 0.1":11683,"Unnamed: 0":11683.0,"anchor":"A recurrent neural network without chaos","positive":"  We introduce an exceptionally simple gated recurrent neural network (RNN)\nthat achieves performance comparable to well-known gated architectures, such as\nLSTMs and GRUs, on the word-level language modeling task. We prove that our\nmodel has simple, predicable and non-chaotic dynamics. This stands in stark\ncontrast to more standard gated architectures, whose underlying dynamical\nsystems exhibit chaotic behavior.\n","negative":"  The majority of online display ads are served through real-time bidding (RTB)\n--- each ad display impression is auctioned off in real-time when it is just\nbeing generated from a user visit. To place an ad automatically and optimally,\nit is critical for advertisers to devise a learning algorithm to cleverly bid\nan ad impression in real-time. Most previous works consider the bid decision as\na static optimization problem of either treating the value of each impression\nindependently or setting a bid price to each segment of ad volume. However, the\nbidding for a given ad campaign would repeatedly happen during its life span\nbefore the budget runs out. As such, each bid is strategically correlated by\nthe constrained budget and the overall effectiveness of the campaign (e.g., the\nrewards from generated clicks), which is only observed after the campaign has\ncompleted. Thus, it is of great interest to devise an optimal bidding strategy\nsequentially so that the campaign budget can be dynamically allocated across\nall the available impressions on the basis of both the immediate and future\nrewards. In this paper, we formulate the bid decision process as a\nreinforcement learning problem, where the state space is represented by the\nauction information and the campaign's real-time parameters, while an action is\nthe bid price to set. By modeling the state transition via auction competition,\nwe build a Markov Decision Process framework for learning the optimal bidding\npolicy to optimize the advertising performance in the dynamic real-time bidding\nenvironment. Furthermore, the scalability problem from the large real-world\nauction volume and campaign budget is well handled by state value approximation\nusing neural networks.\n","id":683}
{"Unnamed: 0.1":11684,"Unnamed: 0":11684.0,"anchor":"Corralling a Band of Bandit Algorithms","positive":"  We study the problem of combining multiple bandit algorithms (that is, online\nlearning algorithms with partial feedback) with the goal of creating a master\nalgorithm that performs almost as well as the best base algorithm if it were to\nbe run on its own. The main challenge is that when run with a master, base\nalgorithms unavoidably receive much less feedback and it is thus critical that\nthe master not starve a base algorithm that might perform uncompetitively\ninitially but would eventually outperform others if given enough feedback. We\naddress this difficulty by devising a version of Online Mirror Descent with a\nspecial mirror map together with a sophisticated learning rate scheme. We show\nthat this approach manages to achieve a more delicate balance between\nexploiting and exploring base algorithms than previous works yielding superior\nregret bounds.\n  Our results are applicable to many settings, such as multi-armed bandits,\ncontextual bandits, and convex bandits. As examples, we present two main\napplications. The first is to create an algorithm that enjoys worst-case\nrobustness while at the same time performing much better when the environment\nis relatively easy. The second is to create an algorithm that works\nsimultaneously under different assumptions of the environment, such as\ndifferent priors or different loss structures.\n","negative":"  Optimization problems with rank constraints appear in many diverse fields\nsuch as control, machine learning and image analysis. Since the rank constraint\nis non-convex, these problems are often approximately solved via convex\nrelaxations. Nuclear norm regularization is the prevailing convexifying\ntechnique for dealing with these types of problem. This paper introduces a\nfamily of low-rank inducing norms and regularizers which includes the nuclear\nnorm as a special case. A posteriori guarantees on solving an underlying rank\nconstrained optimization problem with these convex relaxations are provided. We\nevaluate the performance of the low-rank inducing norms on three matrix\ncompletion problems. In all examples, the nuclear norm heuristic is\noutperformed by convex relaxations based on other low-rank inducing norms. For\ntwo of the problems there exist low-rank inducing norms that succeed in\nrecovering the partially unknown matrix, while the nuclear norm fails. These\nlow-rank inducing norms are shown to be representable as semi-definite\nprograms. Moreover, these norms have cheaply computable proximal mappings,\nwhich makes it possible to also solve problems of large size using first-order\nmethods.\n","id":684}
{"Unnamed: 0.1":11685,"Unnamed: 0":11685.0,"anchor":"VAST : The Virtual Acoustic Space Traveler Dataset","positive":"  This paper introduces a new paradigm for sound source lo-calization referred\nto as virtual acoustic space traveling (VAST) and presents a first dataset\ndesigned for this purpose. Existing sound source localization methods are\neither based on an approximate physical model (physics-driven) or on a\nspecific-purpose calibration set (data-driven). With VAST, the idea is to learn\na mapping from audio features to desired audio properties using a massive\ndataset of simulated room impulse responses. This virtual dataset is designed\nto be maximally representative of the potential audio scenes that the\nconsidered system may be evolving in, while remaining reasonably compact. We\nshow that virtually-learned mappings on this dataset generalize to real data,\novercoming some intrinsic limitations of traditional binaural sound\nlocalization methods based on time differences of arrival.\n","negative":"  We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes.\n","id":685}
{"Unnamed: 0.1":11686,"Unnamed: 0":11686.0,"anchor":"Simple Black-Box Adversarial Perturbations for Deep Networks","positive":"  Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks.\n","negative":"  We study matrix sketching methods for regularized variants of linear\nregression, low rank approximation, and canonical correlation analysis. Our\nmain focus is on sketching techniques which preserve the objective function\nvalue for regularized problems, which is an area that has remained largely\nunexplored. We study regularization both in a fairly broad setting, and in the\nspecific context of the popular and widely used technique of ridge\nregularization; for the latter, as applied to each of these problems, we show\nalgorithmic resource bounds in which the {\\em statistical dimension} appears in\nplaces where in previous bounds the rank would appear. The statistical\ndimension is always smaller than the rank, and decreases as the amount of\nregularization increases. In particular, for the ridge low-rank approximation\nproblem $\\min_{Y,X} \\lVert YX - A \\rVert_F^2 + \\lambda \\lVert Y\\rVert_F^2 +\n\\lambda\\lVert X \\rVert_F^2$, where $Y\\in\\mathbb{R}^{n\\times k}$ and\n$X\\in\\mathbb{R}^{k\\times d}$, we give an approximation algorithm needing \\[\nO(\\mathtt{nnz}(A)) + \\tilde{O}((n+d)\\varepsilon^{-1}k \\min\\{k,\n\\varepsilon^{-1}\\mathtt{sd}_\\lambda(Y^*)\\})+\n\\mathtt{poly}(\\mathtt{sd}_\\lambda(Y^*) \\varepsilon^{-1}) \\] time, where\n$s_{\\lambda}(Y^*)\\le k$ is the statistical dimension of $Y^*$, $Y^*$ is an\noptimal $Y$, $\\varepsilon$ is an error parameter, and $\\mathtt{nnz}(A)$ is the\nnumber of nonzero entries of $A$.This is faster than prior work, even when\n$\\lambda=0$.\n  We also study regularization in a much more general setting. For example, we\nobtain sketching-based algorithms for the low-rank approximation problem\n$\\min_{X,Y} \\lVert YX - A \\rVert_F^2 + f(Y,X)$ where $f(\\cdot,\\cdot)$ is a\nregularizing function satisfying some very general conditions (chiefly,\ninvariance under orthogonal transformations).\n","id":686}
{"Unnamed: 0.1":11687,"Unnamed: 0":11687.0,"anchor":"Computing Human-Understandable Strategies","positive":"  Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans.\n","negative":"  Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.\n","id":687}
{"Unnamed: 0.1":11688,"Unnamed: 0":11688.0,"anchor":"Learning Features by Watching Objects Move","positive":"  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n","negative":"  We describe a method to train spiking deep networks that can be run using\nleaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for\nspiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012\nbenchmark. Our method for transforming deep artificial neural networks into\nspiking networks is scalable and works with a wide range of neural\nnonlinearities. We achieve these results by softening the neural response\nfunction, such that its derivative remains bounded, and by training the network\nwith noise to provide robustness against the variability introduced by spikes.\nOur analysis shows that implementations of these networks on neuromorphic\nhardware will be many times more power-efficient than the equivalent\nnon-spiking networks on traditional hardware.\n","id":688}
{"Unnamed: 0.1":11689,"Unnamed: 0":11689.0,"anchor":"Randomized Clustered Nystrom for Large-Scale Kernel Machines","positive":"  The Nystrom method has been popular for generating the low-rank approximation\nof kernel matrices that arise in many machine learning problems. The\napproximation quality of the Nystrom method depends crucially on the number of\nselected landmark points and the selection procedure. In this paper, we present\na novel algorithm to compute the optimal Nystrom low-approximation when the\nnumber of landmark points exceed the target rank. Moreover, we introduce a\nrandomized algorithm for generating landmark points that is scalable to\nlarge-scale data sets. The proposed method performs K-means clustering on\nlow-dimensional random projections of a data set and, thus, leads to\nsignificant savings for high-dimensional data sets. Our theoretical results\ncharacterize the tradeoffs between the accuracy and efficiency of our proposed\nmethod. Extensive experiments demonstrate the competitive performance as well\nas the efficiency of our proposed method.\n","negative":"  Model-free deep reinforcement learning (RL) methods have been successful in a\nwide variety of simulated domains. However, a major obstacle facing deep RL in\nthe real world is their high sample complexity. Batch policy gradient methods\noffer stable learning, but at the cost of high variance, which often requires\nlarge batches. TD-style methods, such as off-policy actor-critic and\nQ-learning, are more sample-efficient but biased, and often require costly\nhyperparameter sweeps to stabilize. In this work, we aim to develop methods\nthat combine the stability of policy gradients with the efficiency of\noff-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor\nexpansion of the off-policy critic as a control variate. Q-Prop is both sample\nefficient and stable, and effectively combines the benefits of on-policy and\noff-policy methods. We analyze the connection between Q-Prop and existing\nmodel-free algorithms, and use control variate theory to derive two variants of\nQ-Prop with conservative and aggressive adaptation. We show that conservative\nQ-Prop provides substantial gains in sample efficiency over trust region policy\noptimization (TRPO) with generalized advantage estimation (GAE), and improves\nstability over deep deterministic policy gradient (DDPG), the state-of-the-art\non-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control\nenvironments.\n","id":689}
{"Unnamed: 0.1":11690,"Unnamed: 0":11690.0,"anchor":"Parallelized Tensor Train Learning of Polynomial Classifiers","positive":"  In pattern classification, polynomial classifiers are well-studied methods as\nthey are capable of generating complex decision surfaces. Unfortunately, the\nuse of multivariate polynomials is limited to kernels as in support vector\nmachines, because polynomials quickly become impractical for high-dimensional\nproblems. In this paper, we effectively overcome the curse of dimensionality by\nemploying the tensor train format to represent a polynomial classifier. Based\non the structure of tensor trains, two learning algorithms are proposed which\ninvolve solving different optimization problems of low computational\ncomplexity. Furthermore, we show how both regularization to prevent overfitting\nand parallelization, which enables the use of large training sets, are\nincorporated into these methods. Both the efficiency and efficacy of our\ntensor-based polynomial classifier are then demonstrated on the two popular\ndatasets USPS and MNIST.\n","negative":"  Datasets with hundreds of variables and many missing values are commonplace.\nIn this setting, it is both statistically and computationally challenging to\ndetect true predictive relationships between variables and also to suppress\nfalse positives. This paper proposes an approach that combines probabilistic\nprogramming, information theory, and non-parametric Bayes. It shows how to use\nBayesian non-parametric modeling to (i) build an ensemble of joint probability\nmodels for all the variables; (ii) efficiently detect marginal independencies;\nand (iii) estimate the conditional mutual information between arbitrary subsets\nof variables, subject to a broad class of constraints. Users can access these\ncapabilities using BayesDB, a probabilistic programming platform for\nprobabilistic data analysis, by writing queries in a simple, SQL-like language.\nThis paper demonstrates empirically that the method can (i) detect\ncontext-specific (in)dependencies on challenging synthetic problems and (ii)\nyield improved sensitivity and specificity over baselines from statistics and\nmachine learning, on a real-world database of over 300 sparsely observed\nindicators of macroeconomic development and public health.\n","id":690}
{"Unnamed: 0.1":11691,"Unnamed: 0":11691.0,"anchor":"Exploring the Design Space of Deep Convolutional Neural Networks at\n  Large Scale","positive":"  In recent years, the research community has discovered that deep neural\nnetworks (DNNs) and convolutional neural networks (CNNs) can yield higher\naccuracy than all previous solutions to a broad array of machine learning\nproblems. To our knowledge, there is no single CNN\/DNN architecture that solves\nall problems optimally. Instead, the \"right\" CNN\/DNN architecture varies\ndepending on the application at hand. CNN\/DNNs comprise an enormous design\nspace. Quantitatively, we find that a small region of the CNN design space\ncontains 30 billion different CNN architectures.\n  In this dissertation, we develop a methodology that enables systematic\nexploration of the design space of CNNs. Our methodology is comprised of the\nfollowing four themes.\n  1. Judiciously choosing benchmarks and metrics.\n  2. Rapidly training CNN models.\n  3. Defining and describing the CNN design space.\n  4. Exploring the design space of CNN architectures.\n  Taken together, these four themes comprise an effective methodology for\ndiscovering the \"right\" CNN architectures to meet the needs of practical\napplications.\n","negative":"  Many events occur in the world. Some event types are stochastically excited\nor inhibited---in the sense of having their probabilities elevated or\ndecreased---by patterns in the sequence of previous events. Discovering such\npatterns can help us predict which type of event will happen next and when. We\nmodel streams of discrete events in continuous time, by constructing a neurally\nself-modulating multivariate point process in which the intensities of multiple\nevent types evolve according to a novel continuous-time LSTM. This generative\nmodel allows past events to influence the future in complex and realistic ways,\nby conditioning future event intensities on the hidden state of a recurrent\nneural network that has consumed the stream of past events. Our model has\ndesirable qualitative properties. It achieves competitive likelihood and\npredictive accuracy on real and synthetic datasets, including under\nmissing-data conditions.\n","id":691}
{"Unnamed: 0.1":11692,"Unnamed: 0":11692.0,"anchor":"WoCE: a framework for clustering ensemble by exploiting the wisdom of\n  Crowds theory","positive":"  The Wisdom of Crowds (WOC), as a theory in the social science, gets a new\nparadigm in computer science. The WOC theory explains that the aggregate\ndecision made by a group is often better than those of its individual members\nif specific conditions are satisfied. This paper presents a novel framework for\nunsupervised and semi-supervised cluster ensemble by exploiting the WOC theory.\nWe employ four conditions in the WOC theory, i.e., diversity, independency,\ndecentralization and aggregation, to guide both the constructing of individual\nclustering results and the final combination for clustering ensemble. Firstly,\nindependency criterion, as a novel mapping system on the raw data set, removes\nthe correlation between features on our proposed method. Then, decentralization\nas a novel mechanism generates high-quality individual clustering results.\nNext, uniformity as a new diversity metric evaluates the generated clustering\nresults. Further, weighted evidence accumulation clustering method is proposed\nfor the final aggregation without using thresholding procedure. Experimental\nstudy on varied data sets demonstrates that the proposed approach achieves\nsuperior performance to state-of-the-art methods.\n","negative":"  SVRG and its variants are among the state of art optimization algorithms for\nlarge scale machine learning problems. It is well known that SVRG converges\nlinearly when the objective function is strongly convex. However this setup can\nbe restrictive, and does not include several important formulations such as\nLasso, group Lasso, logistic regression, and some non-convex models including\ncorrected Lasso and SCAD. In this paper, we prove that, for a class of\nstatistical M-estimators covering examples mentioned above, SVRG solves the\nformulation with {\\em a linear convergence rate} without strong convexity or\neven convexity. Our analysis makes use of {\\em restricted strong convexity},\nunder which we show that SVRG converges linearly to the fundamental statistical\nprecision of the model, i.e., the difference between true unknown parameter\n$\\theta^*$ and the optimal solution $\\hat{\\theta}$ of the model.\n","id":692}
{"Unnamed: 0.1":11693,"Unnamed: 0":11693.0,"anchor":"Supervised Learning for Optimal Power Flow as a Real-Time Proxy","positive":"  In this work we design and compare different supervised learning algorithms\nto compute the cost of Alternating Current Optimal Power Flow (ACOPF). The\nmotivation for quick calculation of OPF cost outcomes stems from the growing\nneed of algorithmic-based long-term and medium-term planning methodologies in\npower networks. Integrated in a multiple time-horizon coordination framework,\nwe refer to this approximation module as a proxy for predicting short-term\ndecision outcomes without the need of actual simulation and optimization of\nthem. Our method enables fast approximate calculation of OPF cost with less\nthan 1% error on average, achieved in run-times that are several orders of\nmagnitude lower than of exact computation. Several test-cases such as\nIEEE-RTS96 are used to demonstrate the efficiency of our approach.\n","negative":"  The idea of applying machine learning(ML) to solve problems in security\ndomains is almost 3 decades old. As information and communications grow more\nubiquitous and more data become available, many security risks arise as well as\nappetite to manage and mitigate such risks. Consequently, research on applying\nand designing ML algorithms and systems for security has grown fast, ranging\nfrom intrusion detection systems(IDS) and malware classification to security\npolicy management(SPM) and information leak checking. In this paper, we\nsystematically study the methods, algorithms, and system designs in academic\npublications from 2008-2015 that applied ML in security domains. 98 percent of\nthe surveyed papers appeared in the 6 highest-ranked academic security\nconferences and 1 conference known for pioneering ML applications in security.\nWe examine the generalized system designs, underlying assumptions,\nmeasurements, and use cases in active research. Our examinations lead to 1) a\ntaxonomy on ML paradigms and security domains for future exploration and\nexploitation, and 2) an agenda detailing open and upcoming challenges. Based on\nour survey, we also suggest a point of view that treats security as a game\ntheory problem instead of a batch-trained ML problem.\n","id":693}
{"Unnamed: 0.1":11694,"Unnamed: 0":11694.0,"anchor":"Enhancing Observability in Distribution Grids using Smart Meter Data","positive":"  Due to limited metering infrastructure, distribution grids are currently\nchallenged by observability issues. On the other hand, smart meter data,\nincluding local voltage magnitudes and power injections, are communicated to\nthe utility operator from grid buses with renewable generation and\ndemand-response programs. This work employs grid data from metered buses\ntowards inferring the underlying grid state. To this end, a coupled formulation\nof the power flow problem (CPF) is put forth. Exploiting the high variability\nof injections at metered buses, the controllability of solar inverters, and the\nrelative time-invariance of conventional loads, the idea is to solve the\nnon-linear power flow equations jointly over consecutive time instants. An\nintuitive and easily verifiable rule pertaining to the locations of metered and\nnon-metered buses on the physical grid is shown to be a necessary and\nsufficient criterion for local observability in radial networks. To account for\nnoisy smart meter readings, a coupled power system state estimation (CPSSE)\nproblem is further developed. Both CPF and CPSSE tasks are tackled via\naugmented semi-definite program relaxations. The observability criterion along\nwith the CPF and CPSSE solvers are numerically corroborated using synthetic and\nactual solar generation and load data on the IEEE 34-bus benchmark feeder.\n","negative":"  Most of researches on image forensics have been mainly focused on detection\nof artifacts introduced by a single processing tool. They lead in the\ndevelopment of many specialized algorithms looking for one or more particular\nfootprints under specific settings. Naturally, the performance of such\nalgorithms are not perfect, and accordingly the provided output might be noisy,\ninaccurate and only partially correct. Furthermore, a forged image in practical\nscenarios is often the result of utilizing several tools available by\nimage-processing software systems. Therefore, reliable tamper detection\nrequires developing more poweful tools to deal with various tempering\nscenarios. Fusion of forgery detection tools based on Fuzzy Inference System\nhas been used before for addressing this problem. Adjusting the membership\nfunctions and defining proper fuzzy rules for attaining to better results are\ntime-consuming processes. This can be accounted as main disadvantage of fuzzy\ninference systems. In this paper, a Neuro-Fuzzy inference system for fusion of\nforgery detection tools is developed. The neural network characteristic of\nthese systems provides appropriate tool for automatically adjusting the\nmembership functions. Moreover, initial fuzzy inference system is generated\nbased on fuzzy clustering techniques. The proposed framework is implemented and\nvalidated on a benchmark image splicing data set in which three forgery\ndetection tools are fused based on adaptive Neuro-Fuzzy inference system. The\noutcome of the proposed method reveals that applying Neuro Fuzzy inference\nsystems could be a better approach for fusion of forgery detection tools.\n","id":694}
{"Unnamed: 0.1":11695,"Unnamed: 0":11695.0,"anchor":"Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault\n  Detection Using an LSTM-based Predictive Data Model","positive":"  We adopted an approach based on an LSTM neural network to monitor and detect\nfaults in industrial multivariate time series data. To validate the approach we\ncreated a Modelica model of part of a real gasoil plant. By introducing hacks\ninto the logic of the Modelica model, we were able to generate both the roots\nand causes of fault behavior in the plant. Having a self-consistent data set\nwith labeled faults, we used an LSTM architecture with a forecasting error\nthreshold to obtain precision and recall quality metrics. The dependency of the\nquality metric on the threshold level is considered. An appropriate mechanism\nsuch as \"one handle\" was introduced for filtering faults that are outside of\nthe plant operator field of interest.\n","negative":"  Water saturation is an important property in reservoir engineering domain.\nThus, satisfactory classification of water saturation from seismic attributes\nis beneficial for reservoir characterization. However, diverse and non-linear\nnature of subsurface attributes makes the classification task difficult. In\nthis context, this paper proposes a generalized Support Vector Data Description\n(SVDD) based novel classification framework to classify water saturation into\ntwo classes (Class high and Class low) from three seismic attributes seismic\nimpedance, amplitude envelop, and seismic sweetness. G-metric means and program\nexecution time are used to quantify the performance of the proposed framework\nalong with established supervised classifiers. The documented results imply\nthat the proposed framework is superior to existing classifiers. The present\nstudy is envisioned to contribute in further reservoir modeling.\n","id":695}
{"Unnamed: 0.1":11696,"Unnamed: 0":11696.0,"anchor":"Action-Driven Object Detection with Top-Down Visual Attentions","positive":"  A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7.\n","negative":"  We consider the Hypothesis Transfer Learning (HTL) problem where one\nincorporates a hypothesis trained on the source domain into the learning\nprocedure of the target domain. Existing theoretical analysis either only\nstudies specific algorithms or only presents upper bounds on the generalization\nerror but not on the excess risk. In this paper, we propose a unified\nalgorithm-dependent framework for HTL through a novel notion of transformation\nfunction, which characterizes the relation between the source and the target\ndomains. We conduct a general risk analysis of this framework and in\nparticular, we show for the first time, if two domains are related, HTL enjoys\nfaster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge\nRegression than those of the classical non-transfer learning settings.\nExperiments on real world data demonstrate the effectiveness of our framework.\n","id":696}
{"Unnamed: 0.1":11697,"Unnamed: 0":11697.0,"anchor":"Beyond Skip Connections: Top-Down Modulation for Object Detection","positive":"  In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.).\n","negative":"  State of the art machine learning algorithms are highly optimized to provide\nthe optimal prediction possible, naturally resulting in complex models. While\nthese models often outperform simpler more interpretable models by order of\nmagnitudes, in terms of understanding the way the model functions, we are often\nfacing a \"black box\".\n  In this paper we suggest a simple method to interpret the behavior of any\npredictive model, both for regression and classification. Given a particular\nmodel, the information required to interpret it can be obtained by studying the\npartial derivatives of the model with respect to the input. We exemplify this\ninsight by interpreting convolutional and multi-layer neural networks in the\nfield of natural language processing.\n","id":697}
{"Unnamed: 0.1":11698,"Unnamed: 0":11698.0,"anchor":"Temporal Feature Selection on Networked Time Series","positive":"  This paper formulates the problem of learning discriminative features\n(\\textit{i.e.,} segments) from networked time series data considering the\nlinked information among time series. For example, social network users are\nconsidered to be social sensors that continuously generate social signals\n(tweets) represented as a time series. The discriminative segments are often\nreferred to as \\emph{shapelets} in a time series. Extracting shapelets for time\nseries classification has been widely studied. However, existing works on\nshapelet selection assume that the time series are independent and identically\ndistributed (i.i.d.). This assumption restricts their applications to social\nnetworked time series analysis, since a user's actions can be correlated to\nhis\/her social affiliations. In this paper we propose a new Network Regularized\nLeast Squares (NetRLS) feature selection model that combines typical time\nseries data and user network data for analysis. Experiments on real-world\nnetworked time series Twitter and DBLP data demonstrate the performance of the\nproposed method. NetRLS performs better than LTS, the state-of-the-art time\nseries feature selection approach, on real-world data.\n","negative":"  We present a framework to understand GAN training as alternating density\nratio estimation and approximate divergence minimization. This provides an\ninterpretation for the mismatched GAN generator and discriminator objectives\noften used in practice, and explains the problem of poor sample diversity. We\nalso derive a family of generator objectives that target arbitrary\n$f$-divergences without minimizing a lower bound, and use them to train\ngenerative image models that target either improved sample quality or greater\nsample diversity.\n","id":698}
{"Unnamed: 0.1":11699,"Unnamed: 0":11699.0,"anchor":"Robust mixture of experts modeling using the skew $t$ distribution","positive":"  Mixture of Experts (MoE) is a popular framework in the fields of statistics\nand machine learning for modeling heterogeneity in data for regression,\nclassification and clustering. MoE for continuous data are usually based on the\nnormal distribution. However, it is known that for data with asymmetric\nbehavior, heavy tails and atypical observations, the use of the normal\ndistribution is unsuitable. We introduce a new robust non-normal mixture of\nexperts modeling using the skew $t$ distribution. The proposed skew $t$ mixture\nof experts, named STMoE, handles these issues of the normal mixtures experts\nregarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated\nexpectation conditional maximization (ECM) algorithm to estimate the model\nparameters by monotonically maximizing the observed data log-likelihood. We\ndescribe how the presented model can be used in prediction and in model-based\nclustering of regression data. Numerical experiments carried out on simulated\ndata show the effectiveness and the robustness of the proposed model in fitting\nnon-linear regression functions as well as in model-based clustering. Then, the\nproposed model is applied to the real-world data of tone perception for musical\ndata analysis, and the one of temperature anomalies for the analysis of climate\nchange data. The obtained results confirm the usefulness of the model for\npractical data analysis applications.\n","negative":"  Instability and variability of Deep Reinforcement Learning (DRL) algorithms\ntend to adversely affect their performance. Averaged-DQN is a simple extension\nto the DQN algorithm, based on averaging previously learned Q-values estimates,\nwhich leads to a more stable training procedure and improved performance by\nreducing approximation error variance in the target values. To understand the\neffect of the algorithm, we examine the source of value function estimation\nerrors and provide an analytical comparison within a simplified model. We\nfurther present experiments on the Arcade Learning Environment benchmark that\ndemonstrate significantly improved stability and performance due to the\nproposed extension.\n","id":699}
{"Unnamed: 0.1":11700,"Unnamed: 0":11700.0,"anchor":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning","positive":"  When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.\n","negative":"  In pattern classification, polynomial classifiers are well-studied methods as\nthey are capable of generating complex decision surfaces. Unfortunately, the\nuse of multivariate polynomials is limited to kernels as in support vector\nmachines, because polynomials quickly become impractical for high-dimensional\nproblems. In this paper, we effectively overcome the curse of dimensionality by\nemploying the tensor train format to represent a polynomial classifier. Based\non the structure of tensor trains, two learning algorithms are proposed which\ninvolve solving different optimization problems of low computational\ncomplexity. Furthermore, we show how both regularization to prevent overfitting\nand parallelization, which enables the use of large training sets, are\nincorporated into these methods. Both the efficiency and efficacy of our\ntensor-based polynomial classifier are then demonstrated on the two popular\ndatasets USPS and MNIST.\n","id":700}
{"Unnamed: 0.1":11701,"Unnamed: 0":11701.0,"anchor":"Personalized Video Recommendation Using Rich Contents from Videos","positive":"  Video recommendation has become an essential way of helping people explore\nthe massive videos and discover the ones that may be of interest to them. In\nthe existing video recommender systems, the models make the recommendations\nbased on the user-video interactions and single specific content features. When\nthe specific content features are unavailable, the performance of the existing\nmodels will seriously deteriorate. Inspired by the fact that rich contents\n(e.g., text, audio, motion, and so on) exist in videos, in this paper, we\nexplore how to use these rich contents to overcome the limitations caused by\nthe unavailability of the specific ones. Specifically, we propose a novel\ngeneral framework that incorporates arbitrary single content feature with\nuser-video interactions, named as collaborative embedding regression (CER)\nmodel, to make effective video recommendation in both in-matrix and\nout-of-matrix scenarios. Our extensive experiments on two real-world\nlarge-scale datasets show that CER beats the existing recommender models with\nany single content feature and is more time efficient. In addition, we propose\na priority-based late fusion (PRI) method to gain the benefit brought by the\nintegrating the multiple content features. The corresponding experiment shows\nthat PRI brings real performance improvement to the baseline and outperforms\nthe existing fusion methods.\n","negative":"  We solve the compressive sensing problem via convolutional factor analysis,\nwhere the convolutional dictionaries are learned {\\em in situ} from the\ncompressed measurements. An alternating direction method of multipliers (ADMM)\nparadigm for compressive sensing inversion based on convolutional factor\nanalysis is developed. The proposed algorithm provides reconstructed images as\nwell as features, which can be directly used for recognition ($e.g.$,\nclassification) tasks. When a deep (multilayer) model is constructed, a\nstochastic unpooling process is employed to build a generative model. During\nreconstruction and testing, we project the upper layer dictionary to the data\nlevel and only a single layer deconvolution is required. We demonstrate that\nusing $\\sim30\\%$ (relative to pixel numbers) compressed measurements, the\nproposed model achieves the classification accuracy comparable to the original\ndata on MNIST. We also observe that when the compressed measurements are very\nlimited ($e.g.$, $<10\\%$), the upper layer dictionary can provide better\nreconstruction results than the bottom layer.\n","id":701}
{"Unnamed: 0.1":11702,"Unnamed: 0":11702.0,"anchor":"Robust Learning with Kernel Mean p-Power Error Loss","positive":"  Correntropy is a second order statistical measure in kernel space, which has\nbeen successfully applied in robust learning and signal processing. In this\npaper, we define a nonsecond order statistical measure in kernel space, called\nthe kernel mean-p power error (KMPE), including the correntropic loss (CLoss)\nas a special case. Some basic properties of KMPE are presented. In particular,\nwe apply the KMPE to extreme learning machine (ELM) and principal component\nanalysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and\nPCA-KMPE. Experimental results on synthetic and benchmark data show that the\ndeveloped algorithms can achieve consistently better performance when compared\nwith some existing methods.\n","negative":"  Active deep learning classification of hyperspectral images is considered in\nthis paper. Deep learning has achieved success in many applications, but\ngood-quality labeled samples are needed to construct a deep learning network.\nIt is expensive getting good labeled samples in hyperspectral images for remote\nsensing applications. An active learning algorithm based on a weighted\nincremental dictionary learning is proposed for such applications. The proposed\nalgorithm selects training samples that maximize two selection criteria, namely\nrepresentative and uncertainty. This algorithm trains a deep network\nefficiently by actively selecting training samples at each iteration. The\nproposed algorithm is applied for the classification of hyperspectral images,\nand compared with other classification algorithms employing active learning. It\nis shown that the proposed algorithm is efficient and effective in classifying\nhyperspectral images.\n","id":702}
{"Unnamed: 0.1":11703,"Unnamed: 0":11703.0,"anchor":"An Empirical Study of Language CNN for Image Captioning","positive":"  Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.\n","negative":"  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n","id":703}
{"Unnamed: 0.1":11704,"Unnamed: 0":11704.0,"anchor":"Classification and Learning-to-rank Approaches for Cross-Device Matching\n  at CIKM Cup 2016","positive":"  In this paper, we propose two methods for tackling the problem of\ncross-device matching for online advertising at CIKM Cup 2016. The first method\nconsiders the matching problem as a binary classification task and solve it by\nutilizing ensemble learning techniques. The second method defines the matching\nproblem as a ranking task and effectively solve it with using learning-to-rank\nalgorithms. The results show that the proposed methods obtain promising\nresults, in which the ranking-based method outperforms the classification-based\nmethod for the task.\n","negative":"  We introduce a novel approach for predicting the progression of adolescent\nidiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray\nimages. Recent progress in machine learning have allowed to improve\nclassification and prognosis rates, but lack a probabilistic framework to\nmeasure uncertainty in the data. We propose a discriminative probabilistic\nmanifold embedding where locally linear mappings transform data points from\nhigh-dimensional space to corresponding low-dimensional coordinates. A\ndiscriminant adjacency matrix is constructed to maximize the separation between\nprogressive and non-progressive groups of patients diagnosed with scoliosis,\nwhile minimizing the distance in latent variables belonging to the same class.\nTo predict the evolution of deformation, a baseline reconstruction is projected\nonto the manifold, from which a spatiotemporal regression model is built from\nparallel transport curves inferred from neighboring exemplars. Rate of\nprogression is modulated from the spine flexibility and curve magnitude of the\n3D spine deformation. The method was tested on 745 reconstructions from 133\nsubjects using longitudinal 3D reconstructions of the spine, with results\ndemonstrating the discriminatory framework can identify between progressive and\nnon-progressive of scoliotic patients with a classification rate of 81% and\nprediction differences of 2.1$^{o}$ in main curve angulation, outperforming\nother manifold learning methods. Our method achieved a higher prediction\naccuracy and improved the modeling of spatiotemporal morphological changes in\nhighly deformed spines compared to other learning methods.\n","id":704}
{"Unnamed: 0.1":11705,"Unnamed: 0":11705.0,"anchor":"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference","positive":"  Research has shown that convolutional neural networks contain significant\nredundancy, and high classification accuracy can be obtained even when weights\nand activations are reduced from floating point to binary values. In this\npaper, we present FINN, a framework for building fast and flexible FPGA\naccelerators using a flexible heterogeneous streaming architecture. By\nutilizing a novel set of optimizations that enable efficient mapping of\nbinarized neural networks to hardware, we implement fully connected,\nconvolutional and pooling layers, with per-layer compute resources being\ntailored to user-provided throughput requirements. On a ZC706 embedded FPGA\nplatform drawing less than 25 W total system power, we demonstrate up to 12.3\nmillion image classifications per second with 0.31 {\\mu}s latency on the MNIST\ndataset with 95.8% accuracy, and 21906 image classifications per second with\n283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1%\nand 94.9% accuracy. To the best of our knowledge, ours are the fastest\nclassification rates reported to date on these benchmarks.\n","negative":"  To sidestep the curse of dimensionality when computing solutions to\nHamilton-Jacobi-Bellman partial differential equations (HJB PDE), we propose an\nalgorithm that leverages a neural network to approximate the value function. We\nshow that our final approximation of the value function generates near optimal\ncontrols which are guaranteed to successfully drive the system to a target\nstate. Our framework is not dependent on state space discretization, leading to\na significant reduction in computation time and space complexity in comparison\nwith dynamic programming-based approaches. Using this grid-free approach also\nenables us to plan over longer time horizons with relatively little additional\ncomputation overhead. Unlike many previous neural network HJB PDE approximating\nformulations, our approximation is strictly conservative and hence any\ntrajectories we generate will be strictly feasible. For demonstration, we\nspecialize our new general framework to the Dubins car model and discuss how\nthe framework can be applied to other models with higher-dimensional state\nspaces.\n","id":705}
{"Unnamed: 0.1":11706,"Unnamed: 0":11706.0,"anchor":"A Survey of Deep Network Solutions for Learning Control in Robotics:\n  From Reinforcement to Imitation","positive":"  Deep learning techniques have been widely applied, achieving state-of-the-art\nresults in various fields of study. This survey focuses on deep learning\nsolutions that target learning control policies for robotics applications. We\ncarry out our discussions on the two main paradigms for learning control with\ndeep networks: deep reinforcement learning and imitation learning. For deep\nreinforcement learning (DRL), we begin from traditional reinforcement learning\nalgorithms, showing how they are extended to the deep context and effective\nmechanisms that could be added on top of the DRL algorithms. We then introduce\nrepresentative works that utilize DRL to solve navigation and manipulation\ntasks in robotics. We continue our discussion on methods addressing the\nchallenge of the reality gap for transferring DRL policies trained in\nsimulation to real-world scenarios, and summarize robotics simulation platforms\nfor conducting DRL research. For imitation leaning, we go through its three\nmain categories, behavior cloning, inverse reinforcement learning and\ngenerative adversarial imitation learning, by introducing their formulations\nand their corresponding robotics applications. Finally, we discuss the open\nchallenges and research frontiers.\n","negative":"  Predicting user response is one of the core machine learning tasks in\ncomputational advertising. Field-aware Factorization Machines (FFM) have\nrecently been established as a state-of-the-art method for that problem and in\nparticular won two Kaggle challenges. This paper presents some results from\nimplementing this method in a production system that predicts click-through and\nconversion rates for display advertising and shows that this method it is not\nonly effective to win challenges but is also valuable in a real-world\nprediction system. We also discuss some specific challenges and solutions to\nreduce the training time, namely the use of an innovative seeding algorithm and\na distributed learning mechanism.\n","id":706}
{"Unnamed: 0.1":11707,"Unnamed: 0":11707.0,"anchor":"Robust Classification of Graph-Based Data","positive":"  A graph-based classification method is proposed for semi-supervised learning\nin the case of Euclidean data and for classification in the case of graph data.\nOur manifold learning technique is based on a convex optimization problem\ninvolving a convex quadratic regularization term and a concave quadratic loss\nfunction with a trade-off parameter carefully chosen so that the objective\nfunction remains convex. As shown empirically, the advantage of considering a\nconcave loss function is that the learning problem becomes more robust in the\npresence of noisy labels. Furthermore, the loss function considered here is\nthen more similar to a classification loss while several other methods treat\ngraph-based classification problems as regression problems.\n","negative":"  Zeroth-order (derivative-free) optimization attracts a lot of attention in\nmachine learning, because explicit gradient calculations may be computationally\nexpensive or infeasible. To handle large scale problems both in volume and\ndimension, recently asynchronous doubly stochastic zeroth-order algorithms were\nproposed. The convergence rate of existing asynchronous doubly stochastic\nzeroth order algorithms is $O(\\frac{1}{\\sqrt{T}})$ (also for the sequential\nstochastic zeroth-order optimization algorithms). In this paper, we focus on\nthe finite sums of smooth but not necessarily convex functions, and propose an\nasynchronous doubly stochastic zeroth-order optimization algorithm using the\naccelerated technology of variance reduction (AsyDSZOVR). Rigorous theoretical\nanalysis show that the convergence rate can be improved from\n$O(\\frac{1}{\\sqrt{T}})$ the best result of existing algorithms to\n$O(\\frac{1}{T})$. Also our theoretical results is an improvement to the ones of\nthe sequential stochastic zeroth-order optimization algorithms.\n","id":707}
{"Unnamed: 0.1":11708,"Unnamed: 0":11708.0,"anchor":"Collaborative Filtering with User-Item Co-Autoregressive Models","positive":"  Deep neural networks have shown promise in collaborative filtering (CF).\nHowever, existing neural approaches are either user-based or item-based, which\ncannot leverage all the underlying information explicitly. We propose CF-UIcA,\na neural co-autoregressive model for CF tasks, which exploits the structural\ncorrelation in the domains of both users and items. The co-autoregression\nallows extra desired properties to be incorporated for different tasks.\nFurthermore, we develop an efficient stochastic learning algorithm to handle\nlarge scale datasets. We evaluate CF-UIcA on two popular benchmarks: MovieLens\n1M and Netflix, and achieve state-of-the-art performance in both rating\nprediction and top-N recommendation tasks, which demonstrates the effectiveness\nof CF-UIcA.\n","negative":"  We use some of the largest order statistics of the random projections of a\nreference signal to construct a binary embedding that is adapted to signals\ncorrelated with such signal. The embedding is characterized from the analytical\nstandpoint and shown to provide improved performance on tasks such as\nclassification in a reduced-dimensionality space.\n","id":708}
{"Unnamed: 0.1":11709,"Unnamed: 0":11709.0,"anchor":"Multi-Agent Cooperation and the Emergence of (Natural) Language","positive":"  The current mainstream approach to train natural language systems is to\nexpose them to large amounts of text. This passive learning is problematic if\nwe are interested in developing interactive machines, such as conversational\nagents. We propose a framework for language learning that relies on multi-agent\ncommunication. We study this learning in the context of referential games. In\nthese games, a sender and a receiver see a pair of images. The sender is told\none of them is the target and is allowed to send a message from a fixed,\narbitrary vocabulary to the receiver. The receiver must rely on this message to\nidentify the target. Thus, the agents develop their own language interactively\nout of the need to communicate. We show that two networks with simple\nconfigurations are able to learn to coordinate in the referential game. We\nfurther explore how to make changes to the game environment to cause the \"word\nmeanings\" induced in the game to better reflect intuitive semantic properties\nof the images. In addition, we present a simple strategy for grounding the\nagents' code into natural language. Both of these are necessary steps towards\ndeveloping machines that are able to communicate with humans productively.\n","negative":"  This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.\n","id":709}
{"Unnamed: 0.1":11710,"Unnamed: 0":11710.0,"anchor":"Bayesian Decision Process for Cost-Efficient Dynamic Ranking via\n  Crowdsourcing","positive":"  Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost.\n","negative":"  The past year saw the introduction of new architectures such as Highway\nnetworks and Residual networks which, for the first time, enabled the training\nof feedforward networks with dozens to hundreds of layers using simple gradient\ndescent. While depth of representation has been posited as a primary reason for\ntheir success, there are indications that these architectures defy a popular\nview of deep learning as a hierarchical computation of increasingly abstract\nfeatures at each layer.\n  In this report, we argue that this view is incomplete and does not adequately\nexplain several recent findings. We propose an alternative viewpoint based on\nunrolled iterative estimation -- a group of successive layers iteratively\nrefine their estimates of the same features instead of computing an entirely\nnew representation. We demonstrate that this viewpoint directly leads to the\nconstruction of Highway and Residual networks. Finally we provide preliminary\nexperiments to discuss the similarities and differences between the two\narchitectures.\n","id":710}
{"Unnamed: 0.1":11711,"Unnamed: 0":11711.0,"anchor":"Loss is its own Reward: Self-Supervision for Reinforcement Learning","positive":"  Reinforcement learning optimizes policies for expected cumulative reward.\nNeed the supervision be so narrow? Reward is delayed and sparse for many tasks,\nmaking it a difficult and impoverished signal for end-to-end optimization. To\naugment reward, we consider a range of self-supervised tasks that incorporate\nstates, actions, and successors to provide auxiliary losses. These losses offer\nubiquitous and instantaneous supervision for representation learning even in\nthe absence of reward. While current results show that learning from reward\nalone is feasible, pure reinforcement learning methods are constrained by\ncomputational and data efficiency issues that can be remedied by auxiliary\nlosses. Self-supervised pre-training and joint optimization improve the data\nefficiency and policy returns of end-to-end reinforcement learning.\n","negative":"  Although much progress has been made in classification with high-dimensional\nfeatures \\citep{Fan_Fan:2008, JGuo:2010, CaiSun:2014, PRXu:2014},\nclassification with ultrahigh-dimensional features, wherein the features much\noutnumber the sample size, defies most existing work. This paper introduces a\nnovel and computationally feasible multivariate screening and classification\nmethod for ultrahigh-dimensional data. Leveraging inter-feature correlations,\nthe proposed method enables detection of marginally weak and sparse signals and\nrecovery of the true informative feature set, and achieves asymptotic optimal\nmisclassification rates. We also show that the proposed procedure provides more\npowerful discovery boundaries compared to those in \\citet{CaiSun:2014} and\n\\citet{JJin:2009}. The performance of the proposed procedure is evaluated using\nsimulation studies and demonstrated via classification of patients with\ndifferent post-transplantation renal functional types.\n","id":711}
{"Unnamed: 0.1":11712,"Unnamed: 0":11712.0,"anchor":"Distributed Dictionary Learning","positive":"  The paper studies distributed Dictionary Learning (DL) problems where the\nlearning task is distributed over a multi-agent network with time-varying\n(nonsymmetric) connectivity. This formulation is relevant, for instance, in\nbig-data scenarios where massive amounts of data are collected\/stored in\ndifferent spatial locations and it is unfeasible to aggregate and\/or process\nall the data in a fusion center, due to resource limitations, communication\noverhead or privacy considerations. We develop a general distributed\nalgorithmic framework for the (nonconvex) DL problem and establish its\nasymptotic convergence. The new method hinges on Successive Convex\nApproximation (SCA) techniques coupled with i) a gradient tracking mechanism\ninstrumental to locally estimate the missing global information; and ii) a\nconsensus step, as a mechanism to distribute the computations among the agents.\nTo the best of our knowledge, this is the first distributed algorithm with\nprovable convergence for the DL problem and, more in general, bi-convex\noptimization problems over (time-varying) directed graphs.\n","negative":"  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification\/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n","id":712}
{"Unnamed: 0.1":11713,"Unnamed: 0":11713.0,"anchor":"Detecting Unusual Input-Output Associations in Multivariate Conditional\n  Data","positive":"  Despite tremendous progress in outlier detection research in recent years,\nthe majority of existing methods are designed only to detect unconditional\noutliers that correspond to unusual data patterns expressed in the joint space\nof all data attributes. Such methods are not applicable when we seek to detect\nconditional outliers that reflect unusual responses associated with a given\ncontext or condition. This work focuses on multivariate conditional outlier\ndetection, a special type of the conditional outlier detection problem, where\ndata instances consist of multi-dimensional input (context) and output\n(responses) pairs. We present a novel outlier detection framework that\nidentifies abnormal input-output associations in data with the help of a\ndecomposable conditional probabilistic model that is learned from all data\ninstances. Since components of this model can vary in their quality, we combine\nthem with the help of weights reflecting their reliability in assessment of\noutliers. We study two ways of calculating the component weights: global that\nrelies on all data, and local that relies only on instances similar to the\ntarget instance. Experimental results on data from various domains demonstrate\nthe ability of our framework to successfully identify multivariate conditional\noutliers.\n","negative":"  This paper addresses the real-time encoding-decoding problem for\nhigh-frame-rate video compressive sensing (CS). Unlike prior works that perform\nreconstruction using iterative optimization-based approaches, we propose a\nnon-iterative model, named \"CSVideoNet\". CSVideoNet directly learns the inverse\nmapping of CS and reconstructs the original input in a single forward\npropagation. To overcome the limitations of existing CS cameras, we propose a\nmulti-rate CNN and a synthesizing RNN to improve the trade-off between\ncompression ratio (CR) and spatial-temporal resolution of the reconstructed\nvideos. The experiment results demonstrate that CSVideoNet significantly\noutperforms the state-of-the-art approaches. With no pre\/post-processing, we\nachieve 25dB PSNR recovery quality at 100x CR, with a frame rate of 125 fps on\na Titan X GPU. Due to the feedforward and high-data-concurrency natures of\nCSVideoNet, it can take advantage of GPU acceleration to achieve three orders\nof magnitude speed-up over conventional iterative-based approaches. We share\nthe source code at https:\/\/github.com\/PSCLab-ASU\/CSVideoNet.\n","id":713}
{"Unnamed: 0.1":11714,"Unnamed: 0":11714.0,"anchor":"Microstructure Representation and Reconstruction of Heterogeneous\n  Materials via Deep Belief Network for Computational Material Design","positive":"  Integrated Computational Materials Engineering (ICME) aims to accelerate\noptimal design of complex material systems by integrating material science and\ndesign automation. For tractable ICME, it is required that (1) a structural\nfeature space be identified to allow reconstruction of new designs, and (2) the\nreconstruction process be property-preserving. The majority of existing\nstructural presentation schemes rely on the designer's understanding of\nspecific material systems to identify geometric and statistical features, which\ncould be biased and insufficient for reconstructing physically meaningful\nmicrostructures of complex material systems. In this paper, we develop a\nfeature learning mechanism based on convolutional deep belief network to\nautomate a two-way conversion between microstructures and their\nlower-dimensional feature representations, and to achieves a 1000-fold\ndimension reduction from the microstructure space. The proposed model is\napplied to a wide spectrum of heterogeneous material systems with distinct\nmicrostructural features including Ti-6Al-4V alloy, Pb63-Sn37 alloy,\nFontainebleau sandstone, and Spherical colloids, to produce material\nreconstructions that are close to the original samples with respect to 2-point\ncorrelation functions and mean critical fracture strength. This capability is\nnot achieved by existing synthesis methods that rely on the Markovian\nassumption of material microstructures.\n","negative":"  In this paper, we consider a new low-quality label learning problem: learning\ntime series detection models from temporally imprecise labels. In this problem,\nthe data consist of a set of input time series, and supervision is provided by\na sequence of noisy time stamps corresponding to the occurrence of positive\nclass events. Such temporally imprecise labels commonly occur in areas like\nmobile health research where human annotators are tasked with labeling the\noccurrence of very short duration events. We propose a general learning\nframework for this problem that can accommodate different base classifiers and\nnoise models. We present results on real mobile health data showing that the\nproposed framework significantly outperforms a number of alternatives including\nassuming that the label time stamps are noise-free, transforming the problem\ninto the multiple instance learning framework, and learning on labels that were\nmanually re-aligned.\n","id":714}
{"Unnamed: 0.1":11715,"Unnamed: 0":11715.0,"anchor":"A Context-aware Attention Network for Interactive Question Answering","positive":"  Neural network based sequence-to-sequence models in an encoder-decoder\nframework have been successfully applied to solve Question Answering (QA)\nproblems, predicting answers from statements and questions. However, almost all\nprevious models have failed to consider detailed context information and\nunknown states under which systems do not have enough information to answer\ngiven questions. These scenarios with incomplete or ambiguous information are\nvery common in the setting of Interactive Question Answering (IQA). To address\nthis challenge, we develop a novel model, employing context-dependent\nword-level attention for more accurate statement representations and\nquestion-guided sentence-level attention for better context modeling. We also\ngenerate unique IQA datasets to test our model, which will be made publicly\navailable. Employing these attention mechanisms, our model accurately\nunderstands when it can output an answer or when it requires generating a\nsupplementary question for additional input depending on different contexts.\nWhen available, user's feedback is encoded and directly applied to update\nsentence-level attention to infer an answer. Extensive experiments on QA and\nIQA datasets quantitatively demonstrate the effectiveness of our model with\nsignificant improvement over state-of-the-art conventional QA models.\n","negative":"  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n","id":715}
{"Unnamed: 0.1":11716,"Unnamed: 0":11716.0,"anchor":"How to Train Your Deep Neural Network with Dictionary Learning","positive":"  Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.\n","negative":"  Acquiring your first language is an incredible feat and not easily\nduplicated. Learning to communicate using nothing but a few pictureless books,\na corpus, would likely be impossible even for humans. Nevertheless, this is the\ndominating approach in most natural language processing today. As an\nalternative, we propose the use of situated interactions between agents as a\ndriving force for communication, and the framework of Deep Recurrent Q-Networks\nfor evolving a shared language grounded in the provided environment. We task\nthe agents with interactive image search in the form of the game Guess Who?.\nThe images from the game provide a non trivial environment for the agents to\ndiscuss and a natural grounding for the concepts they decide to encode in their\ncommunication. Our experiments show that the agents learn not only to encode\nphysical concepts in their words, i.e. grounding, but also that the agents\nlearn to hold a multi-step dialogue remembering the state of the dialogue from\nstep to step.\n","id":716}
{"Unnamed: 0.1":11717,"Unnamed: 0":11717.0,"anchor":"On Coreset Constructions for the Fuzzy $K$-Means Problem","positive":"  The fuzzy $K$-means problem is a popular generalization of the well-known\n$K$-means problem to soft clusterings. We present the first coresets for fuzzy\n$K$-means with size linear in the dimension, polynomial in the number of\nclusters, and poly-logarithmic in the number of points. We show that these\ncoresets can be employed in the computation of a $(1+\\epsilon)$-approximation\nfor fuzzy $K$-means, improving previously presented results. We further show\nthat our coresets can be maintained in an insertion-only streaming setting,\nwhere data points arrive one-by-one.\n","negative":"  This MSc dissertation considers the effects of the current corporate interest\non researchers in the field of machine learning. Situated within the field's\ncyclical history of academic, public and corporate interest, this dissertation\ninvestigates how current researchers view recent developments and negotiate\ntheir own research practices within an environment of increased commercial\ninterest and funding. The original research consists of in-depth interviews\nwith 12 machine learning researchers working in both academia and industry.\nBuilding on theory from science, technology and society studies, this\ndissertation problematizes the traditional narratives of the neoliberalization\nof academic research by allowing the researchers themselves to discuss how\ntheir career choices, working environments and interactions with others in the\nfield have been affected by the reinvigorated corporate interest of recent\nyears.\n","id":717}
{"Unnamed: 0.1":11718,"Unnamed: 0":11718.0,"anchor":"Robustness of Voice Conversion Techniques Under Mismatched Conditions","positive":"  Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions.\n","negative":"  This paper formulates the problem of learning discriminative features\n(\\textit{i.e.,} segments) from networked time series data considering the\nlinked information among time series. For example, social network users are\nconsidered to be social sensors that continuously generate social signals\n(tweets) represented as a time series. The discriminative segments are often\nreferred to as \\emph{shapelets} in a time series. Extracting shapelets for time\nseries classification has been widely studied. However, existing works on\nshapelet selection assume that the time series are independent and identically\ndistributed (i.i.d.). This assumption restricts their applications to social\nnetworked time series analysis, since a user's actions can be correlated to\nhis\/her social affiliations. In this paper we propose a new Network Regularized\nLeast Squares (NetRLS) feature selection model that combines typical time\nseries data and user network data for analysis. Experiments on real-world\nnetworked time series Twitter and DBLP data demonstrate the performance of the\nproposed method. NetRLS performs better than LTS, the state-of-the-art time\nseries feature selection approach, on real-world data.\n","id":718}
{"Unnamed: 0.1":11719,"Unnamed: 0":11719.0,"anchor":"Non-Deterministic Policy Improvement Stabilizes Approximated\n  Reinforcement Learning","positive":"  This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\n","negative":"  Previous research has shown that computation of convolution in the frequency\ndomain provides a significant speedup versus traditional convolution network\nimplementations. However, this performance increase comes at the expense of\nrepeatedly computing the transform and its inverse in order to apply other\nnetwork operations such as activation, pooling, and dropout. We show,\nmathematically, how convolution and activation can both be implemented in the\nfrequency domain using either the Fourier or Laplace transformation. The main\ncontributions are a description of spectral activation under the Fourier\ntransform and a further description of an efficient algorithm for computing\nboth convolution and activation under the Laplace transform. By computing both\nthe convolution and activation functions in the frequency domain, we can reduce\nthe number of transforms required, as well as reducing overall complexity. Our\ndescription of a spectral activation function, together with existing spectral\nanalogs of other network functions may then be used to compose a fully spectral\nimplementation of a convolution network.\n","id":719}
{"Unnamed: 0.1":11720,"Unnamed: 0":11720.0,"anchor":"On the function approximation error for risk-sensitive reinforcement\n  learning","positive":"  In this paper we obtain several informative error bounds on function\napproximation for the policy evaluation algorithm proposed by Basu et al. when\nthe aim is to find the risk-sensitive cost represented using exponential\nutility. The main idea is to use classical Bapat's inequality and to use\nPerron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to\nget the new bounds. The novelty of our approach is that we use the\nirreduciblity of Markov chain to get the new bounds whereas the earlier work by\nBasu et al. used spectral variation bound which is true for any matrix. We also\ngive examples where all our bounds achieve the \"actual error\" whereas the\nearlier bound given by Basu et al. is much weaker in comparison. We show that\nthis happens due to the absence of difference term in the earlier bound which\nis always present in all our bounds when the state space is large.\nAdditionally, we discuss how all our bounds compare with each other. As a\ncorollary of our main result we provide a bound between largest eigenvalues of\ntwo irreducibile matrices in terms of the matrix entries.\n","negative":"  The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.\n","id":720}
{"Unnamed: 0.1":11721,"Unnamed: 0":11721.0,"anchor":"Finding Statistically Significant Attribute Interactions","positive":"  In many data exploration tasks it is meaningful to identify groups of\nattribute interactions that are specific to a variable of interest. For\ninstance, in a dataset where the attributes are medical markers and the\nvariable of interest (class variable) is binary indicating presence\/absence of\ndisease, we would like to know which medical markers interact with respect to\nthe binary class label. These interactions are useful in several practical\napplications, for example, to gain insight into the structure of the data, in\nfeature selection, and in data anonymisation. We present a novel method, based\non statistical significance testing, that can be used to verify if the data set\nhas been created by a given factorised class-conditional joint distribution,\nwhere the distribution is parametrised by a partition of its attributes.\nFurthermore, we provide a method, named ASTRID, for automatically finding a\npartition of attributes describing the distribution that has generated the\ndata. State-of-the-art classifiers are utilised to capture the interactions\npresent in the data by systematically breaking attribute interactions and\nobserving the effect of this breaking on classifier performance. We empirically\ndemonstrate the utility of the proposed method with examples using real and\nsynthetic data.\n","negative":"  We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions.\n","id":721}
{"Unnamed: 0.1":11722,"Unnamed: 0":11722.0,"anchor":"Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey","positive":"  Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.\n","negative":"  Binarized neural networks (BNNs) are gaining interest in the deep learning\ncommunity due to their significantly lower computational and memory cost. They\nare particularly well suited to reconfigurable logic devices, which contain an\nabundance of fine-grained compute resources and can result in smaller, lower\npower implementations, or conversely in higher classification rates. Towards\nthis end, the Finn framework was recently proposed for building fast and\nflexible field programmable gate array (FPGA) accelerators for BNNs. Finn\nutilized a novel set of optimizations that enable efficient mapping of BNNs to\nhardware and implemented fully connected, non-padded convolutional and pooling\nlayers, with per-layer compute resources being tailored to user-provided\nthroughput requirements. However, FINN was not evaluated on larger topologies\ndue to the size of the chosen FPGA, and exhibited decreased accuracy due to\nlack of padding. In this paper, we improve upon Finn to show how padding can be\nemployed on BNNs while still maintaining a 1-bit datapath and high accuracy.\nBased on this technique, we demonstrate numerous experiments to illustrate\nflexibility and scalability of the approach. In particular, we show that a\nlarge BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5\nplatform can classify images at 12 kFPS with 671 us latency while drawing less\nthan 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our\nimplementation of this network achieves 14.8 trillion operations per second. We\nbelieve this is the fastest classification rate reported to date on this\nbenchmark at this level of accuracy.\n","id":722}
{"Unnamed: 0.1":11723,"Unnamed: 0":11723.0,"anchor":"Structured Sequence Modeling with Graph Convolutional Recurrent Networks","positive":"  This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep\nlearning model able to predict structured sequences of data. Precisely, GCRN is\na generalization of classical recurrent neural networks (RNN) to data\nstructured by an arbitrary graph. Such structured sequences can represent\nseries of frames in videos, spatio-temporal measurements on a network of\nsensors, or random walks on a vocabulary graph for natural language modeling.\nThe proposed model combines convolutional neural networks (CNN) on graphs to\nidentify spatial structures and RNN to find dynamic patterns. We study two\npossible architectures of GCRN, and apply the models to two practical problems:\npredicting moving MNIST data, and modeling natural language with the Penn\nTreebank dataset. Experiments show that exploiting simultaneously graph spatial\nand dynamic information about data can improve both precision and learning\nspeed.\n","negative":"  In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.\n","id":723}
{"Unnamed: 0.1":11724,"Unnamed: 0":11724.0,"anchor":"Stacking machine learning classifiers to identify Higgs bosons at the\n  LHC","positive":"  Machine learning (ML) algorithms have been employed in the problem of\nclassifying signal and background events with high accuracy in particle\nphysics. In this paper, we compare the performance of a widespread ML\ntechnique, namely, \\emph{stacked generalization}, against the results of two\nstate-of-art algorithms: (1) a deep neural network (DNN) in the task of\ndiscovering a new neutral Higgs boson and (2) a scalable machine learning\nsystem for tree boosting, in the Standard Model Higgs to tau leptons channel,\nboth at the 8 TeV LHC. In a cut-and-count analysis, \\emph{stacking} three\nalgorithms performed around 16\\% worse than DNN but demanding far less\ncomputation efforts, however, the same \\emph{stacking} outperforms boosted\ndecision trees. Using the stacked classifiers in a multivariate statistical\nanalysis (MVA), on the other hand, significantly enhances the statistical\nsignificance compared to cut-and-count in both Higgs processes, suggesting that\ncombining an ensemble of simpler and faster ML algorithms with MVA tools is a\nbetter approach than building a complex state-of-art algorithm for\ncut-and-count.\n","negative":"  The significant computational costs of deploying neural networks in\nlarge-scale or resource constrained environments, such as data centers and\nmobile devices, has spurred interest in model compression, which can achieve a\nreduction in both arithmetic operations and storage memory. Several techniques\nhave been proposed for reducing or compressing the parameters for feed-forward\nand convolutional neural networks, but less is understood about the effect of\nparameter compression on recurrent neural networks (RNN). In particular, the\nextent to which the recurrent parameters can be compressed and the impact on\nshort-term memory performance, is not well understood. In this paper, we study\nthe effect of complexity reduction, through singular value decomposition rank\nreduction, on RNN and minimal gated recurrent unit (MGRU) networks for several\ntasks. We show that considerable rank reduction is possible when compressing\nrecurrent weights, even without fine tuning. Furthermore, we propose a\nperturbation model for the effect of general perturbations, such as a\ncompression, on the recurrent parameters of RNNs. The model is tested against a\nnoiseless memorization experiment that elucidates the short-term memory\nperformance. In this way, we demonstrate that the effect of compression of\nrecurrent parameters is dependent on the degree of temporal coherence present\nin the data and task. This work can guide on-the-fly RNN compression for novel\nenvironments or tasks, and provides insight for applying RNN compression in\nlow-power devices, such as hearing aids.\n","id":724}
{"Unnamed: 0.1":11725,"Unnamed: 0":11725.0,"anchor":"Highway and Residual Networks learn Unrolled Iterative Estimation","positive":"  The past year saw the introduction of new architectures such as Highway\nnetworks and Residual networks which, for the first time, enabled the training\nof feedforward networks with dozens to hundreds of layers using simple gradient\ndescent. While depth of representation has been posited as a primary reason for\ntheir success, there are indications that these architectures defy a popular\nview of deep learning as a hierarchical computation of increasingly abstract\nfeatures at each layer.\n  In this report, we argue that this view is incomplete and does not adequately\nexplain several recent findings. We propose an alternative viewpoint based on\nunrolled iterative estimation -- a group of successive layers iteratively\nrefine their estimates of the same features instead of computing an entirely\nnew representation. We demonstrate that this viewpoint directly leads to the\nconstruction of Highway and Residual networks. Finally we provide preliminary\nexperiments to discuss the similarities and differences between the two\narchitectures.\n","negative":"  Gravitational wave astronomy has set in motion a scientific revolution. To\nfurther enhance the science reach of this emergent field, there is a pressing\nneed to increase the depth and speed of the gravitational wave algorithms that\nhave enabled these groundbreaking discoveries. To contribute to this effort, we\nintroduce Deep Filtering, a new highly scalable method for end-to-end\ntime-series signal processing, based on a system of two deep convolutional\nneural networks, which we designed for classification and regression to rapidly\ndetect and estimate parameters of signals in highly noisy time-series data\nstreams. We demonstrate a novel training scheme with gradually increasing noise\nlevels, and a transfer learning procedure between the two networks. We showcase\nthe application of this method for the detection and parameter estimation of\ngravitational waves from binary black hole mergers. Our results indicate that\nDeep Filtering significantly outperforms conventional machine learning\ntechniques, achieves similar performance compared to matched-filtering while\nbeing several orders of magnitude faster thus allowing real-time processing of\nraw big data with minimal resources. More importantly, Deep Filtering extends\nthe range of gravitational wave signals that can be detected with ground-based\ngravitational wave detectors. This framework leverages recent advances in\nartificial intelligence algorithms and emerging hardware architectures, such as\ndeep-learning-optimized GPUs, to facilitate real-time searches of gravitational\nwave sources and their electromagnetic and astro-particle counterparts.\n","id":725}
{"Unnamed: 0.1":11726,"Unnamed: 0":11726.0,"anchor":"Logic-based Clustering and Learning for Time-Series Data","positive":"  To effectively analyze and design cyberphysical systems (CPS), designers\ntoday have to combat the data deluge problem, i.e., the burden of processing\nintractably large amounts of data produced by complex models and experiments.\nIn this work, we utilize monotonic Parametric Signal Temporal Logic (PSTL) to\ndesign features for unsupervised classification of time series data. This\nenables using off-the-shelf machine learning tools to automatically cluster\nsimilar traces with respect to a given PSTL formula. We demonstrate how this\ntechnique produces interpretable formulas that are amenable to analysis and\nunderstanding using a few representative examples. We illustrate this with case\nstudies related to automotive engine testing, highway traffic analysis, and\nauto-grading massively open online courses.\n","negative":"  Many practical environments contain catastrophic states that an optimal agent\nwould visit infrequently or never. Even on toy problems, Deep Reinforcement\nLearning (DRL) agents tend to periodically revisit these states upon forgetting\ntheir existence under a new policy. We introduce intrinsic fear (IF), a learned\nreward shaping that guards DRL agents against periodic catastrophes. IF agents\npossess a fear model trained to predict the probability of imminent\ncatastrophe. This score is then used to penalize the Q-learning objective. Our\ntheoretical analysis bounds the reduction in average return due to learning on\nthe perturbed objective. We also prove robustness to classification errors. As\na bonus, IF models tend to learn faster, owing to reward shaping. Experiments\ndemonstrate that intrinsic-fear DQNs solve otherwise pathological environments\nand improve on several Atari games.\n","id":726}
{"Unnamed: 0.1":11727,"Unnamed: 0":11727.0,"anchor":"Learning from Simulated and Unsupervised Images through Adversarial\n  Training","positive":"  With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data.\n","negative":"  In this paper, we focus on training and evaluating effective word embeddings\nwith both text and visual information. More specifically, we introduce a\nlarge-scale dataset with 300 million sentences describing over 40 million\nimages crawled and downloaded from publicly available Pins (i.e. an image with\nsentence descriptions uploaded by users) on Pinterest. This dataset is more\nthan 200 times larger than MS COCO, the standard large-scale image dataset with\nsentence descriptions. In addition, we construct an evaluation dataset to\ndirectly assess the effectiveness of word embeddings in terms of finding\nsemantically similar or related words and phrases. The word\/phrase pairs in\nthis evaluation dataset are collected from the click data with millions of\nusers in an image search system, thus contain rich semantic relationships.\nBased on these datasets, we propose and compare several Recurrent Neural\nNetworks (RNNs) based multimodal (text and image) models. Experiments show that\nour model benefits from incorporating the visual information into the word\nembeddings, and a weight sharing strategy is crucial for learning such\nmultimodal embeddings. The project page is:\nhttp:\/\/www.stat.ucla.edu\/~junhua.mao\/multimodal_embedding.html\n","id":727}
{"Unnamed: 0.1":11728,"Unnamed: 0":11728.0,"anchor":"\"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach","positive":"  Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.\n","negative":"  The problem of learning long-term dependencies in sequences using Recurrent\nNeural Networks (RNNs) is still a major challenge. Recent methods have been\nsuggested to solve this problem by constraining the transition matrix to be\nunitary during training which ensures that its norm is equal to one and\nprevents exploding gradients. These methods either have limited expressiveness\nor scale poorly with the size of the network when compared with the simple RNN\ncase, especially when using stochastic gradient descent with a small mini-batch\nsize. Our contributions are as follows; we first show that constraining the\ntransition matrix to be unitary is a special case of an orthogonal constraint.\nThen we present a new parametrisation of the transition matrix which allows\nefficient training of an RNN while ensuring that the matrix is always\northogonal. Our results show that the orthogonal constraint on the transition\nmatrix applied through our parametrisation gives similar benefits to the\nunitary constraint, without the time complexity limitations.\n","id":728}
{"Unnamed: 0.1":11729,"Unnamed: 0":11729.0,"anchor":"Human Action Attribute Learning From Video Data Using Low-Rank\n  Representations","positive":"  Representation of human actions as a sequence of human body movements or\naction attributes enables the development of models for human activity\nrecognition and summarization. We present an extension of the low-rank\nrepresentation (LRR) model, termed the clustering-aware structure-constrained\nlow-rank representation (CS-LRR) model, for unsupervised learning of human\naction attributes from video data. Our model is based on the union-of-subspaces\n(UoS) framework, and integrates spectral clustering into the LRR optimization\nproblem for better subspace clustering results. We lay out an efficient linear\nalternating direction method to solve the CS-LRR optimization problem. We also\nintroduce a hierarchical subspace clustering approach, termed hierarchical\nCS-LRR, to learn the attributes without the need for a priori specification of\ntheir number. By visualizing and labeling these action attributes, the\nhierarchical model can be used to semantically summarize long video sequences\nof human actions at multiple resolutions. A human action or activity can also\nbe uniquely represented as a sequence of transitions from one action attribute\nto another, which can then be used for human action recognition. We demonstrate\nthe effectiveness of the proposed model for semantic summarization and action\nrecognition through comprehensive experiments on five real-world human action\ndatasets.\n","negative":"  Learning tasks such as those involving genomic data often poses a serious\nchallenge: the number of input features can be orders of magnitude larger than\nthe number of training examples, making it difficult to avoid overfitting, even\nwhen using the known regularization techniques. We focus here on tasks in which\nthe input is a description of the genetic variation specific to a patient, the\nsingle nucleotide polymorphisms (SNPs), yielding millions of ternary inputs.\nImproving the ability of deep learning to handle such datasets could have an\nimportant impact in precision medicine, where high-dimensional data regarding a\nparticular patient is used to make predictions of interest. Even though the\namount of data for such tasks is increasing, this mismatch between the number\nof examples and the number of inputs remains a concern. Naive implementations\nof classifier neural networks involve a huge number of free parameters in their\nfirst layer: each input feature is associated with as many parameters as there\nare hidden units. We propose a novel neural network parametrization which\nconsiderably reduces the number of free parameters. It is based on the idea\nthat we can first learn or provide a distributed representation for each input\nfeature (e.g. for each position in the genome where variations are observed),\nand then learn (with another neural network called the parameter prediction\nnetwork) how to map a feature's distributed representation to the vector of\nparameters specific to that feature in the classifier neural network (the\nweights which link the value of the feature to each of the hidden units). We\nshow experimentally on a population stratification task of interest to medical\nstudies that the proposed approach can significantly reduce both the number of\nparameters and the error rate of the classifier.\n","id":729}
{"Unnamed: 0.1":11730,"Unnamed: 0":11730.0,"anchor":"A Base Camp for Scaling AI","positive":"  Modern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach.\n","negative":"  We present a distributed (non-Bayesian) learning algorithm for the problem of\nparameter estimation with Gaussian noise. The algorithm is expressed as\nexplicit updates on the parameters of the Gaussian beliefs (i.e. means and\nprecision). We show a convergence rate of $O(1\/k)$ with the constant term\ndepending on the number of agents and the topology of the network. Moreover, we\nshow almost sure convergence to the optimal solution of the estimation problem\nfor the general case of time-varying directed graphs.\n","id":730}
{"Unnamed: 0.1":11731,"Unnamed: 0":11731.0,"anchor":"Supervised Opinion Aspect Extraction by Exploiting Past Extraction\n  Results","positive":"  One of the key tasks of sentiment analysis of product reviews is to extract\nproduct aspects or features that users have expressed opinions on. In this\nwork, we focus on using supervised sequence labeling as the base approach to\nperforming the task. Although several extraction methods using sequence\nlabeling methods such as Conditional Random Fields (CRF) and Hidden Markov\nModels (HMM) have been proposed, we show that this supervised approach can be\nsignificantly improved by exploiting the idea of concept sharing across\nmultiple domains. For example, \"screen\" is an aspect in iPhone, but not only\niPhone has a screen, many electronic devices have screens too. When \"screen\"\nappears in a review of a new domain (or product), it is likely to be an aspect\ntoo. Knowing this information enables us to do much better extraction in the\nnew domain. This paper proposes a novel extraction method exploiting this idea\nin the context of supervised sequence labeling. Experimental results show that\nit produces markedly better results than without using the past information.\n","negative":"  Anti-discrimination is an increasingly important task in data science. In\nthis paper, we investigate the problem of discovering both direct and indirect\ndiscrimination from the historical data, and removing the discriminatory\neffects before the data is used for predictive analysis (e.g., building\nclassifiers). We make use of the causal network to capture the causal structure\nof the data. Then we model direct and indirect discrimination as the\npath-specific effects, which explicitly distinguish the two types of\ndiscrimination as the causal effects transmitted along different paths in the\nnetwork. Based on that, we propose an effective algorithm for discovering\ndirect and indirect discrimination, as well as an algorithm for precisely\nremoving both types of discrimination while retaining good data utility.\nDifferent from previous works, our approaches can ensure that the predictive\nmodels built from the modified data will not incur discrimination in decision\nmaking. Experiments using real datasets show the effectiveness of our\napproaches.\n","id":731}
{"Unnamed: 0.1":11732,"Unnamed: 0":11732.0,"anchor":"DeMIAN: Deep Modality Invariant Adversarial Network","positive":"  Obtaining common representations from different modalities is important in\nthat they are interchangeable with each other in a classification problem. For\nexample, we can train a classifier on image features in the common\nrepresentations and apply it to the testing of the text features in the\nrepresentations. Existing multi-modal representation learning methods mainly\naim to extract rich information from paired samples and train a classifier by\nthe corresponding labels; however, collecting paired samples and their labels\nsimultaneously involves high labor costs. Addressing paired modal samples\nwithout their labels and single modal data with their labels independently is\nmuch easier than addressing labeled multi-modal data. To obtain the common\nrepresentations under such a situation, we propose to make the distributions\nover different modalities similar in the learned representations, namely\nmodality-invariant representations. In particular, we propose a novel algorithm\nfor modality-invariant representation learning, named Deep Modality Invariant\nAdversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation\n(DA). Using the modality-invariant representations learned by DeMIAN, we\nachieved better classification accuracy than with the state-of-the-art methods,\nespecially for some benchmark datasets of zero-shot learning.\n","negative":"  In this paper we discuss the stability properties of convolutional neural\nnetworks. Convolutional neural networks are widely used in machine learning. In\nclassification they are mainly used as feature extractors. Ideally, we expect\nsimilar features when the inputs are from the same class. That is, we hope to\nsee a small change in the feature vector with respect to a deformation on the\ninput signal. This can be established mathematically, and the key step is to\nderive the Lipschitz properties. Further, we establish that the stability\nresults can be extended for more general networks. We give a formula for\ncomputing the Lipschitz bound, and compare it with other methods to show it is\ncloser to the optimal value.\n","id":732}
{"Unnamed: 0.1":11733,"Unnamed: 0":11733.0,"anchor":"RSSL: Semi-supervised Learning in R","positive":"  In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature.\n","negative":"  Antimicrobial resistance is an important public health concern that has\nimplications in the practice of medicine worldwide. Accurately predicting\nresistance phenotypes from genome sequences shows great promise in promoting\nbetter use of antimicrobial agents, by determining which antibiotics are likely\nto be effective in specific clinical cases. In healthcare, this would allow for\nthe design of treatment plans tailored for specific individuals, likely\nresulting in better clinical outcomes for patients with bacterial infections.\nIn this work, we present the recent work of Drouin et al. (2016) on using Set\nCovering Machines to learn highly interpretable models of antibiotic resistance\nand complement it by providing a large scale application of their method to the\nentire PATRIC database. We report prediction results for 36 new datasets and\npresent the Kover AMR platform, a new web-based tool allowing the visualization\nand interpretation of the generated models.\n","id":733}
{"Unnamed: 0.1":11734,"Unnamed: 0":11734.0,"anchor":"Constructing Effective Personalized Policies Using Counterfactual\n  Inference from Biased Data Sets with Many Features","positive":"  This paper proposes a novel approach for constructing effective personalized\npolicies when the observed data lacks counter-factual information, is biased\nand possesses many features. The approach is applicable in a wide variety of\nsettings from healthcare to advertising to education to finance. These settings\nhave in common that the decision maker can observe, for each previous instance,\nan array of features of the instance, the action taken in that instance, and\nthe reward realized -- but not the rewards of actions that were not taken: the\ncounterfactual information. Learning in such settings is made even more\ndifficult because the observed data is typically biased by the existing policy\n(that generated the data) and because the array of features that might affect\nthe reward in a particular instance -- and hence should be taken into account\nin deciding on an action in each particular instance -- is often vast. The\napproach presented here estimates propensity scores for the observed data,\ninfers counterfactuals, identifies a (relatively small) number of features that\nare (most) relevant for each possible action and instance, and prescribes a\npolicy to be followed. Comparison of the proposed algorithm against the\nstate-of-art algorithm on actual datasets demonstrates that the proposed\nalgorithm achieves a significant improvement in performance.\n","negative":"  We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark.\n","id":734}
{"Unnamed: 0.1":11735,"Unnamed: 0":11735.0,"anchor":"On Spectral Analysis of Directed Signed Graphs","positive":"  It has been shown that the adjacency eigenspace of a network contains key\ninformation of its underlying structure. However, there has been no study on\nspectral analysis of the adjacency matrices of directed signed graphs. In this\npaper, we derive theoretical approximations of spectral projections from such\ndirected signed networks using matrix perturbation theory. We use the derived\ntheoretical results to study the influences of negative intra cluster and inter\ncluster directed edges on node spectral projections. We then develop a spectral\nclustering based graph partition algorithm, SC-DSG, and conduct evaluations on\nboth synthetic and real datasets. Both theoretical analysis and empirical\nevaluation demonstrate the effectiveness of the proposed algorithm.\n","negative":"  We learn recurrent neural network optimizers trained on simple synthetic\nfunctions by gradient descent. We show that these learned optimizers exhibit a\nremarkable degree of transfer in that they can be used to efficiently optimize\na broad range of derivative-free black-box functions, including Gaussian\nprocess bandits, simple control objectives, global optimization benchmarks and\nhyper-parameter tuning tasks. Up to the training horizon, the learned\noptimizers learn to trade-off exploration and exploitation, and compare\nfavourably with heavily engineered Bayesian optimization packages for\nhyper-parameter tuning.\n","id":735}
{"Unnamed: 0.1":11736,"Unnamed: 0":11736.0,"anchor":"Image-Text Multi-Modal Representation Learning by Adversarial\n  Backpropagation","positive":"  We present novel method for image-text multi-modal representation learning.\nIn our knowledge, this work is the first approach of applying adversarial\nlearning concept to multi-modal learning and not exploiting image-text pair\ninformation to learn multi-modal feature. We only use category information in\ncontrast with most previous methods using image-text pair information for\nmulti-modal embedding. In this paper, we show that multi-modal feature can be\nachieved without image-text pair information and our method makes more similar\ndistribution with image and text in multi-modal feature space than other\nmethods which use image-text pair information. And we show our multi-modal\nfeature has universal semantic information, even though it was trained for\ncategory prediction. Our model is end-to-end backpropagation, intuitive and\neasily extended to other multi-modal learning work.\n","negative":"  We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\nupdate.\n","id":736}
{"Unnamed: 0.1":11737,"Unnamed: 0":11737.0,"anchor":"Clustering Algorithms: A Comparative Approach","positive":"  Many real-world systems can be studied in terms of pattern recognition tasks,\nso that proper use (and understanding) of machine learning methods in practical\napplications becomes essential. While a myriad of classification methods have\nbeen proposed, there is no consensus on which methods are more suitable for a\ngiven dataset. As a consequence, it is important to comprehensively compare\nmethods in many possible scenarios. In this context, we performed a systematic\ncomparison of 7 well-known clustering methods available in the R language. In\norder to account for the many possible variations of data, we considered\nartificial datasets with several tunable properties (number of classes,\nseparation between classes, etc). In addition, we also evaluated the\nsensitivity of the clustering methods with regard to their parameters\nconfiguration. The results revealed that, when considering the default\nconfigurations of the adopted methods, the spectral approach usually\noutperformed the other clustering algorithms. We also found that the default\nconfiguration of the adopted implementations was not accurate. In these cases,\na simple approach based on random selection of parameters values proved to be a\ngood alternative to improve the performance. All in all, the reported approach\nprovides subsidies guiding the choice of clustering algorithms.\n","negative":"  Learning from the crowd has become increasingly popular in the Web and social\nmedia. There is a wide variety of crowdlearning sites in which, on the one\nhand, users learn from the knowledge that other users contribute to the site,\nand, on the other hand, knowledge is reviewed and curated by the same users\nusing assessment measures such as upvotes or likes.\n  In this paper, we present a probabilistic modeling framework of\ncrowdlearning, which uncovers the evolution of a user's expertise over time by\nleveraging other users' assessments of her contributions. The model allows for\nboth off-site and on-site learning and captures forgetting of knowledge. We\nthen develop a scalable estimation method to fit the model parameters from\nmillions of recorded learning and contributing events. We show the\neffectiveness of our model by tracing activity of ~25 thousand users in Stack\nOverflow over a 4.5 year period. We find that answers with high knowledge value\nare rare. Newbies and experts tend to acquire less knowledge than users in the\nmiddle range. Prolific learners tend to be also proficient contributors that\npost answers with high knowledge value.\n","id":737}
{"Unnamed: 0.1":11738,"Unnamed: 0":11738.0,"anchor":"Multi-Region Neural Representation: A novel model for decoding visual\n  stimuli in human brains","positive":"  Multivariate Pattern (MVP) classification holds enormous potential for\ndecoding visual stimuli in the human brain by employing task-based fMRI data\nsets. There is a wide range of challenges in the MVP techniques, i.e.\ndecreasing noise and sparsity, defining effective regions of interest (ROIs),\nvisualizing results, and the cost of brain studies. In overcoming these\nchallenges, this paper proposes a novel model of neural representation, which\ncan automatically detect the active regions for each visual stimulus and then\nutilize these anatomical regions for visualizing and analyzing the functional\nactivities. Therefore, this model provides an opportunity for neuroscientists\nto ask this question: what is the effect of a stimulus on each of the detected\nregions instead of just study the fluctuation of voxels in the manually\nselected ROIs. Moreover, our method introduces analyzing snapshots of brain\nimage for decreasing sparsity rather than using the whole of fMRI time series.\nFurther, a new Gaussian smoothing method is proposed for removing noise of\nvoxels in the level of ROIs. The proposed method enables us to combine\ndifferent fMRI data sets for reducing the cost of brain studies. Experimental\nstudies on 4 visual categories (words, consonants, objects and nonsense photos)\nconfirm that the proposed method achieves superior performance to\nstate-of-the-art methods.\n","negative":"  The increasing availability of implicit feedback datasets has raised the\ninterest in developing effective collaborative filtering techniques able to\ndeal asymmetrically with unambiguous positive feedback and ambiguous negative\nfeedback. In this paper, we propose a principled kernel-based collaborative\nfiltering method for top-N item recommendation with implicit feedback. We\npresent an efficient implementation using the linear kernel, and we show how to\ngeneralize it to kernels of the dot product family preserving the efficiency.\nWe also investigate on the elements which influence the sparsity of a standard\ncosine kernel. This analysis shows that the sparsity of the kernel strongly\ndepends on the properties of the dataset, in particular on the long tail\ndistribution. We compare our method with state-of-the-art algorithms achieving\ngood results both in terms of efficiency and effectiveness.\n","id":738}
{"Unnamed: 0.1":11739,"Unnamed: 0":11739.0,"anchor":"Correlated signal inference by free energy exploration","positive":"  The inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations.\n","negative":"  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n","id":739}
{"Unnamed: 0.1":11740,"Unnamed: 0":11740.0,"anchor":"Unsupervised Learning for Computational Phenotyping","positive":"  With large volumes of health care data comes the research area of\ncomputational phenotyping, making use of techniques such as machine learning to\ndescribe illnesses and other clinical concepts from the data itself. The\n\"traditional\" approach of using supervised learning relies on a domain expert,\nand has two main limitations: requiring skilled humans to supply correct labels\nlimits its scalability and accuracy, and relying on existing clinical\ndescriptions limits the sorts of patterns that can be found. For instance, it\nmay fail to acknowledge that a disease treated as a single condition may really\nhave several subtypes with different phenotypes, as seems to be the case with\nasthma and heart disease. Some recent papers cite successes instead using\nunsupervised learning. This shows great potential for finding patterns in\nElectronic Health Records that would otherwise be hidden and that can lead to\ngreater understanding of conditions and treatments. This work implements a\nmethod derived strongly from Lasko et al., but implements it in Apache Spark\nand Python and generalizes it to laboratory time-series data in MIMIC-III. It\nis released as an open-source tool for exploration, analysis, and\nvisualization, available at https:\/\/github.com\/Hodapp87\/mimic3_phenotyping\n","negative":"  Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases.\n","id":740}
{"Unnamed: 0.1":11741,"Unnamed: 0":11741.0,"anchor":"Randomized Block Frank-Wolfe for Convergent Large-Scale Learning","positive":"  Owing to their low-complexity iterations, Frank-Wolfe (FW) solvers are well\nsuited for various large-scale learning tasks. When block-separable constraints\nare present, randomized block FW (RB-FW) has been shown to further reduce\ncomplexity by updating only a fraction of coordinate blocks per iteration. To\ncircumvent the limitations of existing methods, the present work develops step\nsizes for RB-FW that enable a flexible selection of the number of blocks to\nupdate per iteration while ensuring convergence and feasibility of the\niterates. To this end, convergence rates of RB-FW are established through\ncomputational bounds on a primal sub-optimality measure and on the duality gap.\nThe novel bounds extend the existing convergence analysis, which only applies\nto a step-size sequence that does not generally lead to feasible iterates.\nFurthermore, two classes of step-size sequences that guarantee feasibility of\nthe iterates are also proposed to enhance flexibility in choosing decay rates.\nThe novel convergence results are markedly broadened to encompass also\nnonconvex objectives, and further assert that RB-FW with exact line-search\nreaches a stationary point at rate $\\mathcal{O}(1\/\\sqrt{t})$. Performance of\nRB-FW with different step sizes and number of blocks is demonstrated in two\napplications, namely charging of electrical vehicles and structural support\nvector machines. Extensive simulated tests demonstrate the performance\nimprovement of RB-FW relative to existing randomized single-block FW methods.\n","negative":"  Existing multi-objective reinforcement learning (MORL) algorithms do not\naccount for objectives that arise from players with differing beliefs.\nConcretely, consider two players with different beliefs and utility functions\nwho may cooperate to build a machine that takes actions on their behalf. A\nrepresentation is needed for how much the machine's policy will prioritize each\nplayer's interests over time. Assuming the players have reached common\nknowledge of their situation, this paper derives a recursion that any Pareto\noptimal policy must satisfy. Two qualitative observations can be made from the\nrecursion: the machine must (1) use each player's own beliefs in evaluating how\nwell an action will serve that player's utility function, and (2) shift the\nrelative priority it assigns to each player's expected utilities over time, by\na factor proportional to how well that player's beliefs predict the machine's\ninputs. Observation (2) represents a substantial divergence from na\\\"{i}ve\nlinear utility aggregation (as in Harsanyi's utilitarian theorem, and existing\nMORL algorithms), which is shown here to be inadequate for Pareto optimal\nsequential decision-making on behalf of players with different beliefs.\n","id":741}
{"Unnamed: 0.1":11742,"Unnamed: 0":11742.0,"anchor":"Steerable CNNs","positive":"  It has long been recognized that the invariance and equivariance properties\nof a representation are critically important for success in many vision tasks.\nIn this paper we present Steerable Convolutional Neural Networks, an efficient\nand flexible class of equivariant convolutional networks. We show that\nsteerable CNNs achieve state of the art results on the CIFAR image\nclassification benchmark. The mathematical theory of steerable representations\nreveals a type system in which any steerable representation is a composition of\nelementary feature types, each one associated with a particular kind of\nsymmetry. We show how the parameter cost of a steerable filter bank depends on\nthe types of the input and output features, and show how to use this knowledge\nto construct CNNs that utilize parameters effectively.\n","negative":"  A deep learning approach has been widely applied in sequence modeling\nproblems. In terms of automatic speech recognition (ASR), its performance has\nsignificantly been improved by increasing large speech corpus and deeper neural\nnetwork. Especially, recurrent neural network and deep convolutional neural\nnetwork have been applied in ASR successfully. Given the arising problem of\ntraining speed, we build a novel deep recurrent convolutional network for\nacoustic modeling and then apply deep residual learning to it. Our experiments\nshow that it has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network. In the\nexperiments, we compare the convergence speed of our novel deep recurrent\nconvolutional networks and traditional deep convolutional recurrent networks.\nWith faster convergence speed, our novel deep recurrent convolutional networks\ncan reach the comparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel deep recurret\nconvolutional networks. Finally, we evaluate all our experimental networks by\nphoneme error rate (PER) with our proposed bidirectional statistical n-gram\nlanguage model. Our evaluation results show that our newly proposed deep\nrecurrent convolutional network applied with deep residual learning can reach\nthe best PER of 17.33\\% with the fastest convergence speed on TIMIT database.\nThe outstanding performance of our novel deep recurrent convolutional neural\nnetwork with deep residual learning indicates that it can be potentially\nadopted in other sequential problems.\n","id":742}
{"Unnamed: 0.1":11743,"Unnamed: 0":11743.0,"anchor":"Theory-guided Data Science: A New Paradigm for Scientific Discovery from\n  Data","positive":"  Data science models, although successful in a number of commercial domains,\nhave had limited applicability in scientific problems involving complex\nphysical phenomena. Theory-guided data science (TGDS) is an emerging paradigm\nthat aims to leverage the wealth of scientific knowledge for improving the\neffectiveness of data science models in enabling scientific discovery. The\noverarching vision of TGDS is to introduce scientific consistency as an\nessential component for learning generalizable models. Further, by producing\nscientifically interpretable models, TGDS aims to advance our scientific\nunderstanding by discovering novel domain insights. Indeed, the paradigm of\nTGDS has started to gain prominence in a number of scientific disciplines such\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\nscience, bio-marker discovery, climate science, and hydrology. In this paper,\nwe formally conceptualize the paradigm of TGDS and present a taxonomy of\nresearch themes in TGDS. We describe several approaches for integrating domain\nknowledge in different research themes using illustrative examples from\ndifferent disciplines. We also highlight some of the promising avenues of novel\nresearch for realizing the full potential of theory-guided data science.\n","negative":"  Cybersecurity is increasingly threatened by advanced and persistent attacks.\nAs these attacks are often designed to disable a system (or a critical\nresource, e.g., a user account) repeatedly, it is crucial for the defender to\nkeep updating its security measures to strike a balance between the risk of\nbeing compromised and the cost of security updates. Moreover, these decisions\noften need to be made with limited and delayed feedback due to the stealthy\nnature of advanced attacks. In addition to targeted attacks, such an optimal\ntiming policy under incomplete information has broad applications in\ncybersecurity. Examples include key rotation, password change, application of\npatches, and virtual machine refreshing. However, rigorous studies of optimal\ntiming are rare. Further, existing solutions typically rely on a pre-defined\nattack model that is known to the defender, which is often not the case in\npractice. In this work, we make an initial effort towards achieving optimal\ntiming of security updates in the face of unknown stealthy attacks. We consider\na variant of the influential FlipIt game model with asymmetric feedback and\nunknown attack time distribution, which provides a general model to consecutive\nsecurity updates. The defender's problem is then modeled as a time associative\nbandit problem with dependent arms. We derive upper confidence bound based\nlearning policies that achieve low regret compared with optimal periodic\ndefense strategies that can only be derived when attack time distributions are\nknown.\n","id":743}
{"Unnamed: 0.1":11744,"Unnamed: 0":11744.0,"anchor":"ASAP: Asynchronous Approximate Data-Parallel Computation","positive":"  Emerging workloads, such as graph processing and machine learning are\napproximate because of the scale of data involved and the stochastic nature of\nthe underlying algorithms. These algorithms are often distributed over multiple\nmachines using bulk-synchronous processing (BSP) or other synchronous\nprocessing paradigms such as map-reduce. However, data parallel processing\nprimitives such as repeated barrier and reduce operations introduce high\nsynchronization overheads. Hence, many existing data-processing platforms use\nasynchrony and staleness to improve data-parallel job performance. Often, these\nsystems simply change the synchronous communication to asynchronous between the\nworker nodes in the cluster. This improves the throughput of data processing\nbut results in poor accuracy of the final output since different workers may\nprogress at different speeds and process inconsistent intermediate outputs.\n  In this paper, we present ASAP, a model that provides asynchronous and\napproximate processing semantics for data-parallel computation. ASAP provides\nfine-grained worker synchronization using NOTIFY-ACK semantics that allows\nindependent workers to run asynchronously. ASAP also provides stochastic reduce\nthat provides approximate but guaranteed convergence to the same result as an\naggregated all-reduce. In our results, we show that ASAP can reduce\nsynchronization costs and provides 2-10X speedups in convergence and up to 10X\nsavings in network costs for distributed machine learning applications and\nprovides strong convergence guarantees.\n","negative":"  Code super-optimization is the task of transforming any given program to a\nmore efficient version while preserving its input-output behaviour. In some\nsense, it is similar to the paraphrase problem from natural language processing\nwhere the intention is to change the syntax of an utterance without changing\nits semantics. Code-optimization has been the subject of years of research that\nhas resulted in the development of rule-based transformation strategies that\nare used by compilers. More recently, however, a class of stochastic search\nbased methods have been shown to outperform these strategies. This approach\ninvolves repeated sampling of modifications to the program from a proposal\ndistribution, which are accepted or rejected based on whether they preserve\ncorrectness, and the improvement they achieve. These methods, however, neither\nlearn from past behaviour nor do they try to leverage the semantics of the\nprogram under consideration. Motivated by this observation, we present a novel\nlearning based approach for code super-optimization. Intuitively, our method\nworks by learning the proposal distribution using unbiased estimators of the\ngradient of the expected improvement. Experiments on benchmarks comprising of\nautomatically generated as well as existing (\"Hacker's Delight\") programs show\nthat the proposed method is able to significantly outperform state of the art\napproaches for code super-optimization.\n","id":744}
{"Unnamed: 0.1":11745,"Unnamed: 0":11745.0,"anchor":"A Sparse Nonlinear Classifier Design Using AUC Optimization","positive":"  AUC (Area under the ROC curve) is an important performance measure for\napplications where the data is highly imbalanced. Learning to maximize AUC\nperformance is thus an important research problem. Using a max-margin based\nsurrogate loss function, AUC optimization problem can be approximated as a\npairwise rankSVM learning problem. Batch learning methods for solving the\nkernelized version of this problem suffer from scalability and may not result\nin sparse classifiers. Recent years have witnessed an increased interest in the\ndevelopment of online or single-pass online learning algorithms that design a\nclassifier by maximizing the AUC performance. The AUC performance of nonlinear\nclassifiers, designed using online methods, is not comparable with that of\nnonlinear classifiers designed using batch learning algorithms on many\nreal-world datasets. Motivated by these observations, we design a scalable\nalgorithm for maximizing AUC performance by greedily adding the required number\nof basis functions into the classifier model. The resulting sparse classifiers\nperform faster inference. Our experimental results show that the level of\nsparsity achievable can be order of magnitude smaller than the Kernel RankSVM\nmodel without affecting the AUC performance much.\n","negative":"  Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks.\n","id":745}
{"Unnamed: 0.1":11746,"Unnamed: 0":11746.0,"anchor":"Reproducible Pattern Recognition Research: The Case of Optimistic SSL","positive":"  In this paper, we discuss the approaches we took and trade-offs involved in\nmaking a paper on a conceptual topic in pattern recognition research fully\nreproducible. We discuss our definition of reproducibility, the tools used, how\nthe analysis was set up, show some examples of alternative analyses the code\nenables and discuss our views on reproducibility.\n","negative":"  We design differentially private algorithms for the problem of online linear\noptimization in the full information and bandit settings with optimal\n$\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our\nresults demonstrate that $\\epsilon$-differential privacy may be ensured for\nfree -- in particular, the regret bounds scale as\n$O(\\sqrt{T})+\\tilde{O}\\left(\\frac{1}{\\epsilon}\\right)$. For bandit linear\noptimization, and as a special case, for non-stochastic multi-armed bandits,\nthe proposed algorithm achieves a regret of\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}\\sqrt{T}\\right)$, while the previously known\nbest regret bound was\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}T^{\\frac{2}{3}}\\right)$.\n","id":746}
{"Unnamed: 0.1":11747,"Unnamed: 0":11747.0,"anchor":"A Hybrid Both Filter and Wrapper Feature Selection Method for Microarray\n  Classification","positive":"  Gene expression data is widely used in disease analysis and cancer diagnosis.\nHowever, since gene expression data could contain thousands of genes\nsimultaneously, successful microarray classification is rather difficult.\nFeature selection is an important pre-treatment for any classification process.\nSelecting a useful gene subset as a classifier not only decreases the\ncomputational time and cost, but also increases classification accuracy. In\nthis study, we applied the information gain method as a filter approach, and an\nimproved binary particle swarm optimization as a wrapper approach to implement\nfeature selection; selected gene subsets were used to evaluate the performance\nof classification. Experimental results show that by employing the proposed\nmethod fewer gene subsets needed to be selected and better classification\naccuracy could be obtained.\n","negative":"  In this paper we explore the performance limits of Apache Spark for machine\nlearning applications. We begin by analyzing the characteristics of a\nstate-of-the-art distributed machine learning algorithm implemented in Spark\nand compare it to an equivalent reference implementation using the high\nperformance computing framework MPI. We identify critical bottlenecks of the\nSpark framework and carefully study their implications on the performance of\nthe algorithm. In order to improve Spark performance we then propose a number\nof practical techniques to alleviate some of its overheads. However, optimizing\ncomputational efficiency and framework related overheads is not the only key to\nperformance -- we demonstrate that in order to get the best performance out of\nany implementation it is necessary to carefully tune the algorithm to the\nrespective trade-off between computation time and communication latency. The\noptimal trade-off depends on both the properties of the distributed algorithm\nas well as infrastructure and framework-related characteristics. Finally, we\napply these technical and algorithmic optimizations to three different\ndistributed linear machine learning algorithms that have been implemented in\nSpark. We present results using five large datasets and demonstrate that by\nusing the proposed optimizations, we can achieve a reduction in the performance\ndifference between Spark and MPI from 20x to 2x.\n","id":747}
{"Unnamed: 0.1":11748,"Unnamed: 0":11748.0,"anchor":"Clustering with Confidence: Finding Clusters with Statistical Guarantees","positive":"  Clustering is a widely used unsupervised learning method for finding\nstructure in the data. However, the resulting clusters are typically presented\nwithout any guarantees on their robustness; slightly changing the used data\nsample or re-running a clustering algorithm involving some stochastic component\nmay lead to completely different clusters. There is, hence, a need for\ntechniques that can quantify the instability of the generated clusters. In this\nstudy, we propose a technique for quantifying the instability of a clustering\nsolution and for finding robust clusters, termed core clusters, which\ncorrespond to clusters where the co-occurrence probability of each data item\nwithin a cluster is at least $1 - \\alpha$. We demonstrate how solving the core\nclustering problem is linked to finding the largest maximal cliques in a graph.\nWe show that the method can be used with both clustering and classification\nalgorithms. The proposed method is tested on both simulated and real datasets.\nThe results show that the obtained clusters indeed meet the guarantees on\nrobustness.\n","negative":"  Motivated by models of human decision making proposed to explain commonly\nobserved deviations from conventional expected value preferences, we formulate\ntwo stochastic multi-armed bandit problems with distorted probabilities on the\ncost distributions: the classic $K$-armed bandit and the linearly parameterized\nbandit. In both settings, we propose algorithms that are inspired by Upper\nConfidence Bound (UCB), incorporate cost distortions, and exhibit sublinear\nregret assuming \\holder continuous weight distortion functions. For the\n$K$-armed setting, we show that the algorithm, called W-UCB, achieves\nproblem-dependent regret $O(L^2 M^2 \\log n\/ \\Delta^{\\frac{2}{\\alpha}-1})$,\nwhere $n$ is the number of plays, $\\Delta$ is the gap in distorted expected\nvalue between the best and next best arm, $L$ and $\\alpha$ are the H\\\"{o}lder\nconstants for the distortion function, and $M$ is an upper bound on costs, and\na problem-independent regret bound of\n$O((KL^2M^2)^{\\alpha\/2}n^{(2-\\alpha)\/2})$. We also present a matching lower\nbound on the regret, showing that the regret of W-UCB is essentially\nunimprovable over the class of H\\\"{o}lder-continuous weight distortions. For\nthe linearly parameterized setting, we develop a new algorithm, a variant of\nthe Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm called\nWOFUL (Weight-distorted OFUL), and show that it has regret $O(d\\sqrt{n} \\;\n\\mbox{polylog}(n))$ with high probability, for sub-Gaussian cost distributions.\nFinally, numerical examples demonstrate the advantages resulting from using\ndistortion-aware learning algorithms.\n","id":748}
{"Unnamed: 0.1":11749,"Unnamed: 0":11749.0,"anchor":"Automatic Composition and Optimization of Multicomponent Predictive\n  Systems With an Extended Auto-WEKA","positive":"  Composition and parameterization of multicomponent predictive systems (MCPSs)\nconsisting of chains of data transformation steps are a challenging task.\nAuto-WEKA is a tool to automate the combined algorithm selection and\nhyperparameter (CASH) optimization problem. In this paper, we extend the CASH\nproblem and Auto-WEKA to support the MCPS, including preprocessing steps for\nboth classification and regression tasks. We define the optimization problem in\nwhich the search space consists of suitably parameterized Petri nets forming\nthe sought MCPS solutions. In the experimental analysis, we focus on examining\nthe impact of considerably extending the search space (from approximately\n22,000 to 812 billion possible combinations of methods and categorical\nhyperparameters). In a range of extensive experiments, three different\noptimization strategies are used to automatically compose MCPSs for 21 publicly\navailable data sets. The diversity of the composed MCPSs found is an indication\nthat fully and automatically exploiting different combinations of data cleaning\nand preprocessing techniques is possible and highly beneficial for different\npredictive models. We also present the results on seven data sets from real\nchemical production processes. Our findings can have a major impact on the\ndevelopment of high-quality predictive models as well as their maintenance and\nscalability aspects needed in modern applications and deployment scenarios.\n","negative":"  The current mainstream approach to train natural language systems is to\nexpose them to large amounts of text. This passive learning is problematic if\nwe are interested in developing interactive machines, such as conversational\nagents. We propose a framework for language learning that relies on multi-agent\ncommunication. We study this learning in the context of referential games. In\nthese games, a sender and a receiver see a pair of images. The sender is told\none of them is the target and is allowed to send a message from a fixed,\narbitrary vocabulary to the receiver. The receiver must rely on this message to\nidentify the target. Thus, the agents develop their own language interactively\nout of the need to communicate. We show that two networks with simple\nconfigurations are able to learn to coordinate in the referential game. We\nfurther explore how to make changes to the game environment to cause the \"word\nmeanings\" induced in the game to better reflect intuitive semantic properties\nof the images. In addition, we present a simple strategy for grounding the\nagents' code into natural language. Both of these are necessary steps towards\ndeveloping machines that are able to communicate with humans productively.\n","id":749}
{"Unnamed: 0.1":11750,"Unnamed: 0":11750.0,"anchor":"Provable learning of Noisy-or Networks","positive":"  Many machine learning applications use latent variable models to explain\nstructure in data, whereby visible variables (= coordinates of the given\ndatapoint) are explained as a probabilistic function of some hidden variables.\nFinding parameters with the maximum likelihood is NP-hard even in very simple\nsettings. In recent years, provably efficient algorithms were nevertheless\ndeveloped for models with linear structures: topic models, mixture models,\nhidden markov models, etc. These algorithms use matrix or tensor decomposition,\nand make some reasonable assumptions about the parameters of the underlying\nmodel.\n  But matrix or tensor decomposition seems of little use when the latent\nvariable model has nonlinearities. The current paper shows how to make\nprogress: tensor decomposition is applied for learning the single-layer {\\em\nnoisy or} network, which is a textbook example of a Bayes net, and used for\nexample in the classic QMR-DT software for diagnosing which disease(s) a\npatient may have by observing the symptoms he\/she exhibits.\n  The technical novelty here, which should be useful in other settings in\nfuture, is analysis of tensor decomposition in presence of systematic error\n(i.e., where the noise\/error is correlated with the signal, and doesn't\ndecrease as number of samples goes to infinity). This requires rethinking all\nsteps of tensor decomposition methods from the ground up.\n  For simplicity our analysis is stated assuming that the network parameters\nwere chosen from a probability distribution but the method seems more generally\napplicable.\n","negative":"  Deep neural network models, though very powerful and highly successful, are\ncomputationally expensive in terms of space and time. Recently, there have been\na number of attempts on binarizing the network weights and activations. This\ngreatly reduces the network size, and replaces the underlying multiplications\nto additions or even XNOR bit operations. However, existing binarization\nschemes are based on simple matrix approximation and ignore the effect of\nbinarization on the loss. In this paper, we propose a proximal Newton algorithm\nwith diagonal Hessian approximation that directly minimizes the loss w.r.t. the\nbinarized weights. The underlying proximal step has an efficient closed-form\nsolution, and the second-order information can be efficiently obtained from the\nsecond moments already computed by the Adam optimizer. Experiments on both\nfeedforward and recurrent networks show that the proposed loss-aware\nbinarization algorithm outperforms existing binarization schemes, and is also\nmore robust for wide and deep networks.\n","id":750}
{"Unnamed: 0.1":11751,"Unnamed: 0":11751.0,"anchor":"The Predictron: End-To-End Learning and Planning","positive":"  One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.\n","negative":"  It is common that a trained classification model is applied to the operating\ndata that is deviated from the training data because of noise. This paper\ndemonstrates that an ensemble classifier, Diversified Multiple Tree (DMT), is\nmore robust in classifying noisy data than other widely used ensemble methods.\nDMT is tested on three real world biomedical data sets from different\nlaboratories in comparison with four benchmark ensemble classifiers.\nExperimental results show that DMT is significantly more accurate than other\nbenchmark ensemble classifiers on noisy test data. We also discuss a limitation\nof DMT and its possible variations.\n","id":751}
{"Unnamed: 0.1":11752,"Unnamed: 0":11752.0,"anchor":"The Pessimistic Limits and Possibilities of Margin-based Losses in\n  Semi-supervised Learning","positive":"  Consider a classification problem where we have both labeled and unlabeled\ndata available. We show that for linear classifiers defined by convex\nmargin-based surrogate losses that are decreasing, it is impossible to\nconstruct any semi-supervised approach that is able to guarantee an improvement\nover the supervised classifier measured by this surrogate loss on the labeled\nand unlabeled data. For convex margin-based loss functions that also increase,\nwe demonstrate safe improvements are possible.\n","negative":"  To effectively analyze and design cyberphysical systems (CPS), designers\ntoday have to combat the data deluge problem, i.e., the burden of processing\nintractably large amounts of data produced by complex models and experiments.\nIn this work, we utilize monotonic Parametric Signal Temporal Logic (PSTL) to\ndesign features for unsupervised classification of time series data. This\nenables using off-the-shelf machine learning tools to automatically cluster\nsimilar traces with respect to a given PSTL formula. We demonstrate how this\ntechnique produces interpretable formulas that are amenable to analysis and\nunderstanding using a few representative examples. We illustrate this with case\nstudies related to automotive engine testing, highway traffic analysis, and\nauto-grading massively open online courses.\n","id":752}
{"Unnamed: 0.1":11753,"Unnamed: 0":11753.0,"anchor":"Efficient iterative policy optimization","positive":"  We tackle the issue of finding a good policy when the number of policy\nupdates is limited. This is done by approximating the expected policy reward as\na sequence of concave lower bounds which can be efficiently maximized,\ndrastically reducing the number of policy updates required to achieve good\nperformance. We also extend existing methods to negative rewards, enabling the\nuse of control variates.\n","negative":"  We consider a firm that sells a large number of products to its customers in\nan online fashion. Each product is described by a high dimensional feature\nvector, and the market value of a product is assumed to be linear in the values\nof its features. Parameters of the valuation model are unknown and can change\nover time. The firm sequentially observes a product's features and can use the\nhistorical sales data (binary sale\/no sale feedbacks) to set the price of\ncurrent product, with the objective of maximizing the collected revenue. We\nmeasure the performance of a dynamic pricing policy via regret, which is the\nexpected revenue loss compared to a clairvoyant that knows the sequence of\nmodel parameters in advance.\n  We propose a pricing policy based on projected stochastic gradient descent\n(PSGD) and characterize its regret in terms of time $T$, features dimension\n$d$, and the temporal variability in the model parameters, $\\delta_t$. We\nconsider two settings. In the first one, feature vectors are chosen\nantagonistically by nature and we prove that the regret of PSGD pricing policy\nis of order $O(\\sqrt{T} + \\sum_{t=1}^T \\sqrt{t}\\delta_t)$. In the second\nsetting (referred to as stochastic features model), the feature vectors are\ndrawn independently from an unknown distribution. We show that in this case,\nthe regret of PSGD pricing policy is of order $O(d^2 \\log T + \\sum_{t=1}^T\nt\\delta_t\/d)$.\n","id":753}
{"Unnamed: 0.1":11754,"Unnamed: 0":11754.0,"anchor":"A Deep Learning Approach To Multiple Kernel Fusion","positive":"  Kernel fusion is a popular and effective approach for combining multiple\nfeatures that characterize different aspects of data. Traditional approaches\nfor Multiple Kernel Learning (MKL) attempt to learn the parameters for\ncombining the kernels through sophisticated optimization procedures. In this\npaper, we propose an alternative approach that creates dense embeddings for\ndata using the kernel similarities and adopts a deep neural network\narchitecture for fusing the embeddings. In order to improve the effectiveness\nof this network, we introduce the kernel dropout regularization strategy\ncoupled with the use of an expanded set of composition kernels. Experiment\nresults on a real-world activity recognition dataset show that the proposed\narchitecture is effective in fusing kernels and achieves state-of-the-art\nperformance.\n","negative":"  We study the problem of recovering an incomplete $m\\times n$ matrix of rank\n$r$ with columns arriving online over time. This is known as the problem of\nlife-long matrix completion, and is widely applied to recommendation system,\ncomputer vision, system identification, etc. The challenge is to design\nprovable algorithms tolerant to a large amount of noises, with small sample\ncomplexity. In this work, we give algorithms achieving strong guarantee under\ntwo realistic noise models. In bounded deterministic noise, an adversary can\nadd any bounded yet unstructured noise to each column. For this problem, we\npresent an algorithm that returns a matrix of a small error, with sample\ncomplexity almost as small as the best prior results in the noiseless case. For\nsparse random noise, where the corrupted columns are sparse and drawn randomly,\nwe give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by\nprobability at least $1-\\delta$ with sample complexity as small as\n$O\\left(\\mu_0rn\\log (r\/\\delta)\\right)$. This result advances the\nstate-of-the-art work and matches the lower bound in a worst case. We also\nstudy the scenario where the hidden matrix lies on a mixture of subspaces and\nshow that the sample complexity can be even smaller. Our proposed algorithms\nperform well experimentally in both synthetic and real-world datasets.\n","id":754}
{"Unnamed: 0.1":11755,"Unnamed: 0":11755.0,"anchor":"Meta-Unsupervised-Learning: A supervised approach to unsupervised\n  learning","positive":"  We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.\n","negative":"  Point patterns are sets or multi-sets of unordered elements that can be found\nin numerous data sources. However, in data analysis tasks such as\nclassification and novelty detection, appropriate statistical models for point\npattern data have not received much attention. This paper proposes the\nmodelling of point pattern data via random finite sets (RFS). In particular, we\npropose appropriate likelihood functions, and a maximum likelihood estimator\nfor learning a tractable family of RFS models. In novelty detection, we propose\nnovel ranking functions based on RFS models, which substantially improve\nperformance.\n","id":755}
{"Unnamed: 0.1":11756,"Unnamed: 0":11756.0,"anchor":"Geometric descent method for convex composite minimization","positive":"  In this paper, we extend the geometric descent method recently proposed by\nBubeck, Lee and Singh to tackle nonsmooth and strongly convex composite\nproblems. We prove that our proposed algorithm, dubbed geometric proximal\ngradient method (GeoPG), converges with a linear rate $(1-1\/\\sqrt{\\kappa})$ and\nthus achieves the optimal rate among first-order methods, where $\\kappa$ is the\ncondition number of the problem. Numerical results on linear regression and\nlogistic regression with elastic net regularization show that GeoPG compares\nfavorably with Nesterov's accelerated proximal gradient method, especially when\nthe problem is ill-conditioned.\n","negative":"  Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions.\n","id":756}
{"Unnamed: 0.1":11757,"Unnamed: 0":11757.0,"anchor":"Deep Learning and Hierarchal Generative Models","positive":"  It is argued that deep learning is efficient for data that is generated from\nhierarchal generative models. Examples of such generative models include\nwavelet scattering networks, functions of compositional structure, and deep\nrendering models. Unfortunately so far, for all such models, it is either not\nrigorously known that they can be learned efficiently, or it is not known that\n\"deep algorithms\" are required in order to learn them.\n  We propose a simple family of \"generative hierarchal models\" which can be\nefficiently learned and where \"deep\" algorithm are necessary for learning. Our\ndefinition of \"deep\" algorithms is based on the empirical observation that deep\nnets necessarily use correlations between features. More formally, we show that\nin a semi-supervised setting, given access to low-order moments of the labeled\ndata and all of the unlabeled data, it is information theoretically impossible\nto perform classification while at the same time there is an efficient\nalgorithm, that given all labelled and unlabeled data, perfectly labels all\nunlabelled data with high probability.\n  For the proof, we use and strengthen the fact that Belief Propagation does\nnot admit a good approximation in terms of linear functions.\n","negative":"  We consider the problem of learning hierarchical policies for Reinforcement\nLearning able to discover options, an option corresponding to a sub-policy over\na set of primitive actions. Different models have been proposed during the last\ndecade that usually rely on a predefined set of options. We specifically\naddress the problem of automatically discovering options in decision processes.\nWe describe a new learning model called Budgeted Option Neural Network (BONN)\nable to discover options based on a budgeted learning objective. The BONN model\nis evaluated on different classical RL problems, demonstrating both\nquantitative and qualitative interesting results.\n","id":757}
{"Unnamed: 0.1":11758,"Unnamed: 0":11758.0,"anchor":"Selecting Bases in Spectral learning of Predictive State Representations\n  via Model Entropy","positive":"  Predictive State Representations (PSRs) are powerful techniques for modelling\ndynamical systems, which represent a state as a vector of predictions about\nfuture observable events (tests). In PSRs, one of the fundamental problems is\nthe learning of the PSR model of the underlying system. Recently, spectral\nmethods have been successfully used to address this issue by treating the\nlearning problem as the task of computing an singular value decomposition (SVD)\nover a submatrix of a special type of matrix called the Hankel matrix. Under\nthe assumptions that the rows and columns of the submatrix of the Hankel Matrix\nare sufficient~(which usually means a very large number of rows and columns,\nand almost fails in practice) and the entries of the matrix can be estimated\naccurately, it has been proven that the spectral approach for learning PSRs is\nstatistically consistent and the learned parameters can converge to the true\nparameters. However, in practice, due to the limit of the computation ability,\nonly a finite set of rows or columns can be chosen to be used for the spectral\nlearning. While different sets of columns usually lead to variant accuracy of\nthe learned model, in this paper, we propose an approach for selecting the set\nof columns, namely basis selection, by adopting a concept of model entropy to\nmeasure the accuracy of the learned model. Experimental results are shown to\ndemonstrate the effectiveness of the proposed approach.\n","negative":"  This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin.\n","id":758}
{"Unnamed: 0.1":11759,"Unnamed: 0":11759.0,"anchor":"Sequence-to-point learning with neural networks for nonintrusive load\n  monitoring","positive":"  Energy disaggregation (a.k.a nonintrusive load monitoring, NILM), a\nsingle-channel blind source separation problem, aims to decompose the mains\nwhich records the whole house electricity consumption into appliance-wise\nreadings. This problem is difficult because it is inherently unidentifiable.\nRecent approaches have shown that the identifiability problem could be reduced\nby introducing domain knowledge into the model. Deep neural networks have been\nshown to be a promising approach for these problems, but sliding windows are\nnecessary to handle the long sequences which arise in signal processing\nproblems, which raises issues about how to combine predictions from different\nsliding windows. In this paper, we propose sequence-to-point learning, where\nthe input is a window of the mains and the output is a single point of the\ntarget appliance. We use convolutional neural networks to train the model.\nInterestingly, we systematically show that the convolutional neural networks\ncan inherently learn the signatures of the target appliances, which are\nautomatically added into the model to reduce the identifiability problem. We\napplied the proposed neural network approaches to real-world household energy\ndata, and show that the methods achieve state-of-the-art performance, improving\ntwo standard error measures by 84% and 92%.\n","negative":"  Hadoop MapReduce is a framework for distributed storage and processing of\nlarge datasets that is quite popular in big data analytics. It has various\nconfiguration parameters (knobs) which play an important role in deciding the\nperformance i.e., the execution time of a given big data processing job.\nDefault values of these parameters do not always result in good performance and\nhence it is important to tune them. However, there is inherent difficulty in\ntuning the parameters due to two important reasons - firstly, the parameter\nsearch space is large and secondly, there are cross-parameter interactions.\nHence, there is a need for a dimensionality-free method which can automatically\ntune the configuration parameters by taking into account the cross-parameter\ndependencies. In this paper, we propose a novel Hadoop parameter tuning\nmethodology, based on a noisy gradient algorithm known as the simultaneous\nperturbation stochastic approximation (SPSA). The SPSA algorithm tunes the\nparameters by directly observing the performance of the Hadoop MapReduce\nsystem. The approach followed is independent of parameter dimensions and\nrequires only $2$ observations per iteration while tuning. We demonstrate the\neffectiveness of our methodology in achieving good performance on popular\nHadoop benchmarks namely \\emph{Grep}, \\emph{Bigram}, \\emph{Inverted Index},\n\\emph{Word Co-occurrence} and \\emph{Terasort}. Our method, when tested on a 25\nnode Hadoop cluster shows 66\\% decrease in execution time of Hadoop jobs on an\naverage, when compared to the default configuration. Further, we also observe a\nreduction of 45\\% in execution times, when compared to prior methods.\n","id":759}
{"Unnamed: 0.1":11760,"Unnamed: 0":11760.0,"anchor":"Modeling documents with Generative Adversarial Networks","positive":"  This paper describes a method for using Generative Adversarial Networks to\nlearn distributed representations of natural language documents. We propose a\nmodel that is based on the recently proposed Energy-Based GAN, but instead uses\na Denoising Autoencoder as the discriminator network. Document representations\nare extracted from the hidden layer of the discriminator and evaluated both\nquantitatively and qualitatively.\n","negative":"  An important problem for HCI researchers is to estimate the parameter values\nof a cognitive model from behavioral data. This is a difficult problem, because\nof the substantial complexity and variety in human behavioral strategies. We\nreport an investigation into a new approach using approximate Bayesian\ncomputation (ABC) to condition model parameters to data and prior knowledge. As\nthe case study we examine menu interaction, where we have click time data only\nto infer a cognitive model that implements a search behaviour with parameters\nsuch as fixation duration and recall probability. Our results demonstrate that\nABC (i) improves estimates of model parameter values, (ii) enables meaningful\ncomparisons between model variants, and (iii) supports fitting models to\nindividual users. ABC provides ample opportunities for theoretical HCI research\nby allowing principled inference of model parameter values and their\nuncertainty.\n","id":760}
{"Unnamed: 0.1":11761,"Unnamed: 0":11761.0,"anchor":"Linear Learning with Sparse Data","positive":"  Linear predictors are especially useful when the data is high-dimensional and\nsparse. One of the standard techniques used to train a linear predictor is the\nAveraged Stochastic Gradient Descent (ASGD) algorithm. We present an efficient\nimplementation of ASGD that avoids dense vector operations. We also describe a\ntranslation invariant extension called Centered Averaged Stochastic Gradient\nDescent (CASGD).\n","negative":"  A key limitation of sampling algorithms for approximate inference is that it\nis difficult to quantify their approximation error. Widely used sampling\nschemes, such as sequential importance sampling with resampling and\nMetropolis-Hastings, produce output samples drawn from a distribution that may\nbe far from the target posterior distribution. This paper shows how to\nupper-bound the symmetric KL divergence between the output distribution of a\nbroad class of sequential Monte Carlo (SMC) samplers and their target posterior\ndistributions, subject to assumptions about the accuracy of a separate\ngold-standard sampler. The proposed method applies to samplers that combine\nmultiple particles, multinomial resampling, and rejuvenation kernels. The\nexperiments show the technique being used to estimate bounds on the divergence\nof SMC samplers for posterior inference in a Bayesian linear regression model\nand a Dirichlet process mixture model.\n","id":761}
{"Unnamed: 0.1":11762,"Unnamed: 0":11762.0,"anchor":"The interplay between system identification and machine learning","positive":"  Learning from examples is one of the key problems in science and engineering.\nIt deals with function reconstruction from a finite set of direct and noisy\nsamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely\nused to solve this task and includes powerful estimators such as regularization\nnetworks. Recent achievements include the proof of the statistical consistency\nof these kernel- based approaches. Parallel to this, many different system\nidentification techniques have been developed but the interaction with machine\nlearning does not appear so strong yet. One reason is that the RKHSs usually\nemployed in machine learning do not embed the information available on dynamic\nsystems, e.g. BIBO stability. In addition, in system identification the\nindependent data assumptions routinely adopted in machine learning are never\nsatisfied in practice. This paper provides new results which strengthen the\nconnection between system identification and machine learning. Our starting\npoint is the introduction of RKHSs of dynamic systems. They contain functionals\nover spaces defined by system inputs and allow to interpret system\nidentification as learning from examples. In both linear and nonlinear\nsettings, it is shown that this perspective permits to derive in a relatively\nsimple way conditions on RKHS stability (i.e. the property of containing only\nBIBO stable systems or predictors), also facilitating the design of new kernels\nfor system identification. Furthermore, we prove the convergence of the\nregularized estimator to the optimal predictor under conditions typical of\ndynamic systems.\n","negative":"  Gaussian processes (GP) provide a prior over functions and allow finding\ncomplex regularities in data. Gaussian processes are successfully used for\nclassification\/regression problems and dimensionality reduction. In this work\nwe consider the classification problem only. The complexity of standard methods\nfor GP-classification scales cubically with the size of the training dataset.\nThis complexity makes them inapplicable to big data problems. Therefore, a\nvariety of methods were introduced to overcome this limitation. In the paper we\nfocus on methods based on so called inducing inputs. This approach is based on\nvariational inference and proposes a particular lower bound for marginal\nlikelihood (evidence). This bound is then maximized w.r.t. parameters of kernel\nfunction of the Gaussian process, thus fitting the model to data. The\ncomputational complexity of this method is $O(nm^2)$, where $m$ is the number\nof inducing inputs used by the model and is assumed to be substantially smaller\nthan the size of the dataset $n$. Recently, a new evidence lower bound for\nGP-classification problem was introduced. It allows using stochastic\noptimization, which makes it suitable for big data problems. However, the new\nlower bound depends on $O(m^2)$ variational parameter, which makes optimization\nchallenging in case of big m. In this work we develop a new approach for\ntraining inducing input GP models for classification problems. Here we use\nquadratic approximation of several terms in the aforementioned evidence lower\nbound, obtaining analytical expressions for optimal values of most of the\nparameters in the optimization, thus sufficiently reducing the dimension of\noptimization space. In our experiments we achieve as well or better results,\ncompared to the existing method. Moreover, our method doesn't require the user\nto manually set the learning rate, making it more practical, than the existing\nmethod.\n","id":762}
{"Unnamed: 0.1":11763,"Unnamed: 0":11763.0,"anchor":"Deep neural heart rate variability analysis","positive":"  Despite of the pain and limited accuracy of blood tests for early recognition\nof cardiovascular disease, they dominate risk screening and triage. On the\nother hand, heart rate variability is non-invasive and cheap, but not\nconsidered accurate enough for clinical practice. Here, we tackle heart beat\ninterval based classification with deep learning. We introduce an end to end\ndifferentiable hybrid architecture, consisting of a layer of biological neuron\nmodels of cardiac dynamics (modified FitzHugh Nagumo neurons) and several\nlayers of a standard feed-forward neural network. The proposed model is\nevaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,\nand 1172 chest pain patients of an emergency department. We show that it can\nsignificantly outperform models based on traditional heart rate variability\npredictors, as well as approaching or in some cases outperforming clinical\nblood tests, based only on 60 seconds of inter-beat intervals.\n","negative":"  Autonomous systems can be used to search for sparse signals in a large space;\ne.g., aerial robots can be deployed to localize threats, detect gas leaks, or\nrespond to distress calls. Intuitively, search algorithms may increase\nefficiency by collecting aggregate measurements summarizing large contiguous\nregions. However, most existing search methods either ignore the possibility of\nsuch region observations (e.g., Bayesian optimization and multi-armed bandits)\nor make strong assumptions about the sensing mechanism that allow each\nmeasurement to arbitrarily encode all signals in the entire environment (e.g.,\ncompressive sensing). We propose an algorithm that actively collects data to\nsearch for sparse signals using only noisy measurements of the average values\non rectangular regions (including single points), based on the greedy\nmaximization of information gain. We analyze our algorithm in 1d and show that\nit requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$\nsignal locations with small Bayes error, where $\\mu$ and $n$ are the signal\nstrength and the size of the search space, respectively. We also show that\nactive designs can be fundamentally more efficient than passive designs with\nregion sensing, contrasting with the results of Arias-Castro, Candes, and\nDavenport (2013). We demonstrate the empirical performance of our algorithm on\na search problem using satellite image data and in high dimensions.\n","id":763}
{"Unnamed: 0.1":11764,"Unnamed: 0":11764.0,"anchor":"Generalized Intersection Kernel","positive":"  Following the very recent line of work on the ``generalized min-max'' (GMM)\nkernel, this study proposes the ``generalized intersection'' (GInt) kernel and\nthe related ``normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the ``asymmetrically transformed'' version of the GInt\nkernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search.\n","negative":"  This position paper advocates a communications-inspired approach to the\ndesign of machine learning systems on energy-constrained embedded `always-on'\nplatforms. The communications-inspired approach has two versions - 1) a\ndeterministic version where existing low-power communication IC design methods\nare repurposed, and 2) a stochastic version referred to as Shannon-inspired\nstatistical information processing employing information-based metrics,\nstatistical error compensation (SEC), and retraining-based methods to implement\nML systems on stochastic circuit\/device fabrics operating at the limits of\nenergy-efficiency. The communications-inspired approach has the potential to\nfully leverage the opportunities afforded by ML algorithms and applications in\norder to address the challenges inherent in their deployment on\nenergy-constrained platforms.\n","id":764}
{"Unnamed: 0.1":11765,"Unnamed: 0":11765.0,"anchor":"Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex\n  Matrix Factorization","positive":"  We propose a general theory for studying the \\xl{landscape} of nonconvex\n\\xl{optimization} with underlying symmetric structures \\tz{for a class of\nmachine learning problems (e.g., low-rank matrix factorization, phase\nretrieval, and deep linear neural networks)}. In specific, we characterize the\nlocations of stationary points and the null space of Hessian matrices \\xl{of\nthe objective function} via the lens of invariant groups\\removed{for associated\noptimization problems, including low-rank matrix factorization, phase\nretrieval, and deep linear neural networks}. As a major motivating example, we\napply the proposed general theory to characterize the global \\xl{landscape} of\nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In\nparticular, we illustrate how the rotational symmetry group gives rise to\ninfinitely many nonisolated strict saddle points and equivalent global minima\nof the objective function. By explicitly identifying all stationary points, we\ndivide the entire parameter space into three regions: ($\\cR_1$) the region\ncontaining the neighborhoods of all strict saddle points, where the objective\nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all\nglobal minima, where the objective enjoys strong convexity along certain\ndirections; and ($\\cR_3$) the complement of the above regions, where the\ngradient has sufficiently large magnitudes. We further extend our result to the\nmatrix sensing problem. Such global landscape implies strong global convergence\nguarantees for popular iterative algorithms with arbitrary initial solutions.\n","negative":"  We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with zero contribution is\ndramatically reduced, resulting in much less noisy images within a fixed time\nbudget.\n","id":765}
{"Unnamed: 0.1":11766,"Unnamed: 0":11766.0,"anchor":"The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point\n  Process","positive":"  Many events occur in the world. Some event types are stochastically excited\nor inhibited---in the sense of having their probabilities elevated or\ndecreased---by patterns in the sequence of previous events. Discovering such\npatterns can help us predict which type of event will happen next and when. We\nmodel streams of discrete events in continuous time, by constructing a neurally\nself-modulating multivariate point process in which the intensities of multiple\nevent types evolve according to a novel continuous-time LSTM. This generative\nmodel allows past events to influence the future in complex and realistic ways,\nby conditioning future event intensities on the hidden state of a recurrent\nneural network that has consumed the stream of past events. Our model has\ndesirable qualitative properties. It achieves competitive likelihood and\npredictive accuracy on real and synthetic datasets, including under\nmissing-data conditions.\n","negative":"  Recent empirical results on long-term dependency tasks have shown that neural\nnetworks augmented with an external memory can learn the long-term dependency\ntasks more easily and achieve better generalization than vanilla recurrent\nneural networks (RNN). We suggest that memory augmented neural networks can\nreduce the effects of vanishing gradients by creating shortcut (or wormhole)\nconnections. Based on this observation, we propose a novel memory augmented\nneural network model called TARDIS (Temporal Automatic Relation Discovery in\nSequences). The controller of TARDIS can store a selective set of embeddings of\nits own previous hidden states into an external memory and revisit them as and\nwhen needed. For TARDIS, memory acts as a storage for wormhole connections to\nthe past to propagate the gradients more effectively and it helps to learn the\ntemporal dependencies. The memory structure of TARDIS has similarities to both\nNeural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but\nboth read and write operations of TARDIS are simpler and more efficient. We use\ndiscrete addressing for read\/write operations which helps to substantially to\nreduce the vanishing gradient problem with very long sequences. Read and write\noperations in TARDIS are tied with a heuristic once the memory becomes full,\nand this makes the learning problem simpler when compared to NTM or D-NTM type\nof architectures. We provide a detailed analysis on the gradient propagation in\ngeneral for MANNs. We evaluate our models on different long-term dependency\ntasks and report competitive results in all of them.\n","id":766}
{"Unnamed: 0.1":11767,"Unnamed: 0":11767.0,"anchor":"Data driven estimation of Laplace-Beltrami operator","positive":"  Approximations of Laplace-Beltrami operators on manifolds through graph\nLapla-cians have become popular tools in data analysis and machine learning.\nThese discretized operators usually depend on bandwidth parameters whose tuning\nremains a theoretical and practical problem. In this paper, we address this\nproblem for the unnormalized graph Laplacian by establishing an oracle\ninequality that opens the door to a well-founded data-driven procedure for the\nbandwidth selection. Our approach relies on recent results by Lacour and\nMassart [LM15] on the so-called Lepski's method.\n","negative":"  Evaluation of hydrocarbon reservoir requires classification of petrophysical\nproperties from available dataset. However, characterization of reservoir\nattributes is difficult due to the nonlinear and heterogeneous nature of the\nsubsurface physical properties. In this context, present study proposes a\ngeneralized one class classification framework based on Support Vector Data\nDescription (SVDD) to classify a reservoir characteristic water saturation into\ntwo classes (Class high and Class low) from four logs namely gamma ray, neutron\nporosity, bulk density, and P sonic using an imbalanced dataset. A comparison\nis carried out among proposed framework and different supervised classification\nalgorithms in terms of g metric means and execution time. Experimental results\nshow that proposed framework has outperformed other classifiers in terms of\nthese performance evaluators. It is envisaged that the classification analysis\nperformed in this study will be useful in further reservoir modeling.\n","id":767}
{"Unnamed: 0.1":11768,"Unnamed: 0":11768.0,"anchor":"Automatic Discoveries of Physical and Semantic Concepts via Association\n  Priors of Neuron Groups","positive":"  The recent successful deep neural networks are largely trained in a\nsupervised manner. It {\\it associates} complex patterns of input samples with\nneurons in the last layer, which form representations of {\\it concepts}. In\nspite of their successes, the properties of complex patterns associated a\nlearned concept remain elusive. In this work, by analyzing how neurons are\nassociated with concepts in supervised networks, we hypothesize that with\nproper priors to regulate learning, neural networks can automatically associate\nneurons in the intermediate layers with concepts that are aligned with real\nworld concepts, when trained only with labels that associate concepts with top\nlevel neurons, which is a plausible way for unsupervised learning. We develop a\nprior to verify the hypothesis and experimentally find the proposed prior help\nneural networks automatically learn both basic physical concepts at the lower\nlayers, e.g., rotation of filters, and highly semantic concepts at the higher\nlayers, e.g., fine-grained categories of an entry-level category.\n","negative":"  This paper addresses the task of set prediction using deep learning. This is\nimportant because the output of many computer vision tasks, including image\ntagging and object detection, are naturally expressed as sets of entities\nrather than vectors. As opposed to a vector, the size of a set is not fixed in\nadvance, and it is invariant to the ordering of entities within it. We define a\nlikelihood for a set distribution and learn its parameters using a deep neural\nnetwork. We also derive a loss for predicting a discrete distribution\ncorresponding to set cardinality. Set prediction is demonstrated on the problem\nof multi-class image classification. Moreover, we show that the proposed\ncardinality loss can also trivially be applied to the tasks of object counting\nand pedestrian detection. Our approach outperforms existing methods in all\nthree cases on standard datasets.\n","id":768}
{"Unnamed: 0.1":11769,"Unnamed: 0":11769.0,"anchor":"Adaptive Lambda Least-Squares Temporal Difference Learning","positive":"  Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in\nthe field of reinforcement learning. However, setting TD's $\\lambda$ parameter,\nwhich controls the timescale of TD updates, is generally left up to the\npractitioner. We formalize the $\\lambda$ selection problem as a bias-variance\ntrade-off where the solution is the value of $\\lambda$ that leads to the\nsmallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest\napplying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the\nspace of $\\lambda$ values. Unfortunately, this approach is too computationally\nexpensive for most practical applications. For Least Squares TD (LSTD) we show\nthat LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and\napply function optimization methods to efficiently search the space of\n$\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our\nexperiments demonstrate that ALLSTD is significantly computationally faster\nthan the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.\n","negative":"  We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds.\n","id":769}
{"Unnamed: 0.1":11770,"Unnamed: 0":11770.0,"anchor":"Linking the Neural Machine Translation and the Prediction of Organic\n  Chemistry Reactions","positive":"  Finding the main product of a chemical reaction is one of the important\nproblems of organic chemistry. This paper describes a method of applying a\nneural machine translation model to the prediction of organic chemical\nreactions. In order to translate 'reactants and reagents' to 'products', a\ngated recurrent unit based sequence-to-sequence model and a parser to generate\ninput tokens for model from reaction SMILES strings were built. Training sets\nare composed of reactions from the patent databases, and reactions manually\ngenerated applying the elementary reactions in an organic chemistry textbook of\nWade. The trained models were tested by examples and problems in the textbook.\nThe prediction process does not need manual encoding of rules (e.g., SMARTS\ntransformations) to predict products, hence it only needs sufficient training\nreaction sets to learn new types of reactions.\n","negative":"  Predictive models are increasingly deployed for the purpose of determining\naccess to services such as credit, insurance, and employment. Despite potential\ngains in productivity and efficiency, several potential problems have yet to be\naddressed, particularly the potential for unintentional discrimination. We\npresent an iterative procedure, based on orthogonal projection of input\nattributes, for enabling interpretability of black-box predictive models.\nThrough our iterative procedure, one can quantify the relative dependence of a\nblack-box model on its input attributes.The relative significance of the inputs\nto a predictive model can then be used to assess the fairness (or\ndiscriminatory extent) of such a model.\n","id":770}
{"Unnamed: 0.1":11771,"Unnamed: 0":11771.0,"anchor":"Counterfactual Prediction with Deep Instrumental Variables Networks","positive":"  We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.\n","negative":"  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n","id":771}
{"Unnamed: 0.1":11772,"Unnamed: 0":11772.0,"anchor":"Deep Neural Networks to Enable Real-time Multimessenger Astrophysics","positive":"  Gravitational wave astronomy has set in motion a scientific revolution. To\nfurther enhance the science reach of this emergent field, there is a pressing\nneed to increase the depth and speed of the gravitational wave algorithms that\nhave enabled these groundbreaking discoveries. To contribute to this effort, we\nintroduce Deep Filtering, a new highly scalable method for end-to-end\ntime-series signal processing, based on a system of two deep convolutional\nneural networks, which we designed for classification and regression to rapidly\ndetect and estimate parameters of signals in highly noisy time-series data\nstreams. We demonstrate a novel training scheme with gradually increasing noise\nlevels, and a transfer learning procedure between the two networks. We showcase\nthe application of this method for the detection and parameter estimation of\ngravitational waves from binary black hole mergers. Our results indicate that\nDeep Filtering significantly outperforms conventional machine learning\ntechniques, achieves similar performance compared to matched-filtering while\nbeing several orders of magnitude faster thus allowing real-time processing of\nraw big data with minimal resources. More importantly, Deep Filtering extends\nthe range of gravitational wave signals that can be detected with ground-based\ngravitational wave detectors. This framework leverages recent advances in\nartificial intelligence algorithms and emerging hardware architectures, such as\ndeep-learning-optimized GPUs, to facilitate real-time searches of gravitational\nwave sources and their electromagnetic and astro-particle counterparts.\n","negative":"  Non-negative matrix factorization is a popular tool for decomposing data into\nfeature and weight matrices under non-negativity constraints. It enjoys\npractical success but is poorly understood theoretically. This paper proposes\nan algorithm that alternates between decoding the weights and updating the\nfeatures, and shows that assuming a generative model of the data, it provably\nrecovers the ground-truth under fairly mild conditions. In particular, its only\nessential requirement on features is linear independence. Furthermore, the\nalgorithm uses ReLU to exploit the non-negativity for decoding the weights, and\nthus can tolerate adversarial noise that can potentially be as large as the\nsignal, and can tolerate unbiased noise much larger than the signal. The\nanalysis relies on a carefully designed coupling between two potential\nfunctions, which we believe is of independent interest.\n","id":772}
{"Unnamed: 0.1":11773,"Unnamed: 0":11773.0,"anchor":"NIPS 2016 Tutorial: Generative Adversarial Networks","positive":"  This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.\n","negative":"  The mixture models have become widely used in clustering, given its\nprobabilistic framework in which its based, however, for modern databases that\nare characterized by their large size, these models behave disappointingly in\nsetting out the model, making essential the selection of relevant variables for\nthis type of clustering. After recalling the basics of clustering based on a\nmodel, this article will examine the variable selection methods for model-based\nclustering, as well as presenting opportunities for improvement of these\nmethods.\n","id":773}
{"Unnamed: 0.1":11774,"Unnamed: 0":11774.0,"anchor":"Very Fast Kernel SVM under Budget Constraints","positive":"  In this paper we propose a fast online Kernel SVM algorithm under tight\nbudget constraints. We propose to split the input space using LVQ and train a\nKernel SVM in each cluster. To allow for online training, we propose to limit\nthe size of the support vector set of each cluster using different strategies.\nWe show in the experiment that our algorithm is able to achieve high accuracy\nwhile having a very high number of samples processed per second both in\ntraining and in the evaluation.\n","negative":"  Finding related published articles is an important task in any science, but\nwith the explosion of new work in the biomedical domain it has become\nespecially challenging. Most existing methodologies use text similarity metrics\nto identify whether two articles are related or not. However biomedical\nknowledge discovery is hypothesis-driven. The most related articles may not be\nones with the highest text similarities. In this study, we first develop an\ninnovative crowd-sourcing approach to build an expert-annotated\ndocument-ranking corpus. Using this corpus as the gold standard, we then\nevaluate the approaches of using text similarity to rank the relatedness of\narticles. Finally, we develop and evaluate a new supervised model to\nautomatically rank related scientific articles. Our results show that authors'\nranking differ significantly from rankings by text-similarity-based models. By\ntraining a learning-to-rank model on a subset of the annotated corpus, we found\nthe best supervised learning-to-rank model (SVM-Rank) significantly surpassed\nstate-of-the-art baseline systems.\n","id":774}
{"Unnamed: 0.1":11775,"Unnamed: 0":11775.0,"anchor":"Lazily Adapted Constant Kinky Inference for Nonparametric Regression and\n  Model-Reference Adaptive Control","positive":"  Techniques known as Nonlinear Set Membership prediction, Lipschitz\nInterpolation or Kinky Inference are approaches to machine learning that\nutilise presupposed Lipschitz properties to compute inferences over unobserved\nfunction values. Provided a bound on the true best Lipschitz constant of the\ntarget function is known a priori they offer convergence guarantees as well as\nbounds around the predictions. Considering a more general setting that builds\non Hoelder continuity relative to pseudo-metrics, we propose an online method\nfor estimating the Hoelder constant online from function value observations\nthat possibly are corrupted by bounded observational errors. Utilising this to\ncompute adaptive parameters within a kinky inference rule gives rise to a\nnonparametric machine learning method, for which we establish strong universal\napproximation guarantees. That is, we show that our prediction rule can learn\nany continuous function in the limit of increasingly dense data to within a\nworst-case error bound that depends on the level of observational uncertainty.\nWe apply our method in the context of nonparametric model-reference adaptive\ncontrol (MRAC). Across a range of simulated aircraft roll-dynamics and\nperformance metrics our approach outperforms recently proposed alternatives\nthat were based on Gaussian processes and RBF-neural networks. For\ndiscrete-time systems, we provide guarantees on the tracking success of our\nlearning-based controllers both for the batch and the online learning setting.\n","negative":"  Superoptimization requires the estimation of the best program for a given\ncomputational task. In order to deal with large programs, superoptimization\ntechniques perform a stochastic search. This involves proposing a modification\nof the current program, which is accepted or rejected based on the improvement\nachieved. The state of the art method uses uniform proposal distributions,\nwhich fails to exploit the problem structure to the fullest. To alleviate this\ndeficiency, we learn a proposal distribution over possible modifications using\nReinforcement Learning. We provide convincing results on the superoptimization\nof \"Hacker's Delight\" programs.\n","id":775}
{"Unnamed: 0.1":11776,"Unnamed: 0":11776.0,"anchor":"Classification of Smartphone Users Using Internet Traffic","positive":"  Today, smartphone devices are owned by a large portion of the population and\nhave become a very popular platform for accessing the Internet. Smartphones\nprovide the user with immediate access to information and services. However,\nthey can easily expose the user to many privacy risks. Applications that are\ninstalled on the device and entities with access to the device's Internet\ntraffic can reveal private information about the smartphone user and steal\nsensitive content stored on the device or transmitted by the device over the\nInternet. In this paper, we present a method to reveal various demographics and\ntechnical computer skills of smartphone users by their Internet traffic\nrecords, using machine learning classification models. We implement and\nevaluate the method on real life data of smartphone users and show that\nsmartphone users can be classified by their gender, smoking habits, software\nprogramming experience, and other characteristics.\n","negative":"  Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data.\n","id":776}
{"Unnamed: 0.1":11777,"Unnamed: 0":11777.0,"anchor":"Outlier Robust Online Learning","positive":"  We consider the problem of learning from noisy data in practical settings\nwhere the size of data is too large to store on a single machine. More\nchallenging, the data coming from the wild may contain malicious outliers. To\naddress the scalability and robustness issues, we present an online robust\nlearning (ORL) approach. ORL is simple to implement and has provable robustness\nguarantee -- in stark contrast to existing online learning approaches that are\ngenerally fragile to outliers. We specialize the ORL approach for two concrete\ncases: online robust principal component analysis and online linear regression.\nWe demonstrate the efficiency and robustness advantages of ORL through\ncomprehensive simulations and predicting image tags on a large-scale data set.\nWe also discuss extension of the ORL to distributed learning and provide\nexperimental evaluations.\n","negative":"  The Why3 IDE and verification system facilitates the use of a wide range of\nSatisfiability Modulo Theories (SMT) solvers through a driver-based\narchitecture. We present Where4: a portfolio-based approach to discharge Why3\nproof obligations. We use data analysis and machine learning techniques on\nstatic metrics derived from program source code. Our approach benefits software\nengineers by providing a single utility to delegate proof obligations to the\nsolvers most likely to return a useful result. It does this in a time-efficient\nway using existing Why3 and solver installations - without requiring low-level\nknowledge about SMT solver operation from the user.\n","id":777}
{"Unnamed: 0.1":11778,"Unnamed: 0":11778.0,"anchor":"Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs\n  by Selective Execution","positive":"  We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward\ndeep neural network that allows selective execution. Given an input, only a\nsubset of D2NN neurons are executed, and the particular subset is determined by\nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs\nprovide a way to improve computational efficiency. To achieve dynamic selective\nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic\ngraph of differentiable modules) with controller modules. Each controller\nmodule is a sub-network whose output is a decision that controls whether other\nmodules can execute. A D2NN is trained end to end. Both regular and controller\nmodules in a D2NN are learnable and are jointly trained to optimize both\naccuracy and efficiency. Such training is achieved by integrating\nbackpropagation with reinforcement learning. With extensive experiments of\nvarious D2NN architectures on image classification tasks, we demonstrate that\nD2NNs are general and flexible, and can effectively optimize\naccuracy-efficiency trade-offs.\n","negative":"  In this work we design and compare different supervised learning algorithms\nto compute the cost of Alternating Current Optimal Power Flow (ACOPF). The\nmotivation for quick calculation of OPF cost outcomes stems from the growing\nneed of algorithmic-based long-term and medium-term planning methodologies in\npower networks. Integrated in a multiple time-horizon coordination framework,\nwe refer to this approximation module as a proxy for predicting short-term\ndecision outcomes without the need of actual simulation and optimization of\nthem. Our method enables fast approximate calculation of OPF cost with less\nthan 1% error on average, achieved in run-times that are several orders of\nmagnitude lower than of exact computation. Several test-cases such as\nIEEE-RTS96 are used to demonstrate the efficiency of our approach.\n","id":778}
{"Unnamed: 0.1":11779,"Unnamed: 0":11779.0,"anchor":"Two-Bit Networks for Deep Learning on Resource-Constrained Embedded\n  Devices","positive":"  With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\n","negative":"  A large body of work in behavioral fields attempts to develop models that\ndescribe the way people, as opposed to rational agents, make decisions. A\nrecent Choice Prediction Competition (2015) challenged researchers to suggest a\nmodel that captures 14 classic choice biases and can predict human decisions\nunder risk and ambiguity. The competition focused on simple decision problems,\nin which human subjects were asked to repeatedly choose between two gamble\noptions.\n  In this paper we present our approach for predicting human decision behavior:\nwe suggest to use machine learning algorithms with features that are based on\nwell-established behavioral theories. The basic idea is that these\npsychological features are essential for the representation of the data and are\nimportant for the success of the learning process. We implement a vanilla model\nin which we train SVM models using behavioral features that rely on the\npsychological properties underlying the competition baseline model. We show\nthat this basic model captures the 14 choice biases and outperforms all the\nother learning-based models in the competition. The preliminary results suggest\nthat such hybrid models can significantly improve the prediction of human\ndecision making, and are a promising direction for future research.\n","id":779}
{"Unnamed: 0.1":11780,"Unnamed: 0":11780.0,"anchor":"Robust method for finding sparse solutions to linear inverse problems\n  using an L2 regularization","positive":"  We analyzed the performance of a biologically inspired algorithm called the\nCorrected Projections Algorithm (CPA) when a sparseness constraint is required\nto unambiguously reconstruct an observed signal using atoms from an\novercomplete dictionary. By changing the geometry of the estimation problem,\nCPA gives an analytical expression for a binary variable that indicates the\npresence or absence of a dictionary atom using an L2 regularizer. The\nregularized solution can be implemented using an efficient real-time\nKalman-filter type of algorithm. The smoother L2 regularization of CPA makes it\nvery robust to noise, and CPA outperforms other methods in identifying known\natoms in the presence of strong novel atoms in the signal.\n","negative":"  Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints.\n","id":780}
{"Unnamed: 0.1":11781,"Unnamed: 0":11781.0,"anchor":"HLA class I binding prediction via convolutional neural networks","positive":"  Many biological processes are governed by protein-ligand interactions. One\nsuch example is the recognition of self and nonself cells by the immune system.\nThis immune response process is regulated by the major histocompatibility\ncomplex (MHC) protein which is encoded by the human leukocyte antigen (HLA)\ncomplex. Understanding the binding potential between MHC and peptides can lead\nto the design of more potent, peptide-based vaccines and immunotherapies for\ninfectious autoimmune diseases.\n  We apply machine learning techniques from the natural language processing\n(NLP) domain to address the task of MHC-peptide binding prediction. More\nspecifically, we introduce a new distributed representation of amino acids,\nname HLA-Vec, that can be used for a variety of downstream proteomic machine\nlearning tasks. We then propose a deep convolutional neural network\narchitecture, name HLA-CNN, for the task of HLA class I-peptide binding\nprediction. Experimental results show combining the new distributed\nrepresentation with our HLA-CNN architecture achieves state-of-the-art results\nin the majority of the latest two Immune Epitope Database (IEDB) weekly\nautomated benchmark datasets. We further apply our model to predict binding on\nthe human genome and identify 15 genes with potential for self binding.\n","negative":"  The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis and management of patients in clinical\npractice. Supervised- or weakly supervised training enables the detection of\nfindings that are known a priori. It does not scale well, and a priori\ndefinition limits the vocabulary of markers to known entities reducing the\naccuracy of diagnosis and prognosis. Here, we propose the identification of\nanomalies in large-scale medical imaging data using healthy examples as a\nreference. We detect and categorize candidates for anomaly findings untypical\nfor the observed data. A deep convolutional autoencoder is trained on healthy\nretinal images. The learned model generates a new feature representation, and\nthe distribution of healthy retinal patches is estimated by a one-class support\nvector machine. Results demonstrate that we can identify pathologic regions in\nimages without using expert annotations. A subsequent clustering categorizes\nfindings into clinically meaningful classes. In addition the learned features\noutperform standard embedding approaches in a classification task.\n","id":781}
{"Unnamed: 0.1":11782,"Unnamed: 0":11782.0,"anchor":"Deep Convolutional Neural Networks for Pairwise Causality","positive":"  Discovering causal models from observational and interventional data is an\nimportant first step preceding what-if analysis or counterfactual reasoning. As\nhas been shown before, the direction of pairwise causal relations can, under\ncertain conditions, be inferred from observational data via standard\ngradient-boosted classifiers (GBC) using carefully engineered statistical\nfeatures. In this paper we apply deep convolutional neural networks (CNNs) to\nthis problem by plotting attribute pairs as 2-D scatter plots that are fed to\nthe CNN as images. We evaluate our approach on the 'Cause- Effect Pairs' NIPS\n2013 Data Challenge. We observe that a weighted ensemble of CNN with the\nearlier GBC approach yields significant improvement. Further, we observe that\nwhen less training data is available, our approach performs better than the GBC\nbased approach suggesting that CNN models pre-trained to determine the\ndirection of pairwise causal direction could have wider applicability in causal\ndiscovery and enabling what-if or counterfactual analysis.\n","negative":"  Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning\nalgorithm that breaks the initial learning task to several, easier sub-tasks by\nfirst constructing a hierarchy of labels from a given label set and secondly\nemploying a given base multi-label classifier (MLC) to the resulting\nsub-problems. The primary goal is to effectively address class imbalance and\nscalability issues that often arise in real-world multi-label classification\nproblems. In this work, we present the general setup for a HOMER model and a\nsimple extension of the algorithm that is suited for MLCs that output rankings.\nFurthermore, we provide a detailed analysis of the properties of the algorithm,\nboth from an aspect of effectiveness and computational complexity. A secondary\ncontribution involves the presentation of a balanced variant of the k means\nalgorithm, which serves in the first step of the label hierarchy construction.\nWe conduct extensive experiments on six real-world datasets, studying\nempirically HOMER's parameters and providing examples of instantiations of the\nalgorithm with different clustering approaches and MLCs, The empirical results\ndemonstrate a significant improvement over the given base MLC.\n","id":782}
{"Unnamed: 0.1":11783,"Unnamed: 0":11783.0,"anchor":"Akid: A Library for Neural Network Research and Production from a\n  Dataism Approach","positive":"  Neural networks are a revolutionary but immature technique that is fast\nevolving and heavily relies on data. To benefit from the newest development and\nnewly available data, we want the gap between research and production as small\nas possibly. On the other hand, differing from traditional machine learning\nmodels, neural network is not just yet another statistic model, but a model for\nthe natural processing engine --- the brain. In this work, we describe a neural\nnetwork library named {\\texttt akid}. It provides higher level of abstraction\nfor entities (abstracted as blocks) in nature upon the abstraction done on\nsignals (abstracted as tensors) by Tensorflow, characterizing the dataism\nobservation that all entities in nature processes input and emit out in some\nways. It includes a full stack of software that provides abstraction to let\nresearchers focus on research instead of implementation, while at the same time\nthe developed program can also be put into production seamlessly in a\ndistributed environment, and be production ready. At the top application stack,\nit provides out-of-box tools for neural network applications. Lower down, akid\nprovides a programming paradigm that lets user easily build customized models.\nThe distributed computing stack handles the concurrency and communication, thus\nletting models be trained or deployed to a single GPU, multiple GPUs, or a\ndistributed environment without affecting how a model is specified in the\nprogramming paradigm stack. Lastly, the distributed deployment stack handles\nhow the distributed computing is deployed, thus decoupling the research\nprototype environment with the actual production environment, and is able to\ndynamically allocate computing resources, so development (Devs) and operations\n(Ops) could be separated. Please refer to http:\/\/akid.readthedocs.io\/en\/latest\/\nfor documentation.\n","negative":"  Safety is a desirable property that can immensely increase the applicability\nof learning algorithms in real-world decision-making problems. It is much\neasier for a company to deploy an algorithm that is safe, i.e., guaranteed to\nperform at least as well as a baseline. In this paper, we study the issue of\nsafety in contextual linear bandits that have application in many different\nfields including personalized ad recommendation in online marketing. We\nformulate a notion of safety for this class of algorithms. We develop a safe\ncontextual linear bandit algorithm, called conservative linear UCB (CLUCB),\nthat simultaneously minimizes its regret and satisfies the safety constraint,\ni.e., maintains its performance above a fixed percentage of the performance of\na baseline strategy, uniformly over time. We prove an upper-bound on the regret\nof CLUCB and show that it can be decomposed into two terms: 1) an upper-bound\nfor the regret of the standard linear UCB algorithm that grows with the time\nhorizon and 2) a constant (does not grow with the time horizon) term that\naccounts for the loss of being conservative in order to satisfy the safety\nconstraint. We empirically show that our algorithm is safe and validate our\ntheoretical analysis.\n","id":783}
{"Unnamed: 0.1":11784,"Unnamed: 0":11784.0,"anchor":"New Methods of Enhancing Prediction Accuracy in Linear Models with\n  Missing Data","positive":"  In this paper, prediction for linear systems with missing information is\ninvestigated. New methods are introduced to improve the Mean Squared Error\n(MSE) on the test set in comparison to state-of-the-art methods, through\nappropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft\nWeighted Prediction (SWP) algorithm and its efficacy are depicted and compared\nto previous works for non-missing scenarios. The algorithm is then modified and\noptimized for missing scenarios. It is shown that controlled over-fitting by\nsuggested algorithms will improve prediction accuracy in various cases.\nSimulation results approve our heuristics in enhancing the prediction accuracy.\n","negative":"  An important way to make large training sets is to gather noisy labels from\ncrowds of non experts. We propose a method to aggregate noisy labels collected\nfrom a crowd of workers or annotators. Eliciting labels is important in tasks\nsuch as judging web search quality and rating products. Our method assumes that\nlabels are generated by a probability distribution over items and labels. We\nformulate the method by drawing parallels between Gaussian Mixture Models\n(GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of\nvote aggregation can be viewed as one of clustering. We use K-RBMs to perform\nclustering. We finally show some empirical evaluations over real datasets.\n","id":784}
{"Unnamed: 0.1":11785,"Unnamed: 0":11785.0,"anchor":"Using Big Data to Enhance the Bosch Production Line Performance: A\n  Kaggle Challenge","positive":"  This paper describes our approach to the Bosch production line performance\nchallenge run by Kaggle.com. Maximizing the production yield is at the heart of\nthe manufacturing industry. At the Bosch assembly line, data is recorded for\nproducts as they progress through each stage. Data science methods are applied\nto this huge data repository consisting records of tests and measurements made\nfor each component along the assembly line to predict internal failures. We\nfound that it is possible to train a model that predicts which parts are most\nlikely to fail. Thus a smarter failure detection system can be built and the\nparts tagged likely to fail can be salvaged to decrease operating costs and\nincrease the profit margins.\n","negative":"  Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning.\n","id":785}
{"Unnamed: 0.1":11786,"Unnamed: 0":11786.0,"anchor":"Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-rank Multi-View Data","positive":"  We consider the multi-view data completion problem, i.e., to complete a\nmatrix $\\mathbf{U}=[\\mathbf{U}_1|\\mathbf{U}_2]$ where the ranks of\n$\\mathbf{U},\\mathbf{U}_1$, and $\\mathbf{U}_2$ are given. In particular, we\ninvestigate the fundamental conditions on the sampling pattern, i.e., locations\nof the sampled entries for finite completability of such a multi-view data\ngiven the corresponding rank constraints. In contrast with the existing\nanalysis on Grassmannian manifold for a single-view matrix, i.e., conventional\nmatrix completion, we propose a geometric analysis on the manifold structure\nfor multi-view data to incorporate more than one rank constraint. We provide a\ndeterministic necessary and sufficient condition on the sampling pattern for\nfinite completability. We also give a probabilistic condition in terms of the\nnumber of samples per column that guarantees finite completability with high\nprobability. Finally, using the developed tools, we derive the deterministic\nand probabilistic guarantees for unique completability.\n","negative":"  In this paper, we consider the problem of predicting demographics of\ngeographic units given geotagged Tweets that are composed within these units.\nTraditional survey methods that offer demographics estimates are usually\nlimited in terms of geographic resolution, geographic boundaries, and time\nintervals. Thus, it would be highly useful to develop computational methods\nthat can complement traditional survey methods by offering demographics\nestimates at finer geographic resolutions, with flexible geographic boundaries\n(i.e. not confined to administrative boundaries), and at different time\nintervals. While prior work has focused on predicting demographics and health\nstatistics at relatively coarse geographic resolutions such as the county-level\nor state-level, we introduce an approach to predict demographics at finer\ngeographic resolutions such as the blockgroup-level. For the task of predicting\ngender and race\/ethnicity counts at the blockgroup-level, an approach adapted\nfrom prior work to our problem achieves an average correlation of 0.389\n(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms\nthis prior approach with an average correlation of 0.671 (gender) and 0.692\n(race).\n","id":786}
{"Unnamed: 0.1":11787,"Unnamed: 0":11787.0,"anchor":"Using Artificial Neural Networks (ANN) to Control Chaos","positive":"  Controlling Chaos could be a big factor in getting great stable amounts of\nenergy out of small amounts of not necessarily stable resources. By definition,\nChaos is getting huge changes in the system's output due to unpredictable small\nchanges in initial conditions, and that means we could take advantage of this\nfact and select the proper control system to manipulate system's initial\nconditions and inputs in general and get a desirable output out of otherwise a\nChaotic system. That was accomplished by first building some known chaotic\ncircuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN\ncontrol system. It was shown that this technique can also be used to stabilize\nsome hard to stabilize electronic systems.\n","negative":"  Effective information analysis generally boils down to properly identifying\nthe structure or geometry of the data, which is often represented by a graph.\nIn some applications, this structure may be partly determined by design\nconstraints or pre-determined sensing arrangements, like in road transportation\nnetworks for example. In general though, the data structure is not readily\navailable and becomes pretty difficult to define. In particular, the global\nsmoothness assumptions, that most of the existing works adopt, are often too\ngeneral and unable to properly capture localized properties of data. In this\npaper, we go beyond this classical data model and rather propose to represent\ninformation as a sparse combination of localized functions that live on a data\nstructure represented by a graph. Based on this model, we focus on the problem\nof inferring the connectivity that best explains the data samples at different\nvertices of a graph that is a priori unknown. We concentrate on the case where\nthe observed data is actually the sum of heat diffusion processes, which is a\nquite common model for data on networks or other irregular structures. We cast\na new graph learning problem and solve it with an efficient nonconvex\noptimization algorithm. Experiments on both synthetic and real world data\nfinally illustrate the benefits of the proposed graph learning framework and\nconfirm that the data structure can be efficiently learned from data\nobservations only. We believe that our algorithm will help solving key\nquestions in diverse application domains such as social and biological network\nanalysis where it is crucial to unveil proper geometry for data understanding\nand inference.\n","id":787}
{"Unnamed: 0.1":11788,"Unnamed: 0":11788.0,"anchor":"Clustering Signed Networks with the Geometric Mean of Laplacians","positive":"  Signed networks allow to model positive and negative relationships. We\nanalyze existing extensions of spectral clustering to signed networks. It turns\nout that existing approaches do not recover the ground truth clustering in\nseveral situations where either the positive or the negative network structures\ncontain no noise. Our analysis shows that these problems arise as existing\napproaches take some form of arithmetic mean of the Laplacians of the positive\nand negative part. As a solution we propose to use the geometric mean of the\nLaplacians of positive and negative part and show that it outperforms the\nexisting approaches. While the geometric mean of matrices is computationally\nexpensive, we show that eigenvectors of the geometric mean can be computed\nefficiently, leading to a numerical scheme for sparse matrices which is of\nindependent interest.\n","negative":"  In training speech recognition systems, labeling audio clips can be\nexpensive, and not all data is equally valuable. Active learning aims to label\nonly the most informative samples to reduce cost. For speech recognition,\nconfidence scores and other likelihood-based active learning methods have been\nshown to be effective. Gradient-based active learning methods, however, are\nstill not well-understood. This work investigates the Expected Gradient Length\n(EGL) approach in active learning for end-to-end speech recognition. We justify\nEGL from a variance reduction perspective, and observe that EGL's measure of\ninformativeness picks novel samples uncorrelated with confidence scores.\nExperimentally, we show that EGL can reduce word errors by 11\\%, or\nalternatively, reduce the number of samples to label by 50\\%, when compared to\nrandom sampling.\n","id":788}
{"Unnamed: 0.1":11789,"Unnamed: 0":11789.0,"anchor":"Collapsing of dimensionality","positive":"  We analyze a new approach to Machine Learning coming from a modification of\nclassical regularization networks by casting the process in the time dimension,\nleading to a sort of collapse of dimensionality in the problem of learning the\nmodel parameters. This approach allows the definition of a online learning\nalgorithm that progressively accumulates the knowledge provided in the input\ntrajectory. The regularization principle leads to a solution based on a\ndynamical system that is paired with a procedure to develop a graph structure\nthat stores the input regularities acquired from the temporal evolution. We\nreport an extensive experimental exploration on the behavior of the parameter\nof the proposed model and an evaluation on artificial dataset.\n","negative":"  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n","id":789}
{"Unnamed: 0.1":11790,"Unnamed: 0":11790.0,"anchor":"Unsupervised neural and Bayesian models for zero-resource speech\n  processing","positive":"  In settings where only unlabelled speech data is available, zero-resource\nspeech technology needs to be developed without transcriptions, pronunciation\ndictionaries, or language modelling text. There are two central problems in\nzero-resource speech processing: (i) finding frame-level feature\nrepresentations which make it easier to discriminate between linguistic units\n(phones or words), and (ii) segmenting and clustering unlabelled speech into\nmeaningful units. In this thesis, we argue that a combination of top-down and\nbottom-up modelling is advantageous in tackling these two problems.\n  To address the problem of frame-level representation learning, we present the\ncorrespondence autoencoder (cAE), a neural network trained with weak top-down\nsupervision from an unsupervised term discovery system. By combining this\ntop-down supervision with unsupervised bottom-up initialization, the cAE yields\nmuch more discriminative features than previous approaches. We then present our\nunsupervised segmental Bayesian model that segments and clusters unlabelled\nspeech into hypothesized words. By imposing a consistent top-down segmentation\nwhile also using bottom-up knowledge from detected syllable boundaries, our\nsystem outperforms several others on multi-speaker conversational English and\nXitsonga speech data. Finally, we show that the clusters discovered by the\nsegmental Bayesian model can be made less speaker- and gender-specific by using\nfeatures from the cAE instead of traditional acoustic features.\n  In summary, the different models and systems presented in this thesis show\nthat both top-down and bottom-up modelling can improve representation learning,\nsegmentation and clustering of unlabelled speech data.\n","negative":"  Modern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach.\n","id":790}
{"Unnamed: 0.1":11791,"Unnamed: 0":11791.0,"anchor":"Neural Probabilistic Model for Non-projective MST Parsing","positive":"  In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.\n","negative":"  This paper proposes a convolutional neural network (CNN)-based method that\nlearns traffic as images and predicts large-scale, network-wide traffic speed\nwith a high accuracy. Spatiotemporal traffic dynamics are converted to images\ndescribing the time and space relations of traffic flow via a two-dimensional\ntime-space matrix. A CNN is applied to the image following two consecutive\nsteps: abstract traffic feature extraction and network-wide traffic speed\nprediction. The effectiveness of the proposed method is evaluated by taking two\nreal-world transportation networks, the second ring road and north-east\ntransportation network in Beijing, as examples, and comparing the method with\nfour prevailing algorithms, namely, ordinary least squares, k-nearest\nneighbors, artificial neural network, and random forest, and three deep\nlearning architectures, namely, stacked autoencoder, recurrent neural network,\nand long-short-term memory network. The results show that the proposed method\noutperforms other algorithms by an average accuracy improvement of 42.91%\nwithin an acceptable execution time. The CNN can train the model in a\nreasonable time and, thus, is suitable for large-scale transportation networks.\n","id":791}
{"Unnamed: 0.1":11792,"Unnamed: 0":11792.0,"anchor":"On the Usability of Probably Approximately Correct Implication Bases","positive":"  We revisit the notion of probably approximately correct implication bases\nfrom the literature and present a first formulation in the language of formal\nconcept analysis, with the goal to investigate whether such bases represent a\nsuitable substitute for exact implication bases in practical use-cases. To this\nend, we quantitatively examine the behavior of probably approximately correct\nimplication bases on artificial and real-world data sets and compare their\nprecision and recall with respect to their corresponding exact implication\nbases. Using a small example, we also provide qualitative insight that\nimplications from probably approximately correct bases can still represent\nmeaningful knowledge from a given data set.\n","negative":"  We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient that maximumly decreases the\nKL divergence with the target distribution. Our method works for any target\ndistribution specified by their unnormalized density function, and can train\nany black-box architectures that are differentiable in terms of the parameters\nwe want to adapt. As an application of our method, we propose an amortized MLE\nalgorithm for training deep energy model, where a neural sampler is adaptively\ntrained to approximate the likelihood function. Our method mimics an\nadversarial game between the deep energy model and the neural sampler, and\nobtains realistic-looking images competitive with the state-of-the-art results.\n","id":792}
{"Unnamed: 0.1":11793,"Unnamed: 0":11793.0,"anchor":"An Interval-Based Bayesian Generative Model for Human Complex Activity\n  Recognition","positive":"  Complex activity recognition is challenging due to the inherent uncertainty\nand diversity of performing a complex activity. Normally, each instance of a\ncomplex activity has its own configuration of atomic actions and their temporal\ndependencies. We propose in this paper an atomic action-based Bayesian model\nthat constructs Allen's interval relation networks to characterize complex\nactivities with structural varieties in a probabilistic generative way: By\nintroducing latent variables from the Chinese restaurant process, our approach\nis able to capture all possible styles of a particular complex activity as a\nunique set of distributions over atomic actions and relations. We also show\nthat local temporal dependencies can be retained and are globally consistent in\nthe resulting interval network. Moreover, network structure can be learned from\nempirical data. A new dataset of complex hand activities has been constructed\nand made publicly available, which is much larger in size than any existing\ndatasets. Empirical evaluations on benchmark datasets as well as our in-house\ndataset demonstrate the competitiveness of our approach.\n","negative":"  We present LBW-Net, an efficient optimization based method for quantization\nand training of the low bit-width convolutional neural networks (CNNs).\nSpecifically, we quantize the weights to zero or powers of two by minimizing\nthe Euclidean distance between full-precision weights and quantized weights\nduring backpropagation. We characterize the combinatorial nature of the low\nbit-width quantization problem. For 2-bit (ternary) CNNs, the quantization of\n$N$ weights can be done by an exact formula in $O(N\\log N)$ complexity. When\nthe bit-width is three and above, we further propose a semi-analytical\nthresholding scheme with a single free parameter for quantization that is\ncomputationally inexpensive. The free parameter is further determined by\nnetwork retraining and object detection tests. LBW-Net has several desirable\nadvantages over full-precision CNNs, including considerable memory savings,\nenergy efficiency, and faster deployment. Our experiments on PASCAL VOC dataset\nshow that compared with its 32-bit floating-point counterpart, the performance\nof the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can\neven do better in some real world visual scenes, while empirically enjoying\nmore than 4$\\times$ faster deployment.\n","id":793}
{"Unnamed: 0.1":11794,"Unnamed: 0":11794.0,"anchor":"Dense Associative Memory is Robust to Adversarial Inputs","positive":"  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n","negative":"  In recent years, Deep Neural Networks (DNN) based methods have achieved\nremarkable performance in a wide range of tasks and have been among the most\npowerful and widely used techniques in computer vision. However, DNN-based\nmethods are both computational-intensive and resource-consuming, which hinders\nthe application of these methods on embedded systems like smart phones. To\nalleviate this problem, we introduce a novel Fixed-point Factorized Networks\n(FFN) for pretrained models to reduce the computational complexity as well as\nthe storage requirement of networks. The resulting networks have only weights\nof -1, 0 and 1, which significantly eliminates the most resource-consuming\nmultiply-accumulate operations (MACs). Extensive experiments on large-scale\nImageNet classification task show the proposed FFN only requires one-thousandth\nof multiply operations with comparable accuracy.\n","id":794}
{"Unnamed: 0.1":11795,"Unnamed: 0":11795.0,"anchor":"Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously\n  for Compressive Sensing","positive":"  This paper considers the problem of simultaneously learning the Sensing\nMatrix and Sparsifying Dictionary (SMSD) on a large training dataset. To\naddress the formulated joint learning problem, we propose an online algorithm\nthat consists of a closed-form solution for optimizing the sensing matrix with\na fixed sparsifying dictionary and a stochastic method for learning the\nsparsifying dictionary on a large dataset when the sensing matrix is given.\nBenefiting from training on a large dataset, the obtained compressive sensing\n(CS) system by the proposed algorithm yields a much better performance in terms\nof signal recovery accuracy than the existing ones. The simulation results on\nnatural images demonstrate the effectiveness of the suggested online algorithm\ncompared with the existing methods.\n","negative":"  Real estate appraisal, which is the process of estimating the price for real\nestate properties, is crucial for both buys and sellers as the basis for\nnegotiation and transaction. Traditionally, the repeat sales model has been\nwidely adopted to estimate real estate price. However, it depends the design\nand calculation of a complex economic related index, which is challenging to\nestimate accurately. Today, real estate brokers provide easy access to detailed\nonline information on real estate properties to their clients. We are\ninterested in estimating the real estate price from these large amounts of\neasily accessed data. In particular, we analyze the prediction power of online\nhouse pictures, which is one of the key factors for online users to make a\npotential visiting decision. The development of robust computer vision\nalgorithms makes the analysis of visual content possible. In this work, we\nemploy a Recurrent Neural Network (RNN) to predict real estate price using the\nstate-of-the-art visual features. The experimental results indicate that our\nmodel outperforms several of other state-of-the-art baseline algorithms in\nterms of both mean absolute error (MAE) and mean absolute percentage error\n(MAPE).\n","id":795}
{"Unnamed: 0.1":11796,"Unnamed: 0":11796.0,"anchor":"Demystifying Neural Style Transfer","positive":"  Neural Style Transfer has recently demonstrated very exciting results which\ncatches eyes in both academia and industry. Despite the amazing results, the\nprinciple of neural style transfer, especially why the Gram matrices could\nrepresent style remains unclear. In this paper, we propose a novel\ninterpretation of neural style transfer by treating it as a domain adaptation\nproblem. Specifically, we theoretically show that matching the Gram matrices of\nfeature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with\nthe second order polynomial kernel. Thus, we argue that the essence of neural\nstyle transfer is to match the feature distributions between the style images\nand the generated images. To further support our standpoint, we experiment with\nseveral other distribution alignment methods, and achieve appealing results. We\nbelieve this novel interpretation connects these two important research fields,\nand could enlighten future researches.\n","negative":"  Given samples from an unknown multivariate distribution $p$, is it possible\nto distinguish whether $p$ is the product of its marginals versus $p$ being far\nfrom every product distribution? Similarly, is it possible to distinguish\nwhether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from\neach other? These problems of testing independence and goodness-of-fit have\nreceived enormous attention in statistics, information theory, and theoretical\ncomputer science, with sample-optimal algorithms known in several interesting\nregimes of parameters. Unfortunately, it has also been understood that these\nproblems become intractable in large dimensions, necessitating exponential\nsample complexity.\n  Motivated by the exponential lower bounds for general distributions as well\nas the ubiquity of Markov Random Fields (MRFs) in the modeling of\nhigh-dimensional distributions, we initiate the study of distribution testing\non structured multivariate distributions, and in particular the prototypical\nexample of MRFs: the Ising Model. We demonstrate that, in this structured\nsetting, we can avoid the curse of dimensionality, obtaining sample and time\nefficient testers for independence and goodness-of-fit. One of the key\ntechnical challenges we face along the way is bounding the variance of\nfunctions of the Ising model.\n","id":796}
{"Unnamed: 0.1":11797,"Unnamed: 0":11797.0,"anchor":"Estimating Quality in Multi-Objective Bandits Optimization","positive":"  Many real-world applications are characterized by a number of conflicting\nperformance measures. As optimizing in a multi-objective setting leads to a set\nof non-dominated solutions, a preference function is required for selecting the\nsolution with the appropriate trade-off between the objectives. The question\nis: how good do estimations of these objectives have to be in order for the\nsolution maximizing the preference function to remain unchanged? In this paper,\nwe introduce the concept of preference radius to characterize the robustness of\nthe preference function and provide guidelines for controlling the quality of\nestimations in the multi-objective setting. More specifically, we provide a\ngeneral formulation of multi-objective optimization under the bandits setting.\nWe show how the preference radius relates to the optimal gap and we use this\nconcept to provide a theoretical analysis of the Thompson sampling algorithm\nfrom multivariate normal priors. We finally present experiments to support the\ntheoretical results and highlight the fact that one cannot simply scalarize\nmulti-objective problems into single-objective problems.\n","negative":"  Safeguarding privacy in machine learning is highly desirable, especially in\ncollaborative studies across many organizations. Privacy-preserving distributed\nmachine learning (based on cryptography) is popular to solve the problem.\nHowever, existing cryptographic protocols still incur excess computational\noverhead. Here, we make a novel observation that this is partially due to naive\nadoption of mainstream numerical optimization (e.g., Newton method) and failing\nto tailor for secure computing. This work presents a contrasting perspective:\ncustomizing numerical optimization specifically for secure settings. We propose\na seemingly less-favorable optimization method that can in fact significantly\naccelerate privacy-preserving logistic regression. Leveraging this new method,\nwe propose two new secure protocols for conducting logistic regression in a\nprivacy-preserving and distributed manner. Extensive theoretical and empirical\nevaluations prove the competitive performance of our two secure proposals while\nwithout compromising accuracy or privacy: with speedup up to 2.3x and 8.1x,\nrespectively, over state-of-the-art; and even faster as data scales up. Such\ndrastic speedup is on top of and in addition to performance improvements from\nexisting (and future) state-of-the-art cryptography. Our work provides a new\nway towards efficient and practical privacy-preserving logistic regression for\nlarge-scale studies which are common for modern science.\n","id":797}
{"Unnamed: 0.1":11798,"Unnamed: 0":11798.0,"anchor":"Overlapping Cover Local Regression Machines","positive":"  We present the Overlapping Domain Cover (ODC) notion for kernel machines, as\na set of overlapping subsets of the data that covers the entire training set\nand optimized to be spatially cohesive as possible. We show how this notion\nbenefit the speed of local kernel machines for regression in terms of both\nspeed while achieving while minimizing the prediction error. We propose an\nefficient ODC framework, which is applicable to various regression models and\nin particular reduces the complexity of Twin Gaussian Processes (TGP)\nregression from cubic to quadratic. Our notion is also applicable to several\nkernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as\nshown in our experiments). We also theoretically justified the idea behind our\nmethod to improve local prediction by the overlapping cover. We validated and\nanalyzed our method on three benchmark human pose estimation datasets and\ninteresting findings are discussed.\n","negative":"  Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in\nthe field of reinforcement learning. However, setting TD's $\\lambda$ parameter,\nwhich controls the timescale of TD updates, is generally left up to the\npractitioner. We formalize the $\\lambda$ selection problem as a bias-variance\ntrade-off where the solution is the value of $\\lambda$ that leads to the\nsmallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest\napplying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the\nspace of $\\lambda$ values. Unfortunately, this approach is too computationally\nexpensive for most practical applications. For Least Squares TD (LSTD) we show\nthat LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and\napply function optimization methods to efficiently search the space of\n$\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our\nexperiments demonstrate that ALLSTD is significantly computationally faster\nthan the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.\n","id":798}
{"Unnamed: 0.1":11799,"Unnamed: 0":11799.0,"anchor":"OpenML: An R Package to Connect to the Machine Learning Platform OpenML","positive":"  OpenML is an online machine learning platform where researchers can easily\nshare data, machine learning tasks and experiments as well as organize them\nonline to work and collaborate more efficiently. In this paper, we present an R\npackage to interface with the OpenML platform and illustrate its usage in\ncombination with the machine learning R package mlr. We show how the OpenML\npackage allows R users to easily search, download and upload data sets and\nmachine learning tasks. Furthermore, we also show how to upload results of\nexperiments, share them with others and download results from other users.\nBeyond ensuring reproducibility of results, the OpenML platform automates much\nof the drudge work, speeds up research, facilitates collaboration and increases\nthe users' visibility online.\n","negative":"  Sparsity-constrained optimization is an important and challenging problem\nthat has wide applicability in data mining, machine learning, and statistics.\nIn this paper, we focus on sparsity-constrained optimization in cases where the\ncost function is a general nonlinear function and, in particular, the sparsity\nconstraint is defined by a graph-structured sparsity model. Existing methods\nexplore this problem in the context of sparse estimation in linear models. To\nthe best of our knowledge, this is the first work to present an efficient\napproximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),\nto optimize a general nonlinear function subject to graph-structured\nconstraints. We prove that our algorithm enjoys the strong guarantees analogous\nto those designed for linear models in terms of convergence rate and\napproximation accuracy. As a case study, we specialize Graph-Mp to optimize a\nnumber of well-known graph scan statistic models for the connected subgraph\ndetection task, and empirical evidence demonstrates that our general algorithm\nperforms superior over state-of-the-art methods that are designed specifically\nfor the task of connected subgraph detection.\n","id":799}
{"Unnamed: 0.1":11800,"Unnamed: 0":11800.0,"anchor":"Toward negotiable reinforcement learning: shifting priorities in Pareto\n  optimal sequential decision-making","positive":"  Existing multi-objective reinforcement learning (MORL) algorithms do not\naccount for objectives that arise from players with differing beliefs.\nConcretely, consider two players with different beliefs and utility functions\nwho may cooperate to build a machine that takes actions on their behalf. A\nrepresentation is needed for how much the machine's policy will prioritize each\nplayer's interests over time. Assuming the players have reached common\nknowledge of their situation, this paper derives a recursion that any Pareto\noptimal policy must satisfy. Two qualitative observations can be made from the\nrecursion: the machine must (1) use each player's own beliefs in evaluating how\nwell an action will serve that player's utility function, and (2) shift the\nrelative priority it assigns to each player's expected utilities over time, by\na factor proportional to how well that player's beliefs predict the machine's\ninputs. Observation (2) represents a substantial divergence from na\\\"{i}ve\nlinear utility aggregation (as in Harsanyi's utilitarian theorem, and existing\nMORL algorithms), which is shown here to be inadequate for Pareto optimal\nsequential decision-making on behalf of players with different beliefs.\n","negative":"  We describe a new method called t-ETE for finding a low-dimensional embedding\nof a set of objects in Euclidean space. We formulate the embedding problem as a\njoint ranking problem over a set of triplets, where each triplet captures the\nrelative similarities between three objects in the set. By exploiting recent\nadvances in robust ranking, t-ETE produces high-quality embeddings even in the\npresence of a significant amount of noise and better preserves local scale than\nknown methods, such as t-STE and t-SNE. In particular, our method produces\nsignificantly better results than t-SNE on signature datasets while also being\nfaster to compute.\n","id":800}
{"Unnamed: 0.1":11801,"Unnamed: 0":11801.0,"anchor":"Outlier Detection for Text Data : An Extended Version","positive":"  The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods.\n","negative":"  Future projection of climate is typically obtained by combining outputs from\nmultiple Earth System Models (ESMs) for several climate variables such as\ntemperature and precipitation. While IPCC has traditionally used a simple model\noutput average, recent work has illustrated potential advantages of using a\nmultitask learning (MTL) framework for projections of individual climate\nvariables. In this paper we introduce a framework for hierarchical multitask\nlearning (HMTL) with two levels of tasks such that each super-task, i.e., task\nat the top level, is itself a multitask learning problem over sub-tasks. For\nclimate projections, each super-task focuses on projections of specific climate\nvariables spatially using an MTL formulation. For the proposed HMTL approach, a\ngroup lasso regularization is added to couple parameters across the\nsuper-tasks, which in the climate context helps exploit relationships among the\nbehavior of different climate variables at a given spatial location. We show\nthat some recent works on MTL based on learning task dependency structures can\nbe viewed as special cases of HMTL. Experiments on synthetic and real climate\ndata show that HMTL produces better results than decoupled MTL methods applied\nseparately on the super-tasks and HMTL significantly outperforms baselines for\nclimate projection.\n","id":801}
{"Unnamed: 0.1":11802,"Unnamed: 0":11802.0,"anchor":"Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\n  Neural Networks","positive":"  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n","negative":"  Ranking a set of objects involves establishing an order allowing for\ncomparisons between any pair of objects in the set. Oftentimes, due to the\nunavailability of a ground truth of ranked orders, researchers resort to\nobtaining judgments from multiple annotators followed by inferring the ground\ntruth based on the collective knowledge of the crowd. However, the aggregation\nis often ad-hoc and involves imposing stringent assumptions in inferring the\nground truth (e.g. majority vote). In this work, we propose\nExpectation-Maximization (EM) based algorithms that rely on the judgments from\nmultiple annotators and the object attributes for inferring the latent ground\ntruth. The algorithm learns the relation between the latent ground truth and\nobject attributes as well as annotator specific probabilities of flipping, a\nmetric to assess annotator quality. We further extend the EM algorithm to allow\nfor a variable probability of flipping based on the pair of objects at hand. We\ntest our algorithms on two data sets with synthetic annotations and investigate\nthe impact of annotator quality and quantity on the inferred ground truth. We\nalso obtain the results on two other data sets with annotations from\nmachine\/human annotators and interpret the output trends based on the data\ncharacteristics.\n","id":802}
{"Unnamed: 0.1":11803,"Unnamed: 0":11803.0,"anchor":"NeuroRule: A Connectionist Approach to Data Mining","positive":"  Classification, which involves finding rules that partition a given data set\ninto disjoint groups, is one class of data mining problems. Approaches proposed\nso far for mining classification rules for large databases are mainly decision\ntree based symbolic learning methods. The connectionist approach based on\nneural networks has been thought not well suited for data mining. One of the\nmajor reasons cited is that knowledge generated by neural networks is not\nexplicitly represented in the form of rules suitable for verification or\ninterpretation by humans. This paper examines this issue. With our newly\ndeveloped algorithms, rules which are similar to, or more concise than those\ngenerated by the symbolic methods can be extracted from the neural networks.\nThe data mining process using neural networks with the emphasis on rule\nextraction is described. Experimental results and comparison with previously\npublished works are presented.\n","negative":"  Training robots to perceive, act and communicate using multiple modalities\nstill represents a challenging problem, particularly if robots are expected to\nlearn efficiently from small sets of example interactions. We describe a\nlearning approach as a step in this direction, where we teach a humanoid robot\nhow to play the game of noughts and crosses. Given that multiple multimodal\nskills can be trained to play this game, we focus our attention to training the\nrobot to perceive the game, and to interact in this game. Our multimodal deep\nreinforcement learning agent perceives multimodal features and exhibits verbal\nand non-verbal actions while playing. Experimental results using simulations\nshow that the robot can learn to win or draw up to 98% of the games. A pilot\ntest of the proposed multimodal system for the targeted game---integrating\nspeech, vision and gestures---reports that reasonable and fluent interactions\ncan be achieved using the proposed approach.\n","id":803}
{"Unnamed: 0.1":11804,"Unnamed: 0":11804.0,"anchor":"On spectral partitioning of signed graphs","positive":"  We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute.\n","negative":"  In an attempt to solve the lengthy training times of neural networks, we\nproposed Parallel Circuits (PCs), a biologically inspired architecture.\nPrevious work has shown that this approach fails to maintain generalization\nperformance in spite of achieving sharp speed gains. To address this issue, and\nmotivated by the way Dropout prevents node co-adaption, in this paper, we\nsuggest an improvement by extending Dropout to the PC architecture. The paper\nprovides multiple insights into this combination, including a variety of fusion\napproaches. Experiments show promising results in which improved error rates\nare achieved in most cases, whilst maintaining the speed advantage of the PC\napproach.\n","id":804}
{"Unnamed: 0.1":11805,"Unnamed: 0":11805.0,"anchor":"Learning local trajectories for high precision robotic tasks :\n  application to KUKA LBR iiwa Cartesian positioning","positive":"  To ease the development of robot learning in industry, two conditions need to\nbe fulfilled. Manipulators must be able to learn high accuracy and precision\ntasks while being safe for workers in the factory. In this paper, we extend\npreviously submitted work which consists in rapid learning of local high\naccuracy behaviors. By exploration and regression, linear and quadratic models\nare learnt for respectively the dynamics and cost function. Iterative Linear\nQuadratic Gaussian Regulator combined with cost quadratic regression can\nconverge rapidly in the final stages towards high accuracy behavior as the cost\nfunction is modelled quite precisely. In this paper, both a different cost\nfunction and a second order improvement method are implemented within this\nframework. We also propose an analysis of the algorithm parameters through\nsimulation for a positioning task. Finally, an experimental validation on a\nKUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be\neasily programmed into safety mode, which makes it qualified for the second\nindustry constraint stated above.\n","negative":"  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n","id":805}
{"Unnamed: 0.1":11806,"Unnamed: 0":11806.0,"anchor":"Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU","positive":"  The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem.\n","negative":"  We study machine learning formulations of inductive program synthesis; that\nis, given input-output examples, synthesize source code that maps inputs to\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\nlanguage for expressing program synthesis problems. A TerpreT model is composed\nof a specification of a program representation and an interpreter that\ndescribes how programs map inputs to outputs. The inference task is to observe\na set of input-output examples and infer the underlying program. From a TerpreT\nmodel we automatically perform inference using four different back-ends:\ngradient descent (thus each TerpreT model can be seen as defining a\ndifferentiable interpreter), linear program (LP) relaxations for graphical\nmodels, discrete satisfiability solving, and the Sketch program synthesis\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\nrange of domains, program representations, and interpreter models. Second, it\nseparates the model specification from the inference algorithm, allowing proper\ncomparisons between different approaches to inference.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an extensive empirical comparison between alternative inference\nalgorithms on a variety of program models. To our knowledge, this is the first\nwork to compare gradient-based search over program space to traditional\nsearch-based alternatives. Our key empirical finding is that constraint solvers\ndominate the gradient descent and LP-based formulations.\n  This is a workshop summary of a longer report at arXiv:1608.04428\n","id":806}
{"Unnamed: 0.1":11807,"Unnamed: 0":11807.0,"anchor":"Deep Learning for Time-Series Analysis","positive":"  In many real-world application, e.g., speech recognition or sleep stage\nclassification, data are captured over the course of time, constituting a\nTime-Series. Time-Series often contain temporal dependencies that cause two\notherwise identical points of time to belong to different classes or predict\ndifferent behavior. This characteristic generally increases the difficulty of\nanalysing them. Existing techniques often depended on hand-crafted features\nthat were expensive to create and required expert knowledge of the field. With\nthe advent of Deep Learning new models of unsupervised learning of features for\nTime-series analysis and forecast have been developed. Such new developments\nare the topic of this paper: a review of the main Deep Learning techniques is\npresented, and some applications on Time-Series analysis are summaried. The\nresults make it clear that Deep Learning has a lot to contribute to the field.\n","negative":"  This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete\/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart.\n","id":807}
{"Unnamed: 0.1":11808,"Unnamed: 0":11808.0,"anchor":"See the Near Future: A Short-Term Predictive Methodology to Traffic Load\n  in ITS","positive":"  The Intelligent Transportation System (ITS) targets to a coordinated traffic\nsystem by applying the advanced wireless communication technologies for road\ntraffic scheduling. Towards an accurate road traffic control, the short-term\ntraffic forecasting to predict the road traffic at the particular site in a\nshort period is often useful and important. In existing works, Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model is a popular approach.\nThe scheme however encounters two challenges: 1) the analysis on related data\nis insufficient whereas some important features of data may be neglected; and\n2) with data presenting different features, it is unlikely to have one\npredictive model that can fit all situations. To tackle above issues, in this\nwork, we develop a hybrid model to improve accuracy of SARIMA. In specific, we\nfirst explore the autocorrelation and distribution features existed in traffic\nflow to revise structure of the time series model. Based on the Gaussian\ndistribution of traffic flow, a hybrid model with a Bayesian learning algorithm\nis developed which can effectively expand the application scenarios of SARIMA.\nWe show the efficiency and accuracy of our proposal using both analysis and\nexperimental studies. Using the real-world trace data, we show that the\nproposed predicting approach can achieve satisfactory performance in practice.\n","negative":"  Knowledge Tracing (KT) is a task of tracing evolving knowledge state of\nstudents with respect to one or more concepts as they engage in a sequence of\nlearning activities. One important purpose of KT is to personalize the practice\nsequence to help students learn knowledge concepts efficiently. However,\nexisting methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing\neither model knowledge state for each predefined concept separately or fail to\npinpoint exactly which concepts a student is good at or unfamiliar with. To\nsolve these problems, this work introduces a new model called Dynamic Key-Value\nMemory Networks (DKVMN) that can exploit the relationships between underlying\nconcepts and directly output a student's mastery level of each concept. Unlike\nstandard memory-augmented neural networks that facilitate a single memory\nmatrix or two static memory matrices, our model has one static matrix called\nkey, which stores the knowledge concepts and the other dynamic matrix called\nvalue, which stores and updates the mastery levels of corresponding concepts.\nExperiments show that our model consistently outperforms the state-of-the-art\nmodel in a range of KT datasets. Moreover, the DKVMN model can automatically\ndiscover underlying concepts of exercises typically performed by human\nannotations and depict the changing knowledge state of a student.\n","id":808}
{"Unnamed: 0.1":11809,"Unnamed: 0":11809.0,"anchor":"Large-scale network motif analysis using compression","positive":"  We introduce a new method for finding network motifs: interesting or\ninformative subgraph patterns in a network. Subgraphs are motifs when their\nfrequency in the data is high compared to the expected frequency under a null\nmodel. To compute this expectation, a full or approximate count of the\noccurrences of a motif is normally repeated on as many as 1000 random graphs\nsampled from the null model; a prohibitively expensive step. We use ideas from\nthe Minimum Description Length (MDL) literature to define a new measure of\nmotif relevance. With our method, samples from the null model are not required.\nInstead we compute the probability of the data under the null model and compare\nthis to the probability under a specially designed alternative model. With this\nnew relevance test, we can search for motifs by random sampling, rather than\nrequiring an accurate count of all instances of a motif. This allows motif\nanalysis to scale to networks with billions of links.\n","negative":"  A network of independently trained Gaussian processes (StackedGP) is\nintroduced to obtain predictions of quantities of interest with quantified\nuncertainties. The main applications of the StackedGP framework are to\nintegrate different datasets through model composition, enhance predictions of\nquantities of interest through a cascade of intermediate predictions, and to\npropagate uncertainties through emulated dynamical systems driven by uncertain\nforcing variables. By using analytical first and second-order moments of a\nGaussian process with uncertain inputs using squared exponential and polynomial\nkernels, approximated expectations of quantities of interests that require an\narbitrary composition of functions can be obtained. The StackedGP model is\nextended to any number of layers and nodes per layer, and it provides\nflexibility in kernel selection for the input nodes. The proposed nonparametric\nstacked model is validated using synthetic datasets, and its performance in\nmodel composition and cascading predictions is measured in two applications\nusing real data.\n","id":809}
{"Unnamed: 0.1":11810,"Unnamed: 0":11810.0,"anchor":"Tunable GMM Kernels","positive":"  The recently proposed \"generalized min-max\" (GMM) kernel can be efficiently\nlinearized, with direct applications in large-scale statistical learning and\nfast near neighbor search. The linearized GMM kernel was extensively compared\nin with linearized radial basis function (RBF) kernel. On a large number of\nclassification tasks, the tuning-free GMM kernel performs (surprisingly) well\ncompared to the best-tuned RBF kernel. Nevertheless, one would naturally expect\nthat the GMM kernel ought to be further improved if we introduce tuning\nparameters.\n  In this paper, we study three simple constructions of tunable GMM kernels:\n(i) the exponentiated-GMM (or eGMM) kernel, (ii) the powered-GMM (or pGMM)\nkernel, and (iii) the exponentiated-powered-GMM (epGMM) kernel. The pGMM kernel\ncan still be efficiently linearized by modifying the original hashing procedure\nfor the GMM kernel. On about 60 publicly available classification datasets, we\nverify that the proposed tunable GMM kernels typically improve over the\noriginal GMM kernel. On some datasets, the improvements can be astonishingly\nsignificant.\n  For example, on 11 popular datasets which were used for testing deep learning\nalgorithms and tree methods, our experiments show that the proposed tunable GMM\nkernels are strong competitors to trees and deep nets. The previous studies\ndeveloped tree methods including \"abc-robust-logitboost\" and demonstrated the\nexcellent performance on those 11 datasets (and other datasets), by\nestablishing the second-order tree-split formula and new derivatives for\nmulti-class logistic loss. Compared to tree methods like\n\"abc-robust-logitboost\" (which are slow and need substantial model sizes), the\ntunable GMM kernels produce largely comparable results.\n","negative":"  Variational inference is a powerful tool for approximate inference. However,\nit mainly focuses on the evidence lower bound as variational objective and the\ndevelopment of other measures for variational inference is a promising area of\nresearch. This paper proposes a robust modification of evidence and a lower\nbound for the evidence, which is applicable when the majority of the training\nset samples are random noise objects. We provide experiments for variational\nautoencoders to show advantage of the objective over the evidence lower bound\non synthetic datasets obtained by adding uninformative noise objects to MNIST\nand OMNIGLOT. Additionally, for the original MNIST and OMNIGLOT datasets we\nobserve a small improvement over the non-robust evidence lower bound.\n","id":810}
{"Unnamed: 0.1":11811,"Unnamed: 0":11811.0,"anchor":"Coupled Compound Poisson Factorization","positive":"  We present a general framework, the coupled compound Poisson factorization\n(CCPF), to capture the missing-data mechanism in extremely sparse data sets by\ncoupling a hierarchical Poisson factorization with an arbitrary data-generating\nmodel. We derive a stochastic variational inference algorithm for the resulting\nmodel and, as examples of our framework, implement three different\ndata-generating models---a mixture model, linear regression, and factor\nanalysis---to robustly model non-random missing data in the context of\nclustering, prediction, and matrix factorization. In all three cases, we test\nour framework against models that ignore the missing-data mechanism on large\nscale studies with non-random missing data, and we show that explicitly\nmodeling the missing-data mechanism substantially improves the quality of the\nresults, as measured using data log likelihood on a held-out test set.\n","negative":"  Population growth and increasing droughts are creating unprecedented strain\non the continued availability of water resources. Since irrigation is a major\nconsumer of fresh water, wastage of resources in this sector could have strong\nconsequences. To address this issue, irrigation water management and prediction\ntechniques need to be employed effectively and should be able to account for\nthe variabilities present in the environment. The different techniques surveyed\nin this paper can be classified into two categories: computational and\nstatistical. Computational methods deal with scientific correlations between\nphysical parameters whereas statistical methods involve specific prediction\nalgorithms that can be used to automate the process of irrigation water\nprediction. These algorithms interpret semantic relationships between the\nvarious parameters of temperature, pressure, evapotranspiration etc. and store\nthem as numerical precomputed entities specific to the conditions and the area\nused as the data for the training corpus used to train it. We focus on\nreviewing the computational methods used to determine Evapotranspiration and\nits implications. We compare the efficiencies of different data mining and\nmachine learning methods implemented in this area, such as Logistic Regression,\nDecision Tress Classifier, SysFor, Support Vector Machine(SVM), Fuzzy Logic\ntechniques, Artifical Neural Networks(ANNs) and various hybrids of Genetic\nAlgorithms (GA) applied to irrigation prediction. We also recommend a possible\ntechnique for the same based on its superior results in other such time series\nanalysis tasks.\n","id":811}
{"Unnamed: 0.1":11812,"Unnamed: 0":11812.0,"anchor":"Deep driven fMRI decoding of visual categories","positive":"  Deep neural networks have been developed drawing inspiration from the brain\nvisual pathway, implementing an end-to-end approach: from image data to video\nobject classes. However building an fMRI decoder with the typical structure of\nConvolutional Neural Network (CNN), i.e. learning multiple level of\nrepresentations, seems impractical due to lack of brain data. As a possible\nsolution, this work presents the first hybrid fMRI and deep features decoding\napproach: collected fMRI and deep learnt representations of video object\nclasses are linked together by means of Kernel Canonical Correlation Analysis.\nIn decoding, this allows exploiting the discriminatory power of CNN by relating\nthe fMRI representation to the last layer of CNN (fc7). We show the\neffectiveness of embedding fMRI data onto a subspace related to deep features\nin distinguishing semantic visual categories based solely on brain imaging\ndata.\n","negative":"  We study the fundamental problems of (i) uniformity testing of a discrete\ndistribution, and (ii) closeness testing between two discrete distributions\nwith bounded $\\ell_2$-norm. These problems have been extensively studied in\ndistribution testing and sample-optimal estimators are known for\nthem~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n  In this work, we show that the original collision-based testers proposed for\nthese problems ~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant\nfactors. Previous analyses showed sample complexity upper bounds for these\ntesters that are optimal as a function of the domain size $n$, but suboptimal\nby polynomial factors in the error parameter $\\epsilon$. Our main contribution\nis a new tight analysis establishing that these collision-based testers are\ninformation-theoretically optimal, up to constant factors, both in the\ndependence on $n$ and in the dependence on $\\epsilon$.\n","id":812}
{"Unnamed: 0.1":11813,"Unnamed: 0":11813.0,"anchor":"Shallow and Deep Networks Intrusion Detection System: A Taxonomy and\n  Survey","positive":"  Intrusion detection has attracted a considerable interest from researchers\nand industries. The community, after many years of research, still faces the\nproblem of building reliable and efficient IDS that are capable of handling\nlarge quantities of data, with changing patterns in real time situations. The\nwork presented in this manuscript classifies intrusion detection systems (IDS).\nMoreover, a taxonomy and survey of shallow and deep networks intrusion\ndetection systems is presented based on previous and current works. This\ntaxonomy and survey reviews machine learning techniques and their performance\nin detecting anomalies. Feature selection which influences the effectiveness of\nmachine learning (ML) IDS is discussed to explain the role of feature selection\nin the classification and training phase of ML IDS. Finally, a discussion of\nthe false and true positive alarm rates is presented to help researchers model\nreliable and efficient machine learning based intrusion detection systems.\n","negative":"  Dictionary learning is the task of determining a data-dependent transform\nthat yields a sparse representation of some observed data. The dictionary\nlearning problem is non-convex, and usually solved via computationally complex\niterative algorithms. Furthermore, the resulting transforms obtained generally\nlack structure that permits their fast application to data. To address this\nissue, this paper develops a framework for learning orthonormal dictionaries\nwhich are built from products of a few Householder reflectors. Two algorithms\nare proposed to learn the reflector coefficients: one that considers a\nsequential update of the reflectors and one with a simultaneous update of all\nreflectors that imposes an additional internal orthogonal constraint. The\nproposed methods have low computational complexity and are shown to converge to\nlocal minimum points which can be described in terms of the spectral properties\nof the matrices involved. The resulting dictionaries balance between the\ncomputational complexity and the quality of the sparse representations by\ncontrolling the number of Householder reflectors in their product. Simulations\nof the proposed algorithms are shown in the image processing setting where\nwell-known fast transforms are available for comparisons. The proposed\nalgorithms have favorable reconstruction error and the advantage of a fast\nimplementation relative to the classical, unstructured, dictionaries.\n","id":813}
{"Unnamed: 0.1":11814,"Unnamed: 0":11814.0,"anchor":"DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning","positive":"  In recent years, Deep Learning (DL) has found great success in domains such\nas multimedia understanding. However, the complex nature of multimedia data\nmakes it difficult to develop DL-based software. The state-of-the art tools,\nsuch as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their\napplicable domains, are programming libraries with fixed user interface,\ninternal representation, and execution environment. This makes it difficult to\nimplement portable and customized DL applications.\n  In this paper, we present DeepDSL, a domain specific language (DSL) embedded\nin Scala, that compiles deep networks written in DeepDSL to Java source code.\nDeep DSL provides (1) intuitive constructs to support compact encoding of deep\nnetworks; (2) symbolic gradient derivation of the networks; (3) static analysis\nfor memory consumption and error detection; and (4) DSL-level optimization to\nimprove memory and runtime efficiency.\n  DeepDSL programs are compiled into compact, efficient, customizable, and\nportable Java source code, which operates the CUDA and CUDNN interfaces running\non Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL\nwith a number of popular DL networks. Our experiments show that the compiled\nprograms have very competitive runtime performance and memory efficiency\ncompared to the existing libraries.\n","negative":"  Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions.\n","id":814}
{"Unnamed: 0.1":11815,"Unnamed: 0":11815.0,"anchor":"QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures","positive":"  We present QuickNet, a fast and accurate network architecture that is both\nfaster and significantly more accurate than other fast deep architectures like\nSqueezeNet. Furthermore, it uses less parameters than previous networks, making\nit more memory efficient. We do this by making two major modifications to the\nreference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable\nconvolutions and 2) The use of parametric rectified linear units. We make the\nobservation that parametric rectified linear units are computationally\nequivalent to leaky rectified linear units at test time and the observation\nthat separable convolutions can be interpreted as a compressed Inception\nnetwork (Chollet, 2016). Using these observations, we derive a network\narchitecture, which we call QuickNet, that is both faster and more accurate\nthan previous models. Our architecture provides at least four major advantages:\n(1) A smaller model size, which is more tenable on memory constrained systems;\n(2) A significantly faster network which is more tenable on computationally\nconstrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10\nDataset which outperforms all but one result published so far, although we note\nthat our works are orthogonal approaches and can be combined (4) Orthogonality\nto previous model compression approaches allowing for further speed gains to be\nrealized.\n","negative":"  Models obtained by decision tree induction techniques excel in being\ninterpretable.However, they can be prone to overfitting, which results in a low\npredictive performance. Ensemble techniques are able to achieve a higher\naccuracy. However, this comes at a cost of losing interpretability of the\nresulting model. This makes ensemble techniques impractical in applications\nwhere decision support, instead of decision making, is crucial.\n  To bridge this gap, we present the GENESIM algorithm that transforms an\nensemble of decision trees to a single decision tree with an enhanced\npredictive performance by using a genetic algorithm. We compared GENESIM to\nprevalent decision tree induction and ensemble techniques using twelve publicly\navailable data sets. The results show that GENESIM achieves a better predictive\nperformance on most of these data sets than decision tree induction techniques\nand a predictive performance in the same order of magnitude as the ensemble\ntechniques. Moreover, the resulting model of GENESIM has a very low complexity,\nmaking it very interpretable, in contrast to ensemble techniques.\n","id":815}
{"Unnamed: 0.1":11816,"Unnamed: 0":11816.0,"anchor":"A Homological Theory of Functions","positive":"  In computational complexity, a complexity class is given by a set of problems\nor functions, and a basic challenge is to show separations of complexity\nclasses $A \\not= B$ especially when $A$ is known to be a subset of $B$. In this\npaper we introduce a homological theory of functions that can be used to\nestablish complexity separations, while also providing other interesting\nconsequences. We propose to associate a topological space $S_A$ to each class\nof functions $A$, such that, to separate complexity classes $A \\subseteq B'$,\nit suffices to observe a change in \"the number of holes\", i.e. homology, in\n$S_A$ as a subclass $B$ of $B'$ is added to $A$. In other words, if the\nhomologies of $S_A$ and $S_{A \\cup B}$ are different, then $A \\not= B'$. We\ndevelop the underlying theory of functions based on combinatorial and\nhomological commutative algebra and Stanley-Reisner theory, and recover Minsky\nand Papert's 1969 result that parity cannot be computed by nonmaximal degree\npolynomial threshold functions. In the process, we derive a \"maximal principle\"\nfor polynomial threshold functions that is used to extend this result further\nto arbitrary symmetric functions. A surprising coincidence is demonstrated,\nwhere the maximal dimension of \"holes\" in $S_A$ upper bounds the VC dimension\nof $A$, with equality for common computational cases such as the class of\npolynomial threshold functions or the class of linear functionals in $\\mathbb\nF_2$, or common algebraic cases such as when the Stanley-Reisner ring of $S_A$\nis Cohen-Macaulay. As another interesting application of our theory, we prove a\nresult that a priori has nothing to do with complexity separation: it\ncharacterizes when a vector subspace intersects the positive cone, in terms of\nhomological conditions. By analogy to Farkas' result doing the same with\n*linear conditions*, we call our theorem the Homological Farkas Lemma.\n","negative":"  The paper characterizes classes of functions for which deep learning can be\nexponentially better than shallow learning. Deep convolutional networks are a\nspecial case of these conditions, though weight sharing is not the main reason\nfor their exponential advantage.\n","id":816}
{"Unnamed: 0.1":11817,"Unnamed: 0":11817.0,"anchor":"The principle of cognitive action - Preliminary experimental analysis","positive":"  In this document we shows a first implementation and some preliminary results\nof a new theory, facing Machine Learning problems in the frameworks of\nClassical Mechanics and Variational Calculus. We give a general formulation of\nthe problem and then we studies basic behaviors of the model on simple\npractical implementations.\n","negative":"  Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability\ntheory in general and in machine learning in particular. A Markov chain is\ndevised so that its stationary distribution is some probability distribution of\ninterest. Then one samples from the given distribution by running the Markov\nchain for a \"long time\" until it appears to be stationary and then collects the\nsample. However these chains are often very complex and there are no\ntheoretical guarantees that stationarity is actually reached. In this paper we\nstudy the Gibbs sampler of the posterior distribution of a very simple case of\nLatent Dirichlet Allocation, the arguably most well known Bayesian unsupervised\nlearning model for text generation and text classification. It is shown that\nwhen the corpus consists of two long documents of equal length $m$ and the\nvocabulary consists of only two different words, the mixing time is at most of\norder $m^2\\log m$ (which corresponds to $m\\log m$ rounds over the corpus). It\nwill be apparent from our analysis that it seems very likely that the mixing\ntime is not much worse in the more relevant case when the number of documents\nand the size of the vocabulary are also large as long as each word is\nrepresented a large number in each document, even though the computations\ninvolved may be intractable.\n","id":817}
{"Unnamed: 0.1":11818,"Unnamed: 0":11818.0,"anchor":"AdaGAN: Boosting Generative Models","positive":"  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an\neffective method for training generative models of complex data such as natural\nimages. However, they are notoriously hard to train and can suffer from the\nproblem of missing modes where the model is not able to produce examples in\ncertain regions of the space. We propose an iterative procedure, called AdaGAN,\nwhere at every step we add a new component into a mixture model by running a\nGAN algorithm on a reweighted sample. This is inspired by boosting algorithms,\nwhere many potentially weak individual predictors are greedily aggregated to\nform a strong composite predictor. We prove that such an incremental procedure\nleads to convergence to the true distribution in a finite number of steps if\neach step is optimal, and convergence at an exponential rate otherwise. We also\nillustrate experimentally that this procedure addresses the problem of missing\nmodes.\n","negative":"  A number of machine learning algorithms are using a metric, or a distance, in\norder to compare individuals. The Euclidean distance is usually employed, but\nit may be more efficient to learn a parametric distance such as Mahalanobis\nmetric. Learning such a metric is a hot topic since more than ten years now,\nand a number of methods have been proposed to efficiently learn it. However,\nthe nature of the problem makes it quite difficult for large scale data, as\nwell as data for which classes overlap. This paper presents a simple way of\nimproving accuracy and scalability of any iterative metric learning algorithm,\nwhere constraints are obtained prior to the algorithm. The proposed approach\nrelies on a loss-dependent weighted selection of constraints that are used for\nlearning the metric. Using the corresponding dedicated loss function, the\nmethod clearly allows to obtain better results than state-of-the-art methods,\nboth in terms of accuracy and time complexity. Some experimental results on\nreal world, and potentially large, datasets are demonstrating the effectiveness\nof our proposition.\n","id":818}
{"Unnamed: 0.1":11819,"Unnamed: 0":11819.0,"anchor":"Reinforcement Learning via Recurrent Convolutional Neural Networks","positive":"  Deep Reinforcement Learning has enabled the learning of policies for complex\ntasks in partially observable environments, without explicitly learning the\nunderlying model of the tasks. While such model-free methods achieve\nconsiderable performance, they often ignore the structure of task. We present a\nnatural representation of to Reinforcement Learning (RL) problems using\nRecurrent Convolutional Neural Networks (RCNNs), to better exploit this\ninherent structure. We define 3 such RCNNs, whose forward passes execute an\nefficient Value Iteration, propagate beliefs of state in partially observable\nenvironments, and choose optimal actions respectively. Backpropagating\ngradients through these RCNNs allows the system to explicitly learn the\nTransition Model and Reward Function associated with the underlying MDP,\nserving as an elegant alternative to classical model-based RL. We evaluate the\nproposed algorithms in simulation, considering a robot planning problem. We\ndemonstrate the capability of our framework to reduce the cost of replanning,\nlearn accurate MDP models, and finally re-plan with learnt models to achieve\nnear-optimal policies.\n","negative":"  PixelCNNs are a recently proposed class of powerful generative models with\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\nmake available at https:\/\/github.com\/openai\/pixel-cnn. Our implementation\ncontains a number of modifications to the original model that both simplify its\nstructure and improve its performance. 1) We use a discretized logistic mixture\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\nup training. 2) We condition on whole pixels, rather than R\/G\/B sub-pixels,\nsimplifying the model structure. 3) We use downsampling to efficiently capture\nstructure at multiple resolutions. 4) We introduce additional short-cut\nconnections to further speed up optimization. 5) We regularize the model using\ndropout. Finally, we present state-of-the-art log likelihood results on\nCIFAR-10 to demonstrate the usefulness of these modifications.\n","id":819}
{"Unnamed: 0.1":11820,"Unnamed: 0":11820.0,"anchor":"Machine Learning of Linear Differential Equations using Gaussian\n  Processes","positive":"  This work leverages recent advances in probabilistic machine learning to\ndiscover conservation laws expressed by parametric linear equations. Such\nequations involve, but are not limited to, ordinary and partial differential,\nintegro-differential, and fractional order operators. Here, Gaussian process\npriors are modified according to the particular form of such operators and are\nemployed to infer parameters of the linear equations from scarce and possibly\nnoisy observations. Such observations may come from experiments or \"black-box\"\ncomputer simulations.\n","negative":"  Continual data collection and widespread deployment of machine learning\nalgorithms, particularly the distributed variants, have raised new privacy\nchallenges. In a distributed machine learning scenario, the dataset is stored\namong several machines and they solve a distributed optimization problem to\ncollectively learn the underlying model. We present a secure multi-party\ncomputation inspired privacy preserving distributed algorithm for optimizing a\nconvex function consisting of several possibly non-convex functions. Each\nindividual objective function is privately stored with an agent while the\nagents communicate model parameters with neighbor machines connected in a\nnetwork. We show that our algorithm can correctly optimize the overall\nobjective function and learn the underlying model accurately. We further prove\nthat under a vertex connectivity condition on the topology, our algorithm\npreserves privacy of individual objective functions. We establish limits on the\nwhat a coalition of adversaries can learn by observing the messages and states\nshared over a network.\n","id":820}
{"Unnamed: 0.1":11821,"Unnamed: 0":11821.0,"anchor":"Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\n  Speech Recognition","positive":"  Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM\/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM\/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB\n","negative":"  Phytoplankton plays an important role in marine ecosystem. It is defined as a\nbiological factor to assess marine quality. The identification of phytoplankton\nspecies has a high potential for monitoring environmental, climate changes and\nfor evaluating water quality. However, phytoplankton species identification is\nnot an easy task owing to their variability and ambiguity due to thousands of\nmicro and pico-plankton species. Therefore, the aim of this paper is to build a\nframework for identifying phytoplankton species and to perform a comparison on\ndifferent features types and classifiers. We propose a new features type\nextracted from raw signals of phytoplankton species. We then analyze the\nperformance of various classifiers on the proposed features type as well as two\nother features types for finding the robust one. Through experiments, it is\nfound that Random Forest using the proposed features gives the best\nclassification results with average accuracy up to 98.24%.\n","id":821}
{"Unnamed: 0.1":11822,"Unnamed: 0":11822.0,"anchor":"Implicitly Incorporating Morphological Information into Word Embedding","positive":"  In this paper, we propose three novel models to enhance word embedding by\nimplicitly using morphological information. Experiments on word similarity and\nsyntactic analogy show that the implicit models are superior to traditional\nexplicit ones. Our models outperform all state-of-the-art baselines and\nsignificantly improve the performance on both tasks. Moreover, our performance\non the smallest corpus is similar to the performance of CBOW on the corpus\nwhich is five times the size of ours. Parameter analysis indicates that the\nimplicit models can supplement semantic information during the word embedding\ntraining process.\n","negative":"  Smart active particles can acquire some limited knowledge of the fluid\nenvironment from simple mechanical cues and exert a control on their preferred\nsteering direction. Their goal is to learn the best way to navigate by\nexploiting the underlying flow whenever possible. As an example, we focus our\nattention on smart gravitactic swimmers. These are active particles whose task\nis to reach the highest altitude within some time horizon, given the\nconstraints enforced by fluid mechanics. By means of numerical experiments, we\nshow that swimmers indeed learn nearly optimal strategies just by experience. A\nreinforcement learning algorithm allows particles to learn effective strategies\neven in difficult situations when, in the absence of control, they would end up\nbeing trapped by flow structures. These strategies are highly nontrivial and\ncannot be easily guessed in advance. This Letter illustrates the potential of\nreinforcement learning algorithms to model adaptive behavior in complex flows\nand paves the way towards the engineering of smart microswimmers that solve\ndifficult navigation problems.\n","id":822}
{"Unnamed: 0.1":11823,"Unnamed: 0":11823.0,"anchor":"Real-Time Bidding by Reinforcement Learning in Display Advertising","positive":"  The majority of online display ads are served through real-time bidding (RTB)\n--- each ad display impression is auctioned off in real-time when it is just\nbeing generated from a user visit. To place an ad automatically and optimally,\nit is critical for advertisers to devise a learning algorithm to cleverly bid\nan ad impression in real-time. Most previous works consider the bid decision as\na static optimization problem of either treating the value of each impression\nindependently or setting a bid price to each segment of ad volume. However, the\nbidding for a given ad campaign would repeatedly happen during its life span\nbefore the budget runs out. As such, each bid is strategically correlated by\nthe constrained budget and the overall effectiveness of the campaign (e.g., the\nrewards from generated clicks), which is only observed after the campaign has\ncompleted. Thus, it is of great interest to devise an optimal bidding strategy\nsequentially so that the campaign budget can be dynamically allocated across\nall the available impressions on the basis of both the immediate and future\nrewards. In this paper, we formulate the bid decision process as a\nreinforcement learning problem, where the state space is represented by the\nauction information and the campaign's real-time parameters, while an action is\nthe bid price to set. By modeling the state transition via auction competition,\nwe build a Markov Decision Process framework for learning the optimal bidding\npolicy to optimize the advertising performance in the dynamic real-time bidding\nenvironment. Furthermore, the scalability problem from the large real-world\nauction volume and campaign budget is well handled by state value approximation\nusing neural networks.\n","negative":"  A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7.\n","id":823}
{"Unnamed: 0.1":11824,"Unnamed: 0":11824.0,"anchor":"Heterogeneous domain adaptation: An unsupervised approach","positive":"  Domain adaptation leverages the knowledge in one domain - the source domain -\nto improve learning efficiency in another domain - the target domain. Existing\nheterogeneous domain adaptation research is relatively well-progressed, but\nonly in situations where the target domain contains at least a few labeled\ninstances. In contrast, heterogeneous domain adaptation with an unlabeled\ntarget domain has not been well-studied. To contribute to the research in this\nemerging field, this paper presents: (1) an unsupervised knowledge transfer\ntheorem that guarantees the correctness of transferring knowledge; and (2) a\nprincipal angle-based metric to measure the distance between two pairs of\ndomains: one pair comprises the original source and target domains and the\nother pair comprises two homogeneous representations of two domains. The\ntheorem and the metric have been implemented in an innovative transfer model,\ncalled a Grassmann-Linear monotonic maps-geodesic flow kernel (GLG), that is\nspecifically designed for heterogeneous unsupervised domain adaptation (HeUDA).\nThe linear monotonic maps meet the conditions of the theorem and are used to\nconstruct homogeneous representations of the heterogeneous domains. The metric\nshows the extent to which the homogeneous representations have preserved the\ninformation in the original source and target domains. By minimizing the\nproposed metric, the GLG model learns the homogeneous representations of\nheterogeneous domains and transfers knowledge through these learned\nrepresentations via a geodesic flow kernel. To evaluate the model, five public\ndatasets were reorganized into ten HeUDA tasks across three applications:\ncancer detection, credit assessment, and text classification. The experiments\ndemonstrate that the proposed model delivers superior performance over the\nexisting baselines.\n","negative":"  There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and\/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space.\n","id":824}
{"Unnamed: 0.1":11825,"Unnamed: 0":11825.0,"anchor":"Unsupervised Image-to-Image Translation with Generative Adversarial\n  Networks","positive":"  It's useful to automatically transform an image from its original form to\nsome synthetic form (style, partial contents, etc.), while keeping the original\nstructure or semantics. We define this requirement as the \"image-to-image\ntranslation\" problem, and propose a general approach to achieve it, based on\ndeep convolutional and conditional generative adversarial networks (GANs),\nwhich has gained a phenomenal success to learn mapping images from noise input\nsince 2014. In this work, we develop a two step (unsupervised) learning method\nto translate images between different domains by using unlabeled images without\nspecifying any correspondence between them, so that to avoid the cost of\nacquiring labeled data. Compared with prior works, we demonstrated the capacity\nof generality in our model, by which variance of translations can be conduct by\na single type of model. Such capability is desirable in applications like\nbidirectional translation\n","negative":"  We present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments.\n","id":825}
{"Unnamed: 0.1":11826,"Unnamed: 0":11826.0,"anchor":"Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks","positive":"  Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models\/Gaussian Mixture Models\n(HMMs\/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information.\n","negative":"  In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we extend a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. Experimental results show reasonable predictions of\ndiagnostic labels given an unseen discomfort drawing. The positive result\nindicates a significant potential of machine learning to be used for parts of\nthe pain diagnostic process and to be a decision support system for physicians\nand other health care personnel.\n","id":826}
{"Unnamed: 0.1":11827,"Unnamed: 0":11827.0,"anchor":"Identifying Best Interventions through Online Importance Sampling","positive":"  Motivated by applications in computational advertising and systems biology,\nwe consider the problem of identifying the best out of several possible soft\ninterventions at a source node $V$ in an acyclic causal directed graph, to\nmaximize the expected value of a target node $Y$ (located downstream of $V$).\nOur setting imposes a fixed total budget for sampling under various\ninterventions, along with cost constraints on different types of interventions.\nWe pose this as a best arm identification bandit problem with $K$ arms where\neach arm is a soft intervention at $V,$ and leverage the information leakage\namong the arms to provide the first gap dependent error and simple regret\nbounds for this problem. Our results are a significant improvement over the\ntraditional best arm identification results. We empirically show that our\nalgorithms outperform the state of the art in the Flow Cytometry data-set, and\nalso apply our algorithm for model interpretation of the Inception-v3 deep net\nthat classifies images.\n","negative":"  Low-variance gradient estimation is crucial for learning directed graphical\nmodels parameterized by neural networks, where the reparameterization trick is\nwidely used for those with continuous variables. While this technique gives\nlow-variance gradient estimates, it has not been directly applicable to\ndiscrete variables, the sampling of which inherently requires discontinuous\noperations. We argue that the discontinuity can be bypassed by marginalizing\nout the variable of interest, which results in a new reparameterization trick\nfor discrete variables. This reparameterization greatly reduces the variance,\nwhich is understood by regarding the method as an application of common random\nnumbers to the estimation. The resulting estimator is theoretically guaranteed\nto have a variance not larger than that of the likelihood-ratio method with the\noptimal input-dependent baseline. We give empirical results for variational\nlearning of sigmoid belief networks.\n","id":827}
{"Unnamed: 0.1":11828,"Unnamed: 0":11828.0,"anchor":"Similarity Function Tracking using Pairwise Comparisons","positive":"  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, and demonstrate\nparameter-free RICE-OCELAD metric learning on both synthetic data and a highly\nnonstationary Twitter dataset. We show significant performance improvements and\nincreased robustness to nonstationary effects relative to previously proposed\nbatch and online distance metric learning algorithms.\n","negative":"  We present a novel layerwise optimization algorithm for the learning\nobjective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large\nclass of convolutional neural networks. Specifically, PL-CNNs employ piecewise\nlinear non-linearities such as the commonly used ReLU and max-pool, and an SVM\nclassifier as the final layer. The key observation of our approach is that the\nproblem corresponding to the parameter estimation of a layer can be formulated\nas a difference-of-convex (DC) program, which happens to be a latent structured\nSVM. We optimize the DC program using the concave-convex procedure, which\nrequires us to iteratively solve a structured SVM problem. This allows to\ndesign an optimization algorithm with an optimal learning rate that does not\nrequire any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that\nour approach always improves over the state of the art variants of\nbackpropagation and scales to large data and large network settings.\n","id":828}
{"Unnamed: 0.1":11829,"Unnamed: 0":11829.0,"anchor":"Stochastic Generative Hashing","positive":"  Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.\n","negative":"  We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss.\n","id":829}
{"Unnamed: 0.1":11830,"Unnamed: 0":11830.0,"anchor":"The empirical Christoffel function with applications in data analysis","positive":"  We illustrate the potential applications in machine learning of the\nChristoffel function, or more precisely, its empirical counterpart associated\nwith a counting measure uniformly supported on a finite set of points. Firstly,\nwe provide a thresholding scheme which allows to approximate the support of a\nmeasure from a finite subset of its moments with strong asymptotic guaranties.\nSecondly, we provide a consistency result which relates the empirical\nChristoffel function and its population counterpart in the limit of large\nsamples. Finally, we illustrate the relevance of our results on simulated and\nreal world datasets for several applications in statistics and machine\nlearning: (a) density and support estimation from finite samples, (b) outlier\nand novelty detection and (c) affine matching.\n","negative":"  Due to limited metering infrastructure, distribution grids are currently\nchallenged by observability issues. On the other hand, smart meter data,\nincluding local voltage magnitudes and power injections, are communicated to\nthe utility operator from grid buses with renewable generation and\ndemand-response programs. This work employs grid data from metered buses\ntowards inferring the underlying grid state. To this end, a coupled formulation\nof the power flow problem (CPF) is put forth. Exploiting the high variability\nof injections at metered buses, the controllability of solar inverters, and the\nrelative time-invariance of conventional loads, the idea is to solve the\nnon-linear power flow equations jointly over consecutive time instants. An\nintuitive and easily verifiable rule pertaining to the locations of metered and\nnon-metered buses on the physical grid is shown to be a necessary and\nsufficient criterion for local observability in radial networks. To account for\nnoisy smart meter readings, a coupled power system state estimation (CPSSE)\nproblem is further developed. Both CPF and CPSSE tasks are tackled via\naugmented semi-definite program relaxations. The observability criterion along\nwith the CPF and CPSSE solvers are numerically corroborated using synthetic and\nactual solar generation and load data on the IEEE 34-bus benchmark feeder.\n","id":830}
{"Unnamed: 0.1":11831,"Unnamed: 0":11831.0,"anchor":"Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications","positive":"  This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives.\n","negative":"  The Intelligent Transportation System (ITS) targets to a coordinated traffic\nsystem by applying the advanced wireless communication technologies for road\ntraffic scheduling. Towards an accurate road traffic control, the short-term\ntraffic forecasting to predict the road traffic at the particular site in a\nshort period is often useful and important. In existing works, Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model is a popular approach.\nThe scheme however encounters two challenges: 1) the analysis on related data\nis insufficient whereas some important features of data may be neglected; and\n2) with data presenting different features, it is unlikely to have one\npredictive model that can fit all situations. To tackle above issues, in this\nwork, we develop a hybrid model to improve accuracy of SARIMA. In specific, we\nfirst explore the autocorrelation and distribution features existed in traffic\nflow to revise structure of the time series model. Based on the Gaussian\ndistribution of traffic flow, a hybrid model with a Bayesian learning algorithm\nis developed which can effectively expand the application scenarios of SARIMA.\nWe show the efficiency and accuracy of our proposal using both analysis and\nexperimental studies. Using the real-world trace data, we show that the\nproposed predicting approach can achieve satisfactory performance in practice.\n","id":831}
{"Unnamed: 0.1":11832,"Unnamed: 0":11832.0,"anchor":"Fast mixing for Latent Dirichlet allocation","positive":"  Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability\ntheory in general and in machine learning in particular. A Markov chain is\ndevised so that its stationary distribution is some probability distribution of\ninterest. Then one samples from the given distribution by running the Markov\nchain for a \"long time\" until it appears to be stationary and then collects the\nsample. However these chains are often very complex and there are no\ntheoretical guarantees that stationarity is actually reached. In this paper we\nstudy the Gibbs sampler of the posterior distribution of a very simple case of\nLatent Dirichlet Allocation, the arguably most well known Bayesian unsupervised\nlearning model for text generation and text classification. It is shown that\nwhen the corpus consists of two long documents of equal length $m$ and the\nvocabulary consists of only two different words, the mixing time is at most of\norder $m^2\\log m$ (which corresponds to $m\\log m$ rounds over the corpus). It\nwill be apparent from our analysis that it seems very likely that the mixing\ntime is not much worse in the more relevant case when the number of documents\nand the size of the vocabulary are also large as long as each word is\nrepresented a large number in each document, even though the computations\ninvolved may be intractable.\n","negative":"  DiffSharp is an algorithmic differentiation or automatic differentiation (AD)\nlibrary for the .NET ecosystem, which is targeted by the C# and F# languages,\namong others. The library has been designed with machine learning applications\nin mind, allowing very succinct implementations of models and optimization\nroutines. DiffSharp is implemented in F# and exposes forward and reverse AD\noperators as general nestable higher-order functions, usable by any .NET\nlanguage. It provides high-performance linear algebra primitives---scalars,\nvectors, and matrices, with a generalization to tensors underway---that are\nfully supported by all the AD operators, and which use a BLAS\/LAPACK backend\nvia the highly optimized OpenBLAS library. DiffSharp currently uses operator\noverloading, but we are developing a transformation-based version of the\nlibrary using F#'s \"code quotation\" metaprogramming facility. Work on a\nCUDA-based GPU backend is also underway.\n","id":832}
{"Unnamed: 0.1":11833,"Unnamed: 0":11833.0,"anchor":"Compressive Sensing via Convolutional Factor Analysis","positive":"  We solve the compressive sensing problem via convolutional factor analysis,\nwhere the convolutional dictionaries are learned {\\em in situ} from the\ncompressed measurements. An alternating direction method of multipliers (ADMM)\nparadigm for compressive sensing inversion based on convolutional factor\nanalysis is developed. The proposed algorithm provides reconstructed images as\nwell as features, which can be directly used for recognition ($e.g.$,\nclassification) tasks. When a deep (multilayer) model is constructed, a\nstochastic unpooling process is employed to build a generative model. During\nreconstruction and testing, we project the upper layer dictionary to the data\nlevel and only a single layer deconvolution is required. We demonstrate that\nusing $\\sim30\\%$ (relative to pixel numbers) compressed measurements, the\nproposed model achieves the classification accuracy comparable to the original\ndata on MNIST. We also observe that when the compressed measurements are very\nlimited ($e.g.$, $<10\\%$), the upper layer dictionary can provide better\nreconstruction results than the bottom layer.\n","negative":"  Generative adversarial networks have been proposed as a way of efficiently\ntraining deep generative neural networks. We propose a generative adversarial\nmodel that works on continuous sequential data, and apply it by training it on\na collection of classical music. We conclude that it generates music that\nsounds better and better as the model is trained, report statistics on\ngenerated music, and let the reader judge the quality by downloading the\ngenerated songs.\n","id":833}
{"Unnamed: 0.1":11834,"Unnamed: 0":11834.0,"anchor":"A General and Adaptive Robust Loss Function","positive":"  We present a generalization of the Cauchy\/Lorentzian, Geman-McClure,\nWelsch\/Leclerc, generalized Charbonnier, Charbonnier\/pseudo-Huber\/L1-L2, and L2\nloss functions. By introducing robustness as a continuous parameter, our loss\nfunction allows algorithms built around robust loss minimization to be\ngeneralized, which improves performance on basic vision tasks such as\nregistration and clustering. Interpreting our loss as the negative log of a\nunivariate density yields a general probability distribution that includes\nnormal and Cauchy distributions as special cases. This probabilistic\ninterpretation enables the training of neural networks in which the robustness\nof the loss automatically adapts itself during training, which improves\nperformance on learning-based tasks such as generative image synthesis and\nunsupervised monocular depth estimation, without requiring any manual parameter\ntuning.\n","negative":"  Several methods exist for a computer to generate music based on data\nincluding Markov chains, recurrent neural networks, recombinancy, and grammars.\nWe explore the use of unit selection and concatenation as a means of generating\nmusic using a procedure based on ranking, where, we consider a unit to be a\nvariable length number of measures of music. We first examine whether a unit\nselection method, that is restricted to a finite size unit library, can be\nsufficient for encompassing a wide spectrum of music. We do this by developing\na deep autoencoder that encodes a musical input and reconstructs the input by\nselecting from the library. We then describe a generative model that combines a\ndeep structured semantic model (DSSM) with an LSTM to predict the next unit,\nwhere units consist of four, two, and one measures of music. We evaluate the\ngenerative model using objective metrics including mean rank and accuracy and\nwith a subjective listening test in which expert musicians are asked to\ncomplete a forced-choiced ranking task. We compare our model to a note-level\ngenerative baseline that consists of a stacked LSTM trained to predict forward\nby one note.\n","id":834}
{"Unnamed: 0.1":11835,"Unnamed: 0":11835.0,"anchor":"Linear Disentangled Representation Learning for Facial Actions","positive":"  Limited annotated data available for the recognition of facial expression and\naction units embarrasses the training of deep networks, which can learn\ndisentangled invariant features. However, a linear model with just several\nparameters normally is not demanding in terms of training data. In this paper,\nwe propose an elegant linear model to untangle confounding factors in\nchallenging realistic multichannel signals such as 2D face videos. The simple\nyet powerful model does not rely on huge training data and is natural for\nrecognizing facial actions without explicitly disentangling the identity. Base\non well-understood intuitive linear models such as Sparse Representation based\nClassification (SRC), previous attempts require a prepossessing of explicit\ndecoupling which is practically inexact. Instead, we exploit the low-rank\nproperty across frames to subtract the underlying neutral faces which are\nmodeled jointly with sparse representation on the action components with group\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\nautomatic method on raw face videos performs as competitive as SRC applied on\nmanually prepared action components and performs even better than SRC in terms\nof true positive rate. We apply the model to the even more challenging task of\nfacial action unit recognition, verified on the MPI Face Video Database\n(MPI-VDB) achieving a decent performance. All the programs and data have been\nmade publicly available.\n","negative":"  In this paper, we establish a theoretical connection between the classical\nLucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer\nNetworks (STNs). STNs are of interest to the vision and learning communities\ndue to their natural ability to combine alignment and classification within the\nsame theoretical framework. Inspired by the Inverse Compositional (IC) variant\nof the LK algorithm, we present Inverse Compositional Spatial Transformer\nNetworks (IC-STNs). We demonstrate that IC-STNs can achieve better performance\nthan conventional STNs with less model capacity; in particular, we show\nsuperior performance in pure image alignment tasks as well as joint\nalignment\/classification problems on real-world problems.\n","id":835}
{"Unnamed: 0.1":11836,"Unnamed: 0":11836.0,"anchor":"Real-time eSports Match Result Prediction","positive":"  In this paper, we try to predict the winning team of a match in the\nmultiplayer eSports game Dota 2. To address the weaknesses of previous work, we\nconsider more aspects of prior (pre-match) features from individual players'\nmatch history, as well as real-time (during-match) features at each minute as\nthe match progresses. We use logistic regression, the proposed Attribute\nSequence Model, and their combinations as the prediction models. In a dataset\nof 78362 matches where 20631 matches contain replay data, our experiments show\nthat adding more aspects of prior features improves accuracy from 58.69% to\n71.49%, and introducing real-time features achieves up to 93.73% accuracy when\npredicting at the 40th minute.\n","negative":"  Exploration in multi-task reinforcement learning is critical in training\nagents to deduce the underlying MDP. Many of the existing exploration\nframeworks such as $E^3$, $R_{max}$, Thompson sampling assume a single\nstationary MDP and are not suitable for system identification in the multi-task\nsetting. We present a novel method to facilitate exploration in multi-task\nreinforcement learning using deep generative models. We supplement our method\nwith a low dimensional energy model to learn the underlying MDP distribution\nand provide a resilient and adaptive exploration signal to the agent. We\nevaluate our method on a new set of environments and provide intuitive\ninterpretation of our results.\n","id":836}
{"Unnamed: 0.1":11837,"Unnamed: 0":11837.0,"anchor":"Unsupervised Latent Behavior Manifold Learning from Acoustic Features:\n  audio2behavior","positive":"  Behavioral annotation using signal processing and machine learning is highly\ndependent on training data and manual annotations of behavioral labels.\nPrevious studies have shown that speech information encodes significant\nbehavioral information and be used in a variety of automated behavior\nrecognition tasks. However, extracting behavior information from speech is\nstill a difficult task due to the sparseness of training data coupled with the\ncomplex, high-dimensionality of speech, and the complex and multiple\ninformation streams it encodes. In this work we exploit the slow varying\nproperties of human behavior. We hypothesize that nearby segments of speech\nshare the same behavioral context and hence share a similar underlying\nrepresentation in a latent space. Specifically, we propose a Deep Neural\nNetwork (DNN) model to connect behavioral context and derive the behavioral\nmanifold in an unsupervised manner. We evaluate the proposed manifold in the\ncouples therapy domain and also provide examples from publicly available data\n(e.g. stand-up comedy). We further investigate training within the couples'\ntherapy domain and from movie data. The results are extremely encouraging and\npromise improved behavioral quantification in an unsupervised manner and\nwarrants further investigation in a range of applications.\n","negative":"  Automated protein function prediction is a challenging problem with\ndistinctive features, such as the hierarchical organization of protein\nfunctions and the scarcity of annotated proteins for most biological functions.\nWe propose a multitask learning algorithm addressing both issues. Unlike\nstandard multitask algorithms, which use task (protein functions) similarity\ninformation as a bias to speed up learning, we show that dissimilarity\ninformation enforces separation of rare class labels from frequent class\nlabels, and for this reason is better suited for solving unbalanced protein\nfunction prediction problems. We support our claim by showing that a multitask\nextension of the label propagation algorithm empirically works best when the\ntask relatedness information is represented using a dissimilarity matrix as\nopposed to a similarity matrix. Moreover, the experimental comparison carried\nout on three model organism shows that our method has a more stable performance\nin both \"protein-centric\" and \"function-centric\" evaluation settings.\n","id":837}
{"Unnamed: 0.1":11838,"Unnamed: 0":11838.0,"anchor":"Sparse-TDA: Sparse Realization of Topological Data Analysis for\n  Multi-Way Classification","positive":"  Topological data analysis (TDA) has emerged as one of the most promising\ntechniques to reconstruct the unknown shapes of high-dimensional spaces from\nobserved data samples. TDA, thus, yields key shape descriptors in the form of\npersistent topological features that can be used for any supervised or\nunsupervised learning task, including multi-way classification. Sparse\nsampling, on the other hand, provides a highly efficient technique to\nreconstruct signals in the spatial-temporal domain from just a few\ncarefully-chosen samples. Here, we present a new method, referred to as the\nSparse-TDA algorithm, that combines favorable aspects of the two techniques.\nThis combination is realized by selecting an optimal set of sparse pixel\nsamples from the persistent features generated by a vector-based TDA algorithm.\nThese sparse samples are selected from a low-rank matrix representation of\npersistent features using QR pivoting. We show that the Sparse-TDA method\ndemonstrates promising performance on three benchmark problems related to human\nposture recognition and image texture classification.\n","negative":"  I aim to show that models, classification or generating functions,\ninvariances and datasets are algorithmically equivalent concepts once properly\ndefined, and provide some concrete examples of them. I then show that a) neural\nnetworks (NNs) of different kinds can be seen to implement models, b) that\nperturbations of inputs and nodes in NNs trained to optimally implement simple\nmodels propagate strongly, c) that there is a framework in which recurrent,\ndeep and shallow networks can be seen to fall into a descriptive power\nhierarchy in agreement with notions from the theory of recursive functions. The\nmotivation for these definitions and following analysis lies in the context of\ncognitive neuroscience, and in particular in Ruffini (2016), where the concept\nof model is used extensively, as is the concept of algorithmic complexity.\n","id":838}
{"Unnamed: 0.1":11839,"Unnamed: 0":11839.0,"anchor":"Prior matters: simple and general methods for evaluating and improving\n  topic quality in topic modeling","positive":"  Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.\n","negative":"  ENIGMA is a learning-based method for guiding given clause selection in\nsaturation-based theorem provers. Clauses from many proof searches are\nclassified as positive and negative based on their participation in the proofs.\nAn efficient classification model is trained on this data, using fast\nfeature-based characterization of the clauses . The learned model is then\ntightly linked with the core prover and used as a basis of a new parameterized\nevaluation heuristic that provides fast ranking of all generated clauses. The\napproach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing\na large increase of E's performance.\n","id":839}
{"Unnamed: 0.1":11840,"Unnamed: 0":11840.0,"anchor":"Modularized Morphing of Neural Networks","positive":"  In this work we study the problem of network morphism, an effective learning\nscheme to morph a well-trained neural network to a new one with the network\nfunction completely preserved. Different from existing work where basic\nmorphing types on the layer level were addressed, we target at the central\nproblem of network morphism at a higher level, i.e., how a convolutional layer\ncan be morphed into an arbitrary module of a neural network. To simplify the\nrepresentation of a network, we abstract a module as a graph with blobs as\nvertices and convolutional layers as edges, based on which the morphing process\nis able to be formulated as a graph transformation problem. Two atomic morphing\noperations are introduced to compose the graphs, based on which modules are\nclassified into two families, i.e., simple morphable modules and complex\nmodules. We present practical morphing solutions for both of these two\nfamilies, and prove that any reasonable module can be morphed from a single\nconvolutional layer. Extensive experiments have been conducted based on the\nstate-of-the-art ResNet on benchmark datasets, and the effectiveness of the\nproposed solution has been verified.\n","negative":"  We describe a method for searching the optimal hyper-parameters in reservoir\ncomputing, which consists of a Gaussian process with Bayesian optimization. It\nprovides an alternative to other frequently used optimization methods such as\ngrid, random, or manual search. In addition to a set of optimal\nhyper-parameters, the method also provides a probability distribution of the\ncost function as a function of the hyper-parameters. We apply this method to\ntwo types of reservoirs: nonlinear delay nodes and echo state networks. It\nshows excellent performance on all considered benchmarks, either matching or\nsignificantly surpassing results found in the literature. In general, the\nalgorithm achieves optimal results in fewer iterations when compared to other\noptimization methods. We have optimized up to six hyper-parameters\nsimultaneously, which would have been infeasible using, e.g., grid search. Due\nto its automated nature, this method significantly reduces the need for expert\nknowledge when optimizing the hyper-parameters in reservoir computing. Existing\nsoftware libraries for Bayesian optimization, such as Spearmint, make the\nimplementation of the algorithm straightforward. A fork of the Spearmint\nframework along with a tutorial on how to use it in practice is available at\nhttps:\/\/bitbucket.org\/uhasseltmachinelearning\/spearmint\/\n","id":840}
{"Unnamed: 0.1":11841,"Unnamed: 0":11841.0,"anchor":"Residual LSTM: Design of a Deep Recurrent Architecture for Distant\n  Speech Recognition","positive":"  In this paper, a novel architecture for a deep recurrent neural network,\nresidual LSTM is introduced. A plain LSTM has an internal memory cell that can\nlearn long term dependencies of sequential data. It also provides a temporal\nshortcut path to avoid vanishing or exploding gradients in the temporal domain.\nThe residual LSTM provides an additional spatial shortcut path from lower\nlayers for efficient training of deep networks with multiple LSTM layers.\nCompared with the previous work, highway LSTM, residual LSTM separates a\nspatial shortcut path with temporal one by using output layers, which can help\nto avoid a conflict between spatial and temporal-domain gradient flows.\nFurthermore, residual LSTM reuses the output projection matrix and the output\ngate of LSTM to control the spatial information flow instead of additional gate\nnetworks, which effectively reduces more than 10% of network parameters. An\nexperiment for distant speech recognition on the AMI SDM corpus shows that\n10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in\nWER over 3-layer aselines, respectively. On the contrary, 10-layer residual\nLSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%\nWER reduction over plain and highway LSTM networks, respectively.\n","negative":"  Existing approaches to online convex optimization (OCO) make sequential\none-slot-ahead decisions, which lead to (possibly adversarial) losses that\ndrive subsequent decision iterates. Their performance is evaluated by the\nso-called regret that measures the difference of losses between the online\nsolution and the best yet fixed overall solution in hindsight. The present\npaper deals with online convex optimization involving adversarial loss\nfunctions and adversarial constraints, where the constraints are revealed after\nmaking decisions, and can be tolerable to instantaneous violations but must be\nsatisfied in the long term. Performance of an online algorithm in this setting\nis assessed by: i) the difference of its losses relative to the best dynamic\nsolution with one-slot-ahead information of the loss function and the\nconstraint (that is here termed dynamic regret); and, ii) the accumulated\namount of constraint violations (that is here termed dynamic fit). In this\ncontext, a modified online saddle-point (MOSP) scheme is developed, and proved\nto simultaneously yield sub-linear dynamic regret and fit, provided that the\naccumulated variations of per-slot minimizers and constraints are sub-linearly\ngrowing with time. MOSP is also applied to the dynamic network resource\nallocation task, and it is compared with the well-known stochastic dual\ngradient method. Under various scenarios, numerical experiments demonstrate the\nperformance gain of MOSP relative to the state-of-the-art.\n","id":841}
{"Unnamed: 0.1":11842,"Unnamed: 0":11842.0,"anchor":"Scaling Binarized Neural Networks on Reconfigurable Logic","positive":"  Binarized neural networks (BNNs) are gaining interest in the deep learning\ncommunity due to their significantly lower computational and memory cost. They\nare particularly well suited to reconfigurable logic devices, which contain an\nabundance of fine-grained compute resources and can result in smaller, lower\npower implementations, or conversely in higher classification rates. Towards\nthis end, the Finn framework was recently proposed for building fast and\nflexible field programmable gate array (FPGA) accelerators for BNNs. Finn\nutilized a novel set of optimizations that enable efficient mapping of BNNs to\nhardware and implemented fully connected, non-padded convolutional and pooling\nlayers, with per-layer compute resources being tailored to user-provided\nthroughput requirements. However, FINN was not evaluated on larger topologies\ndue to the size of the chosen FPGA, and exhibited decreased accuracy due to\nlack of padding. In this paper, we improve upon Finn to show how padding can be\nemployed on BNNs while still maintaining a 1-bit datapath and high accuracy.\nBased on this technique, we demonstrate numerous experiments to illustrate\nflexibility and scalability of the approach. In particular, we show that a\nlarge BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5\nplatform can classify images at 12 kFPS with 671 us latency while drawing less\nthan 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our\nimplementation of this network achieves 14.8 trillion operations per second. We\nbelieve this is the fastest classification rate reported to date on this\nbenchmark at this level of accuracy.\n","negative":"  In recent years, the research community has discovered that deep neural\nnetworks (DNNs) and convolutional neural networks (CNNs) can yield higher\naccuracy than all previous solutions to a broad array of machine learning\nproblems. To our knowledge, there is no single CNN\/DNN architecture that solves\nall problems optimally. Instead, the \"right\" CNN\/DNN architecture varies\ndepending on the application at hand. CNN\/DNNs comprise an enormous design\nspace. Quantitatively, we find that a small region of the CNN design space\ncontains 30 billion different CNN architectures.\n  In this dissertation, we develop a methodology that enables systematic\nexploration of the design space of CNNs. Our methodology is comprised of the\nfollowing four themes.\n  1. Judiciously choosing benchmarks and metrics.\n  2. Rapidly training CNN models.\n  3. Defining and describing the CNN design space.\n  4. Exploring the design space of CNN architectures.\n  Taken together, these four themes comprise an effective methodology for\ndiscovering the \"right\" CNN architectures to meet the needs of practical\napplications.\n","id":842}
{"Unnamed: 0.1":11843,"Unnamed: 0":11843.0,"anchor":"Manifold Alignment Determination: finding correspondences across\n  different data views","positive":"  We present Manifold Alignment Determination (MAD), an algorithm for learning\nalignments between data points from multiple views or modalities. The approach\nis capable of learning correspondences between views as well as correspondences\nbetween individual data-points. The proposed method requires only a few aligned\nexamples from which it is capable to recover a global alignment through a\nprobabilistic model. The strong, yet flexible regularization provided by the\ngenerative model is sufficient to align the views. We provide experiments on\nboth synthetic and real data to highlight the benefit of the proposed approach.\n","negative":"  In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature.\n","id":843}
{"Unnamed: 0.1":11844,"Unnamed: 0":11844.0,"anchor":"An Asynchronous Parallel Approach to Sparse Recovery","positive":"  Asynchronous parallel computing and sparse recovery are two areas that have\nreceived recent interest. Asynchronous algorithms are often studied to solve\noptimization problems where the cost function takes the form $\\sum_{i=1}^M\nf_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each\n$f_i$ acts only on a small number of components of $x\\in\\mathbb{R}^n$. Sparse\nrecovery problems, such as compressed sensing, can be formulated as\noptimization problems, however, the cost functions $f_i$ are dense with respect\nto the components of $x$, and instead the signal $x$ is assumed to be sparse,\nmeaning that it has only $s$ non-zeros where $s\\ll n$. Here we address how one\nmay use an asynchronous parallel architecture when the cost functions $f_i$ are\nnot sparse in $x$, but rather the signal $x$ is sparse. We propose an\nasynchronous parallel approach to sparse recovery via a stochastic greedy\nalgorithm, where multiple processors asynchronously update a vector in shared\nmemory containing information on the estimated signal support. We include\nnumerical simulations that illustrate the potential benefits of our proposed\nasynchronous method.\n","negative":"  Contextual bandit algorithms -- a class of multi-armed bandit algorithms that\nexploit the contextual information -- have been shown to be effective in\nsolving sequential decision making problems under uncertainty. A common\nassumption adopted in the literature is that the realized (ground truth) reward\nby taking the selected action is observed by the learner at no cost, which,\nhowever, is not realistic in many practical scenarios. When observing the\nground truth reward is costly, a key challenge for the learner is how to\njudiciously acquire the ground truth by assessing the benefits and costs in\norder to balance learning efficiency and learning cost. From the information\ntheoretic perspective, a perhaps even more interesting question is how much\nefficiency might be lost due to this cost. In this paper, we design a novel\ncontextual bandit-based learning algorithm and endow it with the active\nlearning capability. The key feature of our algorithm is that in addition to\nsending a query to an annotator for the ground truth, prior information about\nthe ground truth learned by the learner is sent together, thereby reducing the\nquery cost. We prove that by carefully choosing the algorithm parameters, the\nlearning regret of the proposed algorithm achieves the same order as that of\nconventional contextual bandit algorithms in cost-free scenarios, implying\nthat, surprisingly, cost due to acquiring the ground truth does not increase\nthe learning regret in the long-run. Our analysis shows that prior information\nabout the ground truth plays a critical role in improving the system\nperformance in scenarios where active learning is necessary.\n","id":844}
{"Unnamed: 0.1":11845,"Unnamed: 0":11845.0,"anchor":"Perishability of Data: Dynamic Pricing under Varying-Coefficient Models","positive":"  We consider a firm that sells a large number of products to its customers in\nan online fashion. Each product is described by a high dimensional feature\nvector, and the market value of a product is assumed to be linear in the values\nof its features. Parameters of the valuation model are unknown and can change\nover time. The firm sequentially observes a product's features and can use the\nhistorical sales data (binary sale\/no sale feedbacks) to set the price of\ncurrent product, with the objective of maximizing the collected revenue. We\nmeasure the performance of a dynamic pricing policy via regret, which is the\nexpected revenue loss compared to a clairvoyant that knows the sequence of\nmodel parameters in advance.\n  We propose a pricing policy based on projected stochastic gradient descent\n(PSGD) and characterize its regret in terms of time $T$, features dimension\n$d$, and the temporal variability in the model parameters, $\\delta_t$. We\nconsider two settings. In the first one, feature vectors are chosen\nantagonistically by nature and we prove that the regret of PSGD pricing policy\nis of order $O(\\sqrt{T} + \\sum_{t=1}^T \\sqrt{t}\\delta_t)$. In the second\nsetting (referred to as stochastic features model), the feature vectors are\ndrawn independently from an unknown distribution. We show that in this case,\nthe regret of PSGD pricing policy is of order $O(d^2 \\log T + \\sum_{t=1}^T\nt\\delta_t\/d)$.\n","negative":"  Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth.\n","id":845}
{"Unnamed: 0.1":11846,"Unnamed: 0":11846.0,"anchor":"Kernel Approximation Methods for Speech Recognition","positive":"  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word\/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n","negative":"  Many biological processes are governed by protein-ligand interactions. One\nsuch example is the recognition of self and nonself cells by the immune system.\nThis immune response process is regulated by the major histocompatibility\ncomplex (MHC) protein which is encoded by the human leukocyte antigen (HLA)\ncomplex. Understanding the binding potential between MHC and peptides can lead\nto the design of more potent, peptide-based vaccines and immunotherapies for\ninfectious autoimmune diseases.\n  We apply machine learning techniques from the natural language processing\n(NLP) domain to address the task of MHC-peptide binding prediction. More\nspecifically, we introduce a new distributed representation of amino acids,\nname HLA-Vec, that can be used for a variety of downstream proteomic machine\nlearning tasks. We then propose a deep convolutional neural network\narchitecture, name HLA-CNN, for the task of HLA class I-peptide binding\nprediction. Experimental results show combining the new distributed\nrepresentation with our HLA-CNN architecture achieves state-of-the-art results\nin the majority of the latest two Immune Epitope Database (IEDB) weekly\nautomated benchmark datasets. We further apply our model to predict binding on\nthe human genome and identify 15 genes with potential for self binding.\n","id":846}
{"Unnamed: 0.1":11847,"Unnamed: 0":11847.0,"anchor":"Diffusion-based nonlinear filtering for multimodal data fusion with\n  application to sleep stage assessment","positive":"  The problem of information fusion from multiple data-sets acquired by\nmultimodal sensors has drawn significant research attention over the years. In\nthis paper, we focus on a particular problem setting consisting of a physical\nphenomenon or a system of interest observed by multiple sensors. We assume that\nall sensors measure some aspects of the system of interest with additional\nsensor-specific and irrelevant components. Our goal is to recover the variables\nrelevant to the observed system and to filter out the nuisance effects of the\nsensor-specific variables. We propose an approach based on manifold learning,\nwhich is particularly suitable for problems with multiple modalities, since it\naims to capture the intrinsic structure of the data and relies on minimal prior\nmodel knowledge. Specifically, we propose a nonlinear filtering scheme, which\nextracts the hidden sources of variability captured by two or more sensors,\nthat are independent of the sensor-specific components. In addition to\npresenting a theoretical analysis, we demonstrate our technique on real\nmeasured data for the purpose of sleep stage assessment based on multiple,\nmultimodal sensor measurements. We show that without prior knowledge on the\ndifferent modalities and on the measured system, our method gives rise to a\ndata-driven representation that is well correlated with the underlying sleep\nprocess and is robust to noise and sensor-specific effects.\n","negative":"  Deep Reinforcement Learning has enabled the learning of policies for complex\ntasks in partially observable environments, without explicitly learning the\nunderlying model of the tasks. While such model-free methods achieve\nconsiderable performance, they often ignore the structure of task. We present a\nnatural representation of to Reinforcement Learning (RL) problems using\nRecurrent Convolutional Neural Networks (RCNNs), to better exploit this\ninherent structure. We define 3 such RCNNs, whose forward passes execute an\nefficient Value Iteration, propagate beliefs of state in partially observable\nenvironments, and choose optimal actions respectively. Backpropagating\ngradients through these RCNNs allows the system to explicitly learn the\nTransition Model and Reward Function associated with the underlying MDP,\nserving as an elegant alternative to classical model-based RL. We evaluate the\nproposed algorithms in simulation, considering a robot planning problem. We\ndemonstrate the capability of our framework to reduce the cost of replanning,\nlearn accurate MDP models, and finally re-plan with learnt models to achieve\nnear-optimal policies.\n","id":847}
{"Unnamed: 0.1":11848,"Unnamed: 0":11848.0,"anchor":"A dissimilarity-based approach to predictive maintenance with\n  application to HVAC systems","positive":"  The goal of predictive maintenance is to forecast the occurrence of faults of\nan appliance, in order to proactively take the necessary actions to ensure its\navailability. In many application scenarios, predictive maintenance is applied\nto a set of homogeneous appliances. In this paper, we firstly review taxonomies\nand main methodologies currently used for condition-based maintenance;\nsecondly, we argue that the mutual dissimilarities of the behaviours of all\nappliances of this set (the \"cohort\") can be exploited to detect upcoming\nfaults. Specifically, inspired by dissimilarity-based representations, we\npropose a novel machine learning approach based on the analysis of concurrent\nmutual differences of the measurements coming from the cohort. We evaluate our\nmethod over one year of historical data from a cohort of 17 HVAC (Heating,\nVentilation and Air Conditioning) systems installed in an Italian hospital. We\nshow that certain kinds of faults can be foreseen with an accuracy, measured in\nterms of area under the ROC curve, as high as 0.96.\n","negative":"  Recurrent neural networks (RNNs) have been used extensively and with\nincreasing success to model various types of sequential data. Much of this\nprogress has been achieved through devising recurrent units and architectures\nwith the flexibility to capture complex statistics in the data, such as long\nrange dependency or localized attention phenomena. However, while many\nsequential data (such as video, speech or language) can have highly variable\ninformation flow, most recurrent models still consume input features at a\nconstant rate and perform a constant number of computations per time step,\nwhich can be detrimental to both speed and model capacity. In this paper, we\nexplore a modification to existing recurrent units which allows them to learn\nto vary the amount of computation they perform at each step, without prior\nknowledge of the sequence's time structure. We show experimentally that not\nonly do our models require fewer operations, they also lead to better\nperformance overall on evaluation tasks.\n","id":848}
{"Unnamed: 0.1":11849,"Unnamed: 0":11849.0,"anchor":"Symbolic Regression Algorithms with Built-in Linear Regression","positive":"  Recently, several algorithms for symbolic regression (SR) emerged which\nemploy a form of multiple linear regression (LR) to produce generalized linear\nmodels. The use of LR allows the algorithms to create models with relatively\nsmall error right from the beginning of the search; such algorithms are thus\nclaimed to be (sometimes by orders of magnitude) faster than SR algorithms\nbased on vanilla genetic programming. However, a systematic comparison of these\nalgorithms on a common set of problems is still missing. In this paper we\nconceptually and experimentally compare several representatives of such\nalgorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf,\nready-to-use techniques, mostly using their default settings. The methods are\ncompared on several synthetic and real-world SR benchmark problems. Their\nperformance is also related to the performance of three conventional machine\nlearning algorithms --- multiple regression, random forests and support vector\nregression.\n","negative":"  Humans can learn concepts or recognize items from just a handful of examples,\nwhile machines require many more samples to perform the same task. In this\npaper, we build a computational model to investigate the possibility of this\nkind of rapid learning. The proposed method aims to improve the learning task\nof input from sensory memory by leveraging the information retrieved from\nlong-term memory. We present a simple and intuitive technique called cognitive\ndiscriminative mappings (CDM) to explore the cognitive problem. First, CDM\nseparates and clusters the data instances retrieved from long-term memory into\ndistinct classes with a discrimination method in working memory when a sensory\ninput triggers the algorithm. CDM then maps each sensory data instance to be as\nclose as possible to the median point of the data group with the same class.\nThe experimental results demonstrate that the CDM approach is effective for\nlearning the discriminative features of supervised classifications with few\ntraining sensory input instances.\n","id":849}
{"Unnamed: 0.1":11850,"Unnamed: 0":11850.0,"anchor":"Restricted Boltzmann Machines with Gaussian Visible Units Guided by\n  Pairwise Constraints","positive":"  Restricted Boltzmann machines (RBMs) and their variants are usually trained\nby contrastive divergence (CD) learning, but the training procedure is an\nunsupervised learning approach, without any guidances of the background\nknowledge. To enhance the expression ability of traditional RBMs, in this\npaper, we propose pairwise constraints restricted Boltzmann machine with\nGaussian visible units (pcGRBM) model, in which the learning procedure is\nguided by pairwise constraints and the process of encoding is conducted under\nthese guidances. The pairwise constraints are encoded in hidden layer features\nof pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and\nanother part of them are separated by the guidances. In order to deal with\nreal-valued data, the binary visible units are replaced by linear units with\nGausian noise in the pcGRBM model. In the learning process of pcGRBM, the\npairwise constraints are iterated transitions between visible and hidden units\nduring CD learning procedure. Then, the proposed model is inferred by\napproximative gradient descent method and the corresponding learning algorithm\nis designed in this paper. In order to compare the availability of pcGRBM and\ntraditional RBMs with Gaussian visible units, the features of the pcGRBM and\nRBMs hidden layer are used as input 'data' for K-means, spectral clustering\n(SP) and affinity propagation (AP) algorithms, respectively. A thorough\nexperimental evaluation is performed with sixteen image datasets of Microsoft\nResearch Asia Multimedia (MSRA-MM). The experimental results show that the\nclustering performance of K-means, SP and AP algorithms based on pcGRBM model\nare significantly better than traditional RBMs. In addition, the pcGRBM model\nfor clustering task shows better performance than some semi-supervised\nclustering algorithms.\n","negative":"  Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.\n","id":850}
{"Unnamed: 0.1":11851,"Unnamed: 0":11851.0,"anchor":"Dictionary Learning from Incomplete Data","positive":"  This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete\/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart.\n","negative":"  It has been shown that the adjacency eigenspace of a network contains key\ninformation of its underlying structure. However, there has been no study on\nspectral analysis of the adjacency matrices of directed signed graphs. In this\npaper, we derive theoretical approximations of spectral projections from such\ndirected signed networks using matrix perturbation theory. We use the derived\ntheoretical results to study the influences of negative intra cluster and inter\ncluster directed edges on node spectral projections. We then develop a spectral\nclustering based graph partition algorithm, SC-DSG, and conduct evaluations on\nboth synthetic and real datasets. Both theoretical analysis and empirical\nevaluation demonstrate the effectiveness of the proposed algorithm.\n","id":851}
{"Unnamed: 0.1":11852,"Unnamed: 0":11852.0,"anchor":"Truncation-free Hybrid Inference for DPMM","positive":"  Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian\nnon-parametrics. While these models free from choosing the number of components\na-priori, computationally attractive variational inference often reintroduces\nthe need to do so, via a truncation on the variational distribution. In this\npaper we present a truncation-free hybrid inference for DPMM, combining the\nadvantages of sampling-based MCMC and variational methods. The proposed\nhybridization enables more efficient variational updates, while increasing\nmodel complexity only if needed. We evaluate the properties of the hybrid\nupdates and their empirical performance in single- as well as mixed-membership\nmodels. Our method is easy to implement and performs favorably compared to\nexisting schemas.\n","negative":"  The Nystrom method has been popular for generating the low-rank approximation\nof kernel matrices that arise in many machine learning problems. The\napproximation quality of the Nystrom method depends crucially on the number of\nselected landmark points and the selection procedure. In this paper, we present\na novel algorithm to compute the optimal Nystrom low-approximation when the\nnumber of landmark points exceed the target rank. Moreover, we introduce a\nrandomized algorithm for generating landmark points that is scalable to\nlarge-scale data sets. The proposed method performs K-means clustering on\nlow-dimensional random projections of a data set and, thus, leads to\nsignificant savings for high-dimensional data sets. Our theoretical results\ncharacterize the tradeoffs between the accuracy and efficiency of our proposed\nmethod. Extensive experiments demonstrate the competitive performance as well\nas the efficiency of our proposed method.\n","id":852}
{"Unnamed: 0.1":11853,"Unnamed: 0":11853.0,"anchor":"Deep Probabilistic Programming","positive":"  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n","negative":"  Clustering is a fundamental problem in statistics and machine learning.\nLloyd's algorithm, proposed in 1957, is still possibly the most widely used\nclustering algorithm in practice due to its simplicity and empirical\nperformance. However, there has been little theoretical investigation on the\nstatistical and computational guarantees of Lloyd's algorithm. This paper is an\nattempt to bridge this gap between practice and theory. We investigate the\nperformance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an\nappropriate initialization for labels or centers, we show that Lloyd's\nalgorithm converges to an exponentially small clustering error after an order\nof $\\log n$ iterations, where $n$ is the sample size. The error rate is shown\nto be minimax optimal. For the two-mixture case, we only require the\ninitializer to be slightly better than random guess.\n  In addition, we extend the Lloyd's algorithm and its analysis to community\ndetection and crowdsourcing, two problems that have received a lot of attention\nrecently in statistics and machine learning. Two variants of Lloyd's algorithm\nare proposed respectively for community detection and crowdsourcing. On the\ntheoretical side, we provide statistical and computational guarantees of the\ntwo algorithms, and the results improve upon some previous signal-to-noise\nratio conditions in literature for both problems. Experimental results on\nsimulated and real data sets demonstrate competitive performance of our\nalgorithms to the state-of-the-art methods.\n","id":853}
{"Unnamed: 0.1":11854,"Unnamed: 0":11854.0,"anchor":"Long Timescale Credit Assignment in NeuralNetworks with External Memory","positive":"  Credit assignment in traditional recurrent neural networks usually involves\nback-propagating through a long chain of tied weight matrices. The length of\nthis chain scales linearly with the number of time-steps as the same network is\nrun at each time-step. This creates many problems, such as vanishing gradients,\nthat have been well studied. In contrast, a NNEM's architecture recurrent\nactivity doesn't involve a long chain of activity (though some architectures\nsuch as the NTM do utilize a traditional recurrent architecture as a\ncontroller). Rather, the externally stored embedding vectors are used at each\ntime-step, but no messages are passed from previous time-steps. This means that\nvanishing gradients aren't a problem, as all of the necessary gradient paths\nare short. However, these paths are extremely numerous (one per embedding\nvector in memory) and reused for a very long time (until it leaves the memory).\nThus, the forward-pass information of each memory must be stored for the entire\nduration of the memory. This is problematic as this additional storage far\nsurpasses that of the actual memories, to the extent that large memories on\ninfeasible to back-propagate through in high dimensional settings. One way to\nget around the need to hold onto forward-pass information is to recalculate the\nforward-pass whenever gradient information is available. However, if the\nobservations are too large to store in the domain of interest, direct\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\nautoencoder to reinstate the observation, and then use the embedding network to\nrecalculate the forward-pass. Since the recalculated embedding vector is\nunlikely to perfectly match the one stored in memory, we try out 2\napproximations to utilize error gradient w.r.t. the vector in memory.\n","negative":"  We present QuickNet, a fast and accurate network architecture that is both\nfaster and significantly more accurate than other fast deep architectures like\nSqueezeNet. Furthermore, it uses less parameters than previous networks, making\nit more memory efficient. We do this by making two major modifications to the\nreference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable\nconvolutions and 2) The use of parametric rectified linear units. We make the\nobservation that parametric rectified linear units are computationally\nequivalent to leaky rectified linear units at test time and the observation\nthat separable convolutions can be interpreted as a compressed Inception\nnetwork (Chollet, 2016). Using these observations, we derive a network\narchitecture, which we call QuickNet, that is both faster and more accurate\nthan previous models. Our architecture provides at least four major advantages:\n(1) A smaller model size, which is more tenable on memory constrained systems;\n(2) A significantly faster network which is more tenable on computationally\nconstrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10\nDataset which outperforms all but one result published so far, although we note\nthat our works are orthogonal approaches and can be combined (4) Orthogonality\nto previous model compression approaches allowing for further speed gains to be\nrealized.\n","id":854}
{"Unnamed: 0.1":11855,"Unnamed: 0":11855.0,"anchor":"Learning to Invert: Signal Recovery via Deep Convolutional Networks","positive":"  The promise of compressive sensing (CS) has been offset by two significant\nchallenges. First, real-world data is not exactly sparse in a fixed basis.\nSecond, current high-performance recovery algorithms are slow to converge,\nwhich limits CS to either non-real-time applications or scenarios where massive\nback-end computing is available. In this paper, we attack both of these\nchallenges head-on by developing a new signal recovery framework we call {\\em\nDeepInverse} that learns the inverse transformation from measurement vectors to\nsignals using a {\\em deep convolutional network}. When trained on a set of\nrepresentative images, the network learns both a representation for the signals\n(addressing challenge one) and an inverse map approximating a greedy or convex\nrecovery algorithm (addressing challenge two). Our experiments indicate that\nthe DeepInverse network closely approximates the solution produced by\nstate-of-the-art CS recovery algorithms yet is hundreds of times faster in run\ntime. The tradeoff for the ultrafast run time is a computationally intensive,\noff-line training procedure typical to deep networks. However, the training\nneeds to be completed only once, which makes the approach attractive for a host\nof sparse recovery problems.\n","negative":"  Recent advances have shown the capability of Fully Convolutional Neural\nNetworks (FCN) to model cost functions for motion planning in the context of\nlearning driving preferences purely based on demonstration data from human\ndrivers. While pure learning from demonstrations in the framework of Inverse\nReinforcement Learning (IRL) is a promising approach, we can benefit from well\ninformed human priors and incorporate them into the learning process. Our work\nachieves this by pretraining a model to regress to a manual cost function and\nrefining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When\ninjecting prior knowledge as pretraining for the network, we achieve higher\nrobustness, more visually distinct obstacle boundaries, and the ability to\ncapture instances of obstacles that elude models that purely learn from\ndemonstration data. Furthermore, by exploiting these human priors, the\nresulting model can more accurately handle corner cases that are scarcely seen\nin the demonstration data, such as stairs, slopes, and underpasses.\n","id":855}
{"Unnamed: 0.1":11856,"Unnamed: 0":11856.0,"anchor":"On H\\\"older projective divergences","positive":"  We describe a framework to build distances by measuring the tightness of\ninequalities, and introduce the notion of proper statistical divergences and\nimproper pseudo-divergences. We then consider the H\\\"older ordinary and reverse\ninequalities, and present two novel classes of H\\\"older divergences and\npseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz\ndivergence. We report closed-form formulas for those statistical\ndissimilarities when considering distributions belonging to the same\nexponential family provided that the natural parameter space is a cone (e.g.,\nmultivariate Gaussians), or affine (e.g., categorical distributions). Those new\nclasses of H\\\"older distances are invariant to rescaling, and thus do not\nrequire distributions to be normalized. Finally, we show how to compute\nstatistical H\\\"older centroids with respect to those divergences, and carry out\ncenter-based clustering toy experiments on a set of Gaussian distributions that\ndemonstrate empirically that symmetrized H\\\"older divergences outperform the\nsymmetric Cauchy-Schwarz divergence.\n","negative":"  An agglomerative clustering of random variables is proposed, where clusters\nof random variables sharing the maximum amount of multivariate mutual\ninformation are merged successively to form larger clusters. Compared to the\nprevious info-clustering algorithms, the agglomerative approach allows the\ncomputation to stop earlier when clusters of desired size and accuracy are\nobtained. An efficient algorithm is also derived based on the submodularity of\nentropy and the duality between the principal sequence of partitions and the\nprincipal sequence for submodular functions.\n","id":856}
{"Unnamed: 0.1":11857,"Unnamed: 0":11857.0,"anchor":"Marked Temporal Dynamics Modeling based on Recurrent Neural Network","positive":"  We are now witnessing the increasing availability of event stream data, i.e.,\na sequence of events with each event typically being denoted by the time it\noccurs and its mark information (e.g., event type). A fundamental problem is to\nmodel and predict such kind of marked temporal dynamics, i.e., when the next\nevent will take place and what its mark will be. Existing methods either\npredict only the mark or the time of the next event, or predict both of them,\nyet separately. Indeed, in marked temporal dynamics, the time and the mark of\nthe next event are highly dependent on each other, requiring a method that\ncould simultaneously predict both of them. To tackle this problem, in this\npaper, we propose to model marked temporal dynamics by using a mark-specific\nintensity function to explicitly capture the dependency between the mark and\nthe time of the next event. Extensive experiments on two datasets demonstrate\nthat the proposed method outperforms state-of-the-art methods at predicting\nmarked temporal dynamics.\n","negative":"  Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images.\n","id":857}
{"Unnamed: 0.1":11858,"Unnamed: 0":11858.0,"anchor":"Scalable and Incremental Learning of Gaussian Mixture Models","positive":"  This work presents a fast and scalable algorithm for incremental learning of\nGaussian mixture models. By performing rank-one updates on its precision\nmatrices and determinants, its asymptotic time complexity is of \\BigO{NKD^2}\nfor $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting\nalgorithm can be applied to high dimensional tasks, and this is confirmed by\napplying it to the classification datasets MNIST and CIFAR-10. Additionally, in\norder to show the algorithm's applicability to function approximation and\ncontrol tasks, it is applied to three reinforcement learning tasks and its\ndata-efficiency is evaluated.\n","negative":"  One of the main challenges of deep learning methods is the choice of an\nappropriate training strategy. In particular, additional steps, such as\nunsupervised pre-training, have been shown to greatly improve the performances\nof deep structures. In this article, we propose an extra training step, called\npost-training, which only optimizes the last layer of the network. We show that\nthis procedure can be analyzed in the context of kernel theory, with the first\nlayers computing an embedding of the data and the last layer a statistical\nmodel to solve the task based on this embedding. This step makes sure that the\nembedding, or representation, of the data is used in the best possible way for\nthe considered task. This idea is then tested on multiple architectures with\nvarious data sets, showing that it consistently provides a boost in\nperformance.\n","id":858}
{"Unnamed: 0.1":11859,"Unnamed: 0":11859.0,"anchor":"Communication-Efficient Algorithms for Decentralized and Stochastic\n  Optimization","positive":"  We present a new class of decentralized first-order methods for nonsmooth and\nstochastic optimization problems defined over multiagent networks. Considering\nthat communication is a major bottleneck in decentralized optimization, our\nmain goal in this paper is to develop algorithmic frameworks which can\nsignificantly reduce the number of inter-node communications. We first propose\na decentralized primal-dual method which can find an $\\epsilon$-solution both\nin terms of functional optimality gap and feasibility residual in\n$O(1\/\\epsilon)$ inter-node communication rounds when the objective functions\nare convex and the local primal subproblems are solved exactly. Our major\ncontribution is to present a new class of decentralized primal-dual type\nalgorithms, namely the decentralized communication sliding (DCS) methods, which\ncan skip the inter-node communications while agents solve the primal\nsubproblems iteratively through linearizations of their local objective\nfunctions. By employing DCS, agents can still find an $\\epsilon$-solution in\n$O(1\/\\epsilon)$ (resp., $O(1\/\\sqrt{\\epsilon})$) communication rounds for\ngeneral convex functions (resp., strongly convex functions), while maintaining\nthe $O(1\/\\epsilon^2)$ (resp., $O(1\/\\epsilon)$) bound on the total number of\nintra-node subgradient evaluations. We also present a stochastic counterpart\nfor these algorithms, denoted by SDCS, for solving stochastic optimization\nproblems whose objective function cannot be evaluated exactly. In comparison\nwith existing results for decentralized nonsmooth and stochastic optimization,\nwe can reduce the total number of inter-node communication rounds by orders of\nmagnitude while still maintaining the optimal complexity bounds on intra-node\nstochastic subgradient evaluations. The bounds on the subgradient evaluations\nare actually comparable to those required for centralized nonsmooth and\nstochastic optimization.\n","negative":"  Adaptive stochastic gradient methods such as AdaGrad have gained popularity\nin particular for training deep neural networks. The most commonly used and\nstudied variant maintains a diagonal matrix approximation to second order\ninformation by accumulating past gradients which are used to tune the step size\nadaptively. In certain situations the full-matrix variant of AdaGrad is\nexpected to attain better performance, however in high dimensions it is\ncomputationally impractical. We present Ada-LR and RadaGrad two computationally\nefficient approximations to full-matrix AdaGrad based on randomized\ndimensionality reduction. They are able to capture dependencies between\nfeatures and achieve similar performance to full-matrix AdaGrad but at a much\nsmaller computational cost. We show that the regret of Ada-LR is close to the\nregret of full-matrix AdaGrad which can have an up-to exponentially smaller\ndependence on the dimension than the diagonal variant. Empirically, we show\nthat Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task\nof training convolutional neural networks as well as recurrent neural networks,\nRadaGrad achieves faster convergence than diagonal AdaGrad.\n","id":859}
{"Unnamed: 0.1":11860,"Unnamed: 0":11860.0,"anchor":"An Online Convex Optimization Approach to Dynamic Network Resource\n  Allocation","positive":"  Existing approaches to online convex optimization (OCO) make sequential\none-slot-ahead decisions, which lead to (possibly adversarial) losses that\ndrive subsequent decision iterates. Their performance is evaluated by the\nso-called regret that measures the difference of losses between the online\nsolution and the best yet fixed overall solution in hindsight. The present\npaper deals with online convex optimization involving adversarial loss\nfunctions and adversarial constraints, where the constraints are revealed after\nmaking decisions, and can be tolerable to instantaneous violations but must be\nsatisfied in the long term. Performance of an online algorithm in this setting\nis assessed by: i) the difference of its losses relative to the best dynamic\nsolution with one-slot-ahead information of the loss function and the\nconstraint (that is here termed dynamic regret); and, ii) the accumulated\namount of constraint violations (that is here termed dynamic fit). In this\ncontext, a modified online saddle-point (MOSP) scheme is developed, and proved\nto simultaneously yield sub-linear dynamic regret and fit, provided that the\naccumulated variations of per-slot minimizers and constraints are sub-linearly\ngrowing with time. MOSP is also applied to the dynamic network resource\nallocation task, and it is compared with the well-known stochastic dual\ngradient method. Under various scenarios, numerical experiments demonstrate the\nperformance gain of MOSP relative to the state-of-the-art.\n","negative":"  This paper introduces a new paradigm for sound source lo-calization referred\nto as virtual acoustic space traveling (VAST) and presents a first dataset\ndesigned for this purpose. Existing sound source localization methods are\neither based on an approximate physical model (physics-driven) or on a\nspecific-purpose calibration set (data-driven). With VAST, the idea is to learn\na mapping from audio features to desired audio properties using a massive\ndataset of simulated room impulse responses. This virtual dataset is designed\nto be maximally representative of the potential audio scenes that the\nconsidered system may be evolving in, while remaining reasonably compact. We\nshow that virtually-learned mappings on this dataset generalize to real data,\novercoming some intrinsic limitations of traditional binaural sound\nlocalization methods based on time differences of arrival.\n","id":860}
{"Unnamed: 0.1":11861,"Unnamed: 0":11861.0,"anchor":"Breeding electric zebras in the fields of Medicine","positive":"  A few notes on the use of machine learning in medicine and the related\nunintended consequences.\n","negative":"  Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.\n","id":861}
{"Unnamed: 0.1":11862,"Unnamed: 0":11862.0,"anchor":"Agent-Agnostic Human-in-the-Loop Reinforcement Learning","positive":"  Providing Reinforcement Learning agents with expert advice can dramatically\nimprove various aspects of learning. Prior work has developed teaching\nprotocols that enable agents to learn efficiently in complex environments; many\nof these methods tailor the teacher's guidance to agents with a particular\nrepresentation or underlying learning scheme, offering effective but\nspecialized teaching procedures. In this work, we explore protocol programs, an\nagent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is\nto incorporate the beneficial properties of a human teacher into Reinforcement\nLearning without making strong assumptions about the inner workings of the\nagent. We show how to represent existing approaches such as action pruning,\nreward shaping, and training in simulation as special cases of our schema and\nconduct preliminary experiments on simple domains.\n","negative":"  In this paper, we investigate the common scenario where every candidate item\nfor recommendation is characterized by a maximum capacity, i.e., number of\nseats in a Point-of-Interest (POI) or size of an item's inventory. Despite the\nprevalence of the task of recommending items under capacity constraints in a\nvariety of settings, to the best of our knowledge, none of the known\nrecommender methods is designed to respect capacity constraints. To close this\ngap, we extend three state-of-the art latent factor recommendation approaches:\nprobabilistic matrix factorization (PMF), geographical matrix factorization\n(GeoMF), and bayesian personalized ranking (BPR), to optimize for both\nrecommendation accuracy and expected item usage that respects the capacity\nconstraints. We introduce the useful concepts of user propensity to listen and\nitem capacity. Our experimental results in real-world datasets, both for the\ndomain of item recommendation and POI recommendation, highlight the benefit of\nour method for the setting of recommendation under capacity constraints.\n","id":862}
{"Unnamed: 0.1":11863,"Unnamed: 0":11863.0,"anchor":"Field-aware Factorization Machines in a Real-world Online Advertising\n  System","positive":"  Predicting user response is one of the core machine learning tasks in\ncomputational advertising. Field-aware Factorization Machines (FFM) have\nrecently been established as a state-of-the-art method for that problem and in\nparticular won two Kaggle challenges. This paper presents some results from\nimplementing this method in a production system that predicts click-through and\nconversion rates for display advertising and shows that this method it is not\nonly effective to win challenges but is also valuable in a real-world\nprediction system. We also discuss some specific challenges and solutions to\nreduce the training time, namely the use of an innovative seeding algorithm and\na distributed learning mechanism.\n","negative":"  Governments and businesses increasingly rely on data analytics and machine\nlearning (ML) for improving their competitive edge in areas such as consumer\nsatisfaction, threat intelligence, decision making, and product efficiency.\nHowever, by cleverly corrupting a subset of data used as input to a target's ML\nalgorithms, an adversary can perturb outcomes and compromise the effectiveness\nof ML technology. While prior work in the field of adversarial machine learning\nhas studied the impact of input manipulation on correct ML algorithms, we\nconsider the exploitation of bugs in ML implementations. In this paper, we\ncharacterize the attack surface of ML programs, and we show that malicious\ninputs exploiting implementation bugs enable strictly more powerful attacks\nthan the classic adversarial machine learning techniques. We propose a\nsemi-automated technique, called steered fuzzing, for exploring this attack\nsurface and for discovering exploitable bugs in machine learning programs, in\norder to demonstrate the magnitude of this threat. As a result of our work, we\nresponsibly disclosed five vulnerabilities, established three new CVE-IDs, and\nilluminated a common insecure practice across many machine learning systems.\nFinally, we outline several research directions for further understanding and\nmitigating this threat.\n","id":863}
{"Unnamed: 0.1":11864,"Unnamed: 0":11864.0,"anchor":"Near Optimal Behavior via Approximate State Abstraction","positive":"  The combinatorial explosion that plagues planning and reinforcement learning\n(RL) algorithms can be moderated using state abstraction. Prohibitively large\ntask representations can be condensed such that essential information is\npreserved, and consequently, solutions are tractably computable. However, exact\nabstractions, which treat only fully-identical situations as equivalent, fail\nto present opportunities for abstraction in environments where no two\nsituations are exactly alike. In this work, we investigate approximate state\nabstractions, which treat nearly-identical situations as equivalent. We present\ntheoretical guarantees of the quality of behaviors derived from four types of\napproximate abstractions. Additionally, we empirically demonstrate that\napproximate abstractions lead to reduction in task complexity and bounded loss\nof optimality of behavior in a variety of environments.\n","negative":"  Recently, a novel family of biologically plausible online algorithms for\nreducing the dimensionality of streaming data has been derived from the\nsimilarity matching principle. In these algorithms, the number of output\ndimensions can be determined adaptively by thresholding the singular values of\nthe input data matrix. However, setting such threshold requires knowing the\nmagnitude of the desired singular values in advance. Here we propose online\nalgorithms where the threshold is self-calibrating based on the singular values\ncomputed from the existing observations. To derive these algorithms from the\nsimilarity matching cost function we propose novel regularizers. As before,\nthese online algorithms can be implemented by Hebbian\/anti-Hebbian neural\nnetworks in which the learning rule depends on the chosen regularizer. We\ndemonstrate both mathematically and via simulation the effectiveness of these\nonline algorithms in various settings.\n","id":864}
{"Unnamed: 0.1":11865,"Unnamed: 0":11865.0,"anchor":"Understanding the Effective Receptive Field in Deep Convolutional Neural\n  Networks","positive":"  We study characteristics of receptive fields of units in deep convolutional\nnetworks. The receptive field size is a crucial issue in many visual tasks, as\nthe output must respond to large enough areas in the image to capture\ninformation about large objects. We introduce the notion of an effective\nreceptive field, and show that it both has a Gaussian distribution and only\noccupies a fraction of the full theoretical receptive field. We analyze the\neffective receptive field in several architecture designs, and the effect of\nnonlinear activations, dropout, sub-sampling and skip connections on it. This\nleads to suggestions for ways to address its tendency to be too small.\n","negative":"  We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex).\n","id":865}
{"Unnamed: 0.1":11866,"Unnamed: 0":11866.0,"anchor":"Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks","positive":"  Deep learning classifiers are known to be inherently vulnerable to\nmanipulation by intentionally perturbed inputs, named adversarial examples. In\nthis work, we establish that reinforcement learning techniques based on Deep\nQ-Networks (DQNs) are also vulnerable to adversarial input perturbations, and\nverify the transferability of adversarial examples across different DQN models.\nFurthermore, we present a novel class of attacks based on this vulnerability\nthat enable policy manipulation and induction in the learning process of DQNs.\nWe propose an attack mechanism that exploits the transferability of adversarial\nexamples to implement policy induction attacks on DQNs, and demonstrate its\nefficacy and impact through experimental study of a game-learning scenario.\n","negative":"  We present the Overlapping Domain Cover (ODC) notion for kernel machines, as\na set of overlapping subsets of the data that covers the entire training set\nand optimized to be spatially cohesive as possible. We show how this notion\nbenefit the speed of local kernel machines for regression in terms of both\nspeed while achieving while minimizing the prediction error. We propose an\nefficient ODC framework, which is applicable to various regression models and\nin particular reduces the complexity of Twin Gaussian Processes (TGP)\nregression from cubic to quadratic. Our notion is also applicable to several\nkernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as\nshown in our experiments). We also theoretically justified the idea behind our\nmethod to improve local prediction by the overlapping cover. We validated and\nanalyzed our method on three benchmark human pose estimation datasets and\ninteresting findings are discussed.\n","id":866}
{"Unnamed: 0.1":11867,"Unnamed: 0":11867.0,"anchor":"Achieving Privacy in the Adversarial Multi-Armed Bandit","positive":"  In this paper, we improve the previously best known regret bound to achieve\n$\\epsilon$-differential privacy in oblivious adversarial bandits from\n$\\mathcal{O}{(T^{2\/3}\/\\epsilon)}$ to $\\mathcal{O}{(\\sqrt{T} \\ln T \/\\epsilon)}$.\nThis is achieved by combining a Laplace Mechanism with EXP3. We show that\nthough EXP3 is already differentially private, it leaks a linear amount of\ninformation in $T$. However, we can improve this privacy by relying on its\nintrinsic exponential mechanism for selecting actions. This allows us to reach\n$\\mathcal{O}{(\\sqrt{\\ln T})}$-DP, with a regret of $\\mathcal{O}{(T^{2\/3})}$\nthat holds against an adaptive adversary, an improvement from the best known of\n$\\mathcal{O}{(T^{3\/4})}$. This is done by using an algorithm that run EXP3 in a\nmini-batch loop. Finally, we run experiments that clearly demonstrate the\nvalidity of our theoretical analysis.\n","negative":"  Portfolio management is the decision-making process of allocating an amount\nof fund into different financial investment products. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. This paper presents a\nmodel-less convolutional neural network with historic prices of a set of\nfinancial assets as its input, outputting portfolio weights of the set. The\nnetwork is trained with 0.7 years' price data from a cryptocurrency exchange.\nThe training is done in a reinforcement manner, maximizing the accumulative\nreturn, which is regarded as the reward function of the network. Backtest\ntrading experiments with trading period of 30 minutes is conducted in the same\nmarket, achieving 10-fold returns in 1.8 months' periods. Some recently\npublished portfolio selection strategies are also used to perform the same\nback-tests, whose results are compared with the neural network. The network is\nnot limited to cryptocurrency, but can be applied to any other financial\nmarkets.\n","id":867}
{"Unnamed: 0.1":11868,"Unnamed: 0":11868.0,"anchor":"Thompson Sampling For Stochastic Bandits with Graph Feedback","positive":"  We present a novel extension of Thompson Sampling for stochastic sequential\ndecision problems with graph feedback, even when the graph structure itself is\nunknown and\/or changing. We provide theoretical guarantees on the Bayesian\nregret of the algorithm, linking its performance to the underlying properties\nof the graph. Thompson Sampling has the advantage of being applicable without\nthe need to construct complicated upper confidence bounds for different\nproblems. We illustrate its performance through extensive experimental results\non real and simulated networks with graph feedback. More specifically, we\ntested our algorithms on power law, planted partitions and Erdo's-Renyi graphs,\nas well as on graphs derived from Facebook and Flixster data. These all show\nthat our algorithms clearly outperform related methods that employ upper\nconfidence bounds, even if the latter use more information about the graph.\n","negative":"  Revealing hidden features in unlabeled data is called unsupervised feature\nlearning, which plays an important role in pretraining a deep neural network.\nHere we provide a statistical mechanics analysis of the unsupervised learning\nin a restricted Boltzmann machine with binary synapses. A message passing\nequation to infer the hidden feature is derived, and furthermore, variants of\nthis equation are analyzed. A statistical analysis by replica theory describes\nthe thermodynamic properties of the model. Our analysis confirms an entropy\ncrisis preceding the non-convergence of the message passing equation,\nsuggesting a discontinuous phase transition as a key characteristic of the\nrestricted Boltzmann machine. Continuous phase transition is also confirmed\ndepending on the embedded feature strength in the data. The mean-field result\nunder the replica symmetric assumption agrees with that obtained by running\nmessage passing algorithms on single instances of finite sizes. Interestingly,\nin an approximate Hopfield model, the entropy crisis is absent, and a\ncontinuous phase transition is observed instead. We also develop an iterative\nequation to infer the hyper-parameter (temperature) hidden in the data, which\nin physics corresponds to iteratively imposing Nishimori condition. Our study\nprovides insights towards understanding the thermodynamic properties of the\nrestricted Boltzmann machine learning, and moreover important theoretical basis\nto build simplified deep networks.\n","id":868}
{"Unnamed: 0.1":11869,"Unnamed: 0":11869.0,"anchor":"Learning Traffic as Images: A Deep Convolutional Neural Network for\n  Large-Scale Transportation Network Speed Prediction","positive":"  This paper proposes a convolutional neural network (CNN)-based method that\nlearns traffic as images and predicts large-scale, network-wide traffic speed\nwith a high accuracy. Spatiotemporal traffic dynamics are converted to images\ndescribing the time and space relations of traffic flow via a two-dimensional\ntime-space matrix. A CNN is applied to the image following two consecutive\nsteps: abstract traffic feature extraction and network-wide traffic speed\nprediction. The effectiveness of the proposed method is evaluated by taking two\nreal-world transportation networks, the second ring road and north-east\ntransportation network in Beijing, as examples, and comparing the method with\nfour prevailing algorithms, namely, ordinary least squares, k-nearest\nneighbors, artificial neural network, and random forest, and three deep\nlearning architectures, namely, stacked autoencoder, recurrent neural network,\nand long-short-term memory network. The results show that the proposed method\noutperforms other algorithms by an average accuracy improvement of 42.91%\nwithin an acceptable execution time. The CNN can train the model in a\nreasonable time and, thus, is suitable for large-scale transportation networks.\n","negative":"  Faecal Calprotectin (FC) is a surrogate marker for intestinal inflammation,\ntermed Inflammatory Bowel Disease (IBD), but not for cancer. In this\nretrospective study of 804 patients, an enhanced benchmark predictive model for\nanalyzing FC is developed, based on a novel state-of-the-art Echo State Network\n(ESN), an advanced dynamic recurrent neural network which implements a\nbiologically plausible architecture, and a supervised learning mechanism. The\nproposed machine learning driven predictive model is benchmarked against a\nconventional logistic regression model, demonstrating statistically significant\nperformance improvements.\n","id":869}
{"Unnamed: 0.1":11870,"Unnamed: 0":11870.0,"anchor":"Geometric features for voxel-based surface recognition","positive":"  We introduce a library of geometric voxel features for CAD surface\nrecognition\/retrieval tasks. Our features include local versions of the\nintrinsic volumes (the usual 3D volume, surface area, integrated mean and\nGaussian curvature) and a few closely related quantities. We also compute Haar\nwavelet and statistical distribution features by aggregating raw voxel\nfeatures. We apply our features to object classification on the ESB data set\nand demonstrate accurate results with a small number of shallow decision trees.\n","negative":"  Two simple proofs of the triangle inequality for the Jaccard distance in\nterms of nonnegative, monotone, submodular functions are given and discussed.\n","id":870}
{"Unnamed: 0.1":11871,"Unnamed: 0":11871.0,"anchor":"Fast Rates for Empirical Risk Minimization of Strict Saddle Problems","positive":"  We derive bounds on the sample complexity of empirical risk minimization\n(ERM) in the context of minimizing non-convex risks that admit the strict\nsaddle property. Recent progress in non-convex optimization has yielded\nefficient algorithms for minimizing such functions. Our results imply that\nthese efficient algorithms are statistically stable and also generalize well.\nIn particular, we derive fast rates which resemble the bounds that are often\nattained in the strongly convex setting. We specify our bounds to Principal\nComponent Analysis and Independent Component Analysis. Our results and\ntechniques may pave the way for statistical analyses of additional strict\nsaddle problems.\n","negative":"  In this work we investigate intra-day patterns of activity on a population of\n7,261 users of mobile health wearable devices and apps. We show that: (1) using\nintra-day step and sleep data recorded from passive trackers significantly\nimproves classification performance on self-reported chronic conditions related\nto mental health and nervous system disorders, (2) Convolutional Neural\nNetworks achieve top classification performance vs. baseline models when\ntrained directly on multivariate time series of activity data, and (3) jointly\npredicting all condition classes via multi-task learning can be leveraged to\nextract features that generalize across data sets and achieve the highest\nclassification performance.\n","id":871}
{"Unnamed: 0.1":11872,"Unnamed: 0":11872.0,"anchor":"End-to-End ASR-free Keyword Search from Speech","positive":"  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n","negative":"  We explore frame-level audio feature learning for chord recognition using\nartificial neural networks. We present the argument that chroma vectors\npotentially hold enough information to model harmonic content of audio for\nchord recognition, but that standard chroma extractors compute too noisy\nfeatures. This leads us to propose a learned chroma feature extractor based on\nartificial neural networks. It is trained to compute chroma features that\nencode harmonic information important for chord recognition, while being robust\nto irrelevant interferences. We achieve this by feeding the network an audio\nspectrum with context instead of a single frame as input. This way, the network\ncan learn to selectively compensate noise and resolve harmonic ambiguities.\n  We compare the resulting features to hand-crafted ones by using a simple\nlinear frame-wise classifier for chord recognition on various data sets. The\nresults show that the learned feature extractor produces superior chroma\nvectors for chord recognition.\n","id":872}
{"Unnamed: 0.1":11873,"Unnamed: 0":11873.0,"anchor":"Classification of MRI data using Deep Learning and Gaussian\n  Process-based Model Selection","positive":"  The classification of MRI images according to the anatomical field of view is\na necessary task to solve when faced with the increasing quantity of medical\nimages. In parallel, advances in deep learning makes it a suitable tool for\ncomputer vision problems. Using a common architecture (such as AlexNet)\nprovides quite good results, but not sufficient for clinical use. Improving the\nmodel is not an easy task, due to the large number of hyper-parameters\ngoverning both the architecture and the training of the network, and to the\nlimited understanding of their relevance. Since an exhaustive search is not\ntractable, we propose to optimize the network first by random search, and then\nby an adaptive search based on Gaussian Processes and Probability of\nImprovement. Applying this method on a large and varied MRI dataset, we show a\nsubstantial improvement between the baseline network and the final one (up to\n20\\% for the most difficult classes).\n","negative":"  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n","id":873}
{"Unnamed: 0.1":11874,"Unnamed: 0":11874.0,"anchor":"The Incredible Shrinking Neural Network: New Perspectives on Learning\n  Representations Through The Lens of Pruning","positive":"  How much can pruning algorithms teach us about the fundamentals of learning\nrepresentations in neural networks? And how much can these fundamentals help\nwhile devising new pruning techniques? A lot, it turns out. Neural network\npruning has become a topic of great interest in recent years, and many\ndifferent techniques have been proposed to address this problem. The decision\nof what to prune and when to prune necessarily forces us to confront our\nassumptions about how neural networks actually learn to represent patterns in\ndata. In this work, we set out to test several long-held hypotheses about\nneural network learning representations, approaches to pruning and the\nrelevance of one in the context of the other. To accomplish this, we argue in\nfavor of pruning whole neurons as opposed to the traditional method of pruning\nweights from optimally trained networks. We first review the historical\nliterature, point out some common assumptions it makes, and propose methods to\ndemonstrate the inherent flaws in these assumptions. We then propose our novel\napproach to pruning and set about analyzing the quality of the decisions it\nmakes. Our analysis led us to question the validity of many widely-held\nassumptions behind pruning algorithms and the trade-offs we often make in the\ninterest of reducing computational complexity. We discovered that there is a\nstraightforward way, however expensive, to serially prune 40-70% of the neurons\nin a trained network with minimal effect on the learning representation and\nwithout any re-training. It is to be noted here that the motivation behind this\nwork is not to propose an algorithm that would outperform all existing methods,\nbut to shed light on what some inherent flaws in these methods can teach us\nabout learning representations and how this can lead us to superior pruning\ntechniques.\n","negative":"  We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing.\n","id":874}
{"Unnamed: 0.1":11875,"Unnamed: 0":11875.0,"anchor":"Towards a New Interpretation of Separable Convolutions","positive":"  In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.\n","negative":"  Multi-task learning aims to improve generalization performance of multiple\nprediction tasks by appropriately sharing relevant information across them. In\nthe context of deep neural networks, this idea is often realized by\nhand-designed network architectures with layers that are shared across tasks\nand branches that encode task-specific features. However, the space of possible\nmulti-task deep architectures is combinatorially large and often the final\narchitecture is arrived at by manual exploration of this space subject to\ndesigner's bias, which can be both error-prone and tedious. In this work, we\npropose a principled approach for designing compact multi-task deep learning\narchitectures. Our approach starts with a thin network and dynamically widens\nit in a greedy manner during training using a novel criterion that promotes\ngrouping of similar tasks together. Our Extensive evaluation on person\nattributes classification tasks involving facial and clothing attributes\nsuggests that the models produced by the proposed method are fast, compact and\ncan closely match or exceed the state-of-the-art accuracy from strong baselines\nby much more expensive models.\n","id":875}
{"Unnamed: 0.1":11876,"Unnamed: 0":11876.0,"anchor":"Deep Learning for Computational Chemistry","positive":"  The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.\n","negative":"  With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum.\n","id":876}
{"Unnamed: 0.1":11877,"Unnamed: 0":11877.0,"anchor":"Online Learning with Regularized Kernel for One-class Classification","positive":"  This paper presents an online learning with regularized kernel based\none-class extreme learning machine (ELM) classifier and is referred as online\nRK-OC-ELM. The baseline kernel hyperplane model considers whole data in a\nsingle chunk with regularized ELM approach for offline learning in case of\none-class classification (OCC). Further, the basic hyper plane model is adapted\nin an online fashion from stream of training samples in this paper. Two\nframeworks viz., boundary and reconstruction are presented to detect the target\nclass in online RKOC-ELM. Boundary framework based one-class classifier\nconsists of single node output architecture and classifier endeavors to\napproximate all data to any real number. However, one-class classifier based on\nreconstruction framework is an autoencoder architecture, where output nodes are\nidentical to input nodes and classifier endeavor to reconstruct input layer at\nthe output layer. Both these frameworks employ regularized kernel ELM based\nonline learning and consistency based model selection has been employed to\nselect learning algorithm parameters. The performance of online RK-OC-ELM has\nbeen evaluated on standard benchmark datasets as well as on artificial datasets\nand the results are compared with existing state-of-the art one-class\nclassifiers. The results indicate that the online learning one-class classifier\nis slightly better or same as batch learning based approaches. As, base\nclassifier used for the proposed classifiers are based on the ELM, hence,\nproposed classifiers would also inherit the benefit of the base classifier i.e.\nit will perform faster computation compared to traditional autoencoder based\none-class classifier.\n","negative":"  We propose an approach for learning the causal structure in stochastic\ndynamical systems with a $1$-step functional dependency in the presence of\nlatent variables. We propose an information-theoretic approach that allows us\nto recover the causal relations among the observed variables as long as the\nlatent variables evolve without exogenous noise. We further propose an\nefficient learning method based on linear regression for the special sub-case\nwhen the dynamics are restricted to be linear. We validate the performance of\nour approach via numerical simulations.\n","id":877}
{"Unnamed: 0.1":11878,"Unnamed: 0":11878.0,"anchor":"On The Construction of Extreme Learning Machine for Online and Offline\n  One-Class Classification - An Expanded Toolbox","positive":"  One-Class Classification (OCC) has been prime concern for researchers and\neffectively employed in various disciplines. But, traditional methods based\none-class classifiers are very time consuming due to its iterative process and\nvarious parameters tuning. In this paper, we present six OCC methods based on\nextreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed\nclassifiers mainly lie in two categories: reconstruction based and boundary\nbased, which supports both types of learning viz., online and offline learning.\nOut of various proposed methods, four are offline and remaining two are online\nmethods. Out of four offline methods, two methods perform random feature\nmapping and two methods perform kernel feature mapping. Kernel feature mapping\nbased approaches have been tested with RBF kernel and online version of\none-class classifiers are tested with both types of nodes viz., additive and\nRBF. It is well known fact that threshold decision is a crucial factor in case\nof OCC, so, three different threshold deciding criteria have been employed so\nfar and analyses the effectiveness of one threshold deciding criteria over\nanother. Further, these methods are tested on two artificial datasets to check\nthere boundary construction capability and on eight benchmark datasets from\ndifferent discipline to evaluate the performance of the classifiers. Our\nproposed classifiers exhibit better performance compared to ten traditional\none-class classifiers and ELM based two one-class classifiers. Through proposed\none-class classifiers, we intend to expand the functionality of the most used\ntoolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with\nall the present features of the toolbox.\n","negative":"  In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based\nlanguage model designed to directly capture the global semantic meaning\nrelating words in a document via latent topics. Because of their sequential\nnature, RNNs are good at capturing the local structure of a word sequence -\nboth semantic and syntactic - but might face difficulty remembering long-range\ndependencies. Intuitively, these long-range dependencies are of semantic\nnature. In contrast, latent topic models are able to capture the global\nunderlying semantic structure of a document but do not account for word\nordering. The proposed TopicRNN model integrates the merits of RNNs and latent\ntopic models: it captures local (syntactic) dependencies using an RNN and\nglobal (semantic) dependencies using latent topics. Unlike previous work on\ncontextual RNN language modeling, our model is learned end-to-end. Empirical\nresults on word prediction show that TopicRNN outperforms existing contextual\nRNN baselines. In addition, TopicRNN can be used as an unsupervised feature\nextractor for documents. We do this for sentiment analysis on the IMDB movie\nreview dataset and report an error rate of $6.28\\%$. This is comparable to the\nstate-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally,\nTopicRNN also yields sensible topics, making it a useful alternative to\ndocument models such as latent Dirichlet allocation.\n","id":878}
{"Unnamed: 0.1":11879,"Unnamed: 0":11879.0,"anchor":"Towards prediction of rapid intensification in tropical cyclones with\n  recurrent neural networks","positive":"  The problem where a tropical cyclone intensifies dramatically within a short\nperiod of time is known as rapid intensification. This has been one of the\nmajor challenges for tropical weather forecasting. Recurrent neural networks\nhave been promising for time series problems which makes them appropriate for\nrapid intensification. In this paper, recurrent neural networks are used to\npredict rapid intensification cases of tropical cyclones from the South Pacific\nand South Indian Ocean regions. A class imbalanced problem is encountered which\nmakes it very challenging to achieve promising performance. A simple strategy\nwas proposed to include more positive cases for detection where the false\npositive rate was slightly improved. The limitations of building an efficient\nsystem remains due to the challenges of addressing the class imbalance problem\nencountered for rapid intensification prediction. This motivates further\nresearch in using innovative machine learning methods.\n","negative":"  We present a general framework, the coupled compound Poisson factorization\n(CCPF), to capture the missing-data mechanism in extremely sparse data sets by\ncoupling a hierarchical Poisson factorization with an arbitrary data-generating\nmodel. We derive a stochastic variational inference algorithm for the resulting\nmodel and, as examples of our framework, implement three different\ndata-generating models---a mixture model, linear regression, and factor\nanalysis---to robustly model non-random missing data in the context of\nclustering, prediction, and matrix factorization. In all three cases, we test\nour framework against models that ignore the missing-data mechanism on large\nscale studies with non-random missing data, and we show that explicitly\nmodeling the missing-data mechanism substantially improves the quality of the\nresults, as measured using data log likelihood on a held-out test set.\n","id":879}
{"Unnamed: 0.1":11880,"Unnamed: 0":11880.0,"anchor":"Faster K-Means Cluster Estimation","positive":"  There has been considerable work on improving popular clustering algorithm\n`K-means' in terms of mean squared error (MSE) and speed, both. However, most\nof the k-means variants tend to compute distance of each data point to each\ncluster centroid for every iteration. We propose a fast heuristic to overcome\nthis bottleneck with only marginal increase in MSE. We observe that across all\niterations of K-means, a data point changes its membership only among a small\nsubset of clusters. Our heuristic predicts such clusters for each data point by\nlooking at nearby clusters after the first iteration of k-means. We augment\nwell known variants of k-means with our heuristic to demonstrate effectiveness\nof our heuristic. For various synthetic and real-world datasets, our heuristic\nachieves speed-up of up-to 3 times when compared to efficient variants of\nk-means.\n","negative":"  Gaussian graphical models (GGMs) are widely used for statistical modeling,\nbecause of ease of inference and the ubiquitous use of the normal distribution\nin practical approximations. However, they are also known for their limited\nmodeling abilities, due to the Gaussian assumption. In this paper, we introduce\na novel variant of GGMs, which relaxes the Gaussian restriction and yet admits\nefficient inference. Specifically, we impose a bipartite structure on the GGM\nand govern the hidden variables by truncated normal distributions. The\nnonlinearity of the model is revealed by its connection to rectified linear\nunit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and\nappealing properties of truncated normals, we are able to train the models\nefficiently using contrastive divergence. We consider three output constructs,\naccounting for real-valued, binary and count data. We further extend the model\nto deep constructions and show that deep models can be used for unsupervised\npre-training of rectifier neural networks. Extensive experimental results are\nprovided to validate the proposed models and demonstrate their superiority over\ncompeting models.\n","id":880}
{"Unnamed: 0.1":11881,"Unnamed: 0":11881.0,"anchor":"Incremental Learning for Robot Perception through HRI","positive":"  Scene understanding and object recognition is a difficult to achieve yet\ncrucial skill for robots. Recently, Convolutional Neural Networks (CNN), have\nshown success in this task. However, there is still a gap between their\nperformance on image datasets and real-world robotics scenarios. We present a\nnovel paradigm for incrementally improving a robot's visual perception through\nactive human interaction. In this paradigm, the user introduces novel objects\nto the robot by means of pointing and voice commands. Given this information,\nthe robot visually explores the object and adds images from it to re-train the\nperception module. Our base perception module is based on recent development in\nobject detection and recognition using deep learning. Our method leverages\nstate of the art CNNs from off-line batch learning, human guidance, robot\nexploration and incremental on-line learning.\n","negative":"  Recently, sentiment analysis has received a lot of attention due to the\ninterest in mining opinions of social media users. Sentiment analysis consists\nin determining the polarity of a given text, i.e., its degree of positiveness\nor negativeness. Traditionally, Sentiment Analysis algorithms have been\ntailored to a specific language given the complexity of having a number of\nlexical variations and errors introduced by the people generating content. In\nthis contribution, our aim is to provide a simple to implement and easy to use\nmultilingual framework, that can serve as a baseline for sentiment analysis\ncontests, and as starting point to build new sentiment analysis systems. We\ncompare our approach in eight different languages, three of them have important\ninternational contests, namely, SemEval (English), TASS (Spanish), and\nSENTIPOLC (Italian). Within the competitions our approach reaches from medium\nto high positions in the rankings; whereas in the remaining languages our\napproach outperforms the reported results.\n","id":881}
{"Unnamed: 0.1":11882,"Unnamed: 0":11882.0,"anchor":"Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks","positive":"  Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement.\n","negative":"  Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones.\n","id":882}
{"Unnamed: 0.1":11883,"Unnamed: 0":11883.0,"anchor":"On the Sample Complexity of Graphical Model Selection for Non-Stationary\n  Processes","positive":"  We characterize the sample size required for accurate graphical model\nselection from non-stationary samples. The observed data is modeled as a\nvector-valued zero-mean Gaussian random process whose samples are uncorrelated\nbut have different covariance matrices. This model contains as special cases\nthe standard setting of i.i.d. samples as well as the case of samples forming a\nstationary or underspread (non-stationary) processes. More generally, our model\napplies to any process model for which an efficient decorrelation can be\nobtained. By analyzing a particular model selection method, we derive a\nsufficient condition on the required sample size for accurate graphical model\nselection based on non-stationary data.\n","negative":"  Despite tremendous progress in outlier detection research in recent years,\nthe majority of existing methods are designed only to detect unconditional\noutliers that correspond to unusual data patterns expressed in the joint space\nof all data attributes. Such methods are not applicable when we seek to detect\nconditional outliers that reflect unusual responses associated with a given\ncontext or condition. This work focuses on multivariate conditional outlier\ndetection, a special type of the conditional outlier detection problem, where\ndata instances consist of multi-dimensional input (context) and output\n(responses) pairs. We present a novel outlier detection framework that\nidentifies abnormal input-output associations in data with the help of a\ndecomposable conditional probabilistic model that is learned from all data\ninstances. Since components of this model can vary in their quality, we combine\nthem with the help of weights reflecting their reliability in assessment of\noutliers. We study two ways of calculating the component weights: global that\nrelies on all data, and local that relies only on instances similar to the\ntarget instance. Experimental results on data from various domains demonstrate\nthe ability of our framework to successfully identify multivariate conditional\noutliers.\n","id":883}
{"Unnamed: 0.1":11884,"Unnamed: 0":11884.0,"anchor":"Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning","positive":"  Governments and businesses increasingly rely on data analytics and machine\nlearning (ML) for improving their competitive edge in areas such as consumer\nsatisfaction, threat intelligence, decision making, and product efficiency.\nHowever, by cleverly corrupting a subset of data used as input to a target's ML\nalgorithms, an adversary can perturb outcomes and compromise the effectiveness\nof ML technology. While prior work in the field of adversarial machine learning\nhas studied the impact of input manipulation on correct ML algorithms, we\nconsider the exploitation of bugs in ML implementations. In this paper, we\ncharacterize the attack surface of ML programs, and we show that malicious\ninputs exploiting implementation bugs enable strictly more powerful attacks\nthan the classic adversarial machine learning techniques. We propose a\nsemi-automated technique, called steered fuzzing, for exploring this attack\nsurface and for discovering exploitable bugs in machine learning programs, in\norder to demonstrate the magnitude of this threat. As a result of our work, we\nresponsibly disclosed five vulnerabilities, established three new CVE-IDs, and\nilluminated a common insecure practice across many machine learning systems.\nFinally, we outline several research directions for further understanding and\nmitigating this threat.\n","negative":"  Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach.\n","id":884}
{"Unnamed: 0.1":11885,"Unnamed: 0":11885.0,"anchor":"Joint Deep Modeling of Users and Items Using Reviews for Recommendation","positive":"  A large amount of information exists in reviews written by users. This source\nof information has been ignored by most of the current recommender systems\nwhile it can potentially alleviate the sparsity problem and improve the quality\nof recommendations. In this paper, we present a deep model to learn item\nproperties and user behaviors jointly from review text. The proposed model,\nnamed Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel\nneural networks coupled in the last layers. One of the networks focuses on\nlearning user behaviors exploiting reviews written by the user, and the other\none learns item properties from the reviews written for the item. A shared\nlayer is introduced on the top to couple these two networks together. The\nshared layer enables latent factors learned for users and items to interact\nwith each other in a manner similar to factorization machine techniques.\nExperimental results demonstrate that DeepCoNN significantly outperforms all\nbaseline recommender systems on a variety of datasets.\n","negative":"  We consider semi-supervised regression when the predictor variables are drawn\nfrom an unknown manifold. A simple two step approach to this problem is to: (i)\nestimate the manifold geodesic distance between any pair of points using both\nthe labeled and unlabeled instances; and (ii) apply a k nearest neighbor\nregressor based on these distance estimates. We prove that given sufficiently\nmany unlabeled points, this simple method of geodesic kNN regression achieves\nthe optimal finite-sample minimax bound on the mean squared error, as if the\nmanifold were known. Furthermore, we show how this approach can be efficiently\nimplemented, requiring only O(k N log N) operations to estimate the regression\nfunction at all N labeled and unlabeled points. We illustrate this approach on\ntwo datasets with a manifold structure: indoor localization using WiFi\nfingerprints and facial pose estimation. In both cases, geodesic kNN is more\naccurate and much faster than the popular Laplacian eigenvector regressor.\n","id":885}
{"Unnamed: 0.1":11886,"Unnamed: 0":11886.0,"anchor":"Towards Principled Methods for Training Generative Adversarial Networks","positive":"  The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem.\n","negative":"  Bayesian Optimization (BO) has become a core method for solving expensive\nblack-box optimization problems. While much research focussed on the choice of\nthe acquisition function, we focus on online length-scale adaption and the\nchoice of kernel function. Instead of choosing hyperparameters in view of\nmaximum likelihood on past data, we propose to use the acquisition function to\ndecide on hyperparameter adaptation more robustly and in view of the future\noptimization progress. Further, we propose a particular kernel function that\nincludes non-stationarity and local anisotropy and thereby implicitly\nintegrates the efficiency of local convex optimization with global Bayesian\noptimization. Comparisons to state-of-the art BO methods underline the\nefficiency of these mechanisms on global optimization benchmarks.\n","id":886}
{"Unnamed: 0.1":11887,"Unnamed: 0":11887.0,"anchor":"3D Morphology Prediction of Progressive Spinal Deformities from\n  Probabilistic Modeling of Discriminant Manifolds","positive":"  We introduce a novel approach for predicting the progression of adolescent\nidiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray\nimages. Recent progress in machine learning have allowed to improve\nclassification and prognosis rates, but lack a probabilistic framework to\nmeasure uncertainty in the data. We propose a discriminative probabilistic\nmanifold embedding where locally linear mappings transform data points from\nhigh-dimensional space to corresponding low-dimensional coordinates. A\ndiscriminant adjacency matrix is constructed to maximize the separation between\nprogressive and non-progressive groups of patients diagnosed with scoliosis,\nwhile minimizing the distance in latent variables belonging to the same class.\nTo predict the evolution of deformation, a baseline reconstruction is projected\nonto the manifold, from which a spatiotemporal regression model is built from\nparallel transport curves inferred from neighboring exemplars. Rate of\nprogression is modulated from the spine flexibility and curve magnitude of the\n3D spine deformation. The method was tested on 745 reconstructions from 133\nsubjects using longitudinal 3D reconstructions of the spine, with results\ndemonstrating the discriminatory framework can identify between progressive and\nnon-progressive of scoliotic patients with a classification rate of 81% and\nprediction differences of 2.1$^{o}$ in main curve angulation, outperforming\nother manifold learning methods. Our method achieved a higher prediction\naccuracy and improved the modeling of spatiotemporal morphological changes in\nhighly deformed spines compared to other learning methods.\n","negative":"  Limited annotated data available for the recognition of facial expression and\naction units embarrasses the training of deep networks, which can learn\ndisentangled invariant features. However, a linear model with just several\nparameters normally is not demanding in terms of training data. In this paper,\nwe propose an elegant linear model to untangle confounding factors in\nchallenging realistic multichannel signals such as 2D face videos. The simple\nyet powerful model does not rely on huge training data and is natural for\nrecognizing facial actions without explicitly disentangling the identity. Base\non well-understood intuitive linear models such as Sparse Representation based\nClassification (SRC), previous attempts require a prepossessing of explicit\ndecoupling which is practically inexact. Instead, we exploit the low-rank\nproperty across frames to subtract the underlying neutral faces which are\nmodeled jointly with sparse representation on the action components with group\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\nautomatic method on raw face videos performs as competitive as SRC applied on\nmanually prepared action components and performs even better than SRC in terms\nof true positive rate. We apply the model to the even more challenging task of\nfacial action unit recognition, verified on the MPI Face Video Database\n(MPI-VDB) achieving a decent performance. All the programs and data have been\nmade publicly available.\n","id":887}
{"Unnamed: 0.1":11888,"Unnamed: 0":11888.0,"anchor":"Agglomerative Info-Clustering","positive":"  An agglomerative clustering of random variables is proposed, where clusters\nof random variables sharing the maximum amount of multivariate mutual\ninformation are merged successively to form larger clusters. Compared to the\nprevious info-clustering algorithms, the agglomerative approach allows the\ncomputation to stop earlier when clusters of desired size and accuracy are\nobtained. An efficient algorithm is also derived based on the submodularity of\nentropy and the duality between the principal sequence of partitions and the\nprincipal sequence for submodular functions.\n","negative":"  Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system.\n","id":888}
{"Unnamed: 0.1":11889,"Unnamed: 0":11889.0,"anchor":"A Machine Learning Alternative to P-values","positive":"  This paper presents an alternative approach to p-values in regression\nsettings. This approach, whose origins can be traced to machine learning, is\nbased on the leave-one-out bootstrap for prediction error. In machine learning\nthis is called the out-of-bag (OOB) error. To obtain the OOB error for a model,\none draws a bootstrap sample and fits the model to the in-sample data. The\nout-of-sample prediction error for the model is obtained by calculating the\nprediction error for the model using the out-of-sample data. Repeating and\naveraging yields the OOB error, which represents a robust cross-validated\nestimate of the accuracy of the underlying model. By a simple modification to\nthe bootstrap data involving \"noising up\" a variable, the OOB method yields a\nvariable importance (VIMP) index, which directly measures how much a specific\nvariable contributes to the prediction precision of a model. VIMP provides a\nscientifically interpretable measure of the effect size of a variable, we call\nthe \"predictive effect size\", that holds whether the researcher's model is\ncorrect or not, unlike the p-value whose calculation is based on the assumed\ncorrectness of the model. We also discuss a marginal VIMP index, also easily\ncalculated, which measures the marginal effect of a variable, or what we call\n\"the discovery effect\". The OOB procedure can be applied to both parametric and\nnonparametric regression models and requires only that the researcher can\nrepeatedly fit their model to bootstrap and modified bootstrap data. We\nillustrate this approach on a survival data set involving patients with\nsystolic heart failure and to a simulated survival data set where the model is\nincorrectly specified to illustrate its robustness to model misspecification.\n","negative":"  When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.\n","id":889}
{"Unnamed: 0.1":11890,"Unnamed: 0":11890.0,"anchor":"A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in\n  Caffe","positive":"  This paper presents the development of several models of a deep convolutional\nauto-encoder in the Caffe deep learning framework and their experimental\nevaluation on the example of MNIST dataset. We have created five models of a\nconvolutional auto-encoder which differ architecturally by the presence or\nabsence of pooling and unpooling layers in the auto-encoder's encoder and\ndecoder parts. Our results show that the developed models provide very good\nresults in dimensionality reduction and unsupervised clustering tasks, and\nsmall classification errors when we used the learned internal code as an input\nof a supervised linear classifier and multi-layer perceptron. The best results\nwere provided by a model where the encoder part contains convolutional and\npooling layers, followed by an analogous decoder part with deconvolution and\nunpooling layers without the use of switch variables in the decoder part. The\npaper also discusses practical details of the creation of a deep convolutional\nauto-encoder in the very popular Caffe deep learning framework. We believe that\nour approach and results presented in this paper could help other researchers\nto build efficient deep neural network architectures in the future.\n","negative":"  We develop a supervised machine learning model that detects anomalies in\nsystems in real time. Our model processes unbounded streams of data into time\nseries which then form the basis of a low-latency anomaly detection model.\nMoreover, we extend our preliminary goal of just anomaly detection to\nsimultaneous anomaly prediction. We approach this very challenging problem by\ndeveloping a Bayesian Network framework that captures the information about the\nparameters of the lagged regressors calibrated in the first part of our\napproach and use this structure to learn local conditional probability\ndistributions.\n","id":890}
{"Unnamed: 0.1":11891,"Unnamed: 0":11891.0,"anchor":"Multilayer Perceptron Algebra","positive":"  Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones.\n","negative":"  We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities.\n","id":891}
{"Unnamed: 0.1":11892,"Unnamed: 0":11892.0,"anchor":"Highly Efficient Hierarchical Online Nonlinear Regression Using Second\n  Order Methods","positive":"  We introduce highly efficient online nonlinear regression algorithms that are\nsuitable for real life applications. We process the data in a truly online\nmanner such that no storage is needed, i.e., the data is discarded after being\nused. For nonlinear modeling we use a hierarchical piecewise linear approach\nbased on the notion of decision trees where the space of the regressor vectors\nis adaptively partitioned based on the performance. As the first time in the\nliterature, we learn both the piecewise linear partitioning of the regressor\nspace as well as the linear models in each region using highly effective second\norder methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known\nover fitting issues by using piecewise linear models, however, since both the\nregion boundaries as well as the linear models in each region are trained using\nthe second order methods, we achieve substantial performance compared to the\nstate of the art. We demonstrate our gains over the well known benchmark data\nsets and provide performance results in an individual sequence manner\nguaranteed to hold without any statistical assumptions. Hence, the introduced\nalgorithms address computational complexity issues widely encountered in real\nlife applications while providing superior guaranteed performance in a strong\ndeterministic sense.\n","negative":"  We present a matrix factorization algorithm that scales to input matrices\nthat are large in both dimensions (i.e., that contains morethan 1TB of data).\nThe algorithm streams the matrix columns while subsampling them, resulting in\nlow complexity per iteration andreasonable memory footprint. In contrast to\nprevious online matrix factorization methods, our approach relies on\nlow-dimensional statistics from past iterates to control the extra variance\nintroduced by subsampling. We present a convergence analysis that guarantees us\nto reach a stationary point of the problem. Large speed-ups can be obtained\ncompared to previous online algorithms that do not perform subsampling, thanks\nto the feature redundancy that often exists in high-dimensional settings.\n","id":892}
{"Unnamed: 0.1":11893,"Unnamed: 0":11893.0,"anchor":"Lipschitz Properties for Deep Convolutional Networks","positive":"  In this paper we discuss the stability properties of convolutional neural\nnetworks. Convolutional neural networks are widely used in machine learning. In\nclassification they are mainly used as feature extractors. Ideally, we expect\nsimilar features when the inputs are from the same class. That is, we hope to\nsee a small change in the feature vector with respect to a deformation on the\ninput signal. This can be established mathematically, and the key step is to\nderive the Lipschitz properties. Further, we establish that the stability\nresults can be extended for more general networks. We give a formula for\ncomputing the Lipschitz bound, and compare it with other methods to show it is\ncloser to the optimal value.\n","negative":"  In machine learning, error back-propagation in multi-layer neural networks\n(deep learning) has been impressively successful in supervised and\nreinforcement learning tasks. As a model for learning in the brain, however,\ndeep learning has long been regarded as implausible, since it relies in its\nbasic form on a non-local plasticity rule. To overcome this problem,\nenergy-based models with local contrastive Hebbian learning were proposed and\ntested on a classification task with networks of rate neurons. We extended this\nwork by implementing and testing such a model with networks of leaky\nintegrate-and-fire neurons. Preliminary results indicate that it is possible to\nlearn a non-linear regression task with hidden layers, spiking neurons and a\nlocal synaptic plasticity rule.\n","id":893}
{"Unnamed: 0.1":11894,"Unnamed: 0":11894.0,"anchor":"Parsimonious Inference on Convolutional Neural Networks: Learning and\n  applying on-line kernel activation rules","positive":"  A new, radical CNN design approach is presented in this paper, considering\nthe reduction of the total computational load during inference. This is\nachieved by a new holistic intervention on both the CNN architecture and the\ntraining procedure, which targets to the parsimonious inference by learning to\nexploit or remove the redundant capacity of a CNN architecture. This is\naccomplished, by the introduction of a new structural element that can be\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\nor even improving its recognition accuracy. Our approach formulates a\nsystematic and data-driven method for developing CNNs that are trained to\neventually change size and form in real-time during inference, targeting to the\nsmaller possible computational footprint. Results are provided for the optimal\nimplementation on a few modern, high-end mobile computing platforms indicating\na significant speed-up of up to x3 times.\n","negative":"  While Shannon's mutual information has widespread applications in many\ndisciplines, for practical applications it is often difficult to calculate its\nvalue accurately for high-dimensional variables because of the curse of\ndimensionality. This paper is focused on effective approximation methods for\nevaluating mutual information in the context of neural population coding. For\nlarge but finite neural populations, we derive several information-theoretic\nasymptotic bounds and approximation formulas that remain valid in\nhigh-dimensional spaces. We prove that optimizing the population density\ndistribution based on these approximation formulas is a convex optimization\nproblem which allows efficient numerical solutions. Numerical simulation\nresults confirmed that our asymptotic formulas were highly accurate for\napproximating mutual information for large neural populations. In special\ncases, the approximation formulas are exactly equal to the true mutual\ninformation. We also discuss techniques of variable transformation and\ndimensionality reduction to facilitate computation of the approximations.\n","id":894}
{"Unnamed: 0.1":11895,"Unnamed: 0":11895.0,"anchor":"Recommendation under Capacity Constraints","positive":"  In this paper, we investigate the common scenario where every candidate item\nfor recommendation is characterized by a maximum capacity, i.e., number of\nseats in a Point-of-Interest (POI) or size of an item's inventory. Despite the\nprevalence of the task of recommending items under capacity constraints in a\nvariety of settings, to the best of our knowledge, none of the known\nrecommender methods is designed to respect capacity constraints. To close this\ngap, we extend three state-of-the art latent factor recommendation approaches:\nprobabilistic matrix factorization (PMF), geographical matrix factorization\n(GeoMF), and bayesian personalized ranking (BPR), to optimize for both\nrecommendation accuracy and expected item usage that respects the capacity\nconstraints. We introduce the useful concepts of user propensity to listen and\nitem capacity. Our experimental results in real-world datasets, both for the\ndomain of item recommendation and POI recommendation, highlight the benefit of\nour method for the setting of recommendation under capacity constraints.\n","negative":"  Despite widespread interests in reinforcement-learning for task-oriented\ndialogue systems, several obstacles can frustrate research and development\nprogress. First, reinforcement learners typically require interaction with the\nenvironment, so conventional dialogue corpora cannot be used directly. Second,\neach task presents specific challenges, requiring separate corpus of\ntask-specific annotated data. Third, collecting and annotating human-machine or\nhuman-human conversations for task-oriented dialogues requires extensive domain\nknowledge. Because building an appropriate dataset can be both financially\ncostly and time-consuming, one popular approach is to build a user simulator\nbased upon a corpus of example dialogues. Then, one can train reinforcement\nlearning agents in an online fashion as they interact with the simulator.\nDialogue agents trained on these simulators can serve as an effective starting\npoint. Once agents master the simulator, they may be deployed in a real\nenvironment to interact with humans, and continue to be trained online. To ease\nempirical algorithmic comparisons in dialogues, this paper introduces a new,\npublicly available simulation framework, where our simulator, designed for the\nmovie-booking domain, leverages both rules and collected data. The simulator\nsupports two tasks: movie ticket booking and movie seeking. Finally, we\ndemonstrate several agents and detail the procedure to add and test your own\nagent in the proposed framework.\n","id":895}
{"Unnamed: 0.1":11896,"Unnamed: 0":11896.0,"anchor":"Online Structure Learning for Sum-Product Networks with Gaussian Leaves","positive":"  Sum-product networks have recently emerged as an attractive representation\ndue to their dual view as a special type of deep neural network with clear\nsemantics and a special type of probabilistic graphical model for which\ninference is always tractable. Those properties follow from some conditions\n(i.e., completeness and decomposability) that must be respected by the\nstructure of the network. As a result, it is not easy to specify a valid\nsum-product network by hand and therefore structure learning techniques are\ntypically used in practice. This paper describes the first online structure\nlearning technique for continuous SPNs with Gaussian leaves. We also introduce\nan accompanying new parameter learning technique.\n","negative":"  Critically ill patients in regular wards are vulnerable to unanticipated\nclinical dete- rioration which requires timely transfer to the intensive care\nunit (ICU). To allow for risk scoring and patient monitoring in such a setting,\nwe develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the\ninpatients' physiol- ogy. The model captures the patients' latent clinical\nstates and their corresponding observable lab tests and vital signs. We present\nan efficient unsupervised learn- ing algorithm that capitalizes on the\ninformatively censored data in the electronic health records (EHR) to learn the\nparameters of the SSLGM; the learned model is then used to assess the new\ninpatients' risk for clinical deterioration in an online fashion, allowing for\ntimely ICU admission. Experiments conducted on a het- erogeneous cohort of\n6,094 patients admitted to a large academic medical center show that the\nproposed model significantly outperforms the currently deployed risk scores\nsuch as Rothman index, MEWS, SOFA and APACHE.\n","id":896}
{"Unnamed: 0.1":11897,"Unnamed: 0":11897.0,"anchor":"Validity of Clusters Produced By kernel-$k$-means With Kernel-Trick","positive":"  This paper corrects the proof of the Theorem 2 from the Gower's paper\n\\cite[page 5]{Gower:1982} as well as corrects the Theorem 7 from Gower's paper\n\\cite{Gower:1986}. The first correction is needed in order to establish the\nexistence of the kernel function used commonly in the kernel trick e.g. for\n$k$-means clustering algorithm, on the grounds of distance matrix. The\ncorrection encompasses the missing if-part proof and dropping unnecessary\nconditions. The second correction deals with transformation of the kernel\nmatrix into a one embeddable in Euclidean space.\n","negative":"  Variational inference (VI) provides fast approximations of a Bayesian\nposterior in part because it formulates posterior approximation as an\noptimization problem: to find the closest distribution to the exact posterior\nover some family of distributions. For practical reasons, the family of\ndistributions in VI is usually constrained so that it does not include the\nexact posterior, even as a limit point. Thus, no matter how long VI is run, the\nresulting approximation will not approach the exact posterior. We propose to\ninstead consider a more flexible approximating family consisting of all\npossible finite mixtures of a parametric base distribution (e.g., Gaussian).\nFor efficient inference, we borrow ideas from gradient boosting to develop an\nalgorithm we call boosting variational inference (BVI). BVI iteratively\nimproves the current approximation by mixing it with a new component from the\nbase distribution family and thereby yields progressively more accurate\nposterior approximations as more computing time is spent. Unlike a number of\ncommon VI variants including mean-field VI, BVI is able to capture\nmultimodality, general posterior covariance, and nonstandard posterior shapes.\n","id":897}
{"Unnamed: 0.1":11898,"Unnamed: 0":11898.0,"anchor":"Stochastic Subsampling for Factorizing Huge Matrices","positive":"  We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and\/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.\n","negative":"  We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.\n","id":898}
{"Unnamed: 0.1":11899,"Unnamed: 0":11899.0,"anchor":"Variational Dropout Sparsifies Deep Neural Networks","positive":"  We explore a recently proposed Variational Dropout technique that provided an\nelegant Bayesian interpretation to Gaussian Dropout. We extend Variational\nDropout to the case when dropout rates are unbounded, propose a way to reduce\nthe variance of the gradient estimator and report first experimental results\nwith individual dropout rates per weight. Interestingly, it leads to extremely\nsparse solutions both in fully-connected and convolutional layers. This effect\nis similar to automatic relevance determination effect in empirical Bayes but\nhas a number of advantages. We reduce the number of parameters up to 280 times\non LeNet architectures and up to 68 times on VGG-like networks with a\nnegligible decrease of accuracy.\n","negative":"  The vast majority of theoretical results in machine learning and statistics\nassume that the available training data is a reasonably reliable reflection of\nthe phenomena to be learned or estimated. Similarly, the majority of machine\nlearning and statistical techniques used in practice are brittle to the\npresence of large amounts of biased or malicious data. In this work we consider\ntwo frameworks in which to study estimation, learning, and optimization in the\npresence of significant fractions of arbitrary data.\n  The first framework, list-decodable learning, asks whether it is possible to\nreturn a list of answers, with the guarantee that at least one of them is\naccurate. For example, given a dataset of $n$ points for which an unknown\nsubset of $\\alpha n$ points are drawn from a distribution of interest, and no\nassumptions are made about the remaining $(1-\\alpha)n$ points, is it possible\nto return a list of $\\operatorname{poly}(1\/\\alpha)$ answers, one of which is\ncorrect? The second framework, which we term the semi-verified learning model,\nconsiders the extent to which a small dataset of trusted data (drawn from the\ndistribution in question) can be leveraged to enable the accurate extraction of\ninformation from a much larger but untrusted dataset (of which only an\n$\\alpha$-fraction is drawn from the distribution).\n  We show strong positive results in both settings, and provide an algorithm\nfor robust learning in a very general stochastic optimization setting. This\ngeneral result has immediate implications for robust estimation in a number of\nsettings, including for robustly estimating the mean of distributions with\nbounded second moments, robustly learning mixtures of such distributions, and\nrobustly finding planted partitions in random graphs in which significant\nportions of the graph have been perturbed by an adversary.\n","id":899}
{"Unnamed: 0.1":11900,"Unnamed: 0":11900.0,"anchor":"Learning first-order definable concepts over structures of small degree","positive":"  We consider a declarative framework for machine learning where concepts and\nhypotheses are defined by formulas of a logic over some background structure.\nWe show that within this framework, concepts defined by first-order formulas\nover a background structure of at most polylogarithmic degree can be learned in\npolylogarithmic time in the \"probably approximately correct\" learning sense.\n","negative":"  This paper presents an online learning with regularized kernel based\none-class extreme learning machine (ELM) classifier and is referred as online\nRK-OC-ELM. The baseline kernel hyperplane model considers whole data in a\nsingle chunk with regularized ELM approach for offline learning in case of\none-class classification (OCC). Further, the basic hyper plane model is adapted\nin an online fashion from stream of training samples in this paper. Two\nframeworks viz., boundary and reconstruction are presented to detect the target\nclass in online RKOC-ELM. Boundary framework based one-class classifier\nconsists of single node output architecture and classifier endeavors to\napproximate all data to any real number. However, one-class classifier based on\nreconstruction framework is an autoencoder architecture, where output nodes are\nidentical to input nodes and classifier endeavor to reconstruct input layer at\nthe output layer. Both these frameworks employ regularized kernel ELM based\nonline learning and consistency based model selection has been employed to\nselect learning algorithm parameters. The performance of online RK-OC-ELM has\nbeen evaluated on standard benchmark datasets as well as on artificial datasets\nand the results are compared with existing state-of-the art one-class\nclassifiers. The results indicate that the online learning one-class classifier\nis slightly better or same as batch learning based approaches. As, base\nclassifier used for the proposed classifiers are based on the ELM, hence,\nproposed classifiers would also inherit the benefit of the base classifier i.e.\nit will perform faster computation compared to traditional autoencoder based\none-class classifier.\n","id":900}
{"Unnamed: 0.1":11901,"Unnamed: 0":11901.0,"anchor":"Fisher consistency for prior probability shift","positive":"  We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities.\n","negative":"  We describe a framework to build distances by measuring the tightness of\ninequalities, and introduce the notion of proper statistical divergences and\nimproper pseudo-divergences. We then consider the H\\\"older ordinary and reverse\ninequalities, and present two novel classes of H\\\"older divergences and\npseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz\ndivergence. We report closed-form formulas for those statistical\ndissimilarities when considering distributions belonging to the same\nexponential family provided that the natural parameter space is a cone (e.g.,\nmultivariate Gaussians), or affine (e.g., categorical distributions). Those new\nclasses of H\\\"older distances are invariant to rescaling, and thus do not\nrequire distributions to be normalized. Finally, we show how to compute\nstatistical H\\\"older centroids with respect to those divergences, and carry out\ncenter-based clustering toy experiments on a set of Gaussian distributions that\ndemonstrate empirically that symmetrized H\\\"older divergences outperform the\nsymmetric Cauchy-Schwarz divergence.\n","id":901}
{"Unnamed: 0.1":11902,"Unnamed: 0":11902.0,"anchor":"PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture\n  Likelihood and Other Modifications","positive":"  PixelCNNs are a recently proposed class of powerful generative models with\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\nmake available at https:\/\/github.com\/openai\/pixel-cnn. Our implementation\ncontains a number of modifications to the original model that both simplify its\nstructure and improve its performance. 1) We use a discretized logistic mixture\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\nup training. 2) We condition on whole pixels, rather than R\/G\/B sub-pixels,\nsimplifying the model structure. 3) We use downsampling to efficiently capture\nstructure at multiple resolutions. 4) We introduce additional short-cut\nconnections to further speed up optimization. 5) We regularize the model using\ndropout. Finally, we present state-of-the-art log likelihood results on\nCIFAR-10 to demonstrate the usefulness of these modifications.\n","negative":"  Advances in machine learning (ML) in recent years have enabled a dizzying\narray of applications such as data analytics, autonomous systems, and security\ndiagnostics. ML is now pervasive---new systems and models are being deployed in\nevery domain imaginable, leading to rapid and widespread deployment of software\nbased inference and decision making. There is growing recognition that ML\nexposes new vulnerabilities in software systems, yet the technical community's\nunderstanding of the nature and extent of these vulnerabilities remains\nlimited. We systematize recent findings on ML security and privacy, focusing on\nattacks identified on these systems and defenses crafted to date. We articulate\na comprehensive threat model for ML, and categorize attacks and defenses within\nan adversarial framework. Key insights resulting from works both in the ML and\nsecurity communities are identified and the effectiveness of approaches are\nrelated to structural elements of ML algorithms and the data used to train\nthem. We conclude by formally exploring the opposing relationship between model\naccuracy and resilience to adversarial manipulation. Through these\nexplorations, we show that there are (possibly unavoidable) tensions between\nmodel complexity, accuracy, and resilience that must be calibrated for the\nenvironments in which they will be used.\n","id":902}
{"Unnamed: 0.1":11903,"Unnamed: 0":11903.0,"anchor":"Deep Neural Networks - A Brief History","positive":"  Introduction to deep neural networks and their history.\n","negative":"  The GANs are generative models whose random samples realistically reflect\nnatural images. It also can generate samples with specific attributes by\nconcatenating a condition vector into the input, yet research on this field is\nnot well studied. We propose novel methods of conditioning generative\nadversarial networks (GANs) that achieve state-of-the-art results on MNIST and\nCIFAR-10. We mainly introduce two models: an information retrieving model that\nextracts conditional information from the samples, and a spatial bilinear\npooling model that forms bilinear features derived from the spatial cross\nproduct of an image and a condition vector. These methods significantly enhance\nlog-likelihood of test data under the conditional distributions compared to the\nmethods of concatenation.\n","id":903}
{"Unnamed: 0.1":11904,"Unnamed: 0":11904.0,"anchor":"Poisson--Gamma Dynamical Systems","positive":"  We introduce a new dynamical system for sequentially observed multivariate\ncount data. This model is based on the gamma--Poisson construction---a natural\nchoice for count data---and relies on a novel Bayesian nonparametric prior that\nties and shrinks the model parameters, thus avoiding overfitting. We present an\nefficient MCMC inference algorithm that advances recent work on augmentation\nschemes for inference in negative binomial models. Finally, we demonstrate the\nmodel's inductive bias using a variety of real-world data sets, showing that it\nexhibits superior predictive performance over other models and infers highly\ninterpretable latent structure.\n","negative":"  The fifth Dialog State Tracking Challenge (DSTC5) introduces a new\ncross-language dialog state tracking scenario, where the participants are asked\nto build their trackers based on the English training corpus, while evaluating\nthem with the unlabeled Chinese corpus. Although the computer-generated\ntranslations for both English and Chinese corpus are provided in the dataset,\nthese translations contain errors and careless use of them can easily hurt the\nperformance of the built trackers. To address this problem, we propose a\nmultichannel Convolutional Neural Networks (CNN) architecture, in which we\ntreat English and Chinese language as different input channels of one single\nCNN model. In the evaluation of DSTC5, we found that such multichannel\narchitecture can effectively improve the robustness against translation errors.\nAdditionally, our method for DSTC5 is purely machine learning based and\nrequires no prior knowledge about the target language. We consider this a\ndesirable property for building a tracker in the cross-language context, as not\nevery developer will be familiar with both languages.\n","id":904}
{"Unnamed: 0.1":11905,"Unnamed: 0":11905.0,"anchor":"Rare Disease Physician Targeting: A Factor Graph Approach","positive":"  In rare disease physician targeting, a major challenge is how to identify\nphysicians who are treating diagnosed or underdiagnosed rare diseases patients.\nRare diseases have extremely low incidence rate. For a specified rare disease,\nonly a small number of patients are affected and a fractional of physicians are\ninvolved. The existing targeting methodologies, such as segmentation and\nprofiling, are developed under mass market assumption. They are not suitable\nfor rare disease market where the target classes are extremely imbalanced. The\nauthors propose a graphical model approach to predict targets by jointly\nmodeling physician and patient features from different data spaces and\nutilizing the extra relational information. Through an empirical example with\nmedical claim and prescription data, the proposed approach demonstrates better\naccuracy in finding target physicians. The graph representation also provides\nvisual interpretability of relationship among physicians and patients. The\nmodel can be extended to incorporate more complex dependency structures. This\narticle contributes to the literature of exploring the benefit of utilizing\nrelational dependencies among entities in healthcare industry.\n","negative":"  Sum-product networks have recently emerged as an attractive representation\ndue to their dual view as a special type of deep neural network with clear\nsemantics and a special type of probabilistic graphical model for which\ninference is always tractable. Those properties follow from some conditions\n(i.e., completeness and decomposability) that must be respected by the\nstructure of the network. As a result, it is not easy to specify a valid\nsum-product network by hand and therefore structure learning techniques are\ntypically used in practice. This paper describes the first online structure\nlearning technique for continuous SPNs with Gaussian leaves. We also introduce\nan accompanying new parameter learning technique.\n","id":905}
{"Unnamed: 0.1":11906,"Unnamed: 0":11906.0,"anchor":"Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete\n  Source Code Fragments","positive":"  Program authorship attribution has implications for the privacy of\nprogrammers who wish to contribute code anonymously. While previous work has\nshown that complete files that are individually authored can be attributed, we\nshow here for the first time that accounts belonging to open source\ncontributors containing short, incomplete, and typically uncompilable fragments\ncan also be effectively attributed.\n  We propose a technique for authorship attribution of contributor accounts\ncontaining small source code samples, such as those that can be obtained from\nversion control systems or other direct comparison of sequential versions. We\nshow that while application of previous methods to individual small source code\nsamples yields an accuracy of about 73% for 106 programmers as a baseline, by\nensembling and averaging the classification probabilities of a sufficiently\nlarge set of samples belonging to the same author we achieve 99% accuracy for\nassigning the set of samples to the correct author. Through these results, we\ndemonstrate that attribution is an important threat to privacy for programmers\neven in real-world collaborative environments such as GitHub. Additionally, we\npropose the use of calibration curves to identify samples by unknown and\npreviously unencountered authors in the open world setting. We show that we can\nalso use these calibration curves in the case that we do not have linking\ninformation and thus are forced to classify individual samples directly. This\nis because the calibration curves allow us to identify which samples are more\nlikely to have been correctly attributed. Using such a curve can help an\nanalyst choose a cut-off point which will prevent most misclassifications, at\nthe cost of causing the rejection of some of the more dubious correct\nattributions.\n","negative":"  In this paper, we investigate the effectiveness of unsupervised feature\nlearning techniques in predicting user engagement on social media.\nSpecifically, we compare two methods to predict the number of feedbacks (i.e.,\ncomments) that a blog post is likely to receive. We compare Principal Component\nAnalysis (PCA) and sparse Autoencoder to a baseline method where the data are\nonly centered and scaled, on each of two models: Linear Regression and\nRegression Tree. We find that unsupervised learning techniques significantly\nimprove the prediction accuracy on both models. For the Linear Regression\nmodel, sparse Autoencoder achieves the best result, with an improvement in the\nroot mean squared error (RMSE) on the test set of 42% over the baseline method.\nFor the Regression Tree model, PCA achieves the best result, with an\nimprovement in RMSE of 15% over the baseline.\n","id":906}
{"Unnamed: 0.1":11907,"Unnamed: 0":11907.0,"anchor":"Real-time Traffic Accident Risk Prediction based on Frequent Pattern\n  Tree","positive":"  Traffic accident data are usually noisy, contain missing values, and\nheterogeneous. How to select the most important variables to improve real-time\ntraffic accident risk prediction has become a concern of many recent studies.\nThis paper proposes a novel variable selection method based on the Frequent\nPattern tree (FP tree) algorithm. First, all the frequent patterns in the\ntraffic accident dataset are discovered. Then for each frequent pattern, a new\ncriterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is\ncalculated. This ROPR is added to the importance score of the variables that\ndifferentiate one frequent pattern from the others. To test the proposed\nmethod, a dataset was compiled from the traffic accidents records detected by\nonly one detector on interstate highway I-64 in Virginia in 2005. This dataset\nwas then linked to other variables such as real-time traffic information and\nweather conditions. Both the proposed method based on the FP tree algorithm, as\nwell as the widely utilized, random forest method, were then used to identify\nthe important variables or the Virginia dataset. The results indicate that\nthere are some differences between the variables deemed important by the FP\ntree and those selected by the random forest method. Following this, two\nbaseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network)\nwere developed to predict accident risk based on the variables identified by\nboth the FP tree method and the random forest method. The results show that the\nmodels based on the variable selection using the FP tree performed better than\nthose based on the random forest method for several versions of the k-NN and\nBayesian network models.The best results were derived from a Bayesian network\nmodel using variables from FP tree. That model could predict 61.11% of\naccidents accurately while having a false alarm rate of 38.16%.\n","negative":"  In this work, we tackle the problem of hidden community detection. We\nconsider Belief Propagation (BP) applied to the problem of detecting a hidden\nErd\\H{o}s-R\\'enyi (ER) graph embedded in a larger and sparser ER graph, in the\npresence of side-information. We derive two related algorithms based on BP to\nperform subgraph detection in the presence of two kinds of side-information.\nThe first variant of side-information consists of a set of nodes, called cues,\nknown to be from the subgraph. The second variant of side-information consists\nof a set of nodes that are cues with a given probability. It was shown in past\nworks that BP without side-information fails to detect the subgraph correctly\nwhen an effective signal-to-noise ratio (SNR) parameter falls below a\nthreshold. In contrast, in the presence of non-trivial side-information, we\nshow that the BP algorithm achieves asymptotically zero error for any value of\nthe SNR parameter. We validate our results through simulations on synthetic\ndatasets as well as on a few real world networks.\n","id":907}
{"Unnamed: 0.1":11908,"Unnamed: 0":11908.0,"anchor":"Empirical Study of Drone Sound Detection in Real-Life Environment with\n  Deep Neural Networks","positive":"  This work aims to investigate the use of deep neural network to detect\ncommercial hobby drones in real-life environments by analyzing their sound\ndata. The purpose of work is to contribute to a system for detecting drones\nused for malicious purposes, such as for terrorism. Specifically, we present a\nmethod capable of detecting the presence of commercial hobby drones as a binary\nclassification problem based on sound event detection. We recorded the sound\nproduced by a few popular commercial hobby drones, and then augmented this data\nwith diverse environmental sound data to remedy the scarcity of drone sound\ndata in diverse environments. We investigated the effectiveness of\nstate-of-the-art event sound classification methods, i.e., a Gaussian Mixture\nModel (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network\n(RNN), for drone sound detection. Our empirical results, which were obtained\nwith a testing dataset collected on an urban street, confirmed the\neffectiveness of these models for operating in a real environment. In summary,\nour RNN models showed the best detection performance with an F-Score of 0.8009\nwith 240 ms of input audio with a short processing time, indicating their\napplicability to real-time detection systems.\n","negative":"  Google's Machine Learning framework TensorFlow was open-sourced in November\n2015 [1] and has since built a growing community around it. TensorFlow is\nsupposed to be flexible for research purposes while also allowing its models to\nbe deployed productively. This work is aimed towards people with experience in\nMachine Learning considering whether they should use TensorFlow in their\nenvironment. Several aspects of the framework important for such a decision are\nexamined, such as the heterogenity, extensibility and its computation graph. A\npure Python implementation of linear classification is compared with an\nimplementation utilizing TensorFlow. I also contrast TensorFlow to other\npopular frameworks with respect to modeling capability, deployment and\nperformance and give a brief description of the current adaption of the\nframework.\n","id":908}
{"Unnamed: 0.1":11909,"Unnamed: 0":11909.0,"anchor":"Disentangling group and link persistence in Dynamic Stochastic Block\n  models","positive":"  We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference.\n","negative":"  We derive bounds on the sample complexity of empirical risk minimization\n(ERM) in the context of minimizing non-convex risks that admit the strict\nsaddle property. Recent progress in non-convex optimization has yielded\nefficient algorithms for minimizing such functions. Our results imply that\nthese efficient algorithms are statistically stable and also generalize well.\nIn particular, we derive fast rates which resemble the bounds that are often\nattained in the strongly convex setting. We specify our bounds to Principal\nComponent Analysis and Independent Component Analysis. Our results and\ntechniques may pave the way for statistical analyses of additional strict\nsaddle problems.\n","id":909}
{"Unnamed: 0.1":11910,"Unnamed: 0":11910.0,"anchor":"Neural Offset Min-Sum Decoding","positive":"  Recently, it was shown that if multiplicative weights are assigned to the\nedges of a Tanner graph used in belief propagation decoding, it is possible to\nuse deep learning techniques to find values for the weights which improve the\nerror-correction performance of the decoder. Unfortunately, this approach\nrequires many multiplications, which are generally expensive operations. In\nthis paper, we suggest a more hardware-friendly approach in which offset\nmin-sum decoding is augmented with learnable offset parameters. Our method uses\nno multiplications and has a parameter count less than half that of the\nmultiplicative algorithm. This both speeds up training and provides a feasible\npath to hardware architectures. After describing our method, we compare the\nperformance of the two neural decoding algorithms and show that our method\nachieves error-correction performance within 0.1 dB of the multiplicative\napproach and as much as 1 dB better than traditional belief propagation for the\ncodes under consideration.\n","negative":"  This paper describes our approach to the Bosch production line performance\nchallenge run by Kaggle.com. Maximizing the production yield is at the heart of\nthe manufacturing industry. At the Bosch assembly line, data is recorded for\nproducts as they progress through each stage. Data science methods are applied\nto this huge data repository consisting records of tests and measurements made\nfor each component along the assembly line to predict internal failures. We\nfound that it is possible to train a model that predicts which parts are most\nlikely to fail. Thus a smarter failure detection system can be built and the\nparts tagged likely to fail can be salvaged to decrease operating costs and\nincrease the profit margins.\n","id":910}
{"Unnamed: 0.1":11911,"Unnamed: 0":11911.0,"anchor":"Learning Policies for Markov Decision Processes from Data","positive":"  We consider the problem of learning a policy for a Markov decision process\nconsistent with data captured on the state-actions pairs followed by the\npolicy. We assume that the policy belongs to a class of parameterized policies\nwhich are defined using features associated with the state-action pairs. The\nfeatures are known a priori, however, only an unknown subset of them could be\nrelevant. The policy parameters that correspond to an observed target policy\nare recovered using $\\ell_1$-regularized logistic regression that best fits the\nobserved state-action samples. We establish bounds on the difference between\nthe average reward of the estimated and the original policy (regret) in terms\nof the generalization error and the ergodic coefficient of the underlying\nMarkov chain. To that end, we combine sample complexity theory and sensitivity\nanalysis of the stationary distribution of Markov chains. Our analysis suggests\nthat to achieve regret within order $O(\\sqrt{\\epsilon})$, it suffices to use\ntraining sample size on the order of $\\Omega(\\log n \\cdot poly(1\/\\epsilon))$,\nwhere $n$ is the number of the features. We demonstrate the effectiveness of\nour method on a synthetic robot navigation example.\n","negative":"  In this paper, we address learning problems for high dimensional data.\nPreviously, oblivious random projection based approaches that project high\ndimensional features onto a random subspace have been used in practice for\ntackling high-dimensionality challenge in machine learning. Recently, various\nnon-oblivious randomized reduction methods have been developed and deployed for\nsolving many numerical problems such as matrix product approximation, low-rank\nmatrix approximation, etc. However, they are less explored for the machine\nlearning tasks, e.g., classification. More seriously, the theoretical analysis\nof excess risk bounds for risk minimization, an important measure of\ngeneralization performance, has not been established for non-oblivious\nrandomized reduction methods. It therefore remains an open problem what is the\nbenefit of using them over previous oblivious random projection based\napproaches. To tackle these challenges, we propose an algorithmic framework for\nemploying non-oblivious randomized reduction method for general empirical risk\nminimizing in machine learning tasks, where the original high-dimensional\nfeatures are projected onto a random subspace that is derived from the data\nwith a small matrix approximation error. We then derive the first excess risk\nbound for the proposed non-oblivious randomized reduction approach without\nrequiring strong assumptions on the training data. The established excess risk\nbound exhibits that the proposed approach provides much better generalization\nperformance and it also sheds more insights about different randomized\nreduction approaches. Finally, we conduct extensive experiments on both\nsynthetic and real-world benchmark datasets, whose dimension scales to\n$O(10^7)$, to demonstrate the efficacy of our proposed approach.\n","id":911}
{"Unnamed: 0.1":11912,"Unnamed: 0":11912.0,"anchor":"Label Propagation on K-partite Graphs with Heterophily","positive":"  In this paper, for the first time, we study label propagation in\nheterogeneous graphs under heterophily assumption. Homophily label propagation\n(i.e., two connected nodes share similar labels) in homogeneous graph (with\nsame types of vertices and relations) has been extensively studied before.\nUnfortunately, real-life networks are heterogeneous, they contain different\ntypes of vertices (e.g., users, images, texts) and relations (e.g.,\nfriendships, co-tagging) and allow for each node to propagate both the same and\nopposite copy of labels to its neighbors. We propose a $\\mathcal{K}$-partite\nlabel propagation model to handle the mystifying combination of heterogeneous\nnodes\/relations and heterophily propagation. With this model, we develop a\nnovel label inference algorithm framework with update rules in near-linear time\ncomplexity. Since real networks change over time, we devise an incremental\napproach, which supports fast updates for both new data and evidence (e.g.,\nground truth labels) with guaranteed efficiency. We further provide a utility\nfunction to automatically determine whether an incremental or a re-modeling\napproach is favored. Extensive experiments on real datasets have verified the\neffectiveness and efficiency of our approach, and its superiority over the\nstate-of-the-art label propagation methods.\n","negative":"  Reasoning about objects, relations, and physics is central to human\nintelligence, and a key goal of artificial intelligence. Here we introduce the\ninteraction network, a model which can reason about how objects in complex\nsystems interact, supporting dynamical predictions, as well as inferences about\nthe abstract properties of the system. Our model takes graphs as input,\nperforms object- and relation-centric reasoning in a way that is analogous to a\nsimulation, and is implemented using deep neural networks. We evaluate its\nability to reason about several challenging physical domains: n-body problems,\nrigid-body collision, and non-rigid dynamics. Our results show it can be\ntrained to accurately simulate the physical trajectories of dozens of objects\nover thousands of time steps, estimate abstract quantities such as energy, and\ngeneralize automatically to systems with different numbers and configurations\nof objects and relations. Our interaction network implementation is the first\ngeneral-purpose, learnable physics engine, and a powerful general framework for\nreasoning about object and relations in a wide variety of complex real-world\ndomains.\n","id":912}
{"Unnamed: 0.1":11913,"Unnamed: 0":11913.0,"anchor":"Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive\n  Patterns in Vowel Acoustics","positive":"  Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system.\n","negative":"  We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X.\n","id":913}
{"Unnamed: 0.1":11914,"Unnamed: 0":11914.0,"anchor":"Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World","positive":"  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1\/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n","negative":"  We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an\nend-to-end deep learning framework, for early classification of time series\ndata. Unlike most existing methods for early classification of time series\ndata, that are designed to solve this problem under the assumption of the\navailability of a good set of pre-defined (often hand-crafted) features, our\nframework can jointly perform feature learning (by learning a deep hierarchy of\n\\emph{shapelets} capturing the salient characteristics in each time series),\nalong with a dynamic truncation model to help our deep feature learning\narchitecture focus on the early parts of each time series. Consequently, our\nframework is able to make highly reliable early predictions, outperforming\nvarious state-of-the-art methods for early time series classification, while\nalso being competitive when compared to the state-of-the-art time series\nclassification algorithms that work with \\emph{fully observed} time series\ndata. To the best of our knowledge, the proposed framework is the first to\nperform data-driven (deep) feature learning in the context of early\nclassification of time series data. We perform a comprehensive set of\nexperiments, on several benchmark data sets, which demonstrate that our method\nyields significantly better predictions than various state-of-the-art methods\ndesigned for early time series classification. In addition to obtaining high\naccuracies, our experiments also show that the learned deep shapelets based\nfeatures are also highly interpretable and can help gain better understanding\nof the underlying characteristics of time series data.\n","id":914}
{"Unnamed: 0.1":11915,"Unnamed: 0":11915.0,"anchor":"Effective and Extensible Feature Extraction Method Using Genetic\n  Algorithm-Based Frequency-Domain Feature Search for Epileptic EEG\n  Multi-classification","positive":"  In this paper, a genetic algorithm-based frequency-domain feature search\n(GAFDS) method is proposed for the electroencephalogram (EEG) analysis of\nepilepsy. In this method, frequency-domain features are first searched and then\ncombined with nonlinear features. Subsequently, these features are selected and\noptimized to classify EEG signals. The extracted features are analyzed\nexperimentally. The features extracted by GAFDS show remarkable independence,\nand they are superior to the nonlinear features in terms of the ratio of\ninter-class distance and intra-class distance. Moreover, the proposed feature\nsearch method can additionally search for features of instantaneous frequency\nin a signal after Hilbert transformation. The classification results achieved\nusing these features are reasonable, thus, GAFDS exhibits good extensibility.\nMultiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant\nanalysis, decision tree, AdaBoost, multilayer perceptron, and Na\\\"ive Bayes)\nachieve good results by using the features generated by GAFDS method and the\noptimized selection. Specifically, the accuracies for the two-classification\nand three-classification problems may reach up to 99% and 97%, respectively.\nResults of several cross-validation experiments illustrate that GAFDS is\neffective in feature extraction for EEG classification. Therefore, the proposed\nfeature selection and optimization model can improve classification accuracy.\n","negative":"  When an agent cannot represent a perfectly accurate model of its\nenvironment's dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model's ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.\n","id":915}
{"Unnamed: 0.1":11916,"Unnamed: 0":11916.0,"anchor":"Optimization on Product Submanifolds of Convolution Kernels","positive":"  Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints.\n","negative":"  The National Basketball Association(NBA) has expanded their data gathering\nand have heavily invested in new technologies to gather advanced performance\nmetrics on players. This expanded data set allows analysts to use unique\nperformance metrics in models to estimate and classify player performance.\nInstead of grouping players together based on physical attributes and positions\nplayed, analysts can group together players that play similar to each other\nbased on these tracked metrics. Existing methods for player classification have\ntypically used offensive metrics for clustering [1]. There have been attempts\nto classify players using past defensive metrics, but the lack of quality\nmetrics has not produced promising results. The classifications presented in\nthe paper use newly introduced defensive metrics to find different defensive\npositions for each player. Without knowing the number of categories that\nplayers can be cast into, Gaussian Mixture Models (GMM) can be applied to find\nthe optimal number of clusters. In the model presented, five different\ndefensive player types can be identified.\n","id":916}
{"Unnamed: 0.1":11917,"Unnamed: 0":11917.0,"anchor":"Predicting Demographics of High-Resolution Geographies with Geotagged\n  Tweets","positive":"  In this paper, we consider the problem of predicting demographics of\ngeographic units given geotagged Tweets that are composed within these units.\nTraditional survey methods that offer demographics estimates are usually\nlimited in terms of geographic resolution, geographic boundaries, and time\nintervals. Thus, it would be highly useful to develop computational methods\nthat can complement traditional survey methods by offering demographics\nestimates at finer geographic resolutions, with flexible geographic boundaries\n(i.e. not confined to administrative boundaries), and at different time\nintervals. While prior work has focused on predicting demographics and health\nstatistics at relatively coarse geographic resolutions such as the county-level\nor state-level, we introduce an approach to predict demographics at finer\ngeographic resolutions such as the blockgroup-level. For the task of predicting\ngender and race\/ethnicity counts at the blockgroup-level, an approach adapted\nfrom prior work to our problem achieves an average correlation of 0.389\n(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms\nthis prior approach with an average correlation of 0.671 (gender) and 0.692\n(race).\n","negative":"  Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models\/Gaussian Mixture Models\n(HMMs\/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information.\n","id":917}
{"Unnamed: 0.1":11918,"Unnamed: 0":11918.0,"anchor":"What the Language You Tweet Says About Your Occupation","positive":"  Many aspects of people's lives are proven to be deeply connected to their\njobs. In this paper, we first investigate the distinct characteristics of major\noccupation categories based on tweets. From multiple social media platforms, we\ngather several types of user information. From users' LinkedIn webpages, we\nlearn their proficiencies. To overcome the ambiguity of self-reported\ninformation, a soft clustering approach is applied to extract occupations from\ncrowd-sourced data. Eight job categories are extracted, including Marketing,\nAdministrator, Start-up, Editor, Software Engineer, Public Relation, Office\nClerk, and Designer. Meanwhile, users' posts on Twitter provide cues for\nunderstanding their linguistic styles, interests, and personalities. Our\nresults suggest that people of different jobs have unique tendencies in certain\nlanguage styles and interests. Our results also clearly reveal distinctive\nlevels in terms of Big Five Traits for different jobs. Finally, a classifier is\nbuilt to predict job types based on the features extracted from tweets. A high\naccuracy indicates a strong discrimination power of language features for job\nprediction task.\n","negative":"  Machine learning (ML) algorithms have been employed in the problem of\nclassifying signal and background events with high accuracy in particle\nphysics. In this paper, we compare the performance of a widespread ML\ntechnique, namely, \\emph{stacked generalization}, against the results of two\nstate-of-art algorithms: (1) a deep neural network (DNN) in the task of\ndiscovering a new neutral Higgs boson and (2) a scalable machine learning\nsystem for tree boosting, in the Standard Model Higgs to tau leptons channel,\nboth at the 8 TeV LHC. In a cut-and-count analysis, \\emph{stacking} three\nalgorithms performed around 16\\% worse than DNN but demanding far less\ncomputation efforts, however, the same \\emph{stacking} outperforms boosted\ndecision trees. Using the stacked classifiers in a multivariate statistical\nanalysis (MVA), on the other hand, significantly enhances the statistical\nsignificance compared to cut-and-count in both Higgs processes, suggesting that\ncombining an ensemble of simpler and faster ML algorithms with MVA tools is a\nbetter approach than building a complex state-of-art algorithm for\ncut-and-count.\n","id":918}
{"Unnamed: 0.1":11919,"Unnamed: 0":11919.0,"anchor":"A Multichannel Convolutional Neural Network For Cross-language Dialog\n  State Tracking","positive":"  The fifth Dialog State Tracking Challenge (DSTC5) introduces a new\ncross-language dialog state tracking scenario, where the participants are asked\nto build their trackers based on the English training corpus, while evaluating\nthem with the unlabeled Chinese corpus. Although the computer-generated\ntranslations for both English and Chinese corpus are provided in the dataset,\nthese translations contain errors and careless use of them can easily hurt the\nperformance of the built trackers. To address this problem, we propose a\nmultichannel Convolutional Neural Networks (CNN) architecture, in which we\ntreat English and Chinese language as different input channels of one single\nCNN model. In the evaluation of DSTC5, we found that such multichannel\narchitecture can effectively improve the robustness against translation errors.\nAdditionally, our method for DSTC5 is purely machine learning based and\nrequires no prior knowledge about the target language. We consider this a\ndesirable property for building a tracker in the cross-language context, as not\nevery developer will be familiar with both languages.\n","negative":"  The recent successful deep neural networks are largely trained in a\nsupervised manner. It {\\it associates} complex patterns of input samples with\nneurons in the last layer, which form representations of {\\it concepts}. In\nspite of their successes, the properties of complex patterns associated a\nlearned concept remain elusive. In this work, by analyzing how neurons are\nassociated with concepts in supervised networks, we hypothesize that with\nproper priors to regulate learning, neural networks can automatically associate\nneurons in the intermediate layers with concepts that are aligned with real\nworld concepts, when trained only with labels that associate concepts with top\nlevel neurons, which is a plausible way for unsupervised learning. We develop a\nprior to verify the hypothesis and experimentally find the proposed prior help\nneural networks automatically learn both basic physical concepts at the lower\nlayers, e.g., rotation of filters, and highly semantic concepts at the higher\nlayers, e.g., fine-grained categories of an entry-level category.\n","id":919}
{"Unnamed: 0.1":11920,"Unnamed: 0":11920.0,"anchor":"dna2vec: Consistent vector representations of variable-length k-mers","positive":"  One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors.\n","negative":"  The superconducting LHC magnets are coupled with an electronic monitoring\nsystem which records and analyses voltage time series reflecting their\nperformance. A currently used system is based on a range of preprogrammed\ntriggers which launches protection procedures when a misbehavior of the magnets\nis detected. All the procedures used in the protection equipment were designed\nand implemented according to known working scenarios of the system and are\nupdated and monitored by human operators.\n  This paper proposes a novel approach to monitoring and fault protection of\nthe Large Hadron Collider (LHC) superconducting magnets which employs\nstate-of-the-art Deep Learning algorithms. Consequently, the authors of the\npaper decided to examine the performance of LSTM recurrent neural networks for\nmodeling of voltage time series of the magnets. In order to address this\nchallenging task different network architectures and hyper-parameters were used\nto achieve the best possible performance of the solution. The regression\nresults were measured in terms of RMSE for different number of future steps and\nhistory length taken into account for the prediction. The best result of\nRMSE=0.00104 was obtained for a network of 128 LSTM cells within the internal\nlayer and 16 steps history buffer.\n","id":920}
{"Unnamed: 0.1":11921,"Unnamed: 0":11921.0,"anchor":"Comparative study on supervised learning methods for identifying\n  phytoplankton species","positive":"  Phytoplankton plays an important role in marine ecosystem. It is defined as a\nbiological factor to assess marine quality. The identification of phytoplankton\nspecies has a high potential for monitoring environmental, climate changes and\nfor evaluating water quality. However, phytoplankton species identification is\nnot an easy task owing to their variability and ambiguity due to thousands of\nmicro and pico-plankton species. Therefore, the aim of this paper is to build a\nframework for identifying phytoplankton species and to perform a comparison on\ndifferent features types and classifiers. We propose a new features type\nextracted from raw signals of phytoplankton species. We then analyze the\nperformance of various classifiers on the proposed features type as well as two\nother features types for finding the robust one. Through experiments, it is\nfound that Random Forest using the proposed features gives the best\nclassification results with average accuracy up to 98.24%.\n","negative":"  This paper presents a new approach for Gaussian process (GP) regression for\nlarge datasets. The approach involves partitioning the regression input domain\ninto multiple local regions with a different local GP model fitted in each\nregion. Unlike existing local partitioned GP approaches, we introduce a\ntechnique for patching together the local GP models nearly seamlessly to ensure\nthat the local GP models for two neighboring regions produce nearly the same\nresponse prediction and prediction error variance on the boundary between the\ntwo regions. This largely mitigates the well-known discontinuity problem that\ndegrades the boundary accuracy of existing local partitioned GP methods. Our\nmain innovation is to represent the continuity conditions as additional\npseudo-observations that the differences between neighboring GP responses are\nidentically zero at an appropriately chosen set of boundary input locations. To\npredict the response at any input location, we simply augment the actual\nresponse observations with the pseudo-observations and apply standard GP\nprediction methods to the augmented data. In contrast to heuristic continuity\nadjustments, this has an advantage of working within a formal GP framework, so\nthat the GP-based predictive uncertainty quantification remains valid. Our\napproach also inherits a sparse block-like structure for the sample covariance\nmatrix, which results in computationally efficient closed-form expressions for\nthe predictive mean and variance. In addition, we provide a new spatial\npartitioning scheme based on a recursive space partitioning along local\nprincipal component directions, which makes the proposed approach applicable\nfor regression domains having more than two dimensions. Using three spatial\ndatasets and three higher dimensional datasets, we investigate the numerical\nperformance of the approach and compare it to several state-of-the-art\napproaches.\n","id":921}
{"Unnamed: 0.1":11922,"Unnamed: 0":11922.0,"anchor":"Learning what to look in chest X-rays with a recurrent visual attention\n  model","positive":"  X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies.\n","negative":"  In this paper, a genetic algorithm-based frequency-domain feature search\n(GAFDS) method is proposed for the electroencephalogram (EEG) analysis of\nepilepsy. In this method, frequency-domain features are first searched and then\ncombined with nonlinear features. Subsequently, these features are selected and\noptimized to classify EEG signals. The extracted features are analyzed\nexperimentally. The features extracted by GAFDS show remarkable independence,\nand they are superior to the nonlinear features in terms of the ratio of\ninter-class distance and intra-class distance. Moreover, the proposed feature\nsearch method can additionally search for features of instantaneous frequency\nin a signal after Hilbert transformation. The classification results achieved\nusing these features are reasonable, thus, GAFDS exhibits good extensibility.\nMultiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant\nanalysis, decision tree, AdaBoost, multilayer perceptron, and Na\\\"ive Bayes)\nachieve good results by using the features generated by GAFDS method and the\noptimized selection. Specifically, the accuracies for the two-classification\nand three-classification problems may reach up to 99% and 97%, respectively.\nResults of several cross-validation experiments illustrate that GAFDS is\neffective in feature extraction for EEG classification. Therefore, the proposed\nfeature selection and optimization model can improve classification accuracy.\n","id":922}
{"Unnamed: 0.1":11923,"Unnamed: 0":11923.0,"anchor":"Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification","positive":"  We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches.\n","negative":"  Understanding how brain functions has been an intriguing topic for years.\nWith the recent progress on collecting massive data and developing advanced\ntechnology, people have become interested in addressing the challenge of\ndecoding brain wave data into meaningful mind states, with many machine\nlearning models and algorithms being revisited and developed, especially the\nones that handle time series data because of the nature of brain waves.\nHowever, many of these time series models, like HMM with hidden state in\ndiscrete space or State Space Model with hidden state in continuous space, only\nwork with one source of data and cannot handle different sources of information\nsimultaneously. In this paper, we propose an extension of State Space Model to\nwork with different sources of information together with its learning and\ninference algorithms. We apply this model to decode the mind state of students\nduring lectures based on their brain waves and reach a significant better\nresults compared to traditional methods.\n","id":923}
{"Unnamed: 0.1":11924,"Unnamed: 0":11924.0,"anchor":"ENIGMA: Efficient Learning-based Inference Guiding Machine","positive":"  ENIGMA is a learning-based method for guiding given clause selection in\nsaturation-based theorem provers. Clauses from many proof searches are\nclassified as positive and negative based on their participation in the proofs.\nAn efficient classification model is trained on this data, using fast\nfeature-based characterization of the clauses . The learned model is then\ntightly linked with the core prover and used as a basis of a new parameterized\nevaluation heuristic that provides fast ranking of all generated clauses. The\napproach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing\na large increase of E's performance.\n","negative":"  Data mining information about people is becoming increasingly important in\nthe data-driven society of the 21st century. Unfortunately, sometimes there are\nreal-world considerations that conflict with the goals of data mining;\nsometimes the privacy of the people being data mined needs to be considered.\nThis necessitates that the output of data mining algorithms be modified to\npreserve privacy while simultaneously not ruining the predictive power of the\noutputted model. Differential privacy is a strong, enforceable definition of\nprivacy that can be used in data mining algorithms, guaranteeing that nothing\nwill be learned about the people in the data that could not already be\ndiscovered without their participation. In this survey, we focus on one\nparticular data mining algorithm -- decision trees -- and how differential\nprivacy interacts with each of the components that constitute decision tree\nalgorithms. We analyze both greedy and random decision trees, and the conflicts\nthat arise when trying to balance privacy requirements with the accuracy of the\nmodel.\n","id":924}
{"Unnamed: 0.1":11925,"Unnamed: 0":11925.0,"anchor":"Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer","positive":"  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n","negative":"  We consider the problem of learning a policy for a Markov decision process\nconsistent with data captured on the state-actions pairs followed by the\npolicy. We assume that the policy belongs to a class of parameterized policies\nwhich are defined using features associated with the state-action pairs. The\nfeatures are known a priori, however, only an unknown subset of them could be\nrelevant. The policy parameters that correspond to an observed target policy\nare recovered using $\\ell_1$-regularized logistic regression that best fits the\nobserved state-action samples. We establish bounds on the difference between\nthe average reward of the estimated and the original policy (regret) in terms\nof the generalization error and the ergodic coefficient of the underlying\nMarkov chain. To that end, we combine sample complexity theory and sensitivity\nanalysis of the stationary distribution of Markov chains. Our analysis suggests\nthat to achieve regret within order $O(\\sqrt{\\epsilon})$, it suffices to use\ntraining sample size on the order of $\\Omega(\\log n \\cdot poly(1\/\\epsilon))$,\nwhere $n$ is the number of the features. We demonstrate the effectiveness of\nour method on a synthetic robot navigation example.\n","id":925}
{"Unnamed: 0.1":11926,"Unnamed: 0":11926.0,"anchor":"Regularizing Neural Networks by Penalizing Confident Output\n  Distributions","positive":"  We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers.\n","negative":"  Reconstruction of structure and parameters of an Ising model from binary\nsamples is a problem of practical importance in a variety of disciplines,\nranging from statistical physics and computational biology to image processing\nand machine learning. The focus of the research community shifted towards\ndeveloping universal reconstruction algorithms which are both computationally\nefficient and require the minimal amount of expensive data. We introduce a new\nmethod, Interaction Screening, which accurately estimates the model parameters\nusing local optimization problems. The algorithm provably achieves perfect\ngraph structure recovery with an information-theoretically optimal number of\nsamples, notably in the low-temperature regime which is known to be the hardest\nfor learning. The efficacy of Interaction Screening is assessed through\nextensive numerical tests on synthetic Ising models of various topologies with\ndifferent types of interactions, as well as on a real data produced by a D-Wave\nquantum computer. This study shows that the Interaction Screening method is an\nexact, tractable and optimal technique universally solving the inverse Ising\nproblem.\n","id":926}
{"Unnamed: 0.1":11927,"Unnamed: 0":11927.0,"anchor":"On the Parametric Study of Lubricating Oil Production using an\n  Artificial Neural Network (ANN) Approach","positive":"  In this study, an Artificial Neural Network (ANN) approach is utilized to\nperform a parametric study on the process of extraction of lubricants from\nheavy petroleum cuts. To train the model, we used field data collected from an\nindustrial plant. Operational conditions of feed and solvent flow rate,\nTemperature of streams and mixing rate were considered as the input to the\nmodel, whereas the flow rate of the main product was considered as the output\nof the ANN model. A feed-forward Multi-Layer Perceptron Neural Network was\nsuccessfully applied to capture the relationship between inputs and output\nparameters.\n","negative":"  In recent years, Deep Learning (DL) has found great success in domains such\nas multimedia understanding. However, the complex nature of multimedia data\nmakes it difficult to develop DL-based software. The state-of-the art tools,\nsuch as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their\napplicable domains, are programming libraries with fixed user interface,\ninternal representation, and execution environment. This makes it difficult to\nimplement portable and customized DL applications.\n  In this paper, we present DeepDSL, a domain specific language (DSL) embedded\nin Scala, that compiles deep networks written in DeepDSL to Java source code.\nDeep DSL provides (1) intuitive constructs to support compact encoding of deep\nnetworks; (2) symbolic gradient derivation of the networks; (3) static analysis\nfor memory consumption and error detection; and (4) DSL-level optimization to\nimprove memory and runtime efficiency.\n  DeepDSL programs are compiled into compact, efficient, customizable, and\nportable Java source code, which operates the CUDA and CUDNN interfaces running\non Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL\nwith a number of popular DL networks. Our experiments show that the compiled\nprograms have very competitive runtime performance and memory efficiency\ncompared to the existing libraries.\n","id":927}
{"Unnamed: 0.1":11928,"Unnamed: 0":11928.0,"anchor":"Identifying Nonlinear 1-Step Causal Influences in Presence of Latent\n  Variables","positive":"  We propose an approach for learning the causal structure in stochastic\ndynamical systems with a $1$-step functional dependency in the presence of\nlatent variables. We propose an information-theoretic approach that allows us\nto recover the causal relations among the observed variables as long as the\nlatent variables evolve without exogenous noise. We further propose an\nefficient learning method based on linear regression for the special sub-case\nwhen the dynamics are restricted to be linear. We validate the performance of\nour approach via numerical simulations.\n","negative":"  The vanishing and exploding gradient problems are well-studied obstacles that\nmake it difficult for recurrent neural networks to learn long-term time\ndependencies. We propose a reparameterization of standard recurrent neural\nnetworks to update linear transformations in a provably norm-preserving way\nthrough Givens rotations. Additionally, we use the absolute value function as\nan element-wise non-linearity to preserve the norm of backpropagated signals\nover the entire network. We show that this reparameterization reduces the\nnumber of parameters and maintains the same algorithmic complexity as a\nstandard recurrent neural network, while outperforming standard recurrent\nneural networks with orthogonal initializations and Long Short-Term Memory\nnetworks on the copy problem.\n","id":928}
{"Unnamed: 0.1":11929,"Unnamed: 0":11929.0,"anchor":"Revenue Forecasting for Enterprise Products","positive":"  For any business, planning is a continuous process, and typically\nbusiness-owners focus on making both long-term planning aligned with a\nparticular strategy as well as short-term planning that accommodates the\ndynamic market situations. An ability to perform an accurate financial forecast\nis crucial for effective planning. In this paper, we focus on providing an\nintelligent and efficient solution that will help in forecasting revenue using\nmachine learning algorithms. We experiment with three different revenue\nforecasting models, and here we provide detailed insights into the methodology\nand their relative performance measured on real finance data. As a real-world\napplication of our models, we partner with Microsoft's Finance organization\n(department that reports Microsoft's finances) to provide them a guidance on\nthe projected revenue for upcoming quarters.\n","negative":"  This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models. Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the art results on the\nWMT English$\\rightarrow$German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from the previous best models on\nboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner.\n","id":929}
{"Unnamed: 0.1":11930,"Unnamed: 0":11930.0,"anchor":"Convex Parameterizations and Fidelity Bounds for Nonlinear\n  Identification and Reduced-Order Modelling","positive":"  Model instability and poor prediction of long-term behavior are common\nproblems when modeling dynamical systems using nonlinear \"black-box\"\ntechniques. Direct optimization of the long-term predictions, often called\nsimulation error minimization, leads to optimization problems that are\ngenerally non-convex in the model parameters and suffer from multiple local\nminima. In this work we present methods which address these problems through\nconvex optimization, based on Lagrangian relaxation, dissipation inequalities,\ncontraction theory, and semidefinite programming. We demonstrate the proposed\nmethods with a model order reduction task for electronic circuit design and the\nidentification of a pneumatic actuator from experiment.\n","negative":"  We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data.\n","id":930}
{"Unnamed: 0.1":11931,"Unnamed: 0":11931.0,"anchor":"Patchwork Kriging for Large-scale Gaussian Process Regression","positive":"  This paper presents a new approach for Gaussian process (GP) regression for\nlarge datasets. The approach involves partitioning the regression input domain\ninto multiple local regions with a different local GP model fitted in each\nregion. Unlike existing local partitioned GP approaches, we introduce a\ntechnique for patching together the local GP models nearly seamlessly to ensure\nthat the local GP models for two neighboring regions produce nearly the same\nresponse prediction and prediction error variance on the boundary between the\ntwo regions. This largely mitigates the well-known discontinuity problem that\ndegrades the boundary accuracy of existing local partitioned GP methods. Our\nmain innovation is to represent the continuity conditions as additional\npseudo-observations that the differences between neighboring GP responses are\nidentically zero at an appropriately chosen set of boundary input locations. To\npredict the response at any input location, we simply augment the actual\nresponse observations with the pseudo-observations and apply standard GP\nprediction methods to the augmented data. In contrast to heuristic continuity\nadjustments, this has an advantage of working within a formal GP framework, so\nthat the GP-based predictive uncertainty quantification remains valid. Our\napproach also inherits a sparse block-like structure for the sample covariance\nmatrix, which results in computationally efficient closed-form expressions for\nthe predictive mean and variance. In addition, we provide a new spatial\npartitioning scheme based on a recursive space partitioning along local\nprincipal component directions, which makes the proposed approach applicable\nfor regression domains having more than two dimensions. Using three spatial\ndatasets and three higher dimensional datasets, we investigate the numerical\nperformance of the approach and compare it to several state-of-the-art\napproaches.\n","negative":"  Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm\nto model the temporal tensor data. It leverages a time constraint to capture\nthe evolving properties of tensor data. Nowadays the exploding dataset demands\na large scale PTTF analysis, and a parallel solution is critical to accommodate\nthe trend. Whereas, the parallelization of PTTF still remains unexplored. In\nthis paper, we propose a simple yet efficient Parallel Probabilistic Temporal\nTensor Factorization, referred to as P$^2$T$^2$F, to provide a scalable PTTF\nsolution. P$^2$T$^2$F is fundamentally disparate from existing parallel tensor\nfactorizations by considering the probabilistic decomposition and the temporal\neffects of tensor data. It adopts a new tensor data split strategy to subdivide\na large tensor into independent sub-tensors, the computation of which is\ninherently parallel. We train P$^2$T$^2$F with an efficient algorithm of\nstochastic Alternating Direction Method of Multipliers, and show that the\nconvergence is guaranteed. Experiments on several real-word tensor datasets\ndemonstrate that P$^2$T$^2$F is a highly effective and efficiently scalable\nalgorithm dedicated for large scale probabilistic temporal tensor analysis.\n","id":931}
{"Unnamed: 0.1":11932,"Unnamed: 0":11932.0,"anchor":"A Contextual Bandit Approach for Stream-Based Active Learning","positive":"  Contextual bandit algorithms -- a class of multi-armed bandit algorithms that\nexploit the contextual information -- have been shown to be effective in\nsolving sequential decision making problems under uncertainty. A common\nassumption adopted in the literature is that the realized (ground truth) reward\nby taking the selected action is observed by the learner at no cost, which,\nhowever, is not realistic in many practical scenarios. When observing the\nground truth reward is costly, a key challenge for the learner is how to\njudiciously acquire the ground truth by assessing the benefits and costs in\norder to balance learning efficiency and learning cost. From the information\ntheoretic perspective, a perhaps even more interesting question is how much\nefficiency might be lost due to this cost. In this paper, we design a novel\ncontextual bandit-based learning algorithm and endow it with the active\nlearning capability. The key feature of our algorithm is that in addition to\nsending a query to an annotator for the ground truth, prior information about\nthe ground truth learned by the learner is sent together, thereby reducing the\nquery cost. We prove that by carefully choosing the algorithm parameters, the\nlearning regret of the proposed algorithm achieves the same order as that of\nconventional contextual bandit algorithms in cost-free scenarios, implying\nthat, surprisingly, cost due to acquiring the ground truth does not increase\nthe learning regret in the long-run. Our analysis shows that prior information\nabout the ground truth plays a critical role in improving the system\nperformance in scenarios where active learning is necessary.\n","negative":"  A major goal of computer vision is to enable computers to interpret visual\nsituations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd\nwaiting for a bus,\" \"a picnic\") whose image instantiations are linked more by\ntheir common spatial and semantic structure than by low-level visual\nsimilarity. In this paper, we propose a novel method for prior learning and\nactive object localization for this kind of knowledge-driven search in static\nimages. In our system, prior situation knowledge is captured by a set of\nflexible, kernel-based density estimations---a situation model---that represent\nthe expected spatial structure of the given situation. These estimations are\nefficiently updated by information gained as the system searches for relevant\nobjects, allowing the system to use context as it is discovered to narrow the\nsearch.\n  More specifically, at any given time in a run on a test image, our system\nuses image features plus contextual information it has discovered to identify a\nsmall subset of training images---an importance cluster---that is deemed most\nsimilar to the given test image, given the context. This subset is used to\ngenerate an updated situation model in an on-line fashion, using an efficient\nmultipole expansion technique.\n  As a proof of concept, we apply our algorithm to a highly varied and\nchallenging dataset consisting of instances of a \"dog-walking\" situation. Our\nresults support the hypothesis that dynamically-rendered, context-based\nprobability models can support efficient object localization in visual\nsituations. Moreover, our approach is general enough to be applied to diverse\nmachine learning paradigms requiring interpretable, probabilistic\nrepresentations generated from partially observed data.\n","id":932}
{"Unnamed: 0.1":11933,"Unnamed: 0":11933.0,"anchor":"Collective Vertex Classification Using Recursive Neural Network","positive":"  Collective classification of vertices is a task of assigning categories to\neach vertex in a graph based on both vertex attributes and link structure.\nNevertheless, some existing approaches do not use the features of neighbouring\nvertices properly, due to the noise introduced by these features. In this\npaper, we propose a graph-based recursive neural network framework for\ncollective vertex classification. In this framework, we generate hidden\nrepresentations from both attributes of vertices and representations of\nneighbouring vertices via recursive neural networks. Under this framework, we\nexplore two types of recursive neural units, naive recursive neural unit and\nlong short-term memory unit. We have conducted experiments on four real-world\nnetwork datasets. The experimental results show that our frame- work with long\nshort-term memory model achieves better results and outperforms several\ncompetitive baseline methods.\n","negative":"  We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare.\n","id":933}
{"Unnamed: 0.1":11934,"Unnamed: 0":11934.0,"anchor":"Discriminative Neural Topic Models","positive":"  We propose a neural network based approach for learning topics from text and\nimage datasets. The model makes no assumptions about the conditional\ndistribution of the observed features given the latent topics. This allows us\nto perform topic modelling efficiently using sentences of documents and patches\nof images as observed features, rather than limiting ourselves to words.\nMoreover, the proposed approach is online, and hence can be used for streaming\ndata. Furthermore, since the approach utilizes neural networks, it can be\nimplemented on GPU with ease, and hence it is very scalable.\n","negative":"  We study characteristics of receptive fields of units in deep convolutional\nnetworks. The receptive field size is a crucial issue in many visual tasks, as\nthe output must respond to large enough areas in the image to capture\ninformation about large objects. We introduce the notion of an effective\nreceptive field, and show that it both has a Gaussian distribution and only\noccupies a fraction of the full theoretical receptive field. We analyze the\neffective receptive field in several architecture designs, and the effect of\nnonlinear activations, dropout, sub-sampling and skip connections on it. This\nleads to suggestions for ways to address its tendency to be too small.\n","id":934}
{"Unnamed: 0.1":11935,"Unnamed: 0":11935.0,"anchor":"A Survey of Quantum Learning Theory","positive":"  This paper surveys quantum learning theory: the theoretical aspects of\nmachine learning using quantum computers. We describe the main results known\nfor three models of learning: exact learning from membership queries, and\nProbably Approximately Correct (PAC) and agnostic learning from classical or\nquantum examples.\n","negative":"  Consider a classification problem where we have both labeled and unlabeled\ndata available. We show that for linear classifiers defined by convex\nmargin-based surrogate losses that are decreasing, it is impossible to\nconstruct any semi-supervised approach that is able to guarantee an improvement\nover the supervised classifier measured by this surrogate loss on the labeled\nand unlabeled data. For convex margin-based loss functions that also increase,\nwe demonstrate safe improvements are possible.\n","id":935}
{"Unnamed: 0.1":11936,"Unnamed: 0":11936.0,"anchor":"Deep Network Guided Proof Search","positive":"  Deep learning techniques lie at the heart of several significant AI advances\nin recent years including object recognition and detection, image captioning,\nmachine translation, speech recognition and synthesis, and playing the game of\nGo. Automated first-order theorem provers can aid in the formalization and\nverification of mathematical theorems and play a crucial role in program\nanalysis, theory reasoning, security, interpolation, and system verification.\nHere we suggest deep learning based guidance in the proof search of the theorem\nprover E. We train and compare several deep neural network models on the traces\nof existing ATP proofs of Mizar statements and use them to select processed\nclauses during proof search. We give experimental evidence that with a hybrid,\ntwo-phase approach, deep learning based guidance can significantly reduce the\naverage number of proof search steps while increasing the number of theorems\nproved. Using a few proof guidance strategies that leverage deep neural\nnetworks, we have found first-order proofs of 7.36% of the first-order logic\ntranslations of the Mizar Mathematical Library theorems that did not previously\nhave ATP generated proofs. This increases the ratio of statements in the corpus\nwith ATP generated proofs from 56% to 59%.\n","negative":"  We introduce a method to stabilize Generative Adversarial Networks (GANs) by\ndefining the generator objective with respect to an unrolled optimization of\nthe discriminator. This allows training to be adjusted between using the\noptimal discriminator in the generator's objective, which is ideal but\ninfeasible in practice, and using the current value of the discriminator, which\nis often unstable and leads to poor solutions. We show how this technique\nsolves the common problem of mode collapse, stabilizes training of GANs with\ncomplex recurrent generators, and increases diversity and coverage of the data\ndistribution by the generator.\n","id":936}
{"Unnamed: 0.1":11937,"Unnamed: 0":11937.0,"anchor":"On the Effectiveness of Discretizing Quantitative Attributes in Linear\n  Classifiers","positive":"  Learning algorithms that learn linear models often have high representation\nbias on real-world problems. In this paper, we show that this representation\nbias can be greatly reduced by discretization. Discretization is a common\nprocedure in machine learning that is used to convert a quantitative attribute\ninto a qualitative one. It is often motivated by the limitation of some\nlearners to qualitative data. Discretization loses information, as fewer\ndistinctions between instances are possible using discretized data relative to\nundiscretized data. In consequence, where discretization is not essential, it\nmight appear desirable to avoid it. However, it has been shown that\ndiscretization often substantially reduces the error of the linear generative\nBayesian classifier naive Bayes. This motivates a systematic study of the\neffectiveness of discretizing quantitative attributes for other linear\nclassifiers. In this work, we study the effect of discretization on the\nperformance of linear classifiers optimizing three distinct discriminative\nobjective functions --- logistic regression (optimizing negative\nlog-likelihood), support vector classifiers (optimizing hinge loss) and a\nzero-hidden layer artificial neural network (optimizing mean-square-error). We\nshow that discretization can greatly increase the accuracy of these linear\ndiscriminative learners by reducing their representation bias, especially on\nbig datasets. We substantiate our claims with an empirical study on $42$\nbenchmark datasets.\n","negative":"  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their generative models\nwithout prior knowledge of the model statistics and the number of generative\nmodels. Two algorithms, both using the $L^1$-distance between estimated power\nspectral densities (PSDs) as a measure of dissimilarity, are analyzed. The\nfirst one, termed nearest neighbor process clustering (NNPC), relies on\npartitioning the nearest neighbor graph of the observations via spectral\nclustering. The second algorithm, simply referred to as $k$-means (KM),\nconsists of a single $k$-means iteration with farthest point initialization and\nwas considered before in the literature, albeit with a different dissimilarity\nmeasure. We prove that both algorithms succeed with high probability in the\npresence of noise and missing entries, and even when the generative process\nPSDs overlap significantly, all provided that the observation length is\nsufficiently large. Our results quantify the tradeoff between the overlap of\nthe generative process PSDs, the observation length, the fraction of missing\nentries, and the noise variance. Finally, we provide extensive numerical\nresults for synthetic and real data and find that NNPC outperforms\nstate-of-the-art algorithms in human motion sequence clustering.\n","id":937}
{"Unnamed: 0.1":11938,"Unnamed: 0":11938.0,"anchor":"jsCoq: Towards Hybrid Theorem Proving Interfaces","positive":"  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n","negative":"  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems.\n","id":938}
{"Unnamed: 0.1":11939,"Unnamed: 0":11939.0,"anchor":"CP-decomposition with Tensor Power Method for Convolutional Neural\n  Networks Compression","positive":"  Convolutional Neural Networks (CNNs) has shown a great success in many areas\nincluding complex image classification tasks. However, they need a lot of\nmemory and computational cost, which hinders them from running in relatively\nlow-end smart devices such as smart phones. We propose a CNN compression method\nbased on CP-decomposition and Tensor Power Method. We also propose an iterative\nfine tuning, with which we fine-tune the whole network after decomposing each\nlayer, but before decomposing the next layer. Significant reduction in memory\nand computation cost is achieved compared to state-of-the-art previous work\nwith no more accuracy loss.\n","negative":"  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n","id":939}
{"Unnamed: 0.1":11940,"Unnamed: 0":11940.0,"anchor":"Personalized Classifier Ensemble Pruning Framework for Mobile\n  Crowdsourcing","positive":"  Ensemble learning has been widely employed by mobile applications, ranging\nfrom environmental sensing to activity recognitions. One of the fundamental\nissue in ensemble learning is the trade-off between classification accuracy and\ncomputational costs, which is the goal of ensemble pruning. During\ncrowdsourcing, the centralized aggregator releases ensemble learning models to\na large number of mobile participants for task evaluation or as the\ncrowdsourcing learning results, while different participants may seek for\ndifferent levels of the accuracy-cost trade-off. However, most of existing\nensemble pruning approaches consider only one identical level of such\ntrade-off. In this study, we present an efficient ensemble pruning framework\nfor personalized accuracy-cost trade-offs via multi-objective optimization.\nSpecifically, for the commonly used linear-combination style of the trade-off,\nwe provide an objective-mixture optimization to further reduce the number of\nensemble candidates. Experimental results show that our framework is highly\nefficient for personalized ensemble pruning, and achieves much better pruning\nperformance with objective-mixture optimization when compared to state-of-art\napproaches.\n","negative":"  With the advent of highly predictive but opaque deep learning models, it has\nbecome more important than ever to understand and explain the predictions of\nsuch models. Existing approaches define interpretability as the inverse of\ncomplexity and achieve interpretability at the cost of accuracy. This\nintroduces a risk of producing interpretable but misleading explanations. As\nhumans, we are prone to engage in this kind of behavior \\cite{mythos}. In this\npaper, we take a step in the direction of tackling the problem of\ninterpretability without compromising the model accuracy. We propose to build a\nTreeview representation of the complex model via hierarchical partitioning of\nthe feature space, which reveals the iterative rejection of unlikely class\nlabels until the correct association is predicted.\n","id":940}
{"Unnamed: 0.1":11941,"Unnamed: 0":11941.0,"anchor":"Malicious URL Detection using Machine Learning: A Survey","positive":"  Malicious URL, a.k.a. malicious website, is a common and serious threat to\ncybersecurity. Malicious URLs host unsolicited content (spam, phishing,\ndrive-by exploits, etc.) and lure unsuspecting users to become victims of scams\n(monetary loss, theft of private information, and malware installation), and\ncause losses of billions of dollars every year. It is imperative to detect and\nact on such threats in a timely manner. Traditionally, this detection is done\nmostly through the usage of blacklists. However, blacklists cannot be\nexhaustive, and lack the ability to detect newly generated malicious URLs. To\nimprove the generality of malicious URL detectors, machine learning techniques\nhave been explored with increasing attention in recent years. This article aims\nto provide a comprehensive survey and a structural understanding of Malicious\nURL Detection techniques using machine learning. We present the formal\nformulation of Malicious URL Detection as a machine learning task, and\ncategorize and review the contributions of literature studies that addresses\ndifferent dimensions of this problem (feature representation, algorithm design,\netc.). Further, this article provides a timely and comprehensive survey for a\nrange of different audiences, not only for machine learning researchers and\nengineers in academia, but also for professionals and practitioners in\ncybersecurity industry, to help them understand the state of the art and\nfacilitate their own research and practical applications. We also discuss\npractical issues in system design, open research challenges, and point out some\nimportant directions for future research.\n","negative":"  The Residual Network (ResNet), proposed in He et al. (2015), utilized\nshortcut connections to significantly reduce the difficulty of training, which\nresulted in great performance boosts in terms of both training and\ngeneralization error.\n  It was empirically observed in He et al. (2015) that stacking more layers of\nresidual blocks with shortcut 2 results in smaller training error, while it is\nnot true for shortcut of length 1 or 3. We provide a theoretical explanation\nfor the uniqueness of shortcut 2.\n  We show that with or without nonlinearities, by adding shortcuts that have\ndepth two, the condition number of the Hessian of the loss function at the zero\ninitial point is depth-invariant, which makes training very deep models no more\ndifficult than shallow ones. Shortcuts of higher depth result in an extremely\nflat (high-order) stationary point initially, from which the optimization\nalgorithm is hard to escape. The shortcut 1, however, is essentially equivalent\nto no shortcuts, which has a condition number exploding to infinity as the\nnumber of layers grows. We further argue that as the number of layers tends to\ninfinity, it suffices to only look at the loss function at the zero initial\npoint.\n  Extensive experiments are provided accompanying our theoretical results. We\nshow that initializing the network to small weights with shortcut 2 achieves\nsignificantly better results than random Gaussian (Xavier) initialization,\northogonal initialization, and shortcuts of deeper depth, from various\nperspectives ranging from final loss, learning dynamics and stability, to the\nbehavior of the Hessian along the learning process.\n","id":941}
{"Unnamed: 0.1":11942,"Unnamed: 0":11942.0,"anchor":"Privileged Multi-label Learning","positive":"  This paper presents privileged multi-label learning (PrML) to explore and\nexploit the relationship between labels in multi-label learning problems. We\nsuggest that for each individual label, it cannot only be implicitly connected\nwith other labels via the low-rank constraint over label predictors, but also\nits performance on examples can receive the explicit comments from other labels\ntogether acting as an \\emph{Oracle teacher}. We generate privileged label\nfeature for each example and its individual label, and then integrate it into\nthe framework of low-rank based multi-label learning. The proposed algorithm\ncan therefore comprehensively explore and exploit label relationships by\ninheriting all the merits of privileged information and low-rank constraints.\nWe show that PrML can be efficiently solved by dual coordinate descent\nalgorithm using iterative optimization strategy with cheap updates. Experiments\non benchmark datasets show that through privileged label features, the\nperformance can be significantly improved and PrML is superior to several\ncompeting methods in most cases.\n","negative":"  It is widely believed that the prediction accuracy of decision tree models is\ninvariant under any strictly monotone transformation of the individual\npredictor variables. However, this statement may be false when predicting new\nobservations with values that were not seen in the training-set and are close\nto the location of the split point of a tree rule. The sensitivity of the\nprediction error to the split point interpolation is high when the split point\nof the tree is estimated based on very few observations, reaching 9%\nmisclassification error when only 10 observations are used for constructing a\nsplit, and shrinking to 1% when relying on 100 observations. This study\ncompares the performance of alternative methods for split point interpolation\nand concludes that the best choice is taking the mid-point between the two\nclosest points to the split point of the tree. Furthermore, if the (continuous)\ndistribution of the predictor variable is known, then using its probability\nintegral for transforming the variable (\"quantile transformation\") will reduce\nthe model's interpolation error by up to about a half on average. Accordingly,\nthis study provides guidelines for both developers and users of decision tree\nmodels (including bagging and random forest).\n","id":942}
{"Unnamed: 0.1":11943,"Unnamed: 0":11943.0,"anchor":"Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D","positive":"  The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension\n$d\\ge 2$, however, for the 1D case there exists exact polynomial time\nalgorithms. Previous literature reported an $O(kn^2)$ time dynamic programming\nalgorithm that uses $O(kn)$ space. It turns out that the problem has been\nconsidered under a different name more than twenty years ago. We present all\nthe existing work that had been overlooked and compare the various solutions\ntheoretically. Moreover, we show how to reduce the space usage for some of\nthem, as well as generalize them to data structures that can quickly report an\noptimal $k$-Means clustering for any $k$. Finally we also generalize all the\nalgorithms to work for the absolute distance and to work for any Bregman\nDivergence. We complement our theoretical contributions by experiments that\ncompare the practical performance of the various algorithms.\n","negative":"  Deep learning, as a promising new area of machine learning, has attracted a\nrapidly increasing attention in the field of medical imaging. Compared to the\nconventional machine learning methods, deep learning requires no hand-tuned\nfeature extractor, and has shown a superior performance in many visual object\nrecognition applications. In this study, we develop a deep convolutional neural\nnetwork (CNN) and apply it to thoracic CT images for the classification of lung\nnodules. We present the CNN architecture and classification accuracy for the\noriginal images of lung nodules. In order to understand the features of lung\nnodules, we further construct new datasets, based on the combination of\nartificial geometric nodules and some transformations of the original images,\nas well as a stochastic nodule shape model. It is found that simplistic\ngeometric nodules cannot capture the important features of lung nodules.\n","id":943}
{"Unnamed: 0.1":11944,"Unnamed: 0":11944.0,"anchor":"Learn&Fuzz: Machine Learning for Input Fuzzing","positive":"  Fuzzing consists of repeatedly testing an application with modified, or\nfuzzed, inputs with the goal of finding security vulnerabilities in\ninput-parsing code. In this paper, we show how to automate the generation of an\ninput grammar suitable for input fuzzing using sample inputs and\nneural-network-based statistical machine-learning techniques. We present a\ndetailed case study with a complex input format, namely PDF, and a large\ncomplex security-critical parser for this format, namely, the PDF parser\nembedded in Microsoft's new Edge browser. We discuss (and measure) the tension\nbetween conflicting learning and fuzzing goals: learning wants to capture the\nstructure of well-formed inputs, while fuzzing wants to break that structure in\norder to cover unexpected code paths and find bugs. We also present a new\nalgorithm for this learn&fuzz challenge which uses a learnt input probability\ndistribution to intelligently guide where to fuzz inputs.\n","negative":"  Understanding prediction errors and determining how to fix them is critical\nto building effective predictive systems. In this paper, we delineate four\ntypes of prediction errors and demonstrate that these four types characterize\nall prediction errors. In addition, we describe potential remedies and tools\nthat can be used to reduce the uncertainty when trying to determine the source\nof a prediction error and when trying to take action to remove a prediction\nerrors.\n","id":944}
{"Unnamed: 0.1":11945,"Unnamed: 0":11945.0,"anchor":"Decoding Epileptogenesis in a Reduced State Space","positive":"  We describe here the recent results of a multidisciplinary effort to design a\nbiomarker that can actively and continuously decode the progressive changes in\nneuronal organization leading to epilepsy, a process known as epileptogenesis.\nUsing an animal model of acquired epilepsy, wechronically record hippocampal\nevoked potentials elicited by an auditory stimulus. Using a set of reduced\ncoordinates, our algorithm can identify universal smooth low-dimensional\nconfigurations of the auditory evoked potentials that correspond to distinct\nstages of epileptogenesis. We use a hidden Markov model to learn the dynamics\nof the evoked potential, as it evolves along these smooth low-dimensional\nsubsets. We provide experimental evidence that the biomarker is able to exploit\nsubtle changes in the evoked potential to reliably decode the stage of\nepileptogenesis and predict whether an animal will eventually recover from the\ninjury, or develop spontaneous seizures.\n","negative":"  Recurrent neural networks (RNNs) have shown promising performance for\nlanguage modeling. However, traditional training of RNNs using back-propagation\nthrough time often suffers from overfitting. One reason for this is that\nstochastic optimization (used for large training sets) does not provide good\nestimates of model uncertainty. This paper leverages recent advances in\nstochastic gradient Markov Chain Monte Carlo (also appropriate for large\ntraining sets) to learn weight uncertainty in RNNs. It yields a principled\nBayesian learning algorithm, adding gradient noise during training (enhancing\nexploration of the model-parameter space) and model averaging when testing.\nExtensive experiments on various RNN models and across a broad range of\napplications demonstrate the superiority of the proposed approach over\nstochastic optimization.\n","id":945}
{"Unnamed: 0.1":11946,"Unnamed: 0":11946.0,"anchor":"k*-Nearest Neighbors: From Global to Local","positive":"  The weighted k-nearest neighbors algorithm is one of the most fundamental\nnon-parametric methods in pattern recognition and machine learning. The\nquestion of setting the optimal number of neighbors as well as the optimal\nweights has received much attention throughout the years, nevertheless this\nproblem seems to have remained unsettled. In this paper we offer a simple\napproach to locally weighted regression\/classification, where we make the\nbias-variance tradeoff explicit. Our formulation enables us to phrase a notion\nof optimal weights, and to efficiently find these weights as well as the\noptimal number of neighbors efficiently and adaptively, for each data point\nwhose value we wish to estimate. The applicability of our approach is\ndemonstrated on several datasets, showing superior performance over standard\nlocally weighted methods.\n","negative":"  Nowadays deep learning is dominating the field of machine learning with\nstate-of-the-art performance in various application areas. Recently, spiking\nneural networks (SNNs) have been attracting a great deal of attention, notably\nowning to their power efficiency, which can potentially allow us to implement a\nlow-power deep learning engine suitable for real-time\/mobile applications.\nHowever, implementing SNN-based deep learning remains challenging, especially\ngradient-based training of SNNs by error backpropagation. We cannot simply\npropagate errors through SNNs in conventional way because of the property of\nSNNs that process discrete data in the form of a series. Consequently, most of\nthe previous studies employ a workaround technique, which first trains a\nconventional weighted-sum deep neural network and then maps the learning\nweights to the SNN under training, instead of training SNN parameters directly.\nIn order to eliminate this workaround, recently proposed is a new class of SNN\nnamed deep spiking networks (DSNs), which can be trained directly (without a\nmapping from conventional deep networks) by error backpropagation with\nstochastic gradient descent. In this paper, we show that the initialization of\nthe membrane potential on the backward path is an important step in DSN\ntraining, through diverse experiments performed under various conditions.\nFurthermore, we propose a simple and efficient method that can improve DSN\ntraining by controlling the initial membrane potential on the backward path. In\nour experiments, adopting the proposed approach allowed us to boost the\nperformance of DSN training in terms of converging time and accuracy.\n","id":946}
{"Unnamed: 0.1":11947,"Unnamed: 0":11947.0,"anchor":"Deep Reinforcement Learning: An Overview","positive":"  We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\nupdate.\n","negative":"  Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.\n","id":947}
{"Unnamed: 0.1":11948,"Unnamed: 0":11948.0,"anchor":"Learning Light Transport the Reinforced Way","positive":"  We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with zero contribution is\ndramatically reduced, resulting in much less noisy images within a fixed time\nbudget.\n","negative":"  Program authorship attribution has implications for the privacy of\nprogrammers who wish to contribute code anonymously. While previous work has\nshown that complete files that are individually authored can be attributed, we\nshow here for the first time that accounts belonging to open source\ncontributors containing short, incomplete, and typically uncompilable fragments\ncan also be effectively attributed.\n  We propose a technique for authorship attribution of contributor accounts\ncontaining small source code samples, such as those that can be obtained from\nversion control systems or other direct comparison of sequential versions. We\nshow that while application of previous methods to individual small source code\nsamples yields an accuracy of about 73% for 106 programmers as a baseline, by\nensembling and averaging the classification probabilities of a sufficiently\nlarge set of samples belonging to the same author we achieve 99% accuracy for\nassigning the set of samples to the correct author. Through these results, we\ndemonstrate that attribution is an important threat to privacy for programmers\neven in real-world collaborative environments such as GitHub. Additionally, we\npropose the use of calibration curves to identify samples by unknown and\npreviously unencountered authors in the open world setting. We show that we can\nalso use these calibration curves in the case that we do not have linking\ninformation and thus are forced to classify individual samples directly. This\nis because the calibration curves allow us to identify which samples are more\nlikely to have been correctly attributed. Using such a curve can help an\nanalyst choose a cut-off point which will prevent most misclassifications, at\nthe cost of causing the rejection of some of the more dubious correct\nattributions.\n","id":948}
{"Unnamed: 0.1":11949,"Unnamed: 0":11949.0,"anchor":"A Convex Similarity Index for Sparse Recovery of Missing Image Samples","positive":"  This paper investigates the problem of recovering missing samples using\nmethods based on sparse representation adapted especially for image signals.\nInstead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality\nmeasure is used as the similarity criterion between the original and the\nreconstructed images. The proposed criterion called Convex SIMilarity (CSIM)\nindex is a modified version of the Structural SIMilarity (SSIM) index, which\ndespite its predecessor, is convex and uni-modal. We derive mathematical\nproperties for the proposed index and show how to optimally choose the\nparameters of the proposed criterion, investigating the Restricted Isometry\n(RIP) and error-sensitivity properties. We also propose an iterative sparse\nrecovery method based on a constrained $l_1$-norm minimization problem,\nincorporating CSIM as the fidelity criterion. The resulting convex optimization\nproblem is solved via an algorithm based on Alternating Direction Method of\nMultipliers (ADMM). Taking advantage of the convexity of the CSIM index, we\nalso prove the convergence of the algorithm to the globally optimal solution of\nthe proposed optimization problem, starting from any arbitrary point.\nSimulation results confirm the performance of the new similarity index as well\nas the proposed algorithm for missing sample recovery of image patch signals.\n","negative":"  This paper proposes a new optimization algorithm called Entropy-SGD for\ntraining deep neural networks that is motivated by the local geometry of the\nenergy landscape. Local extrema with low generalization error have a large\nproportion of almost-zero eigenvalues in the Hessian with very few positive or\nnegative eigenvalues. We leverage upon this observation to construct a\nlocal-entropy-based objective function that favors well-generalizable solutions\nlying in large flat regions of the energy landscape, while avoiding\npoorly-generalizable solutions located in the sharp valleys. Conceptually, our\nalgorithm resembles two nested loops of SGD where we use Langevin dynamics in\nthe inner loop to compute the gradient of the local entropy before each update\nof the weights. We show that the new objective has a smoother energy landscape\nand show improved generalization over SGD using uniform stability, under\ncertain assumptions. Our experiments on convolutional and recurrent networks\ndemonstrate that Entropy-SGD compares favorably to state-of-the-art techniques\nin terms of generalization error and training time.\n","id":949}
{"Unnamed: 0.1":11950,"Unnamed: 0":11950.0,"anchor":"Robust mixture of experts modeling using the $t$ distribution","positive":"  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification, and clustering. For regression and cluster\nanalyses of continuous data, MoE usually use normal experts following the\nGaussian distribution. However, for a set of data containing a group or groups\nof observations with heavy tails or atypical observations, the use of normal\nexperts is unsuitable and can unduly affect the fit of the MoE model. We\nintroduce a robust MoE modeling using the $t$ distribution. The proposed $t$\nMoE (TMoE) deals with these issues regarding heavy-tailed and noisy data. We\ndevelop a dedicated expectation-maximization (EM) algorithm to estimate the\nparameters of the proposed model by monotonically maximizing the observed data\nlog-likelihood. We describe how the presented model can be used in prediction\nand in model-based clustering of regression data. The proposed model is\nvalidated on numerical experiments carried out on simulated data, which show\nthe effectiveness and the robustness of the proposed model in terms of modeling\nnon-linear regression functions as well as in model-based clustering. Then, it\nis applied to the real-world data of tone perception for musical data analysis,\nand the one of temperature anomalies for the analysis of climate change data.\nThe obtained results show the usefulness of the TMoE model for practical\napplications.\n","negative":"  The promise of compressive sensing (CS) has been offset by two significant\nchallenges. First, real-world data is not exactly sparse in a fixed basis.\nSecond, current high-performance recovery algorithms are slow to converge,\nwhich limits CS to either non-real-time applications or scenarios where massive\nback-end computing is available. In this paper, we attack both of these\nchallenges head-on by developing a new signal recovery framework we call {\\em\nDeepInverse} that learns the inverse transformation from measurement vectors to\nsignals using a {\\em deep convolutional network}. When trained on a set of\nrepresentative images, the network learns both a representation for the signals\n(addressing challenge one) and an inverse map approximating a greedy or convex\nrecovery algorithm (addressing challenge two). Our experiments indicate that\nthe DeepInverse network closely approximates the solution produced by\nstate-of-the-art CS recovery algorithms yet is hundreds of times faster in run\ntime. The tradeoff for the ultrafast run time is a computationally intensive,\noff-line training procedure typical to deep networks. However, the training\nneeds to be completed only once, which makes the approach attractive for a host\nof sparse recovery problems.\n","id":950}
{"Unnamed: 0.1":11951,"Unnamed: 0":11951.0,"anchor":"Exploiting Convolutional Neural Network for Risk Prediction with Medical\n  Feature Embedding","positive":"  The widespread availability of electronic health records (EHRs) promises to\nusher in the era of personalized medicine. However, the problem of extracting\nuseful clinical representations from longitudinal EHR data remains challenging.\nIn this paper, we explore deep neural network models with learned medical\nfeature embedding to deal with the problems of high dimensionality and\ntemporality. Specifically, we use a multi-layer convolutional neural network\n(CNN) to parameterize the model and is thus able to capture complex non-linear\nlongitudinal evolution of EHRs. Our model can effectively capture local\/short\ntemporal dependency in EHRs, which is beneficial for risk prediction. To\naccount for high dimensionality, we use the embedding medical features in the\nCNN model which hold the natural medical concepts. Our initial experiments\nproduce promising results and demonstrate the effectiveness of both the medical\nfeature embedding and the proposed convolutional neural network in risk\nprediction on cohorts of congestive heart failure and diabetes patients\ncompared with several strong baselines.\n","negative":"  A new, radical CNN design approach is presented in this paper, considering\nthe reduction of the total computational load during inference. This is\nachieved by a new holistic intervention on both the CNN architecture and the\ntraining procedure, which targets to the parsimonious inference by learning to\nexploit or remove the redundant capacity of a CNN architecture. This is\naccomplished, by the introduction of a new structural element that can be\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\nor even improving its recognition accuracy. Our approach formulates a\nsystematic and data-driven method for developing CNNs that are trained to\neventually change size and form in real-time during inference, targeting to the\nsmaller possible computational footprint. Results are provided for the optimal\nimplementation on a few modern, high-end mobile computing platforms indicating\na significant speed-up of up to x3 times.\n","id":951}
{"Unnamed: 0.1":11952,"Unnamed: 0":11952.0,"anchor":"A Model-based Projection Technique for Segmenting Customers","positive":"  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n","negative":"  Natural image modeling is a landmark challenge of unsupervised learning.\nVariational Autoencoders (VAEs) learn a useful latent representation and model\nglobal structure well but have difficulty capturing small details. PixelCNN\nmodels details very well, but lacks a latent code and is difficult to scale for\ncapturing large structures. We present PixelVAE, a VAE model with an\nautoregressive decoder based on PixelCNN. Our model requires very few expensive\nautoregressive layers compared to PixelCNN and learns latent codes that are\nmore compressed than a standard VAE while still capturing most non-trivial\nstructure. Finally, we extend our model to a hierarchy of latent variables at\ndifferent scales. Our model achieves state-of-the-art performance on binarized\nMNIST, competitive performance on 64x64 ImageNet, and high-quality samples on\nthe LSUN bedrooms dataset.\n","id":952}
{"Unnamed: 0.1":11953,"Unnamed: 0":11953.0,"anchor":"FPGA Architecture for Deep Learning and its application to Planetary\n  Robotics","positive":"  Autonomous control systems onboard planetary rovers and spacecraft benefit\nfrom having cognitive capabilities like learning so that they can adapt to\nunexpected situations in-situ. Q-learning is a form of reinforcement learning\nand it has been efficient in solving certain class of learning problems.\nHowever, embedded systems onboard planetary rovers and spacecraft rarely\nimplement learning algorithms due to the constraints faced in the field, like\nprocessing power, chip size, convergence rate and costs due to the need for\nradiation hardening. These challenges present a compelling need for a portable,\nlow-power, area efficient hardware accelerator to make learning algorithms\npractical onboard space hardware. This paper presents a FPGA implementation of\nQ-learning with Artificial Neural Networks (ANN). This method matches the\nmassive parallelism inherent in neural network software with the fine-grain\nparallelism of an FPGA hardware thereby dramatically reducing processing time.\nMars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices\nfor image processing, pyrotechnic operation control and obstacle avoidance. We\nsimulate and program our architecture on a Xilinx Virtex 7 FPGA. The\narchitectural implementation for a single neuron Q-learning and a more complex\nMultilayer Perception (MLP) Q-learning accelerator has been demonstrated. The\nresults show up to a 43-fold speed up by Virtex 7 FPGAs compared to a\nconventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed\narchitecture using the Symphony simulator and compiler from Xilinx, and\nevaluate the performance and power consumption.\n","negative":"  In this paper, for the first time, we study label propagation in\nheterogeneous graphs under heterophily assumption. Homophily label propagation\n(i.e., two connected nodes share similar labels) in homogeneous graph (with\nsame types of vertices and relations) has been extensively studied before.\nUnfortunately, real-life networks are heterogeneous, they contain different\ntypes of vertices (e.g., users, images, texts) and relations (e.g.,\nfriendships, co-tagging) and allow for each node to propagate both the same and\nopposite copy of labels to its neighbors. We propose a $\\mathcal{K}$-partite\nlabel propagation model to handle the mystifying combination of heterogeneous\nnodes\/relations and heterophily propagation. With this model, we develop a\nnovel label inference algorithm framework with update rules in near-linear time\ncomplexity. Since real networks change over time, we devise an incremental\napproach, which supports fast updates for both new data and evidence (e.g.,\nground truth labels) with guaranteed efficiency. We further provide a utility\nfunction to automatically determine whether an incremental or a re-modeling\napproach is favored. Extensive experiments on real datasets have verified the\neffectiveness and efficiency of our approach, and its superiority over the\nstate-of-the-art label propagation methods.\n","id":953}
{"Unnamed: 0.1":11954,"Unnamed: 0":11954.0,"anchor":"Dynamic Regret of Strongly Adaptive Methods","positive":"  To cope with changing environments, recent developments in online learning\nhave introduced the concepts of adaptive regret and dynamic regret\nindependently. In this paper, we illustrate an intrinsic connection between\nthese two concepts by showing that the dynamic regret can be expressed in terms\nof the adaptive regret and the functional variation. This observation implies\nthat strongly adaptive algorithms can be directly leveraged to minimize the\ndynamic regret. As a result, we present a series of strongly adaptive\nalgorithms that have small dynamic regrets for convex functions, exponentially\nconcave functions, and strongly convex functions, respectively. To the best of\nour knowledge, this is the first time that exponential concavity is utilized to\nupper bound the dynamic regret. Moreover, all of those adaptive algorithms do\nnot need any prior knowledge of the functional variation, which is a\nsignificant advantage over previous specialized methods for minimizing dynamic\nregret.\n","negative":"  We consider the problem of metric learning subject to a set of constraints on\nrelative-distance comparisons between the data items. Such constraints are\nmeant to reflect side-information that is not expressed directly in the feature\nvectors of the data items. The relative-distance constraints used in this work\nare particularly effective in expressing structures at finer level of detail\nthan must-link (ML) and cannot-link (CL) constraints, which are most commonly\nused for semi-supervised clustering. Relative-distance constraints are thus\nuseful in settings where providing an ML or a CL constraint is difficult\nbecause the granularity of the true clustering is unknown.\n  Our main contribution is an efficient algorithm for learning a kernel matrix\nusing the log determinant divergence --- a variant of the Bregman divergence\n--- subject to a set of relative-distance constraints. The learned kernel\nmatrix can then be employed by many different kernel methods in a wide range of\napplications. In our experimental evaluations, we consider a semi-supervised\nclustering setting and show empirically that kernels found by our algorithm\nyield clusterings of higher quality than existing approaches that either use\nML\/CL constraints or a different means to implement the supervision using\nrelative comparisons.\n","id":954}
{"Unnamed: 0.1":11955,"Unnamed: 0":11955.0,"anchor":"Fast and Accurate Time Series Classification with WEASEL","positive":"  Time series (TS) occur in many scientific and commercial applications,\nranging from earth surveillance to industry automation to the smart grids. An\nimportant type of TS analysis is classification, which can, for instance,\nimprove energy load forecasting in smart grids by detecting the types of\nelectronic devices based on their energy consumption profiles recorded by\nautomatic sensors. Such sensor-driven applications are very often characterized\nby (a) very long TS and (b) very large TS datasets needing classification.\nHowever, current methods to time series classification (TSC) cannot cope with\nsuch data volumes at acceptable accuracy; they are either scalable but offer\nonly inferior classification quality, or they achieve state-of-the-art\nclassification quality but cannot scale to large data volumes.\n  In this paper, we present WEASEL (Word ExtrAction for time SEries\ncLassification), a novel TSC method which is both scalable and accurate. Like\nother state-of-the-art TSC methods, WEASEL transforms time series into feature\nvectors, using a sliding-window approach, which are then analyzed through a\nmachine learning classifier. The novelty of WEASEL lies in its specific method\nfor deriving features, resulting in a much smaller yet much more discriminative\nfeature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more\naccurate than the best current non-ensemble algorithms at orders-of-magnitude\nlower classification and training times, and it is almost as accurate as\nensemble classifiers, whose computational complexity makes them inapplicable\neven for mid-size datasets. The outstanding robustness of WEASEL is also\nconfirmed by experiments on two real smart grid datasets, where it\nout-of-the-box achieves almost the same accuracy as highly tuned,\ndomain-specific methods.\n","negative":"  Given a knowledge base or KB containing (noisy) facts about common nouns or\ngenerics, such as \"all trees produce oxygen\" or \"some animals live in forests\",\nwe consider the problem of inferring additional such facts at a precision\nsimilar to that of the starting KB. Such KBs capture general knowledge about\nthe world, and are crucial for various applications such as question answering.\nDifferent from commonly studied named entity KBs such as Freebase, generics KBs\ninvolve quantification, have more complex underlying regularities, tend to be\nmore incomplete, and violate the commonly used locally closed world assumption\n(LCWA). We show that existing KB completion methods struggle with this new\ntask, and present the first approach that is successful. Our results\ndemonstrate that external information, such as relation schemas and entity\ntaxonomies, if used appropriately, can be a surprisingly powerful tool in this\nsetting. First, our simple yet effective knowledge guided tensor factorization\napproach achieves state-of-the-art results on two generics KBs (80% precise)\nfor science, doubling their size at 74%-86% precision. Second, our novel\ntaxonomy guided, submodular, active learning method for collecting annotations\nabout rare entities (e.g., oriole, a bird) is 6x more effective at inferring\nfurther new facts about them than multiple active learning baselines.\n","id":955}
{"Unnamed: 0.1":11956,"Unnamed: 0":11956.0,"anchor":"Theoretical Foundations of Forward Feature Selection Methods based on\n  Mutual Information","positive":"  Feature selection problems arise in a variety of applications, such as\nmicroarray analysis, clinical prediction, text categorization, image\nclassification and face recognition, multi-label learning, and classification\nof internet traffic. Among the various classes of methods, forward feature\nselection methods based on mutual information have become very popular and are\nwidely used in practice. However, comparative evaluations of these methods have\nbeen limited by being based on specific datasets and classifiers. In this\npaper, we develop a theoretical framework that allows evaluating the methods\nbased on their theoretical properties. Our framework is grounded on the\nproperties of the target objective function that the methods try to\napproximate, and on a novel categorization of features, according to their\ncontribution to the explanation of the class; we derive upper and lower bounds\nfor the target objective function and relate these bounds with the feature\ntypes. Then, we characterize the types of approximations taken by the methods,\nand analyze how these approximations cope with the good properties of the\ntarget objective function. Additionally, we develop a distributional setting\ndesigned to illustrate the various deficiencies of the methods, and provide\nseveral examples of wrong feature selections. Based on our work, we identify\nclearly the methods that should be avoided, and the methods that currently have\nthe best performance.\n","negative":"  An emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks.\n","id":956}
{"Unnamed: 0.1":11957,"Unnamed: 0":11957.0,"anchor":"Riemannian-geometry-based modeling and clustering of network-wide\n  non-stationary time series: The brain-network case","positive":"  This paper advocates Riemannian multi-manifold modeling in the context of\nnetwork-wide non-stationary time-series analysis. Time-series data, collected\nsequentially over time and across a network, yield features which are viewed as\npoints in or close to a union of multiple submanifolds of a Riemannian\nmanifold, and distinguishing disparate time series amounts to clustering\nmultiple Riemannian submanifolds. To support the claim that exploiting the\nlatent Riemannian geometry behind many statistical features of time series is\nbeneficial to learning from network data, this paper focuses on brain networks\nand puts forth two feature-generation schemes for network-wide dynamic time\nseries. The first is motivated by Granger-causality arguments and uses an\nauto-regressive moving average model to map low-rank linear vector subspaces,\nspanned by column vectors of appropriately defined observability matrices, to\npoints into the Grassmann manifold. The second utilizes (non-linear)\ndependencies among network nodes by introducing kernel-based partial\ncorrelations to generate points in the manifold of positive-definite matrices.\nCapitilizing on recently developed research on clustering Riemannian\nsubmanifolds, an algorithm is provided for distinguishing time series based on\ntheir geometrical properties, revealed within Riemannian feature spaces.\nExtensive numerical tests demonstrate that the proposed framework outperforms\nclassical and state-of-the-art techniques in clustering brain-network\nstates\/structures hidden beneath synthetic fMRI time series and brain-activity\nsignals generated from real brain-network structural connectivity matrices.\n","negative":"  Variational auto-encoders (VAE) are scalable and powerful generative models.\nHowever, the choice of the variational posterior determines tractability and\nflexibility of the VAE. Commonly, latent variables are modeled using the normal\ndistribution with a diagonal covariance matrix. This results in computational\nefficiency but typically it is not flexible enough to match the true posterior\ndistribution. One fashion of enriching the variational posterior distribution\nis application of normalizing flows, i.e., a series of invertible\ntransformations to latent variables with a simple posterior. In this paper, we\nfollow this line of thinking and propose a volume-preserving flow that uses a\nseries of Householder transformations. We show empirically on MNIST dataset and\nhistopathology data that the proposed flow allows to obtain more flexible\nvariational posterior and competitive results comparing to other normalizing\nflows.\n","id":957}
{"Unnamed: 0.1":11958,"Unnamed: 0":11958.0,"anchor":"Linear convergence of SDCA in statistical estimation","positive":"  In this paper, we consider stochastic dual coordinate (SDCA) {\\em without}\nstrongly convex assumption or convex assumption. We show that SDCA converges\nlinearly under mild conditions termed restricted strong convexity. This covers\na wide array of popular statistical models including Lasso, group Lasso, and\nlogistic regression with $\\ell_1$ regularization, corrected Lasso and linear\nregression with SCAD regularizer. This significantly improves previous\nconvergence results on SDCA for problems that are not strongly convex. As a by\nproduct, we derive a dual free form of SDCA that can handle general\nregularization term, which is of interest by itself.\n","negative":"  An intriguing property of deep neural networks is the existence of\nadversarial examples, which can transfer among different architectures. These\ntransferable adversarial examples may severely hinder deep neural network-based\napplications. Previous works mostly study the transferability using small scale\ndatasets. In this work, we are the first to conduct an extensive study of the\ntransferability over large models and a large scale dataset, and we are also\nthe first to study the transferability of targeted adversarial examples with\ntheir target labels. We study both non-targeted and targeted adversarial\nexamples, and show that while transferable non-targeted adversarial examples\nare easy to find, targeted adversarial examples generated using existing\napproaches almost never transfer with their target labels. Therefore, we\npropose novel ensemble-based approaches to generating transferable adversarial\nexamples. Using such approaches, we observe a large proportion of targeted\nadversarial examples that are able to transfer with their target labels for the\nfirst time. We also present some geometric studies to help understanding the\ntransferable adversarial examples. Finally, we show that the adversarial\nexamples generated using ensemble-based approaches can successfully attack\nClarifai.com, which is a black-box image classification system.\n","id":958}
{"Unnamed: 0.1":11959,"Unnamed: 0":11959.0,"anchor":"DroidStar: Callback Typestates for Android Classes","positive":"  Event-driven programming frameworks, such as Android, are based on components\nwith asynchronous interfaces. The protocols for interacting with these\ncomponents can often be described by finite-state machines we dub *callback\ntypestates*. Callback typestates are akin to classical typestates, with the\ndifference that their outputs (callbacks) are produced asynchronously. While\nuseful, these specifications are not commonly available, because writing them\nis difficult and error-prone.\n  Our goal is to make the task of producing callback typestates significantly\neasier. We present a callback typestate assistant tool, DroidStar, that\nrequires only limited user interaction to produce a callback typestate. Our\napproach is based on an active learning algorithm, L*. We improved the\nscalability of equivalence queries (a key component of L*), thus making active\nlearning tractable on the Android system.\n  We use DroidStar to learn callback typestates for Android classes both for\ncases where one is already provided by the documentation, and for cases where\nthe documentation is unclear. The results show that DroidStar learns callback\ntypestates accurately and efficiently. Moreover, in several cases, the\nsynthesized callback typestates uncovered surprising and undocumented\nbehaviors.\n","negative":"  Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article.\n","id":959}
{"Unnamed: 0.1":11960,"Unnamed: 0":11960.0,"anchor":"An Empirical Analysis of Feature Engineering for Predictive Modeling","positive":"  Machine learning models, such as neural networks, decision trees, random\nforests, and gradient boosting machines, accept a feature vector, and provide a\nprediction. These models learn in a supervised fashion where we provide feature\nvectors mapped to the expected output. It is common practice to engineer new\nfeatures from the provided feature set. Such engineered features will either\naugment or replace portions of the existing feature vector. These engineered\nfeatures are essentially calculated fields based on the values of the other\nfeatures.\n  Engineering such features is primarily a manual, time-consuming task.\nAdditionally, each type of model will respond differently to different kinds of\nengineered features. This paper reports empirical research to demonstrate what\nkinds of engineered features are best suited to various machine learning model\ntypes. We provide this recommendation by generating several datasets that we\ndesigned to benefit from a particular type of engineered feature. The\nexperiment demonstrates to what degree the machine learning model can\nsynthesize the needed feature on its own. If a model can synthesize a planned\nfeature, it is not necessary to provide that feature. The research demonstrated\nthat the studied models do indeed perform differently with various types of\nengineered features.\n","negative":"  We explore learning-based approaches for feedback control of a dexterous\nfive-finger hand performing non-prehensile manipulation. First, we learn local\ncontrollers that are able to perform the task starting at a predefined initial\nstate. These controllers are constructed using trajectory optimization with\nrespect to locally-linear time-varying models learned directly from sensor\ndata. In some cases, we initialize the optimizer with human demonstrations\ncollected via teleoperation in a virtual environment. We demonstrate that such\ncontrollers can perform the task robustly, both in simulation and on the\nphysical platform, for a limited range of initial conditions around the trained\nstarting state. We then consider two interpolation methods for generalizing to\na wider range of initial conditions: deep learning, and nearest neighbors. We\nfind that nearest neighbors achieve higher performance. Nevertheless, the\nneural network has its advantages: it uses only tactile and proprioceptive\nfeedback but no visual feedback about the object (i.e. it performs the task\nblind) and learns a time-invariant policy. In contrast, the nearest neighbors\nmethod switches between time-varying local controllers based on the proximity\nof initial object states sensed via motion capture. While both generalization\nmethods leave room for improvement, our work shows that (i) local\ntrajectory-based controllers for complex non-prehensile manipulation tasks can\nbe constructed from surprisingly small amounts of training data, and (ii)\ncollections of such controllers can be interpolated to form more global\ncontrollers. Results are summarized in the supplementary video:\nhttps:\/\/youtu.be\/E0wmO6deqjo\n","id":960}
{"Unnamed: 0.1":11961,"Unnamed: 0":11961.0,"anchor":"Wasserstein GAN","positive":"  We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.\n","negative":"  Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.\n","id":961}
{"Unnamed: 0.1":11962,"Unnamed: 0":11962.0,"anchor":"Information Theoretic Limits for Linear Prediction with Graph-Structured\n  Sparsity","positive":"  We analyze the necessary number of samples for sparse vector recovery in a\nnoisy linear prediction setup. This model includes problems such as linear\nregression and classification. We focus on structured graph models. In\nparticular, we prove that sufficient number of samples for the weighted graph\nmodel proposed by Hegde and others is also necessary. We use the Fano's\ninequality on well constructed ensembles as our main tool in establishing\ninformation theoretic lower bounds.\n","negative":"  In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.).\n","id":962}
{"Unnamed: 0.1":11963,"Unnamed: 0":11963.0,"anchor":"The Price of Differential Privacy For Online Learning","positive":"  We design differentially private algorithms for the problem of online linear\noptimization in the full information and bandit settings with optimal\n$\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our\nresults demonstrate that $\\epsilon$-differential privacy may be ensured for\nfree -- in particular, the regret bounds scale as\n$O(\\sqrt{T})+\\tilde{O}\\left(\\frac{1}{\\epsilon}\\right)$. For bandit linear\noptimization, and as a special case, for non-stochastic multi-armed bandits,\nthe proposed algorithm achieves a regret of\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}\\sqrt{T}\\right)$, while the previously known\nbest regret bound was\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}T^{\\frac{2}{3}}\\right)$.\n","negative":"  We analyze online \\cite{BottouBengio} and mini-batch \\cite{Sculley} $k$-means\nvariants. Both scale up the widely used $k$-means algorithm via stochastic\napproximation, and have become popular for large-scale clustering and\nunsupervised feature learning. We show, for the first time, that starting with\nany initial solution, they converge to a \"local optimum\" at rate\n$O(\\frac{1}{t})$ (in terms of the $k$-means objective) under general\nconditions. In addition, we show if the dataset is clusterable, when\ninitialized with a simple and scalable seeding algorithm, mini-batch $k$-means\nconverges to an optimal $k$-means solution at rate $O(\\frac{1}{t})$ with high\nprobability. The $k$-means objective is non-convex and non-differentiable: we\nexploit ideas from recent work on stochastic gradient descent for non-convex\nproblems \\cite{ge:sgd_tensor, balsubramani13} by providing a novel\ncharacterization of the trajectory of $k$-means algorithm on its solution\nspace, and circumvent the non-differentiability problem via geometric insights\nabout $k$-means update.\n","id":963}
{"Unnamed: 0.1":11964,"Unnamed: 0":11964.0,"anchor":"Reinforced stochastic gradient descent for deep neural network learning","positive":"  Stochastic gradient descent (SGD) is a standard optimization method to\nminimize a training error with respect to network parameters in modern neural\nnetwork learning. However, it typically suffers from proliferation of saddle\npoints in the high-dimensional parameter space. Therefore, it is highly\ndesirable to design an efficient algorithm to escape from these saddle points\nand reach a parameter region of better generalization capabilities. Here, we\npropose a simple extension of SGD, namely reinforced SGD, which simply adds\nprevious first-order gradients in a stochastic manner with a probability that\nincreases with learning time. As verified in a simple synthetic dataset, this\nmethod significantly accelerates learning compared with the original SGD.\nSurprisingly, it dramatically reduces over-fitting effects, even compared with\nstate-of-the-art adaptive learning algorithm---Adam. For a benchmark\nhandwritten digits dataset, the learning performance is comparable to Adam, yet\nwith an extra advantage of requiring one-fold less computer memory. The\nreinforced SGD is also compared with SGD with fixed or adaptive momentum\nparameter and Nesterov's momentum, which shows that the proposed framework is\nable to reach a similar generalization accuracy with less computational costs.\nOverall, our method introduces stochastic memory into gradients, which plays an\nimportant role in understanding how gradient-based training algorithms can work\nand its relationship with generalization abilities of deep networks.\n","negative":"  Generative Adversarial Networks (GAN) have limitations when the goal is to\ngenerate sequences of discrete elements. The reason for this is that samples\nfrom a distribution on discrete objects such as the multinomial are not\ndifferentiable with respect to the distribution parameters. This problem can be\navoided by using the Gumbel-softmax distribution, which is a continuous\napproximation to a multinomial distribution parameterized in terms of the\nsoftmax function. In this work, we evaluate the performance of GANs based on\nrecurrent neural networks with Gumbel-softmax output distributions in the task\nof generating sequences of discrete elements.\n","id":964}
{"Unnamed: 0.1":11965,"Unnamed: 0":11965.0,"anchor":"Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes","positive":"  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins\/draws\/losses, batch\/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n","negative":"  We show that the matching problem that underlies optical flow requires\nmultiple strategies, depending on the amount of image motion and other factors.\nWe then study the implications of this observation on training a deep neural\nnetwork for representing image patches in the context of descriptor based\noptical flow. We propose a metric learning method, which selects suitable\nnegative samples based on the nature of the true match. This type of training\nproduces a network that displays multiple strategies depending on the input and\nleads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow\nbenchmarks.\n","id":965}
{"Unnamed: 0.1":11966,"Unnamed: 0":11966.0,"anchor":"Model-Free Control of Thermostatically Controlled Loads Connected to a\n  District Heating Network","positive":"  Optimal control of thermostatically controlled loads connected to a district\nheating network is considered a sequential decision- making problem under\nuncertainty. The practicality of a direct model-based approach is compromised\nby two challenges, namely scalability due to the large dimensionality of the\nproblem and the system identification required to identify an accurate model.\nTo help in mitigating these problems, this paper leverages on recent\ndevelopments in reinforcement learning in combination with a market-based\nmulti-agent system to obtain a scalable solution that obtains a significant\nperformance improvement in a practical learning time. The control approach is\napplied on a scenario comprising 100 thermostatically controlled loads\nconnected to a radial district heating network supplied by a central combined\nheat and power plant. Both for an energy arbitrage and a peak shaving\nobjective, the control approach requires 60 days to obtain a performance within\n65% of a theoretical lower bound on the cost.\n","negative":"  We study the task of teaching a machine to classify objects using features\nand labels. We introduce the Error-Driven-Featuring design pattern for teaching\nusing features and labels in which a teacher prefers to introduce features only\nif they are needed. We analyze the potential risks and benefits of this\nteaching pattern through the use of teaching protocols, illustrative examples,\nand by providing bounds on the effort required for an optimal machine teacher\nusing a linear learning algorithm, the most commonly used type of learners in\ninteractive machine learning systems. Our analysis provides a deeper\nunderstanding of potential trade-offs of using different learning algorithms\nand between the effort required for featuring (creating new features) and\nlabeling (providing labels for objects).\n","id":966}
{"Unnamed: 0.1":11967,"Unnamed: 0":11967.0,"anchor":"Faster Discovery of Faster System Configurations with Spectral Learning","positive":"  Despite the huge spread and economical importance of configurable software\nsystems, there is unsatisfactory support in utilizing the full potential of\nthese systems with respect to finding performance-optimal configurations. Prior\nwork on predicting the performance of software configurations suffered from\neither (a) requiring far too many sample configurations or (b) large variances\nin their predictions. Both these problems can be avoided using the WHAT\nspectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of\nthe distance matrix between the configurations of a configurable software\nsystem, to perform dimensionality reduction. Within that reduced configuration\nspace, many closely associated configurations can be studied by executing only\na few sample configurations. For the subject systems studied here, a few dozen\nsamples yield accurate and stable predictors - less than 10% prediction error,\nwith a standard deviation of less than 2%. When compared to the state of the\nart, WHAT (a) requires 2 to 10 times fewer samples to achieve similar\nprediction accuracies, and (b) its predictions are more stable (i.e., have\nlower standard deviation). Furthermore, we demonstrate that predictive models\ngenerated by WHAT can be used by optimizers to discover system configurations\nthat closely approach the optimal performance.\n","negative":"  Regular medical records are useful for medical practitioners to analyze and\nmonitor patient health status especially for those with chronic disease, but\nsuch records are usually incomplete due to unpunctuality and absence of\npatients. In order to resolve the missing data problem over time, tensor-based\nmodel is suggested for missing data imputation in recent papers because this\napproach makes use of low rank tensor assumption for highly correlated data.\nHowever, when the time intervals between records are long, the data correlation\nis not high along temporal direction and such assumption is not valid. To\naddress this problem, we propose to decompose a matrix with missing data into\nits latent factors. Then, the locally linear constraint is imposed on these\nfactors for matrix completion in this paper. By using a publicly available\ndataset and two medical datasets collected from hospital, experimental results\nshow that the proposed algorithm achieves the best performance by comparing\nwith the existing methods.\n","id":967}
{"Unnamed: 0.1":11968,"Unnamed: 0":11968.0,"anchor":"Multiclass MinMax Rank Aggregation","positive":"  We introduce a new family of minmax rank aggregation problems under two\ndistance measures, the Kendall {\\tau} and the Spearman footrule. As the\nproblems are NP-hard, we proceed to describe a number of constant-approximation\nalgorithms for solving them. We conclude with illustrative applications of the\naggregation methods on the Mallows model and genomic data.\n","negative":"  Discrimination discovery and prevention\/removal are increasingly important\ntasks in data mining. Discrimination discovery aims to unveil discriminatory\npractices on the protected attribute (e.g., gender) by analyzing the dataset of\nhistorical decision records, and discrimination prevention aims to remove\ndiscrimination by modifying the biased data before conducting predictive\nanalysis. In this paper, we show that the key to discrimination discovery and\nprevention is to find the meaningful partitions that can be used to provide\nquantitative evidences for the judgment of discrimination. With the support of\nthe causal graph, we present a graphical condition for identifying a meaningful\npartition. Based on that, we develop a simple criterion for the claim of\nnon-discrimination, and propose discrimination removal algorithms which\naccurately remove discrimination while retaining good data utility. Experiments\nusing real datasets show the effectiveness of our approaches.\n","id":968}
{"Unnamed: 0.1":11969,"Unnamed: 0":11969.0,"anchor":"Deep Recurrent Neural Network for Protein Function Prediction from\n  Sequence","positive":"  As high-throughput biological sequencing becomes faster and cheaper, the need\nto extract useful information from sequencing becomes ever more paramount,\noften limited by low-throughput experimental characterizations. For proteins,\naccurate prediction of their functions directly from their primary amino-acid\nsequences has been a long standing challenge. Here, machine learning using\nartificial recurrent neural networks (RNN) was applied towards classification\nof protein function directly from primary sequence without sequence alignment,\nheuristic scoring or feature engineering. The RNN models containing\nlong-short-term-memory (LSTM) units trained on public, annotated datasets from\nUniProt achieved high performance for in-class prediction of four important\nprotein functions tested, particularly compared to other machine learning\nalgorithms using sequence-derived protein features. RNN models were used also\nfor out-of-class predictions of phylogenetically distinct protein families with\nsimilar functions, including proteins of the CRISPR-associated nuclease,\nferritin-like iron storage and cytochrome P450 families. Applying the trained\nRNN models on the partially unannotated UniRef100 database predicted not only\ncandidates validated by existing annotations but also currently unannotated\nsequences. Some RNN predictions for the ferritin-like iron sequestering\nfunction were experimentally validated, even though their sequences differ\nsignificantly from known, characterized proteins and from each other and cannot\nbe easily predicted using popular bioinformatics methods. As sequencing and\nexperimental characterization data increases rapidly, the machine-learning\napproach based on RNN could be useful for discovery and prediction of\nhomologues for a wide range of protein functions.\n","negative":"  Software estimation is a crucial task in software engineering. Software\nestimation encompasses cost, effort, schedule, and size. The importance of\nsoftware estimation becomes critical in the early stages of the software life\ncycle when the details of software have not been revealed yet. Several\ncommercial and non-commercial tools exist to estimate software in the early\nstages. Most software effort estimation methods require software size as one of\nthe important metric inputs and consequently, software size estimation in the\nearly stages becomes essential. One of the approaches that has been used for\nabout two decades in the early size and effort estimation is called use case\npoints. Use case points method relies on the use case diagram to estimate the\nsize and effort of software projects. Although the use case points method has\nbeen widely used, it has some limitations that might adversely affect the\naccuracy of estimation. This paper presents some techniques using fuzzy logic\nand neural networks to improve the accuracy of the use case points method.\nResults showed that an improvement up to 22% can be obtained using the proposed\napproach.\n","id":969}
{"Unnamed: 0.1":11970,"Unnamed: 0":11970.0,"anchor":"Feature base fusion for splicing forgery detection based on neuro fuzzy","positive":"  Most of researches on image forensics have been mainly focused on detection\nof artifacts introduced by a single processing tool. They lead in the\ndevelopment of many specialized algorithms looking for one or more particular\nfootprints under specific settings. Naturally, the performance of such\nalgorithms are not perfect, and accordingly the provided output might be noisy,\ninaccurate and only partially correct. Furthermore, a forged image in practical\nscenarios is often the result of utilizing several tools available by\nimage-processing software systems. Therefore, reliable tamper detection\nrequires developing more poweful tools to deal with various tempering\nscenarios. Fusion of forgery detection tools based on Fuzzy Inference System\nhas been used before for addressing this problem. Adjusting the membership\nfunctions and defining proper fuzzy rules for attaining to better results are\ntime-consuming processes. This can be accounted as main disadvantage of fuzzy\ninference systems. In this paper, a Neuro-Fuzzy inference system for fusion of\nforgery detection tools is developed. The neural network characteristic of\nthese systems provides appropriate tool for automatically adjusting the\nmembership functions. Moreover, initial fuzzy inference system is generated\nbased on fuzzy clustering techniques. The proposed framework is implemented and\nvalidated on a benchmark image splicing data set in which three forgery\ndetection tools are fused based on adaptive Neuro-Fuzzy inference system. The\noutcome of the proposed method reveals that applying Neuro Fuzzy inference\nsystems could be a better approach for fusion of forgery detection tools.\n","negative":"  Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST.\n","id":970}
{"Unnamed: 0.1":11971,"Unnamed: 0":11971.0,"anchor":"When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum","positive":"  The study of complex systems benefits from graph models and their analysis.\nIn particular, the eigendecomposition of the graph Laplacian lets emerge\nproperties of global organization from local interactions; e.g., the Fiedler\nvector has the smallest non-zero eigenvalue and plays a key role for graph\nclustering. Graph signal processing focusses on the analysis of signals that\nare attributed to the graph nodes. The eigendecomposition of the graph\nLaplacian allows to define the graph Fourier transform and extend conventional\nsignal-processing operations to graphs. Here, we introduce the design of\nSlepian graph signals, by maximizing energy concentration in a predefined\nsubgraph for a graph spectral bandlimit. We establish a novel link with\nclassical Laplacian embedding and graph clustering, which provides a meaning to\nlocalized graph frequencies.\n","negative":"  Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.\n","id":971}
{"Unnamed: 0.1":11972,"Unnamed: 0":11972.0,"anchor":"On the Local Structure of Stable Clustering Instances","positive":"  We study the classic $k$-median and $k$-means clustering objectives in the\nbeyond-worst-case scenario. We consider three well-studied notions of\nstructured data that aim at characterizing real-world inputs: Distribution\nStability (introduced by Awasthi, Blum, and Sheffet, FOCS 2010), Spectral\nSeparability (introduced by Kumar and Kannan, FOCS 2010), Perturbation\nResilience (introduced by Bilu and Linial, ICS 2010).\n  We prove structural results showing that inputs satisfying at least one of\nthe conditions are inherently \"local\". Namely, for any such input, any local\noptimum is close both in term of structure and in term of objective value to\nthe global optima.\n  As a corollary we obtain that the widely-used Local Search algorithm has\nstrong performance guarantees for both the tasks of recovering the underlying\noptimal clustering and obtaining a clustering of small cost. This is a\nsignificant step toward understanding the success of local search heuristics in\nclustering applications.\n","negative":"  Asynchronous parallel computing and sparse recovery are two areas that have\nreceived recent interest. Asynchronous algorithms are often studied to solve\noptimization problems where the cost function takes the form $\\sum_{i=1}^M\nf_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each\n$f_i$ acts only on a small number of components of $x\\in\\mathbb{R}^n$. Sparse\nrecovery problems, such as compressed sensing, can be formulated as\noptimization problems, however, the cost functions $f_i$ are dense with respect\nto the components of $x$, and instead the signal $x$ is assumed to be sparse,\nmeaning that it has only $s$ non-zeros where $s\\ll n$. Here we address how one\nmay use an asynchronous parallel architecture when the cost functions $f_i$ are\nnot sparse in $x$, but rather the signal $x$ is sparse. We propose an\nasynchronous parallel approach to sparse recovery via a stochastic greedy\nalgorithm, where multiple processors asynchronously update a vector in shared\nmemory containing information on the estimated signal support. We include\nnumerical simulations that illustrate the potential benefits of our proposed\nasynchronous method.\n","id":972}
{"Unnamed: 0.1":11973,"Unnamed: 0":11973.0,"anchor":"Transformation-Based Models of Video Sequences","positive":"  In this work we propose a simple unsupervised approach for next frame\nprediction in video. Instead of directly predicting the pixels in a frame given\npast frames, we predict the transformations needed for generating the next\nframe in a sequence, given the transformations of the past frames. This leads\nto sharper results, while using a smaller prediction model.\n  In order to enable a fair comparison between different video frame prediction\nmodels, we also propose a new evaluation protocol. We use generated frames as\ninput to a classifier trained with ground truth sequences. This criterion\nguarantees that models scoring high are those producing sequences which\npreserve discrim- inative features, as opposed to merely penalizing any\ndeviation, plausible or not, from the ground truth. Our proposed approach\ncompares favourably against more sophisticated ones on the UCF-101 data set,\nwhile also being more efficient in terms of the number of parameters and\ncomputational cost.\n","negative":"  In this paper, we propose an end-to-end neural network (NN) based EEG-speech\n(NES) modeling framework, in which three network structures are developed to\nmap imagined EEG signals to phonemes. The proposed NES models incorporate a\nlanguage model based EEG feature extraction layer, an acoustic feature mapping\nlayer, and a restricted Boltzmann machine (RBM) based the feature learning\nlayer. The NES models can jointly realize the representation of multichannel\nEEG signals and the projection of acoustic speech signals. Among three proposed\nNES models, two augmented networks utilize spoken EEG signals as either bias or\ngate information to strengthen the feature learning and translation of imagined\nEEG signals. Experimental results show that all three proposed NES models\noutperform the baseline support vector machine (SVM) method on EEG-speech\nclassification. With respect to binary classification, our approach achieves\ncomparable results relative to deep believe network approach.\n","id":973}
{"Unnamed: 0.1":11974,"Unnamed: 0":11974.0,"anchor":"Predicting SMT Solver Performance for Software Verification","positive":"  The Why3 IDE and verification system facilitates the use of a wide range of\nSatisfiability Modulo Theories (SMT) solvers through a driver-based\narchitecture. We present Where4: a portfolio-based approach to discharge Why3\nproof obligations. We use data analysis and machine learning techniques on\nstatic metrics derived from program source code. Our approach benefits software\nengineers by providing a single utility to delegate proof obligations to the\nsolvers most likely to return a useful result. It does this in a time-efficient\nway using existing Why3 and solver installations - without requiring low-level\nknowledge about SMT solver operation from the user.\n","negative":"  Normalization techniques have only recently begun to be exploited in\nsupervised learning tasks. Batch normalization exploits mini-batch statistics\nto normalize the activations. This was shown to speed up training and result in\nbetter models. However its success has been very limited when dealing with\nrecurrent neural networks. On the other hand, layer normalization normalizes\nthe activations across all activities within a layer. This was shown to work\nwell in the recurrent setting. In this paper we propose a unified view of\nnormalization techniques, as forms of divisive normalization, which includes\nlayer and batch normalization as special cases. Our second contribution is the\nfinding that a small modification to these normalization schemes, in\nconjunction with a sparse regularizer on the activations, leads to significant\nbenefits over standard normalization techniques. We demonstrate the\neffectiveness of our unified divisive normalization framework in the context of\nconvolutional neural nets and recurrent neural networks, showing improvements\nover baselines in image classification, language modeling as well as\nsuper-resolution.\n","id":974}
{"Unnamed: 0.1":11975,"Unnamed: 0":11975.0,"anchor":"Model-based Classification and Novelty Detection For Point Pattern Data","positive":"  Point patterns are sets or multi-sets of unordered elements that can be found\nin numerous data sources. However, in data analysis tasks such as\nclassification and novelty detection, appropriate statistical models for point\npattern data have not received much attention. This paper proposes the\nmodelling of point pattern data via random finite sets (RFS). In particular, we\npropose appropriate likelihood functions, and a maximum likelihood estimator\nfor learning a tractable family of RFS models. In novelty detection, we propose\nnovel ranking functions based on RFS models, which substantially improve\nperformance.\n","negative":"  How much can pruning algorithms teach us about the fundamentals of learning\nrepresentations in neural networks? And how much can these fundamentals help\nwhile devising new pruning techniques? A lot, it turns out. Neural network\npruning has become a topic of great interest in recent years, and many\ndifferent techniques have been proposed to address this problem. The decision\nof what to prune and when to prune necessarily forces us to confront our\nassumptions about how neural networks actually learn to represent patterns in\ndata. In this work, we set out to test several long-held hypotheses about\nneural network learning representations, approaches to pruning and the\nrelevance of one in the context of the other. To accomplish this, we argue in\nfavor of pruning whole neurons as opposed to the traditional method of pruning\nweights from optimally trained networks. We first review the historical\nliterature, point out some common assumptions it makes, and propose methods to\ndemonstrate the inherent flaws in these assumptions. We then propose our novel\napproach to pruning and set about analyzing the quality of the decisions it\nmakes. Our analysis led us to question the validity of many widely-held\nassumptions behind pruning algorithms and the trade-offs we often make in the\ninterest of reducing computational complexity. We discovered that there is a\nstraightforward way, however expensive, to serially prune 40-70% of the neurons\nin a trained network with minimal effect on the learning representation and\nwithout any re-training. It is to be noted here that the motivation behind this\nwork is not to propose an algorithm that would outperform all existing methods,\nbut to shed light on what some inherent flaws in these methods can teach us\nabout learning representations and how this can lead us to superior pruning\ntechniques.\n","id":975}
{"Unnamed: 0.1":11976,"Unnamed: 0":11976.0,"anchor":"Binary adaptive embeddings from order statistics of random projections","positive":"  We use some of the largest order statistics of the random projections of a\nreference signal to construct a binary embedding that is adapted to signals\ncorrelated with such signal. The embedding is characterized from the analytical\nstandpoint and shown to provide improved performance on tasks such as\nclassification in a reduced-dimensionality space.\n","negative":"  The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension\n$d\\ge 2$, however, for the 1D case there exists exact polynomial time\nalgorithms. Previous literature reported an $O(kn^2)$ time dynamic programming\nalgorithm that uses $O(kn)$ space. It turns out that the problem has been\nconsidered under a different name more than twenty years ago. We present all\nthe existing work that had been overlooked and compare the various solutions\ntheoretically. Moreover, we show how to reduce the space usage for some of\nthem, as well as generalize them to data structures that can quickly report an\noptimal $k$-Means clustering for any $k$. Finally we also generalize all the\nalgorithms to work for the absolute distance and to work for any Bregman\nDivergence. We complement our theoretical contributions by experiments that\ncompare the practical performance of the various algorithms.\n","id":976}
{"Unnamed: 0.1":11977,"Unnamed: 0":11977.0,"anchor":"Self-Adaptation of Activity Recognition Systems to New Sensors","positive":"  Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach.\n","negative":"  The weighted k-nearest neighbors algorithm is one of the most fundamental\nnon-parametric methods in pattern recognition and machine learning. The\nquestion of setting the optimal number of neighbors as well as the optimal\nweights has received much attention throughout the years, nevertheless this\nproblem seems to have remained unsettled. In this paper we offer a simple\napproach to locally weighted regression\/classification, where we make the\nbias-variance tradeoff explicit. Our formulation enables us to phrase a notion\nof optimal weights, and to efficiently find these weights as well as the\noptimal number of neighbors efficiently and adaptively, for each data point\nwhose value we wish to estimate. The applicability of our approach is\ndemonstrated on several datasets, showing superior performance over standard\nlocally weighted methods.\n","id":977}
{"Unnamed: 0.1":11978,"Unnamed: 0":11978.0,"anchor":"Variational Policy for Guiding Point Processes","positive":"  Temporal point processes have been widely applied to model event sequence\ndata generated by online users. In this paper, we consider the problem of how\nto design the optimal control policy for point processes, such that the\nstochastic system driven by the point process is steered to a target state. In\nparticular, we exploit the key insight to view the stochastic optimal control\nproblem from the perspective of optimal measure and variational inference. We\nfurther propose a convex optimization framework and an efficient algorithm to\nupdate the policy adaptively to the current system state. Experiments on\nsynthetic and real-world data show that our algorithm can steer the user\nactivities much more accurately and efficiently than other stochastic control\nmethods.\n","negative":"  We introduce highly efficient online nonlinear regression algorithms that are\nsuitable for real life applications. We process the data in a truly online\nmanner such that no storage is needed, i.e., the data is discarded after being\nused. For nonlinear modeling we use a hierarchical piecewise linear approach\nbased on the notion of decision trees where the space of the regressor vectors\nis adaptively partitioned based on the performance. As the first time in the\nliterature, we learn both the piecewise linear partitioning of the regressor\nspace as well as the linear models in each region using highly effective second\norder methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known\nover fitting issues by using piecewise linear models, however, since both the\nregion boundaries as well as the linear models in each region are trained using\nthe second order methods, we achieve substantial performance compared to the\nstate of the art. We demonstrate our gains over the well known benchmark data\nsets and provide performance results in an individual sequence manner\nguaranteed to hold without any statistical assumptions. Hence, the introduced\nalgorithms address computational complexity issues widely encountered in real\nlife applications while providing superior guaranteed performance in a strong\ndeterministic sense.\n","id":978}
{"Unnamed: 0.1":11979,"Unnamed: 0":11979.0,"anchor":"A Comparative Study on Different Types of Approaches to Bengali document\n  Categorization","positive":"  Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article.\n","negative":"  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins\/draws\/losses, batch\/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n","id":979}
{"Unnamed: 0.1":11980,"Unnamed: 0":11980.0,"anchor":"Predicting Auction Price of Vehicle License Plate with Deep Recurrent\n  Neural Network","positive":"  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n","negative":"  Factor analysis is broadly used as a powerful unsupervised machine learning\ntool for reconstruction of hidden features in recorded mixtures of signals. In\nthe case of a linear approximation, the mixtures can be decomposed by a variety\nof model-free Blind Source Separation (BSS) algorithms. Most of the available\nBSS algorithms consider an instantaneous mixing of signals, while the case when\nthe mixtures are linear combinations of signals with delays is less explored.\nEspecially difficult is the case when the number of sources of the signals with\ndelays is unknown and has to be determined from the data as well. To address\nthis problem, in this paper, we present a new method based on Nonnegative\nMatrix Factorization (NMF) that is capable of identifying: (a) the unknown\nnumber of the sources, (b) the delays and speed of propagation of the signals,\nand (c) the locations of the sources. Our method can be used to decompose\nrecords of mixtures of signals with delays emitted by an unknown number of\nsources in a nondispersive medium, based only on recorded data. This is the\ncase, for example, when electromagnetic signals from multiple antennas are\nreceived asynchronously; or mixtures of acoustic or seismic signals recorded by\nsensors located at different positions; or when a shift in frequency is induced\nby the Doppler effect. By applying our method to synthetic datasets, we\ndemonstrate its ability to identify the unknown number of sources as well as\nthe waveforms, the delays, and the strengths of the signals. Using Bayesian\nanalysis, we also evaluate estimation uncertainties and identify the region of\nlikelihood where the positions of the sources can be found.\n","id":980}
{"Unnamed: 0.1":11981,"Unnamed: 0":11981.0,"anchor":"Does Weather Matter? Causal Analysis of TV Logs","positive":"  Weather affects our mood and behaviors, and many aspects of our life. When it\nis sunny, most people become happier; but when it rains, some people get\ndepressed. Despite this evidence and the abundance of data, weather has mostly\nbeen overlooked in the machine learning and data science research. This work\npresents a causal analysis of how weather affects TV watching patterns. We show\nthat some weather attributes, such as pressure and precipitation, cause major\nchanges in TV watching patterns. To the best of our knowledge, this is the\nfirst large-scale causal study of the impact of weather on TV watching\npatterns.\n","negative":"  We study a variant of the variational autoencoder model (VAE) with a Gaussian\nmixture as a prior distribution, with the goal of performing unsupervised\nclustering through deep generative models. We observe that the known problem of\nover-regularisation that has been shown to arise in regular VAEs also manifests\nitself in our model and leads to cluster degeneracy. We show that a heuristic\ncalled minimum information constraint that has been shown to mitigate this\neffect in VAEs can also be applied to improve unsupervised clustering\nperformance with our model. Furthermore we analyse the effect of this heuristic\nand provide an intuition of the various processes with the help of\nvisualizations. Finally, we demonstrate the performance of our model on\nsynthetic data, MNIST and SVHN, showing that the obtained clusters are\ndistinct, interpretable and result in achieving competitive performance on\nunsupervised clustering to the state-of-the-art results.\n","id":981}
{"Unnamed: 0.1":11982,"Unnamed: 0":11982.0,"anchor":"Memory Augmented Neural Networks with Wormhole Connections","positive":"  Recent empirical results on long-term dependency tasks have shown that neural\nnetworks augmented with an external memory can learn the long-term dependency\ntasks more easily and achieve better generalization than vanilla recurrent\nneural networks (RNN). We suggest that memory augmented neural networks can\nreduce the effects of vanishing gradients by creating shortcut (or wormhole)\nconnections. Based on this observation, we propose a novel memory augmented\nneural network model called TARDIS (Temporal Automatic Relation Discovery in\nSequences). The controller of TARDIS can store a selective set of embeddings of\nits own previous hidden states into an external memory and revisit them as and\nwhen needed. For TARDIS, memory acts as a storage for wormhole connections to\nthe past to propagate the gradients more effectively and it helps to learn the\ntemporal dependencies. The memory structure of TARDIS has similarities to both\nNeural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but\nboth read and write operations of TARDIS are simpler and more efficient. We use\ndiscrete addressing for read\/write operations which helps to substantially to\nreduce the vanishing gradient problem with very long sequences. Read and write\noperations in TARDIS are tied with a heuristic once the memory becomes full,\nand this makes the learning problem simpler when compared to NTM or D-NTM type\nof architectures. We provide a detailed analysis on the gradient propagation in\ngeneral for MANNs. We evaluate our models on different long-term dependency\ntasks and report competitive results in all of them.\n","negative":"  Programming by Example (PBE) is the task of inducing computer programs from\ninput-output examples. It can be seen as a type of machine learning where the\nhypothesis space is the set of legal programs in some programming language.\nRecent work on differentiable interpreters relaxes the discrete space of\nprograms into a continuous space so that search over programs can be performed\nusing gradient-based optimization. While conceptually powerful, so far\ndifferentiable interpreter-based program synthesis has only been capable of\nsolving very simple problems. In this work, we study modeling choices that\narise when constructing a differentiable programming language and their impact\non the success of synthesis. The main motivation for the modeling choices comes\nfrom functional programming: we study the effect of memory allocation schemes,\nimmutable data, type systems, and built-in control-flow structures. Empirically\nwe show that incorporating functional programming ideas into differentiable\nprogramming languages allows us to learn much more complex programs than is\npossible with existing differentiable languages.\n","id":982}
{"Unnamed: 0.1":11983,"Unnamed: 0":11983.0,"anchor":"PathNet: Evolution Channels Gradient Descent in Super Neural Networks","positive":"  For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).\n","negative":"  Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.\n","id":983}
{"Unnamed: 0.1":11984,"Unnamed: 0":11984.0,"anchor":"Click Through Rate Prediction for Contextual Advertisment Using Linear\n  Regression","positive":"  This research presents an innovative and unique way of solving the\nadvertisement prediction problem which is considered as a learning problem over\nthe past several years. Online advertising is a multi-billion-dollar industry\nand is growing every year with a rapid pace. The goal of this research is to\nenhance click through rate of the contextual advertisements using Linear\nRegression. In order to address this problem, a new technique propose in this\npaper to predict the CTR which will increase the overall revenue of the system\nby serving the advertisements more suitable to the viewers with the help of\nfeature extraction and displaying the advertisements based on context of the\npublishers. The important steps include the data collection, feature\nextraction, CTR prediction and advertisement serving. The statistical results\nobtained from the dynamically used technique show an efficient outcome by\nfitting the data close to perfection for the LR technique using optimized\nfeature selection.\n","negative":"  Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.\n","id":984}
{"Unnamed: 0.1":11985,"Unnamed: 0":11985.0,"anchor":"Bayesian Learning of Consumer Preferences for Residential Demand\n  Response","positive":"  In coming years residential consumers will face real-time electricity tariffs\nwith energy prices varying day to day, and effective energy saving will require\nautomation - a recommender system, which learns consumer's preferences from her\nactions. A consumer chooses a scenario of home appliance use to balance her\ncomfort level and the energy bill. We propose a Bayesian learning algorithm to\nestimate the comfort level function from the history of appliance use. In\nnumeric experiments with datasets generated from a simulation model of a\nconsumer interacting with small home appliances the algorithm outperforms\npopular regression analysis tools. Our approach can be extended to control an\nair heating and conditioning system, which is responsible for up to half of a\nhousehold's energy bill.\n","negative":"  Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges.\n","id":985}
{"Unnamed: 0.1":11986,"Unnamed: 0":11986.0,"anchor":"Dynamic Task Allocation for Crowdsourcing Settings","positive":"  We consider the problem of optimal budget allocation for crowdsourcing\nproblems, allocating users to tasks to maximize our final confidence in the\ncrowdsourced answers. Such an optimized worker assignment method allows us to\nboost the efficacy of any popular crowdsourcing estimation algorithm. We\nconsider a mutual information interpretation of the crowdsourcing problem,\nwhich leads to a stochastic subset selection problem with a submodular\nobjective function. We present experimental simulation results which\ndemonstrate the effectiveness of our dynamic task allocation method for\nachieving higher accuracy, possibly requiring fewer labels, as well as\nimproving upon a previous method which is sensitive to the proportion of users\nto questions.\n","negative":"  This paper demonstrates the feasibility of learning to retrieve short\nsnippets of sheet music (images) when given a short query excerpt of music\n(audio) -- and vice versa --, without any symbolic representation of music or\nscores. This would be highly useful in many content-based musical retrieval\nscenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)\nand learns correlated latent spaces allowing for cross-modality retrieval in\nboth directions. Initial experiments with relatively simple monophonic music\nshow promising results.\n","id":986}
{"Unnamed: 0.1":11987,"Unnamed: 0":11987.0,"anchor":"Learning from various labeling strategies for suicide-related messages\n  on social media: An experimental study","positive":"  Suicide is an important but often misunderstood problem, one that researchers\nare now seeking to better understand through social media. Due in large part to\nthe fuzzy nature of what constitutes suicidal risks, most supervised approaches\nfor learning to automatically detect suicide-related activity in social media\nrequire a great deal of human labor to train. However, humans themselves have\ndiverse or conflicting views on what constitutes suicidal thoughts. So how to\nobtain reliable gold standard labels is fundamentally challenging and, we\nhypothesize, depends largely on what is asked of the annotators and what slice\nof the data they label. We conducted multiple rounds of data labeling and\ncollected annotations from crowdsourcing workers and domain experts. We\naggregated the resulting labels in various ways to train a series of supervised\nmodels. Our preliminary evaluations show that using unanimously agreed labels\nfrom multiple annotators is helpful to achieve robust machine models.\n","negative":"  Student-$t$ processes have recently been proposed as an appealing alternative\nnon-parameteric function prior. They feature enhanced flexibility and\npredictive variance. In this work the use of Student-$t$ processes are explored\nfor multi-objective Bayesian optimization. In particular, an analytical\nexpression for the hypervolume-based probability of improvement is developed\nfor independent Student-$t$ process priors of the objectives. Its effectiveness\nis shown on a multi-objective optimization problem which is known to be\ndifficult with traditional Gaussian processes.\n","id":987}
{"Unnamed: 0.1":11988,"Unnamed: 0":11988.0,"anchor":"Reinforcement Learning Algorithm Selection","positive":"  This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin.\n","negative":"  In this paper, we propose a generative model, Temporal Generative Adversarial\nNets (TGAN), which can learn a semantic representation of unlabeled videos, and\nis capable of generating videos. Unlike existing Generative Adversarial Nets\n(GAN)-based methods that generate videos with a single generator consisting of\n3D deconvolutional layers, our model exploits two different types of\ngenerators: a temporal generator and an image generator. The temporal generator\ntakes a single latent variable as input and outputs a set of latent variables,\neach of which corresponds to an image frame in a video. The image generator\ntransforms a set of such latent variables into a video. To deal with\ninstability in training of GAN with such advanced networks, we adopt a recently\nproposed model, Wasserstein GAN, and propose a novel method to train it stably\nin an end-to-end manner. The experimental results demonstrate the effectiveness\nof our methods.\n","id":988}
{"Unnamed: 0.1":11989,"Unnamed: 0":11989.0,"anchor":"Fully Convolutional Architectures for Multi-Class Segmentation in Chest\n  Radiographs","positive":"  The success of deep convolutional neural networks on image classification and\nrecognition tasks has led to new applications in very diversified contexts,\nincluding the field of medical imaging. In this paper we investigate and\npropose neural network architectures for automated multi-class segmentation of\nanatomical organs in chest radiographs, namely for lungs, clavicles and heart.\nWe address several open challenges including model overfitting, reducing number\nof parameters and handling of severely imbalanced data in CXR by fusing recent\nconcepts in convolutional networks and adapting them to the segmentation\nproblem task in CXR. We demonstrate that our architecture combining delayed\nsubsampling, exponential linear units, highly restrictive regularization and a\nlarge number of high resolution low level abstract features outperforms\nstate-of-the-art methods on all considered organs, as well as the human\nobserver on lungs and heart. The models use a multi-class configuration with\nthree target classes and are trained and tested on the publicly available JSRT\ndatabase, consisting of 247 X-ray images the ground-truth masks for which are\navailable in the SCR database. Our best performing model, trained with the loss\nfunction based on the Dice coefficient, reached mean Jaccard overlap scores of\n95.0\\% for lungs, 86.8\\% for clavicles and 88.2\\% for heart. This architecture\noutperformed the human observer results for lungs and heart.\n","negative":"  Limbo is an open-source C++11 library for Bayesian optimization which is\ndesigned to be both highly flexible and very fast. It can be used to optimize\nfunctions for which the gradient is unknown, evaluations are expensive, and\nruntime cost matters (e.g., on embedded systems or robots). Benchmarks on\nstandard functions show that Limbo is about 2 times faster than BayesOpt\n(another C++ library) for a similar accuracy.\n","id":989}
{"Unnamed: 0.1":11990,"Unnamed: 0":11990.0,"anchor":"Emergence of Selective Invariance in Hierarchical Feed Forward Networks","positive":"  Many theories have emerged which investigate how in- variance is generated in\nhierarchical networks through sim- ple schemes such as max and mean pooling.\nThe restriction to max\/mean pooling in theoretical and empirical studies has\ndiverted attention away from a more general way of generating invariance to\nnuisance transformations. We con- jecture that hierarchically building\nselective invariance (i.e. carefully choosing the range of the transformation\nto be in- variant to at each layer of a hierarchical network) is im- portant\nfor pattern recognition. We utilize a novel pooling layer called adaptive\npooling to find linear pooling weights within networks. These networks with the\nlearnt pooling weights have performances on object categorization tasks that\nare comparable to max\/mean pooling networks. In- terestingly, adaptive pooling\ncan converge to mean pooling (when initialized with random pooling weights),\nfind more general linear pooling schemes or even decide not to pool at all. We\nillustrate the general notion of selective invari- ance through object\ncategorization experiments on large- scale datasets such as SVHN and ILSVRC\n2012.\n","negative":"  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https:\/\/github.com\/imatge-upc\/VNeAT.\n","id":990}
{"Unnamed: 0.1":11991,"Unnamed: 0":11991.0,"anchor":"Spatial Projection of Multiple Climate Variables using Hierarchical\n  Multitask Learning","positive":"  Future projection of climate is typically obtained by combining outputs from\nmultiple Earth System Models (ESMs) for several climate variables such as\ntemperature and precipitation. While IPCC has traditionally used a simple model\noutput average, recent work has illustrated potential advantages of using a\nmultitask learning (MTL) framework for projections of individual climate\nvariables. In this paper we introduce a framework for hierarchical multitask\nlearning (HMTL) with two levels of tasks such that each super-task, i.e., task\nat the top level, is itself a multitask learning problem over sub-tasks. For\nclimate projections, each super-task focuses on projections of specific climate\nvariables spatially using an MTL formulation. For the proposed HMTL approach, a\ngroup lasso regularization is added to couple parameters across the\nsuper-tasks, which in the climate context helps exploit relationships among the\nbehavior of different climate variables at a given spatial location. We show\nthat some recent works on MTL based on learning task dependency structures can\nbe viewed as special cases of HMTL. Experiments on synthetic and real climate\ndata show that HMTL produces better results than decoupled MTL methods applied\nseparately on the super-tasks and HMTL significantly outperforms baselines for\nclimate projection.\n","negative":"  Feature subspace selection is an important part in speech emotion\nrecognition. Most of the studies are devoted to finding a feature subspace for\nrepresenting all emotions. However, some studies have indicated that the\nfeatures associated with different emotions are not exactly the same. Hence,\ntraditional methods may fail to distinguish some of the emotions with just one\nglobal feature subspace. In this work, we propose a new divide and conquer idea\nto solve the problem. First, the feature subspaces are constructed for all the\ncombinations of every two different emotions (emotion-pair). Bi-classifiers are\nthen trained on these feature subspaces respectively. The final emotion\nrecognition result is derived by the voting and competition method.\nExperimental results demonstrate that the proposed method can get better\nresults than the traditional multi-classification method.\n","id":991}
{"Unnamed: 0.1":11992,"Unnamed: 0":11992.0,"anchor":"Flow Navigation by Smart Microswimmers via Reinforcement Learning","positive":"  Smart active particles can acquire some limited knowledge of the fluid\nenvironment from simple mechanical cues and exert a control on their preferred\nsteering direction. Their goal is to learn the best way to navigate by\nexploiting the underlying flow whenever possible. As an example, we focus our\nattention on smart gravitactic swimmers. These are active particles whose task\nis to reach the highest altitude within some time horizon, given the\nconstraints enforced by fluid mechanics. By means of numerical experiments, we\nshow that swimmers indeed learn nearly optimal strategies just by experience. A\nreinforcement learning algorithm allows particles to learn effective strategies\neven in difficult situations when, in the absence of control, they would end up\nbeing trapped by flow structures. These strategies are highly nontrivial and\ncannot be easily guessed in advance. This Letter illustrates the potential of\nreinforcement learning algorithms to model adaptive behavior in complex flows\nand paves the way towards the engineering of smart microswimmers that solve\ndifficult navigation problems.\n","negative":"  Spectral dimensionality reduction algorithms are widely used in numerous\ndomains, including for recognition, segmentation, tracking and visualization.\nHowever, despite their popularity, these algorithms suffer from a major\nlimitation known as the \"repeated Eigen-directions\" phenomenon. That is, many\nof the embedding coordinates they produce typically capture the same direction\nalong the data manifold. This leads to redundant and inefficient\nrepresentations that do not reveal the true intrinsic dimensionality of the\ndata. In this paper, we propose a general method for avoiding redundancy in\nspectral algorithms. Our approach relies on replacing the orthogonality\nconstraints underlying those methods by unpredictability constraints.\nSpecifically, we require that each embedding coordinate be unpredictable (in\nthe statistical sense) from all previous ones. We prove that these constraints\nnecessarily prevent redundancy, and provide a simple technique to incorporate\nthem into existing methods. As we illustrate on challenging high-dimensional\nscenarios, our approach produces significantly more informative and compact\nrepresentations, which improve visualization and classification tasks.\n","id":992}
{"Unnamed: 0.1":11993,"Unnamed: 0":11993.0,"anchor":"SenseGen: A Deep Learning Architecture for Synthetic Sensor Data\n  Generation","positive":"  Our ability to synthesize sensory data that preserves specific statistical\nproperties of the real data has had tremendous implications on data privacy and\nbig data analytics. The synthetic data can be used as a substitute for\nselective real data segments,that are sensitive to the user, thus protecting\nprivacy and resulting in improved analytics.However, increasingly adversarial\nroles taken by data recipients such as mobile apps, or other cloud-based\nanalytics services, mandate that the synthetic data, in addition to preserving\nstatistical properties, should also be difficult to distinguish from the real\ndata. Typically, visual inspection has been used as a test to distinguish\nbetween datasets. But more recently, sophisticated classifier models\n(discriminators), corresponding to a set of events, have also been employed to\ndistinguish between synthesized and real data. The model operates on both\ndatasets and the respective event outputs are compared for consistency. In this\npaper, we take a step towards generating sensory data that can pass a deep\nlearning based discriminator model test, and make two specific contributions:\nfirst, we present a deep learning based architecture for synthesizing sensory\ndata. This architecture comprises of a generator model, which is a stack of\nmultiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network.\nsecond, we use another LSTM network based discriminator model for\ndistinguishing between the true and the synthesized data. Using a dataset of\naccelerometer traces, collected using smartphones of users doing their daily\nactivities, we show that the deep learning based discriminator model can only\ndistinguish between the real and synthesized traces with an accuracy in the\nneighborhood of 50%.\n","negative":"  The vast majority of current machine learning algorithms are designed to\npredict single responses or a vector of responses, yet many types of response\nare more naturally organized as matrices or higher-order tensor objects where\ncharacteristics are shared across modes. We present a new machine learning\nalgorithm BaTFLED (Bayesian Tensor Factorization Linked to External Data) that\npredicts values in a three-dimensional response tensor using input features for\neach of the dimensions. BaTFLED uses a probabilistic Bayesian framework to\nlearn projection matrices mapping input features for each mode into latent\nrepresentations that multiply to form the response tensor. By utilizing a\nTucker decomposition, the model can capture weights for interactions between\nlatent factors for each mode in a small core tensor. Priors that encourage\nsparsity in the projection matrices and core tensor allow for feature selection\nand model regularization. This method is shown to far outperform elastic net\nand neural net models on 'cold start' tasks from data simulated in a three-mode\nstructure. Additionally, we apply the model to predict dose-response curves in\na panel of breast cancer cell lines treated with drug compounds that was used\nas a Dialogue for Reverse Engineering Assessments and Methods (DREAM)\nchallenge.\n","id":993}
{"Unnamed: 0.1":11994,"Unnamed: 0":11994.0,"anchor":"Deep Reinforcement Learning for Visual Object Tracking in Videos","positive":"  In this paper we introduce a fully end-to-end approach for visual tracking in\nvideos that learns to predict the bounding box locations of a target object at\nevery frame. An important insight is that the tracking problem can be\nconsidered as a sequential decision-making process and historical semantics\nencode highly relevant information for future decisions. Based on this\nintuition, we formulate our model as a recurrent convolutional neural network\nagent that interacts with a video overtime, and our model can be trained with\nreinforcement learning (RL) algorithms to learn good tracking policies that pay\nattention to continuous, inter-frame correlation and maximize tracking\nperformance in the long run. The proposed tracking algorithm achieves\nstate-of-the-art performance in an existing tracking benchmark and operates at\nframe-rates faster than real-time. To the best of our knowledge, our tracker is\nthe first neural-network tracker that combines convolutional and recurrent\nnetworks with RL algorithms.\n","negative":"  Stein's method for measuring convergence to a continuous target distribution\nrelies on an operator characterizing the target and Stein factor bounds on the\nsolutions of an associated differential equation. While such operators and\nbounds are readily available for a diversity of univariate targets, few\nmultivariate targets have been analyzed. We introduce a new class of\ncharacterizing operators based on Ito diffusions and develop explicit\nmultivariate Stein factor bounds for any target with a fast-coupling Ito\ndiffusion. As example applications, we develop computable and\nconvergence-determining diffusion Stein discrepancies for log-concave,\nheavy-tailed, and multimodal targets and use these quality measures to select\nthe hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare\nrandom and deterministic quadrature rules, and quantify bias-variance tradeoffs\nin approximate MCMC. Our results establish a near-linear relationship between\ndiffusion Stein discrepancies and Wasserstein distances, improving upon past\nwork even for strongly log-concave targets. The exposed relationship between\nStein factors and Markov process coupling may be of independent interest.\n","id":994}
{"Unnamed: 0.1":11995,"Unnamed: 0":11995.0,"anchor":"Deep Submodular Functions","positive":"  We start with an overview of a class of submodular functions called SCMMs\n(sums of concave composed with non-negative modular functions plus a final\narbitrary modular). We then define a new class of submodular functions we call\n{\\em deep submodular functions} or DSFs. We show that DSFs are a flexible\nparametric family of submodular functions that share many of the properties and\nadvantages of deep neural networks (DNNs). DSFs can be motivated by considering\na hierarchy of descriptive concepts over ground elements and where one wishes\nto allow submodular interaction throughout this hierarchy. Results in this\npaper show that DSFs constitute a strictly larger class of submodular functions\nthan SCMMs. We show that, for any integer $k>0$, there are $k$-layer DSFs that\ncannot be represented by a $k'$-layer DSF for any $k'<k$. This implies that,\nlike DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs\nstrictly increase with depth. Despite this, we show (using a \"backpropagation\"\nlike method) that DSFs, even with arbitrarily large $k$, do not comprise all\nsubmodular functions. In offering the above results, we also define the notion\nof an antitone superdifferential of a concave function and show how this\nrelates to submodular functions (in general), DSFs (in particular), negative\nsecond-order partial derivatives, continuous submodularity, and concave\nextensions. To further motivate our analysis, we provide various special case\nresults from matroid theory, comparing DSFs with forms of matroid rank, in\nparticular the laminar matroid. Lastly, we discuss strategies to learn DSFs,\nand define the classes of deep supermodular functions, deep difference of\nsubmodular functions, and deep multivariate submodular functions, and discuss\nwhere these can be useful in applications.\n","negative":"  Given a dataset, the task of learning a transform that allows sparse\nrepresentations of the data bears the name of dictionary learning. In many\napplications, these learned dictionaries represent the data much better than\nthe static well-known transforms (Fourier, Hadamard etc.). The main downside of\nlearned transforms is that they lack structure and therefore they are not\ncomputationally efficient, unlike their classical counterparts. These posse\nseveral difficulties especially when using power limited hardware such as\nmobile devices, therefore discouraging the application of sparsity techniques\nin such scenarios. In this paper we construct orthogonal and non-orthogonal\ndictionaries that are factorized as a product of a few basic transformations.\nIn the orthogonal case, we solve exactly the dictionary update problem for one\nbasic transformation, which can be viewed as a generalized Givens rotation, and\nthen propose to construct orthogonal dictionaries that are a product of these\ntransformations, guaranteeing their fast manipulation. We also propose a method\nto construct fast square but non-orthogonal dictionaries that are factorized as\na product of few transforms that can be viewed as a further generalization of\nGivens rotations to the non-orthogonal setting. We show how the proposed\ntransforms can balance very well data representation performance and\ncomputational complexity. We also compare with classical fast and learned\ngeneral and orthogonal transforms.\n","id":995}
{"Unnamed: 0.1":11996,"Unnamed: 0":11996.0,"anchor":"Variable selection for clustering with Gaussian mixture models: state of\n  the art","positive":"  The mixture models have become widely used in clustering, given its\nprobabilistic framework in which its based, however, for modern databases that\nare characterized by their large size, these models behave disappointingly in\nsetting out the model, making essential the selection of relevant variables for\nthis type of clustering. After recalling the basics of clustering based on a\nmodel, this article will examine the variable selection methods for model-based\nclustering, as well as presenting opportunities for improvement of these\nmethods.\n","negative":"  Label space expansion for multi-label classification (MLC) is a methodology\nthat encodes the original label vectors to higher dimensional codes before\ntraining and decodes the predicted codes back to the label vectors during\ntesting. The methodology has been demonstrated to improve the performance of\nMLC algorithms when coupled with off-the-shelf error-correcting codes for\nencoding and decoding. Nevertheless, such a coding scheme can be complicated to\nimplement, and cannot easily satisfy a common application need of\ncost-sensitive MLC---adapting to different evaluation criteria of interest. In\nthis work, we show that a simpler coding scheme based on the concept of a\nreference pair of label vectors achieves cost-sensitivity more naturally. In\nparticular, our proposed cost-sensitive reference pair encoding (CSRPE)\nalgorithm contains cluster-based encoding, weight-based training and\nvoting-based decoding steps, all utilizing the cost information. Furthermore,\nwe leverage the cost information embedded in the code space of CSRPE to propose\na novel active learning algorithm for cost-sensitive MLC. Extensive\nexperimental results verify that CSRPE performs better than state-of-the-art\nalgorithms across different MLC criteria. The results also demonstrate that the\nCSRPE-backed active learning algorithm is superior to existing algorithms for\nactive MLC, and further justify the usefulness of CSRPE.\n","id":996}
{"Unnamed: 0.1":11997,"Unnamed: 0":11997.0,"anchor":"CommAI: Evaluating the first steps towards a useful general AI","positive":"  With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum.\n","negative":"  We describe here the recent results of a multidisciplinary effort to design a\nbiomarker that can actively and continuously decode the progressive changes in\nneuronal organization leading to epilepsy, a process known as epileptogenesis.\nUsing an animal model of acquired epilepsy, wechronically record hippocampal\nevoked potentials elicited by an auditory stimulus. Using a set of reduced\ncoordinates, our algorithm can identify universal smooth low-dimensional\nconfigurations of the auditory evoked potentials that correspond to distinct\nstages of epileptogenesis. We use a hidden Markov model to learn the dynamics\nof the evoked potential, as it evolves along these smooth low-dimensional\nsubsets. We provide experimental evidence that the biomarker is able to exploit\nsubtle changes in the evoked potential to reliably decode the stage of\nepileptogenesis and predict whether an animal will eventually recover from the\ninjury, or develop spontaneous seizures.\n","id":997}
{"Unnamed: 0.1":11998,"Unnamed: 0":11998.0,"anchor":"Towards Adversarial Retinal Image Synthesis","positive":"  Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality.\n","negative":"  In this paper, we consider the patient similarity matching problem over a\ncancer cohort of more than 220,000 patients. Our approach first leverages on\nWord2Vec framework to embed ICD codes into vector-valued representation. We\nthen propose a sequential algorithm for case-control matching on this\nrepresentation space of diagnosis codes. The novel practice of applying the\nsequential matching on the vector representation lifted the matching accuracy\nmeasured through multiple clinical outcomes. We reported the results on a\nlarge-scale dataset to demonstrate the effectiveness of our method. For such a\nlarge dataset where most clinical information has been codified, the new method\nis particularly relevant.\n","id":998}
{"Unnamed: 0.1":11999,"Unnamed: 0":11999.0,"anchor":"Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point","positive":"  We propose a cluster-based quantization method to convert pre-trained full\nprecision weights into ternary weights with minimal impact on the accuracy. In\naddition, we also constrain the activations to 8-bits thus enabling sub 8-bit\nfull integer inference pipeline. Our method uses smaller clusters of N filters\nwith a common scaling factor to minimize the quantization loss, while also\nmaximizing the number of ternary operations. We show that with a cluster size\nof N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best\nfull precision results while replacing ~85% of all multiplications with 8-bit\naccumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1\naccuracy which within 2% of the full precision result. We also study the impact\nof the size of the cluster on both performance and accuracy, larger cluster\nsizes N=64 can replace ~98% of the multiplications with ternary operations but\nintroduces significant drop in accuracy which necessitates fine tuning the\nparameters with retraining the network at lower precision. To address this we\nhave also trained low-precision Resnet-50 with 8-bit activations and ternary\nweights by pre-initializing the network with full precision weights and achieve\n68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can\nrun on a full 8-bit compute pipeline, with a potential 16x improvement in\nperformance compared to baseline full-precision models.\n","negative":"  Distributionally Robust Supervised Learning (DRSL) is necessary for building\nreliable machine learning systems. When machine learning is deployed in the\nreal world, its performance can be significantly degraded because test data may\nfollow a different distribution from training data. DRSL with f-divergences\nexplicitly considers the worst-case distribution shift by minimizing the\nadversarially reweighted training loss. In this paper, we analyze this DRSL,\nfocusing on the classification scenario. Since the DRSL is explicitly\nformulated for a distribution shift scenario, we naturally expect it to give a\nrobust classifier that can aggressively handle shifted distributions. However,\nsurprisingly, we prove that the DRSL just ends up giving a classifier that\nexactly fits the given training distribution, which is too pessimistic. This\npessimism comes from two sources: the particular losses used in classification\nand the fact that the variety of distributions to which the DRSL tries to be\nrobust is too wide. Motivated by our analysis, we propose simple DRSL that\novercomes this pessimism and empirically demonstrate its effectiveness.\n","id":999}
